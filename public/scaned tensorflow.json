[
  {
    "title": "TensorFlow 2.15 update: hot-fix for Linux installation issue",
    "content": "Posted by the TensorFlow team\nWe are releasing a hot-fix for an installation issue affecting the TensorFlow installation process. The TensorFlow 2.15.0 Python package was released such that it requested tensorrt-related packages that cannot be found unless the user installs them beforehand or provides additional installation flags. This dependency affected anyone installing TensorFlow 2.15 alongside NVIDIA CUDA dependencies via pip install tensorflow[and-cuda]. Depending on the installation method, TensorFlow 2.14 would be installed instead of 2.15, or users could receive an installation error due to those missing dependencies.\nTo solve this issue as quickly as possible, we have released TensorFlow 2.15.0.post1 for the Linux x86_64 platform. This version removes the tensorrt Python package dependencies from the tensorflow[and-cuda] installation method. Support for TensorRT is otherwise unaffected as long as TensorRT is already installed on the system. Now, pip install tensorflow[and-cuda] works as originally intended for TensorFlow 2.15.\nUsing .post1 instead of a full minor release allowed us to push this release out quickly. However, please be aware of the following caveat: for users wishing to pin their Python dependency in a requirements file or other situation, under Python's version specification rules, tensorflow[and-cuda]==2.15.0 will not install this fixed version. Please use ==2.15.0.post1 to specify this exact version on Linux platforms, or a fuzzy version specification, such as ==2.15.*, to specify the most recent compatible version of TensorFlow 2.15 on all platforms.",
    "link": "https://blog.tensorflow.org/2023/12/tensorflow-215-update-hot-fix-linux-installation-issue.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVyX9h9jqUfN0LpVCwPg0iWNd4XDkKWd5hL7Wn9CwGLBZ5NyrEFTHHXu6yCxCaLL2szdaQMCkDuRdV4-JapbV329DYTjaH6ThpbzJlbvgunZK9dUgii1kAqcs-4zhyphenhyphenccaLsfffj7K3-8HFLybyOeaHjVl_n7jdPmD33o0TERamUgTdH7H3qhBLS2GuwtY/s1600/Tensorflow-septmber-update-social%20%282%29%20%281%29.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEja7X29KGgTQBVvr3Xl2pyRZ-KQZJOM1-rq5XUr7AKu1vc_umj40H8y3mMoYq3wccQlA9XZ8OPtSx8SFJOOy8uSeX_MpoIAz7x44Cov-P95v9h85TLGgWCW2gqL6x3fbFfL1Xg6gZYkhylQKMQG7_8ilCTsm81bG87vT-3ttwn8IGdCPU1KfVVDjatuofs/s1600/Tensorflow-septmber-update-header%20%284%29.png"
    ],
    "time": "2023/12/12 00:58:43"
  },
  {
    "title": "What's new in TensorFlow 2.15",
    "content": "Posted by the TensorFlow team\nTensorFlow 2.15 has been released! Highlights of this release (and 2.14) include a much simpler installation method for NVIDIA CUDA libraries for Linux, oneDNN CPU performance optimizations for Windows x64 and x86, full availability of tf.function types, an upgrade to Clang 17.0.1, and much more! For the full release note, please check here.\nNote: Release updates on the new multi-backend Keras will be published on keras.io starting with Keras 3.0. For more information, please check here.\nTensorFlow Core\nNVIDIA CUDA libraries for Linux\nThe tensorflow pip package has a new, optional installation method for Linux that installs necessary NVIDIA CUDA libraries through pip. As long as the NVIDIA driver is already installed on the system, you may now run pip install tensorflow[and-cuda] to install TensorFlow's NVIDIA CUDA library dependencies in the Python environment. Aside from the NVIDIA driver, no other pre-existing NVIDIA CUDA packages are necessary. In TensorFlow 2.15, CUDA has been upgraded to version 12.2.\noneDNN CPU performance optimizations\nFor Windows x64 & x86 packages, oneDNN optimizations are now enabled by default on X86 CPUs. These optimizations can be enabled or disabled by setting the environment variable TF_ENABLE_ONEDNN_OPTS to 1 (enable) or 0 (disable) before running TensorFlow. To fall back to default settings, simply unset the environment variable.\ntf.function\ntf.function types are now fully available.\ntf.types.experimental.TraceType now allows custom tf.function inputs to declare Tensor decomposition and type casting support. \nIntroducing tf.types.experimental.FunctionType as the comprehensive representation of the signature of tf.function callables. It can be accessed through the function_type property of tf.function\u2019s and ConcreteFunctions. See the tf.types.experimental.FunctionType documentation for more details. \nIntroducing tf.types.experimental.AtomicFunction as the fastest way to perform TF computations in Python. This capability can be accessed through the inference_fn property of ConcreteFunctions. (Does not support gradients.) See the tf.types.experimental.AtomicFunction documentation for how to call and use it.\nUpgrade to Clang 17.0.1 and CUDA 12.2\nTensorFlow PIP packages are now being built with Clang 17 and CUDA 12.2 to improve performance for NVIDIA Hopper-based GPUs. Moving forward, Clang 17 will be the default C++ compiler for TensorFlow. We recommend upgrading your compiler to Clang 17 when building TensorFlow from source.",
    "link": "https://blog.tensorflow.org/2023/11/whats-new-in-tensorflow-2-15.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVyX9h9jqUfN0LpVCwPg0iWNd4XDkKWd5hL7Wn9CwGLBZ5NyrEFTHHXu6yCxCaLL2szdaQMCkDuRdV4-JapbV329DYTjaH6ThpbzJlbvgunZK9dUgii1kAqcs-4zhyphenhyphenccaLsfffj7K3-8HFLybyOeaHjVl_n7jdPmD33o0TERamUgTdH7H3qhBLS2GuwtY/s1600/Tensorflow-septmber-update-social%20%282%29%20%281%29.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEja7X29KGgTQBVvr3Xl2pyRZ-KQZJOM1-rq5XUr7AKu1vc_umj40H8y3mMoYq3wccQlA9XZ8OPtSx8SFJOOy8uSeX_MpoIAz7x44Cov-P95v9h85TLGgWCW2gqL6x3fbFfL1Xg6gZYkhylQKMQG7_8ilCTsm81bG87vT-3ttwn8IGdCPU1KfVVDjatuofs/s1600/Tensorflow-septmber-update-header%20%284%29.png"
    ],
    "time": "2023/12/12 00:58:43"
  },
  {
    "title": "Distributed Fast Fourier Transform in TensorFlow",
    "content": "Posted by Ruijiao Sun, Google Intern - DTensor team\nFast Fourier Transform is an important method of signal processing, which is commonly used in a number of ways, including speeding up convolutions, extracting features, and regularizing models. Distributed Fast Fourier Transform (Distributed FFT) offers a way to compute Fourier Transforms in models that work with image-like datasets that are too large to fit into the memory of a single accelerator device. In a previous Google Research Paper, \u201cLarge-Scale Discrete Fourier Transform on TPUs\u201d by Tianjian Lu, a Distributed FFT algorithm was implemented for TensorFlow v1 as a library. This work presents the newly added native support in TensorFlow v2 for Distributed FFT, through the new TensorFlow distribution API, DTensor.\nAbout DTensor\nDTensor is an extension to TensorFlow for synchronous distributed computing. It distributes the program and tensors through a procedure called Single program, multiple data (SPMD) extension. DTensor offers an uniform API for traditional data and model parallelism patterns used widely in Machine Learning.\nExample Usage\nThe API interface for distributed FFT is the same as the original FFT in TensorFlow. Users just need to pass a sharded tensor as an input to the existing FFT ops in TensorFlow, such as tf.signal.fft2d. The output of a distributed FFT becomes sharded too.\nimport TensorFlow as tf\nfrom TensorFlow.experimental import dtensor\n\n\n# Set up devices\ndevice_type = dtensor.preferred_device_type()\nif device_type == 'CPU':\ncpu = tf.config.list_physical_devices(device_type)\ntf.config.set_logical_device_configuration(cpu[0], [tf.config.LogicalDeviceConfiguration()] * 8)\nif device_type == 'GPU':\ngpu = tf.config.list_physical_devices(device_type)\ntf.config.set_logical_device_configuration(gpu[0], [tf.config.LogicalDeviceConfiguration(memory_limit=1000)] * 8)\ndtensor.initialize_accelerator_system()\n\n\n# Create a mesh\nmesh = dtensor.create_distributed_mesh(mesh_dims=[('x', 1), ('y', 2), ('z', 4)], device_type=device_type)\n\n\n# Set up a distributed input Tensor\ninput = tf.complex(\ntf.random.stateless_normal(shape=(2, 2, 4), seed=(1, 2), dtype=tf.float32),\ntf.random.stateless_normal(shape=(2, 2, 4), seed=(2, 4), dtype=tf.float32))\ninit_layout = dtensor.Layout(['x', 'y', 'z'], mesh)\nd_input = dtensor.relayout(input, layout=init_layout)\n\n\n# Run distributed fft2d. DTensor determines the most efficient\n# layout of of d_output.\nd_output = tf.signal.fft2d(d_input)\nPerformance Analysis\nThe following experiment demonstrates that the distributed FFT can process more data than the non-distributed one by utilizing memory across multiple devices. The tradeoff is spending additional time on communication and data transposes that slow down the calculation speed.\nThis phenomenon is shown in detail from the profiling result of the 10K*10K distributed FFT experiment. The current implementation of distributed FFT in TensorFlow follows the simple shuffle+local FFT method, which is also used by other popular distributed FFT libraries such as FFTW and PFFT. Notably, the two local FFT ops only take 3.6% of the total time (15ms). This is around 1/3 of the time for non-distributed fft2d. Most of the computing time is spent on data shuffling, represented by the ncclAllToAll Operation. Note that these experiments were conducted on an 8xV100 GPU system.\nNext steps\nThe feature is new and we have adopted a simplest distributed FFT algorithm. A few ideas to fine tune or improve the performance are:\nSwitch to a different DFT/FFT algorithm.\nTweaks on the NCCL communication settings for the particular FFT sizes may improve utilization of the network bandwidth and increase the speed.\nReducing the number of collectives to minimize bandwidth requirements.\nUse N-d local FFTs, rather than multiple 1-d local FFTs.\nTry the new distributed FFT! We welcome your feedback on the TensorFlow Forum and look forward to working with you on improving the performance. Your input would be invaluable!",
    "link": "https://blog.tensorflow.org/2023/08/distributed-fast-fourier-transform-in-tensorflow.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgYIMKKNGUkhWyF3gL1qLpn7cQ3iQu5GleP-36OHbTJwn90YdRUk8vGTefc9ctjwPcJhyBbFlprd581nDsWEKfMAeAo9xuX8zKfxti8Fvl2f2v69Qmvt695cCJY1dfVPbMIlfWqMFKMEyBCgIaRLXypYCrHlob-OiAb0mvVbhmBEt65-agfmRWDMuaI/s1600/image1.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRgwmbW2Tp7Q_Y3GTKSmkfboFNHuUizDaYh1uLrQSCZarfkvs1mK7OodevH9l318Ls8ddEmKNPewlpVhMJzKvtpktP6TeKniEEMAzgRrHq-D-kIEsoQnZyvc7n4pUVsn1RkFF066dnujQZ1htprWST0uSJftVZxQyc2Qm8aijQTMhrtJlj7rrEc6s7/s1600/Tensorflow-septmber-update-header%20%282%29.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEigTOJgosBdpn5Wr7djYt3eYpmcxmRy3nn_UQbG1aW5nO96v5fMFFzsx9p-ahhpTuWJPNRQ5IMRvUjdon-CollI87lu6mAEnnW7KcaIHyrZVLK0tUMQJ2jDBVxtYz9zcXBH2FI5kou775n-Yt4iBGVfzO3rAwaCo4vgkXrWU3BWqv-_YPKf2mkU8kihSxc/s1600/image2.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhNIgERau2VMVV1Uz5jqsjNow9OVkfhd_WI0Irr424u7sUAEo-iILbMOxhrp_dbjMsKhssjCw6iYfe8m5OCBGsldiqM-6_SjAjY0ZamWkBSJUGuW7Xwubw5fYfrNVY9WSr9B2SXX2m0fQHRS7Gqr-jjKMNxIzTax2xz5ntAGZhIMMb4biWvK98tqd4M7H4/s1600/image1.png"
    ],
    "time": "2023/12/12 00:58:43"
  },
  {
    "title": "What's new in TensorFlow 2.13 and Keras 2.13?",
    "content": "Posted by the TensorFlow and Keras Teams\nTensorFlow 2.13 and Keras 2.13 have been released! Highlights of this release include publishing Apple Silicon wheels, the new Keras V3 format being default for .keras extension files and many more!\nTensorFlow Core\nApple Silicon wheels for TensorFlow\nTensorFlow 2.13 is the first version to provide Apple Silicon wheels, which means when you install TensorFlow on an Apple Silicon Mac, you will be able to use the latest version of TensorFlow. The nightly builds for Apple Silicon wheels were released in March 2023 and this new support will enable more fine-grained testing, thanks to technical collaboration between Apple, MacStadium, and Google.\ntf.lite\nThe Python TensorFlow Lite Interpreter bindings now have an option to use experimental_disable_delegate_clustering flag to turn-off delegate clustering during delegate graph partitioning phase. You can set this flag in TensorFlow Lite interpreter Python API\ninterpreter = new Interpreter(file_of_a_tensorflowlite_model, experimental_preserve_all_tensors=False)\nThe flag is set to False by default. This is an advanced feature in experimental that is designed for people who insert explicit control dependencies via with tf.control_dependencies() or need to change graph execution order.\nBesides, there are several operator improvements in TensorFlow Lite in 2.13\nadd operation now supports broadcasting up to 6 dimensions. This will remove explicit broadcast ops from many models. The new implementation is also much faster than the current one which calculates the entire index for both inputs the the input instead of only calculating the part that changes.\nImprove the coverage for 16x8 quantization by enabling int16x8 ops for exp, mirror_pad, space_to_batch_nd, batch_to_space_nd\nIncrease the coverage of integer data types\nenabled int16 for less, greater_than, equal, bitcast, bitwise_xor, right_shift, top_k, mul, and int16 indices for gather and gather_nd\nenabled int8 for floor_div and floor_mod, bitwise_xor, bitwise_xor\nenabled 32-bit int for bitcast, bitwise_xor, right_shift\ntf.data\nWe have improved usability and added functionality for tf.data APIs.\ntf.data.Dataset.zip now supports Python-style zipping. Previously users were required to provide an extra set of parentheses when zipping datasets as in Dataset.zip((a, b, c)). With this change, users can specify the datasets to be zipped simply as Dataset.zip(a, b, c) making it more intuitive.\nAdditionally, tf.data.Dataset.shuffle now supports full shuffling. To specify that data should be fully shuffled, use dataset = dataset.shuffle(dataset.cardinality()). This will load the full dataset into memory so that it can be shuffled, so make sure to only use this with datasets of filenames or other small datasets.\nWe have also added a new tf.data.experimental.pad_to_cardinality transformation which pads a dataset with zero elements up to a specified cardinality. This is useful for avoiding partial batches while not dropping any data.\nExample usage:\nds = tf.data.Dataset.from_tensor_slices({'a': [1, 2]})\nds = ds.apply(tf.data.experimental.pad_to_cardinality(3))\nlist(ds.as_numpy_iterator())\n[{'a': 1, 'valid': True}, {'a': 2, 'valid': True}, {'a': 0, 'valid': False}]\n\nThis can be useful, e.g. during eval, when partial batches are undesirable but it is also important not to drop any data.\noneDNN BF16 Math Mode on CPU\noneDNN supports BF16 math mode where full FP32 tensors are implicitly down-converted to BF16 during computations for faster execution time. TensorFlow CPU users can enable this by setting the environment variable TF_SET_ONEDNN_FPMATH_MODE to BF16. This mode may negatively impact model accuracy. To go back to full FP32 math mode, unset the variable.\nKeras\nKeras Saving format\nThe new Keras V3 saving format, released in TF 2.12, is now the default for all files with the .keras extension.\nYou can start using it now by calling model.save(\u201cyour_model.keras\u201d).\nIt provides richer Python-side model saving and reloading with numerous advantages:\nA lightweight, faster format:\nHuman-readable: The new format is name-based, with a more detailed serialization format that makes debugging much easier. What you load is exactly what you saved, from Python\u2019s perspective.\nSafer: Unlike SavedModel, there is no reliance on loading via bytecode or pickling \u2013 a big advancement for secure ML, as pickle files can be exploited to cause arbitrary code execution at loading time.\nMore general: Support for non-numerical states, such as vocabularies and lookup tables, is included in the new format.\nExtensible: You can add support for saving and loading exotic state elements in custom layers using save_assets(), such as a FIFOQueue \u2013 or anything else you want. You have full control of disk I/O for custom assets.\nThe legacy formats (h5 and Keras SavedModel) will stay supported in perpetuity. However, we recommend that you consider adopting the new Keras v3 format for saving/reloading in Python runtimes, and using model.export() for inference in all other runtimes (such as TF Serving).",
    "link": "https://blog.tensorflow.org/2023/07/whats-new-in-tensorflow-213-and-keras-213.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgYIMKKNGUkhWyF3gL1qLpn7cQ3iQu5GleP-36OHbTJwn90YdRUk8vGTefc9ctjwPcJhyBbFlprd581nDsWEKfMAeAo9xuX8zKfxti8Fvl2f2v69Qmvt695cCJY1dfVPbMIlfWqMFKMEyBCgIaRLXypYCrHlob-OiAb0mvVbhmBEt65-agfmRWDMuaI/s1600/image1.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRgwmbW2Tp7Q_Y3GTKSmkfboFNHuUizDaYh1uLrQSCZarfkvs1mK7OodevH9l318Ls8ddEmKNPewlpVhMJzKvtpktP6TeKniEEMAzgRrHq-D-kIEsoQnZyvc7n4pUVsn1RkFF066dnujQZ1htprWST0uSJftVZxQyc2Qm8aijQTMhrtJlj7rrEc6s7/s1600/Tensorflow-septmber-update-header%20%282%29.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXZQ34cVqejCXufc77ohHqz8xlV2B1rLelRk-35axBFELs1gfc1WG8Xs6Xk2KLZ1jzkb-pGqd807UgGTu17ncZ6ioKkogwM8OEh34gH8opFgdFcF0GmaoUtV-R5YBkHFWNoyfqh4kLY69tkiV8dnauiM0FG3cjbnSSIg20Ym50x3Tki20FlAwPEY19PY4/s1600/image1.png"
    ],
    "time": "2023/12/12 00:58:43"
  },
  {
    "title": "Training a recommendation model with dynamic embeddings",
    "content": "Posted by Thushan Ganegedara (GDE), Haidong Rong (Nvidia), Wei Wei (Google)\nModern recommenders heavily leverage embeddings to create vector representations of each user and candidate item. These embedding can then be used to calculate the similarity between users and items, so that users are recommended candidate items that are more interesting and relevant. But when working with data at scale, particularly in an online machine learning setting, embedding tables can grow in size dramatically, accumulating millions (and sometimes billions) of items. At this scale, it becomes impossible to store these embedding tables in memory. Furthermore, a large portion of the items might be rarely seen, so it does not make sense to keep dedicated embeddings for such rarely occurring items. A better solution would be to represent those items with one common embedding. This can dramatically reduce the size of the embedding table at a very small fraction of the performance cost. This is the main motivation behind dynamic embedding tables.\nTensorFlow's built-in tf.keras.layers.Embedding layer has a fixed size at creation time, so we need another approach. Fortunately, there is a TensorFlow SIG project exactly for this purpose: TensorFlow Recommenders Addons (TFRA). You can learn more from its repository, but at a high level TFRA leverages dynamic embedding technology to dynamically change embedding size and achieve better recommendation results than static embeddings. TFRA is fully TF2.0-compatible and works smoothly with the familiar Keras API interfaces, so it can be easily integrated with other TensorFlow products, such as TensorFlow Recommenders (TFRS).\nIn this tutorial we will build a movie recommender model by leveraging both TFRS and TFRA. We will use the MovieLens dataset, which contains anonymized data showing ratings given to movies by users. Our primary focus is to show how the dynamic embeddings provided in the TensorFlow Recommenders Addons library can be used to dynamically grow and shrink the size of the embedding tables in the recommendation setting. You can find the full implementation here and a walkthrough here.\nImport Libraries\nWe will first import the required libraries.\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\n#TFRA does some patching on TensorFlow so it MUST be imported after importing TF\nimport tensorflow_recommenders as tfrs\nimport tensorflow_recommenders_addons as tfra\nimport tensorflow_recommenders_addons.dynamic_embedding as de\nNote how we are importing the TFRA library after importing TensorFlow. It is recommended to follow this ordering as the TFRA library will be applying some patches on TensorFlow.\nProcessing the data\nLet\u2019s first build a baseline model with TensorFlow Recommenders. We will follow the pattern of this TFRS retrieval tutorial to build a two-tower retrieval model. The user tower will take the user ID as the input, but the item tower will use the tokenized movie title as the input.\nTo handle the movie titles, we define a helper function that converts the movie titles to lowercase, removes any punctuation in a given movie title, and splits using spaces to generate a list of tokens. Finally we take only the up to max_token_length tokens (from the start) from the movie title. If a movie title has fewer tokens, all the tokens will be taken. This number is chosen based on some analysis and represents the 90th percentile in the title lengths in the dataset.\nmax_token_length = 6\npad_token = \"[PAD]\"\npunctuation_regex = \"[\\!\\\"#\\$%&\\(\\)\\*\\+,-\\.\\/\\:;\\<\\=\\>\\?@\\[\\]\\\\\\^_`\\{\\|\\}~\\\\t\\\\n]\"\n\n#First we\u2019ll define a helper function that will process the movie titles for us.\n\ndef process_text(x: tf.Tensor, max_token_length: int, punctuation_regex: str) -> tf.Tensor:\n    \n return tf.strings.split(\n      tf.strings.regex_replace(\n           tf.strings.lower(x[\"movie_title\"]), punctuation_regex, \"\"\n      )\n )[:max_token_length]\nWe also pad the tokenized movie titles to a fixed length and split the dataset using the same random seed so that we get consistent validation results across training epochs. You can find detailed code in the \u2018Processing datasets\u2019 section of the notebook.\nBuilding the two tower model\nOur user tower is pretty much the same as in the TFRS retrieval tutorial (except it\u2019s deeper), but for the movie tower there is a GlobalAveragePooling1D layer after the embedding lookup, which averages the embedding of movie title tokens to a single embedding.\ndef get_movie_title_lookup_layer(dataset: tf.data.Dataset) -> tf.keras.layers.Layer:\n movie_title_lookup_layer = tf.keras.layers.StringLookup(mask_token=pad_token)\n movie_title_lookup_layer.adapt(dataset.map(lambda x: x[\"movie_title\"]))\n return movie_title_lookup_layer\n\ndef build_item_model(movie_title_lookup_layer: tf.keras.layers.StringLookup):\n vocab_size = movie_title_lookup_layer.vocabulary_size()\n return tf.keras.models.Sequential([\n     tf.keras.layers.InputLayer(input_shape=(max_token_length), dtype=tf.string),\n     movie_title_lookup_layer,\n     tf.keras.layers.Embedding(vocab_size, 64),\n     tf.keras.layers.GlobalAveragePooling1D(),\n     tf.keras.layers.Dense(64, activation=\"gelu\"),\n     tf.keras.layers.Dense(32),\n     tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))\n ])\nNext we are going to train the model.\nTraining the model\nTraining the model is simply calling fit() on the model with the required arguments. We will be using our validation dataset validation_ds to measure the performance of our model.\nhistory = model.fit(datasets.training_datasets.train_ds, epochs=3, validation_data=datasets.training_datasets.validation_ds)\nAt the end, the output looks like below:\nEpoch 3/3\n220/220 [==============================] - 146s 633ms/step  \n......\nval_factorized_top_k/top_10_categorical_accuracy: 0.0179 - val_factorized_top_k/top_50_categorical_accuracy: 0.0766 - val_factorized_top_k/top_100_categorical_accuracy: 0.1338 - val_loss: 12359.0557 - val_regularization_loss: 0.0000e+00 - val_total_loss: 12359.0557\nWe have achieved a top 100 categorical accuracy of 13.38% on the validation dataset.\nBuilding the model with dynamic embeddings\nOverview\nWe will now learn how we can use the dynamic embedding in the TensorFlow Recommenders Addons (TFRA) library, rather than a static embedding table. As the name suggests, as opposed to creating embeddings for all the items in the vocabulary up front, dynamic embedding would only grow the size of the embedding table on demand. This behavior really shines when dealing with millions and billions of items and users as some companies do. For these companies, it\u2019s not surprising to find static embedding tables that would not fit in memory. Static embedding tables can grow up to hundreds of Gigabytes or even Terabytes, incapacitating even the highest memory instances available in cloud environments.\nWhen you have an embedding table with large cardinality, the accessing weights will be quite sparse. Therefore, a hash-table based data structure is used to hold the weights and required weights for each iteration are retrieved from the underlying table structure. Here, to focus on the core functionality of the library, we will focus on a non-distributed setting. In this case, TFRA will choose cuckoo hashtable by default. But there are other solutions such as Redis, nvhash available.\nWhen using the dynamic embedding, we initialize the table with some initial capacity and the table will grow in size on demand as it sees more IDs during model training. For more information about motivation and inner mechanics, please refer to the RFC.\nTypes of embedding\nCurrently in the TFRA dynamic_embedding module, there are three types of embedding available:\nEmbedding - The most basic form of embeddings. This expects a 1D ([batch_size]) or 2D ([batch_size, time_steps]) tensor of IDs and outputs a [batch_size, embedding_dim] or [batch_size, time_steps, embedding_dim] sized tensor respectively.\nSquashedEmbedding - This layer squashes the time step dimension based on some reduction operation (e.g. mean/sum) to transform a [batch_size, time_steps] sized tensor of IDs to a [batch_size, embedding_dim] tensor.\nFieldwiseEmbedding - This type can handle multiple features (i.e. fields) at once. The layer takes n_slots as an argument and IDs are mapped to a slot within the layer. The layer would return a tensor of size [batch_size, n_slots, embedding_dim].\nDefining the embedding layers\nWe will be using the Embedding to represent the user IDs and SquashedEmbedding to represent token IDs. Remember that each movie title has multiple tokens, therefore, we need a way to reduce the resulting token embeddings to a single representative embedding.\nNote: The behavior of Embedding has changed from version 0.5 to 0.6. Please make sure to use version 0.6 for this tutorial.\nWith that, we can define the two towers as we did in the standard model. However, this time we\u2019ll be using the dynamic embedding layers instead of static embedding layers.\ndef build_de_user_model(user_id_lookup_layer: tf.keras.layers.StringLookup) -> tf.keras.layers.Layer:\n vocab_size = user_id_lookup_layer.vocabulary_size()\n return tf.keras.Sequential([\n     tf.keras.layers.InputLayer(input_shape=(), dtype=tf.string),\n     user_id_lookup_layer,\n     de.keras.layers.Embedding(\n         embedding_size=64,\n         initializer=tf.random_uniform_initializer(),\n         init_capacity=int(vocab_size*0.8),\n         restrict_policy=de.FrequencyRestrictPolicy,\n         name=\"UserDynamicEmbeddingLayer\"\n     ),\n     tf.keras.layers.Dense(64, activation=\"gelu\"),\n     tf.keras.layers.Dense(32),\n     tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))\n ], name='user_model')\n\ndef build_de_item_model(movie_title_lookup_layer: tf.keras.layers.StringLookup) -> tf.keras.layers.Layer:\n vocab_size = movie_title_lookup_layer.vocabulary_size()\n return tf.keras.models.Sequential([\n      tf.keras.layers.InputLayer(input_shape=(max_token_length), dtype=tf.string),\n     movie_title_lookup_layer,\n     de.keras.layers.SquashedEmbedding(\n         embedding_size=64,\n         initializer=tf.random_uniform_initializer(),\n         init_capacity=int(vocab_size*0.8),\n         restrict_policy=de.FrequencyRestrictPolicy,\n         combiner=\"mean\",\n         name=\"ItemDynamicEmbeddingLayer\"\n     ),\n     tf.keras.layers.Dense(64, activation=\"gelu\"),\n     tf.keras.layers.Dense(32),\n     tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))\n ])\nWith the user tower and movie tower models defined, we can define the retrieval model as usual.\nCreating and compiling the final model\nAs a final step in model building, we\u2019ll create the model and compile it.\ndef create_de_two_tower_model(dataset: tf.data.Dataset, candidate_dataset: tf.data.Dataset) -> tf.keras.Model:\n    \n user_id_lookup_layer = get_user_id_lookup_layer(dataset)\n movie_title_lookup_layer = get_movie_title_lookup_layer(dataset)\n user_model = build_de_user_model(user_id_lookup_layer)\n item_model = build_de_item_model(movie_title_lookup_layer)\n task = tfrs.tasks.Retrieval(\n     metrics=tfrs.metrics.FactorizedTopK(\n         candidate_dataset.map(item_model)\n     ),\n )\n\n model = DynamicEmbeddingTwoTowerModel(user_model, item_model, task)\n optimizer = de.DynamicEmbeddingOptimizer(tf.keras.optimizers.Adam())\n model.compile(optimizer=optimizer)\n    \n return model\n\ndatasets = create_datasets()\nde_model = create_de_two_tower_model(datasets.training_datasets.train_ds, datasets.candidate_dataset)\nNote the usage of the DynamicEmbeddingOptimizer wrapper around the standard TensorFlow optimizer. It is mandatory to wrap the standard optimizer in a DynamicEmbeddingOpitmizer as it will provide specialized functionality needed to train the weights stored in a hashtable. We can now train our model.\nTraining the model\nTraining the model is quite straightforward, but will involve a bit more extra effort as we\u2019d like to log some extra information. We will perform the logging through a tf.keras.callbacks.Callback object. We\u2019ll name this DynamicEmbeddingCallback.\nepochs = 3\nhistory_de = {}\nhistory_de_size = {}\nde_callback = DynamicEmbeddingCallback(de_model, steps_per_logging=20)\n\nfor epoch in range(epochs):\n\n datasets = create_datasets()\n train_steps = len(datasets.training_datasets.train_ds)\n    \n hist = de_model.fit(\n     datasets.training_datasets.train_ds,\n     epochs=1,\n     validation_data=datasets.training_datasets.validation_ds,\n     callbacks=[de_callback]\n )\n    \n for k,v in de_model.dynamic_embedding_history.items():\n     if k==\"step\":\n          v = [vv+(epoch*train_steps) for vv in v]\n     history_de_size.setdefault(k, []).extend(v)\n     \n for k,v in hist.history.items():\n      history_de.setdefault(k, []).extend(v)\nWe have taken the loop that goes through the epochs out of the fit() function. Then in every epoch we re-create the dataset, as that will provide a different shuffling of the training dataset. We will train the model for a single epoch within the loop. Finally we accumulate the logged embedding sizes in history_de_size (this is provided by our custom callback) and performance metrics in history_de.\nThe callback is implemented as follows.\nclass DynamicEmbeddingCallback(tf.keras.callbacks.Callback):\n    \n    def __init__(self, model, steps_per_logging, steps_per_restrict=None, restrict=False):\n        self.model = model\n        self.steps_per_logging = steps_per_logging\n        self.steps_per_restrict = steps_per_restrict\n        self.restrict = restrict\n    \n    def on_train_begin(self, logs=None):\n        self.model.dynamic_embedding_history = {}\n        \n    def on_train_batch_end(self, batch, logs=None):\n                \n        if self.restrict and self.steps_per_restrict and (batch+1) % self.steps_per_restrict == 0:\n            \n            [\n                self.model.embedding_layers[k].params.restrict(\n                    num_reserved=int(self.model.lookup_vocab_sizes[k]*0.8), \n                    trigger=self.model.lookup_vocab_sizes[k]-2 # UNK & PAD tokens\n                ) for k in self.model.embedding_layers.keys()\n            ] \n        \n        if (batch+1) % self.steps_per_logging == 0:\n            \n            embedding_size_dict = {\n                k:self.model.embedding_layers[k].params.size().numpy() \n                for k in self.model.embedding_layers.keys()\n            }\n\n            for k, v in embedding_size_dict.items():\n                self.model.dynamic_embedding_history.setdefault(f\"embedding_size_{k}\", []).append(v)\n            self.model.dynamic_embedding_history.setdefault(f\"step\", []).append(batch+1)\n\nThe callback does two things:\nLogs the sizes of the embedding layers every steps_per_logging iterations\nReduces the size of the embedding table to an 80% size of the total vocabulary size if restrict=True(This is set to False by default)\nLet\u2019s understand what reducing the size means and why it is important.\nReducing the size of the embedding table\nAn important topic we still haven\u2019t discussed is how to reduce the size of the embedding table, should it grow over some predefined threshold. This is a powerful functionality as it allows us to define a threshold over which the embedding table should not grow. This will allow us to work with large vocabularies while keeping the memory requirement under the memory limitations we may have. We achieve this by calling restrict() on the underlying variables of the embedding layer as shown in the DynamicEmbeddingCallback. restrict() takes two arguments in: num_reserved (the size after the reduction) and trigger (size at which the reduction should be triggered). The policy that governs how the reduction is performed is defined using the restrict_policy argument in the layer construct. You can see that we are using the FrequencyRestrictPolicy. This means the least frequent items will be removed from the embedding table. The callback enables a user to set how frequently the reduction should get triggered by setting the steps_per_restrict and restrict arguments in the DynamicEmbeddingCallback.\nReducing the size of the embedding table makes more sense when you have streaming data. Think about an online learning setting, where you are training the model every day (or even every hour) on some incoming data. You can think of the outer for loop (i.e. epochs) representing days. Each day you receive a dataset (containing user interactions from the previous day for example) and you train the model from the previous checkpoint. In this case, you can use the DynamicEmbeddingCallback to trigger a restrict if the embedding table grows over the size defined in the trigger argument.\nAnalyzing performance\nHere we analyze the performance of three variants.\nThe standard retrieval model (which uses a static embedding table)\nRetrieval model using dynamic embedding but no restrict performed\nRetrieval model using dynamic embedding with restrict performed\nYou can see that the model using dynamic embeddings (solid green line) has comparative validation performance to the baseline (solid red line). You can see a similar trend in the training accuracy as well. In practice, dynamic embeddings can often be seen to improve accuracy in a large-scale online learning setup.\nFinally, we can see that restrict has a somewhat detrimental effect on the validation accuracy, which is understandable. Since we\u2019re working with a relatively small dataset with a small number of items, the reduction could be getting rid of embeddings that are best kept in the table. For example, you can increase the num_reserved argument (e.g. set it to int(self.model.lookup_vocab_sizes[k]*0.95)) in the restrict function which would yield performance that improves towards the performance of without restrict.\nNext we look at how dynamic the embedding tables really are over time.\nWe can see that when restrict is not used, the embedding table grows to the full size of the vocabulary (dashed line) and stays there. However when restrict is triggered (dotted line), the size drops and grows in size again as it encounters new IDs.\nIt is also important to note that constructing a proper validation is not a trivial task. There are considerations such as out-of-sample validation, out-of-time validation, stratification, etc. that needs to be taken into account carefully. However for this exercise, we have not focused on such factors and created a validation set by sampling randomly from the existing dataset.\nConclusion\nUsing dynamic embedding tables is a powerful way to perform representation learning when working with large sets of items containing millions or billions of entities. In this tutorial, we learnt how to use the dynamic_embedding module provided in the TensorFlow Recommender Addons library to achieve this. We first explored the data and constructed tf.data.Dataset objects by extracting the features we\u2019ll be using for our model training and evaluation. Next we defined a model that uses static embedding tables to use as an evaluation baseline. We then created a model that uses dynamic embedding and trained it on the data. We saw that using dynamic embeddings, the embedding tables grow only on demand and still achieve comparable performance with the baseline. We also discussed how the restrict functionality can be used to shrink the embedding table if it grows past a pre-defined threshold.\nWe hope this tutorial gives you a good conceptual introduction to TFRA and dynamic embeddings, and helps you think about how you can leverage it to enhance your own recommenders. If you would like to have a more in-depth discussion, please visit the TFRA repository.",
    "link": "https://blog.tensorflow.org/2023/04/training-recommendation-model-with-dynamic-embeddings.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdc76CVXCePD_2qIelWfi3dw_4P4d8tq8X6er753xrvUUGEDYtAqZiPkU1v_RitRm8bxXevsE54jARVORGnTXuC29DWDPIiN9dBr3nEHHVHbDo40EUJnqDvSu2k2nhwMoNuwQ4EEPiWWXIKc12j59vaSp_e_e7V1XL2iAaVNk-seCsNe5NuxYpY-G-/s1600/social-TensorFlow%20-%20Training%20a%20recommendation%20model%20with%20dynamic%20embeddings.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhdW4kHBd4n_Tz0ygjDJqqwoKpxgpu5Q9l-OmgfvAbpTo8smw0uBemAHobAN7sYt3U7v5slXFNudpyzY7JF7rxLuDbRB_v2S8UPDZQoP_edPjXl_U3UFMOyIusJK_H_DcYTlXhow7SjDozxoKpWO9z87SfF7on4WTaQgMOWWwtz0BQ-crja1DTHfCge/s1600/header-TensorFlow%20-%20Training%20a%20recommendation%20model%20with%20dynamic%20embeddings.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdLeEjNdYb4EiUhh3EvNOf9xpno3w87zdjNHKZjbTORI3Wf5FhNcLj6qTUt0eXKrlTT-ey1DXV7nkFRdy2V1W4EQBXyyuH8jNtbdt-FbyK4BXaHuaAaTasVJVzAa78xdlHajnM9mYPe1WKXo7i_L5e8d8nLNim3iXLoAymx2SMcJkWCyKD3uFbnW2Z/s1600/image2.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEje2YQ6vDZwTnU2zZdELInx02rk28bCVFyZzJD_HwwmoyEwludk9wpikmB3JXgxtjoSVcgKknZgse0qkytkqTb_cS7-VEdE4i4tfQahblY4sbvWAR5WRkspleur7pYMULRcmsRh59bLKQKsbHkd4Y-RaBhAimi6pN6fzcCXPhSHVFBcqMnXSftGisfv/s1600/image1.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjX6J6cCq6gc3DzlSKWdPUCz8vOo5IPFMhjdMUckDz2K3TwfSxlODzLX8vIcfDDQ_WjRxtSix4jsEAm71cQU27tFMyqXKeZgpiqk9OlFCRp76Vd39hovyqegVFpbHFEHk6Tne9UwVpKXSSw6JghuxkMZB2qVItCIIswpyfkLzTTsvEl4-tOePwLgRqe/s1600/image3.png"
    ],
    "time": "2023/12/12 00:58:43"
  },
  {
    "title": "What\u2019s new in TensorFlow 2.12 and Keras 2.12?",
    "content": "Posted by the TensorFlow & Keras teams\nTensorFlow 2.12 and Keras 2.12 have been released! Highlights of this release include the new Keras model saving and exporting format, the keras.utils.FeatureSpace utility, SavedModel fingerprinting, Python 3.11 wheels for TensorFlow and many more.\nTensorFlow Core\nSavedModel Fingerprinting\nModels saved with tf.saved_model.save now come with a fingerprint file containing hashes to uniquely identify the SavedModel. Multiple fingerprints are derived from the model content, allowing you to compare the structure, graph, signatures, and weights across models. Read more about fingerprinting in the RFC and check out the read_fingerprint API and Fingerprint class.\ntf.function\ntf.function now uses the Python inspect library to consistently mimic the decorated function\u2019s signature. WYSIWYG: decorated and non-decorated behavior is identical, even for complex uses like wrapping (functools.wraps) and partial application (functools.partial).\nWe now detect incompatible tf.function input types (such as mismatched functools.wraps calls). Additionally, we have improved type constraining logic (input_signature) for better error messages and consistency (e.g. a function with no parameters now automatically has input_signature=[]).\nAdditionally, we have added experimental.extension_type.as_dict() to convert tf.experimental.ExtensionTypes to Python dicts.\nKeras\nNew model format\nThe biggest new Keras feature in this release is the new model export formats. We've completely reworked Keras saving and serialization to cleanly separate two key use cases:\n1. Python saving & reloading. This is when you save a Keras model to re-instantiate it later in a Python runtime, exactly as it was. We achieve this with a new file format, called the \"Keras v3\" format (.keras). You can start using it by calling model.save(\"your_model.keras\", save_format=\"keras_v3\").\n 2. Model export for inference in a runtime that might not support Python at all (e.g. the TF Serving runtime). You can create a lightweight (single-file) export via model.export(\"your_model\") \u2013 and reload it in TF Serving or Python via tf.saved_model.load(\"your_model\"). By default, this format only preserves a single serving endpoint, the forward pass of the model, available upon reload as .serve(). Further customization is available through the keras.export.ExportArchive class.\nIn the 2.13 release, keras_v3 will become the default for all files with the .keras extension. The format supports non-numerical state such as vocabulary files and lookup tables, and it is easy to save custom layers with exotic state elements (such as a FIFOQueue). The format does not rely on loading arbitrary code through bytecode or pickling, so it is safe by default. This is a big advance for secure ML. Note that due to this safety-first mindset, Python lambdas are disallowed at loading time. If you want to use a lambda, and you trust the source of the model, you can pass safe_mode=False to the loading method.\nThe legacy formats (\"h5\" and \"Keras SavedModel\" format based on TF SavedModel) will stay supported in perpetuity. However, we recommend that you consider adopting the new Keras v3 format for richer Python-side model saving/reloading, and using export() for inference-optimized model export.\nFeatureSpace\nAnother exciting feature is the introduction of the keras.utils.FeatureSpace utility. It enables one-step indexing and preprocessing of structured data \u2013 including feature hashing and feature crossing. See the [feature space tutorial](https://keras.io/examples/structured_data/structured_data_classification_with_feature_space/) for more information.\nLike all Keras APIs, FeatureSpace is built with progressive disclosure of complexity in mind, so it is fully customizable \u2013 you can even specify custom feature types that rely on your own preprocessing layers. For instance, if you want to create a feature that encodes a text paragraph, that's just two lines of code:\nfrom tensorflow.keras import layers, utils\n\ncustom_layer = layers.TextVectorization(output_mode=\"tf_idf\")\n\nfeature_space = utils.FeatureSpace(\n    features={\n        \"text\": FeatureSpace.feature(\n            preprocessor=custom_layer, dtype=\"string\", output_mode=\"float\"\n        ),\n    },\n    output_mode=\"concat\",\n)\n\nThere are just the release highlights \u2013 there are many more Keras-related improvements included, so be sure to check out the release notes!\ntf.data\nWarm starting\ntf.data has added support for warm-starting input processing. If warm_start=True (on tf.data.experimental.OptimizationOptions), tf.data will start preemptively start background threads during iterator creation (instead of waiting for the first call to GetNext). This allows users to improve latency to the initial GetNext call at the expense of higher memory usage.\nRe-randomizing across epochs\ntf.data added a new rerandomize_each_iteration argument to tf.data.Dataset.random(), to control whether the sequence of generated random numbers should be re-randomized every epoch, or not (the default behavior). If seed is set and rerandomize_each_iteration=True, random() will produce a different (deterministic) sequence of numbers every epoch. This can be useful when training over a relatively smaller number of input examples to ensure that the model doesn\u2019t learn the sequence itself.\nInfra Updates\nProtobuf python runtime version was upgraded to 4.21.9. All protobuf *_pb2.py stubs are generated now with protoc 3.21.9. The minimal supported protobuf runtime version is 3.20.3.\nWe released Python 3.11 wheels for the TensorFlow packages with this release!\nWe removed Python 3.7 support from this version. Going forward, we will no longer release patches for Python 3.7.",
    "link": "https://blog.tensorflow.org/2023/03/whats-new-in-tensorflow-212.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4Rm9rtDB4jMm-DzSAPH-_DS6S0qjrnmIz5WZ__2KT22zDhQUGPvbS0FgR5vz0TFw62PTrwP_y0jIH47s9VZRj0uOSHQMzyO-GAoWwGpXvYY693DZ9r3StwgsxVzqNdhlFp2hnzn-KKbzakS1sX0dxlQzB0wyxzO5nmDRO3mRCP8yZogvNrKS3RGIO/s1600/Tensorflow-septmber-update-social%20%282%29.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3eSMyPF-kny1HPktcQ1Co7_hqfQ_aWCk6ybKJ6esMrj2TQjuBQ0ohfjQes3p_XFBNoFwoJr1OQM_lnwfsQ1ThYfogUGqlNx6A_Sjva9ofiW3KZwzl9lA3mT4qGijjxG29DldHKCstnrsJljYqc8Dj1fqLcIvzLK2Ta2Om9O112GapJrLdCIy3lGOd/s1600/Tensorflow-septmber-update-header%20%283%29.png"
    ],
    "time": "2023/12/12 00:58:43"
  },
  {
    "title": "Optimizing TensorFlow for 4th Gen Intel Xeon Processors",
    "content": "Posted by Ashraf Bhuiyan, AG Ramesh from Intel, Penporn Koanantakool from Google\nTensorFlow 2.9.1 was the first release to include, by default, optimizations driven by the Intel\u00ae oneAPI Deep Neural Network (oneDNN) library, for 3rd Gen Intel \u00ae 3rd Xeon\u00ae processors (Cascade Lake). Since then, Intel and Google have continued our collaboration to introduce new TensorFlow optimizations for the next generation of Intel Xeon processors.\nThese optimizations accelerate TensorFlow models using the new matrix-based instructions set, Intel\u00ae Advanced Matrix Extension (AMX). The Intel AMX instructions are designed to accelerate deep learning operations such as matrix multiplication and convolutions that use Google\u2019s bfloat16 and 8-bit low precision data types. Low precision data types are widely used and provide significant improvement over the default 32-bit floating format without significant loss in accuracy.\nWe are happy to announce that these features are now available as a preview in the nightly build of TensorFlow on Github, and also in the Intel optimized build. TensorFlow developers can now use Intel AMX on the 4th Gen Intel\u00ae Xeon\u00ae Scalable processor (formerly known as Sapphire Rapids) using the existing mixed precision support available in TensorFlow. We are excited by the results - several popular AI models run up to 19x faster by moving from 3rd Gen to 4th Gen Intel Xeon processors using Intel AMX.\nIntel\u2019s Advanced Matrix Extension (AMX) Accelerations in 4th Gen Intel Xeon Processor\nThe Intel\u00ae Advanced Matrix Extension (AMX) is an X86-based extension which introduces a new programming framework for dot products of two matrices. Intel AMX serves as an AI acceleration engine and builds on capabilities such as AVX-512 (for optimized vector operations) and Deep Learning Boost (through Vector Neural network Instructions for optimized resource utilization/caching and for lower precision AI optimizations) in previous generations of Intel Xeon processors.\nIn Intel AMX, a new type of 2-dimensional register file, called \u201ctiles\u201d, and a set of 12 new X86 instructions to operate on the tiles, are introduced. New instruction TDPBF16PS performs a dot product of bfloat16 tiles, and TDPBSSD performs dot product of signed 8-bit integer tiles. Other instructions include tile configuration and data movement to the Intel AMX unit. Further details can be found in the document published by Intel.\nHow to take advantage of AMX optimizations on 4th Gen Intel Xeon.\nIntel AMX optimizations are included in the official TensorFlow nightly releases. The latest stable release 2.11 includes preliminary support, however full support will be available in a subsequent stable release.\nUsers running TensorFlow on Intel 4th gen Intel Xeon can take advantage of the optimizations with minimal changes:\na)    For bfloat16 mixed precision, developers can accelerate their models using Keras mixed precision API, as explained here. You can easily invoke auto mixed precision by including these lines in your code, that\u2019s it! \n   from tensorflow.keras import mixed_precision\npolicy = mixed_precision.Policy('mixed_bfloat16')\nmixed_precision.set_global_policy(policy)\n\nb)    Using Intel AMX with 8-bit quantized models requires the models to be quantized to use int8. Any existing standard models, for example RN50, BERT, SSD-RN34 that have been previously quantized with Intel Neural Compressor will run with no changes needed.\nPerformance improvements\nThe following charts show performance improvement on a 2-socket, 56-core 4th Gen Intel Xeon using Intel AMX low precision on various popular vision and language models, where the baseline is a 2-socket, 40-core 3rd Gen Intel Xeon with FP32 precision. We use Intel Optimization for TensorFlow* preview and the launch_benchmark script from Model Zoo for Intel\u00ae Architecture .\nHere in the chart, inference with mixed precision models on a 4th Gen Intel Xeon was 1.9x to 9.6x faster than FP32 models on a 3rd Gen Intel Xeon. (BS=x indicates a large batch size, depending on the model)\nTraining models with auto-mixed-precision on a 4th Gen Intel Xeon was 2.3x to 5.5x faster than FP32 models on a 3rd Gen Intel Xeon.\nSimilarly, quantized model inference on a 4th Gen Intel Xeon was 3.3x to 19x faster than FP32 precision on a 3rd Gen Intel Xeon.\nIn addition to the above popular models, we have tested 100s of other models to ensure that the performance gain is observed across the board.\nNext Steps\nWe are working to continuously tune and improve the Intel AMX optimizations in future releases of TensorFlow. We encourage users to optimize their AI models with Intel AMX on Intel 4th Gen processors to get a significant performance boost; not just for inference, but also for pre-training, fine tuning and transfer learning. We would like to hear from you, please provide feedback through the TensorFlow Github page or the oneAPI Deep Neural Network library GitHub page.\nAcknowledgements\nThe results presented in this blog is the work of many people including the TensorFlow and oneDNN teams at Intel and our collaborators in Google\u2019s TensorFlow team.\n\nFrom Intel: Md Faijul Amin, Mahmoud Abuzaina, Gauri Deshpande, Ashiq Imran, Kanvi Khanna, Geetanjali Krishna, Sachin Muradi, Srinivasan Narayanamoorthy, Bhavani Subramanian, Yimei Sun, Om Thakkar, Jojimon Varghese, Tatyana Primak, Shamima Najnin, Mona Minakshi, Haihao Shen, Shufan Wu, Feng Tian, Chandan Damannagari.\n\nFrom Google: Eugene Zhulenev, Antonio Sanchez, Emilio Cota.\n\n*For configuration details see www.intel.com/performanceindex\n\nNotices and Disclaimers:\nIntel's compilers may or may not optimize to the same degree for non-Intel microprocessors for optimizations that are not unique to Intel microprocessors. These optimizations include SSE2, SSE3, and SSSE3 instruction sets and other optimizations. Intel does not guarantee the availability, functionality, or effectiveness of any optimization on microprocessors not manufactured list by Intel. Microprocessor-dependent optimizations in this product are intended for use with Intel microprocessors. Certain optimizations not specific to Intel microarchitecture are reserved for Intel microprocessors. Please refer to the applicable product User and Reference Guides for more information regarding the specific instruction sets covered by this notice.",
    "link": "https://blog.tensorflow.org/2023/01/optimizing-tensorflow-for-4th-gen-intel-xeon-processors.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgcpeRgS7EyhfnXR1XKTfYrkJFSe3whZFVhysr3Cp6Ly_iykW2JukiXWpy7A9hFSwjm1nvJZ0zlXy-_K_24rqlFktO-E2_031iRkcVneQ1xeB9QAiZb5vJC8A4DabTKQJtUxL_-YXc1gSfyu7XYHrSOr_-Rxi2rZsVB6X1zmI37ij5LqoP60aiG3C3K/s1600/image2.jpg",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6BRBg_Ij2Pn7BJ_sJyHMJ_hVYh3yG4FnwOEFitOEkbzqvk9Jrxmx0mmlrT6PmtuLiDEE47zKOaNMFp9aONWEkdQNGzwUPTgLHAPWqUoyXA6y7dMBh1ugFBZOqBt4chwigQoAeeDTGJLpgrrteI2wkSvTe7fAbYMAmUBOq1N1kKTN0qChdS_P7hHvr/s1600/image3.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_Smdbw-JRbJ55Qs_pGsl9NnscaHGRS0-NnK6stsO7sc0qf15Cqcc-f7UnsRX_rQQHrZ4gGV53hkv3tB4c_i_VdlFPqeZ3-naGUbm0excGUTQZyH0ocr3t5z5OrqZyYuDS5y6Z4f5pAplILwEI0NnwcORcXT79sCKgvd_cRt5PDXiIjHU2Opr1lgkB/s1600/image1.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjzorC4DFeHE-IrwW4k-aVzHzOQ9nkQd98cGDxvVu6i2-w1N3BXubGher5Hp2ZXs_VfmKqAqmMKk6iVBn4nXQ3gJXro8Qhv28xhwCaTNQJdwuYpnaHkrVOzVunZDwE62JcBWP7Y3w-VDm3YX0AternxcwzDkC5PSxfIN6atpgrzt5mYAIBbkWtnrETY/s1600/image4.png"
    ],
    "time": "2023/12/12 00:58:43"
  },
  {
    "title": "New State-of-the-Art Quantized Models Added in TF Model Garden",
    "content": "Posted by Jaehong Kim, Fan Yang, Shixin Luo, and Jiyang Kang\nThe TensorFlow Model Garden provides implementations of many state-of-the-art machine learning models for vision and natural language processing, and workflow tools to let you quickly configure and run those models on standard datasets. These models are implemented using modern best practices.\nPreviously, we have announced the quantization aware training (QAT) support for various on-device vision models using TensorFlow Model Optimization Toolkit (TFMOT). In this post, we introduce new SOTA models optimized using QAT in object detection, semantic segmentation, and natural language processing.\nRetinaNet+MobileNetV2\nA new QAT supported object detection model has been added to the Model Garden. Specifically, we use a MobileNetV2 with 1x depth multiplier as the backbone and a lightweight RetinaNet as the decoder. MobileNetV2 is a widely used mobile model backbone and we have provided QAT support since our last release. RetinaNet is the SOTA one-stage detection framework used for detection tasks and we make it more efficient on mobile devices by using separable convolution and reducing the number of filters. We train the model from scratch without any pre-trained checkpoints. \nResults show that with QAT, we can successfully preserve the model quality while reducing the latency significantly. In comparison, post-training quantization (PTQ) does not work out-of-the-box smoothly due to the complexity of the RetinaNet decoder, thus leading to low box average precision (AP).\nTable 1. Box AP and latency comparison of the RetinaNet models. Latency is measured on a Samsung Galaxy S21 using 1-thread CPU. FP32 refers to the unquantized floating point TFLite model. PTQ INT8 refers to full integer post-training quantization. QAT INT8 refers to the quantized QAT model.\nThe QAT support for object detection model is critical to many on-device use cases, such as product recognition using hand-held devices, enabling a more pleasant user journey.\nMOSAIC\nMOSAIC is a neural network architecture for efficient and accurate semantic image segmentation on mobile devices. With a simple asymmetric encoder-decoder structure which consists of an efficient multi-scale context encoder and a light-weight hybrid decoder to recover spatial details from aggregated information, MOSAIC achieves better balanced performance while considering accuracy and computational cost. MLCommons MLPerf adopted MOSAIC as the new industry standard model for mobile image segmentation benchmark.\nWe have added QAT support for MOSAIC as part of the open source release. In Table 2, we provide the benchmark comparison between DeepLabv3+ and MOSAIC. We can clearly observe that MOSAIC achieves better performance (mIoU: mean intersection-over-union) with significantly lower latency. The negligible gap between QAT INT8 and FP32 also demonstrates the effectiveness of QAT. Please refer to the paper for more benchmark results.\nTable 2. mIoU and latency comparison of a MobileNet Multi-HW AVG + MOSAIC. Latency is measured on a Samsung Galaxy S21 using 1-thread CPU. FP32 refers to the unquantized floating point TFLite model. PTQ INT8 refers to full integer post-training quantization. QAT INT8 refers to the quantized QAT model.\nMOSAIC is designed using commonly supported neural operations, and can be easily deployed to diverse mobile hardware platforms for efficient and accurate semantic image segmentation.\nMobileBERT\nMobileBERT is a thin version of BERT_LARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. (code)\nWe applied QAT to the MobileBERT model to show our QAT toolkit can apply to the Transformer based mode, which has become very popular these days.\nTable 3. F1 score and latency comparison of a MobileBERT. Latency is measured on a Samsung Galaxy S21 using 1-thread CPU. FP32 refers to the unquantized floating point TFLite model. PTQ INT8 refers to full integer post-training quantization. QAT INT8 refers to the quantized QAT model.\nMobileBERT-EdgeTPU-XS model\nApply QAT on MobileBERT to enable mobile use-case for the NLP model, such as next word prediction or answer generation. This model only trained on Q&A tasks but it can leverage other on-device NLP tasks.\nNext steps\nIn this post, we expanded the coverage of QAT support and introduced new state-of-the-art quantized models in Model Garden for object detection, semantic segmentation, and natural language processing. TensorFlow practitioners can easily utilize these SOTA quantized models for their problems achieving lower latency or smaller model size with minimal accuracy loss.\nTo learn more about the Model Garden and its Model Optimization Toolkit support, check out the following blog posts:\nIntroducing the Model Garden for TensorFlow 2\nAdding Quantization-aware Training and Pruning to the TensorFlow Model Garden\nModel Garden provides implementation of various vision and language models, and the pipeline to train models from scratch or from checkpoints. To get started with Model Garden, you can check out the examples in the Model Garden Official repository. Model libraries in this repository are optimized for fast performance and actively maintained by Google engineers. Simple colab examples for training and inference using these models are also provided.\nAcknowledgements\nWe would like to thank everyone who contributed to this work including the Model Garden team, Model Optimization team and Google Research team. Special thanks to Abdullah Rashwan, Yeqing Li, Hongkun Yu from the Model Garden team; Jaesung Chung from the Model Optimization team, Weijun Wang from the Google Research team.",
    "link": "https://blog.tensorflow.org/2022/12/new-state-of-art-quantized-models-added-in-tf-model-garden.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJ5hYm0w9e7Uo91ZJ0ve-r-V2UMA0s9sH651kAW_OFKJ4KKQknt21X_PSWy2T1iKcgge_CgAqIymuWMkhijU6RlyJEYRq8NIhYQEdaixXeSh_TfWHAA9FrdWBqdRFGNN2elHDkXgURVwRze4Huc1y1lukmCXxXhN8S9nHyO7rIxmLw3Wsu_H7vVB7h/s1600/Tensorflow-new-state-of-the-art-model-garden.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh3s49cUprXtLr_BbPJpJhRure1jzUwYuoMTscMbGPK-v0xpHNZzyZgRiQ67fnbftoA_9E2d-SGqv3LTbMiDtw1cEfUKyky14QaET1WHoQ8QADXwD0E9G1zgc9DdA5xkMHWMclVXMOkwE0QBE5yxkmAy2ch321ZXeOMWPA_RAAS8zNkRn3Aj06j8lim/s1600/Tensorflow-new-state-of-the-art-model-garden-4209x1253.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1Ii6EvlkDVuGkaXsjzPP3ImRexcTG5S4nzo9u9k2H0G2pkSXO1uB884WFlQ_SMh_4RFQI-BbQcMRURW1qONue94wPcSgVdW6Cq6EviF6ARsGhp2hDCSvhPlfwGTEq6agwO6x9NMelpuEYcwHI4fx1LfkPHlVtkxTkUI5Yf26VXLkBdIcdYYcZlDgA/s1022/Screen%20Shot%202022-12-12%20at%205.24.18%20PM.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhV3B_ZqXCHiPdKk70wSHf9b3aDJYmbkF0MHvjMo0IsgsT69o7EG5yFVcl4v5ycELMJKtvdzrQVqYnqdQ5x2ZOpJdY-6Od0MpWXZHGxyc1GcDHSHczodnGJp0netCdjxns5_HepdMDhlCswXubdDjQ5qzqOGdEwycpi7Tb_rH6OshVPU4J9yOQUZnIU/s1028/Screen%20Shot%202022-12-12%20at%205.26.08%20PM.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhC7Fcb8D6CbbNjALe6EopO0IKlt3R3YL23mHyph7FRurBnT_PscmLya84hs1GhPeCdvaTaDhLNfOoXzkKSd_nOY2vOByxEjF-33KaB3xL9bPtQ8HzjXQJlCsuvVcnOp85R-ShyyGtw7hNPYl3BiKO_RMEfj0HtLgsycF9TA_ef3pbaGm88nsieLdUT/s1600/Screen%20Shot%202022-12-12%20at%205.35.50%20PM.png"
    ],
    "time": "2023/12/12 00:58:43"
  },
  {
    "title": "What\u2019s new in TensorFlow 2.11?",
    "content": "Posted by the TensorFlow & Keras teams\nTensorFlow 2.11 has been released! Highlights of this release include enhancements to DTensor, the completion of the Keras Optimizer migration, the introduction of an experimental StructuredTensor, a new warmstart embedding utility for Keras, a new group normalization Keras layer, native TF Serving support for TensorFlow Decision Forest models, and more. Let's take a look at these new features.\nTensorFlow Core\nDTensor\nDTensor is a TensorFlow API for distributed processing that allows models to seamlessly move from data parallelism to single program multiple data (SPMD) based model parallelism, including spatial partitioning. It gives you tools to easily train models where the model weights or inputs are so large they don\u2019t fit on a single device. We\u2019ve made several updates in TensorFlow v2.11.\nDTensor supports tf.train.Checkpoint\nYou can now checkpoint a DTensor model using tf.train.Checkpoint. Saving and restoring sharded DVariables will perform an efficient sharded save and restore. All DVariables must have the same host mesh, and DVariables and regular variables cannot be saved together. The old DCheckpoint based checkpointing API will be removed in the next release. You can learn more about checkpointing in this tutorial.\n\nA new unified accelerator initialization API\nWe\u2019ve introduced a new unified accelerator initialization API tf.experimental.dtensor.initialize_accelerator_system that shall be called for all three supported accelerator types (CPU, GPU and TPU), and all supported deployment modes (multi-client and local). The old initialization API, which had specialized functions for CPU/GPU multi-client and TPU, will be removed in the next release.\n\nAll-reduce optimizations enabled by default\nDTensor enables by default an All-reduce optimization pass for GPU and CPU to combine all the independent all-reduces into one. The optimization is expected to reduce overhead of small all-reduce operations, and our experiments showed significant improvements to training step time on BERT. The optimization can be disabled by setting the environment variable \u2018DTENSOR_ENABLE_COMBINE_ALL_REDUCES_OPTIMIZATION\u2019 to 0.\n\nA new wrapper for a distributed tf.data.Dataset\nWe\u2019ve introduced a wrapper for a distributed tf.data.Dataset, tf.experimental.dtensor.DTensorDataset. The DTensorDataset API can be used to efficiently handle loading the input data directly as DTensors by correctly packing it to the corresponding devices. It can be used for both data and model parallel training setups. See the API documentation linked above for more examples.\nKeras\nThe new Keras Optimizers API is ready\nIn TensorFlow 2.9, we released an experimental version of the new Keras Optimizer API, tf.keras.optimizers.experimental, to provide a more unified and expanded catalog of built-in optimizers which can be more easily customized and extended. In TensorFlow 2.11, we\u2019re happy to share that the Optimizer migration is complete, and the new optimizers are on by default.\nThe old Keras Optimizers are available under tf.keras.optimizers.legacy. These will never be deleted, but they will not see any new feature additions. New optimizers will only be implemented based on tf.keras.optimizers.Optimizer, the new base class.\nMost users won\u2019t be affected by this change, but if you find your workflow failing, please check out the release notes for possible issues, and the API doc to see if any API used in your workflow has changed.\nThe new GroupNormalization layer\nTensorFlow 2.11 adds a new group normalization layer, keras.layers.GroupNormalization. Group Normalization divides the channels into groups and computes within each group the mean and variance for normalization. Empirically, its accuracy can be more stable than batch norm in a wide range of small batch sizes, if learning rate is adjusted linearly with batch sizes. See the API doc for more details, and try it out!\nA diagram showing the differences between normalization techniques.\nWarmstart embedding utility\nTensorFlow 2.11 includes a new utility function: keras.utils.warmstart_embedding_matrix. It lets you initialize embedding vectors for a new vocabulary from another set of embedding vectors, usually trained on a previous run.\nnew_embedding = layers.Embedding(vocab_size, embedding_depth)\nnew_embedding.build(input_shape=[None])\nnew_embedding.embeddings.assign(\n    tf.keras.utils.warmstart_embedding_matrix(\n        base_vocabulary=base_vectorization.get_vocabulary(),\n        new_vocabulary=new_vectorization.get_vocabulary(),\n        base_embeddings=base_embedding.embeddings,\n        new_embeddings_initializer=\"uniform\")\nSee the Warmstart embedding tutorial for a full walkthrough.\nTensorFlow Decision Forests\nWith the release of TensorFlow 2.11, TensorFlow Serving adds native support for TensorFlow Decision Forests models. This greatly simplifies serving TF-DF models in Google Cloud and other production systems. Check out the new TensorFlow Decision Forests and TensorFlow Serving tutorial, and the new Making predictions tutorial, to learn more.\nAnd did you know that TF-DF comes preinstalled in Kaggle notebooks? Simply import TF-DF with import tensorflow_decision_forests as tfdf and start modeling.\nTensorFlow Lite\nTensorFlow Lite now supports new operations including tf.unsorted_segment_min, tf.atan2 and tf.sign. We\u2019ve also updated tfl.mul to support complex32 inputs.\nStructured Tensor\nThe tf.experimental.StructuredTensor class has been added. This class provides a flexible and TensorFlow-native way to encode structured data such as protocol buffers or pandas dataframes. StructuredTensor allows you to write readable code that can be used with tf.function, Keras, and tf.data. Here\u2019s a quick example.\ndocuments = tf.constant([\n    \"Hello world\",\n    \"StructuredTensor is cool\"])\n\n@tf.function\ndef parse_document(documents):\n tokens = tf.strings.split(documents)\n token_lengths = tf.strings.length(tokens)\n\n ext_tokens = tf.experimental.StructuredTensor.from_fields_and_rank(\n     {\"tokens\":tokens,\n      \"length\":token_lengths}, rank=documents.shape.rank + 1)\n\n return tf.experimental.StructuredTensor.from_fields_and_rank({\n     \"document\":documents,\n     \"tokens\":ext_tokens}, rank=documents.shape.rank)\n\nst = parse_document(documents)\n\nA StructuredTensor can be accessed either by index, or by field name(s).\n\n>>> st[0].to_pyval()\n{'document': b'Hello world',\n 'tokens': [{'length': 5, 'token': b'Hello'},\n  {'length': 5, 'token': b'world'}]}\n\nUnder the hood, the fields are encoded as Tensors and RaggedTensors.\n\n>>> st.field_value((\"tokens\", \"length\"))\n\n<tf.RaggedTensor [[5, 5], [16, 2, 4]]>\nYou can learn more in the API doc linked above.\nComing soon\nDeprecating Estimator and Feature Column\nEffective with the release of TensorFlow 2.12, TensorFlow 1\u2019s Estimator and Feature Column APIs will be considered fully deprecated, in favor of their robust and complete equivalents in Keras. As modules running v1.Session-style code, Estimators and Feature Columns are difficult to write correctly and are especially prone to behave unexpectedly, especially when combined with code from TensorFlow 2.\nAs the primary gateways into most of the model development done in TensorFlow 1, we\u2019ve taken care to ensure their replacements have feature parity and are actively supported. Going forward, model building with Estimator APIs should be migrated to Keras APIs, with feature preprocessing via Feature Columns specifically migrated to Keras\u2019s preprocessing layers - either directly or through the TF 2.12 one-stop utility tf.keras.utils.FeatureSpace built on top of them.\nDeprecation will be reflected throughout the TensorFlow documentation as well as via warnings raised at runtime, both detailing how to avoid the deprecated behavior and adopt its replacement.\nDeprecating Python 3.7 Support after TF 2.11\nTensorFlow 2.11 will be the last TF version to support Python 3.7. Since TensorFlow depends on NumPy, we are aiming to follow numpy's Python version support policy which will benefit our internal and external users and keep our software secure. Additionally, a few vulnerabilities reported recently required that we bump our numpy version, which turned out not compatible with Python 3.7, further supporting the decision to drop support for Python 3.7.\nNext steps\nCheck out the release notes for more information. To stay up to date, you can read the TensorFlow blog, follow twitter.com/tensorflow, or subscribe to youtube.com/tensorflow. If you\u2019ve built something you\u2019d like to share, please submit it for our Community Spotlight at goo.gle/TFCS. For feedback, please file an issue on GitHub or post to the TensorFlow Forum. Thank you!",
    "link": "https://blog.tensorflow.org/2022/11/whats-new-in-tensorflow-211.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4Rm9rtDB4jMm-DzSAPH-_DS6S0qjrnmIz5WZ__2KT22zDhQUGPvbS0FgR5vz0TFw62PTrwP_y0jIH47s9VZRj0uOSHQMzyO-GAoWwGpXvYY693DZ9r3StwgsxVzqNdhlFp2hnzn-KKbzakS1sX0dxlQzB0wyxzO5nmDRO3mRCP8yZogvNrKS3RGIO/s1600/Tensorflow-septmber-update-social%20%282%29.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3eSMyPF-kny1HPktcQ1Co7_hqfQ_aWCk6ybKJ6esMrj2TQjuBQ0ohfjQes3p_XFBNoFwoJr1OQM_lnwfsQ1ThYfogUGqlNx6A_Sjva9ofiW3KZwzl9lA3mT4qGijjxG29DldHKCstnrsJljYqc8Dj1fqLcIvzLK2Ta2Om9O112GapJrLdCIy3lGOd/s1600/Tensorflow-septmber-update-header%20%283%29.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgLn0rZAyW-qQQGOvxsaBrjPwj3hrMqyJdHdwJASR9taiH6XDLOB4zxDiuuwccTZY3AkxKKSLWXm9DFjEagDtsZwN-iL1tqFOtuWFateoxkfKPExeah_0jzV-gKKFbu3Fns20q4lel2vy7nOyEAwu0vxLcZEsCPTFGqG6yFWMJEmQ5SZIUiFnEvgmBF/s1600/image1.png"
    ],
    "time": "2023/12/12 00:58:43"
  },
  {
    "title": "Building the Future of TensorFlow",
    "content": "Posted by the TensorFlow team\nWe\u2019ve started planning the future of TensorFlow! In this article, we\u2019d like to share our vision.\nWe open-sourced TensorFlow nearly seven years ago, on November 9, 2015. Since then, thanks to thousands of open-source contributors and our incredible community of Google Developer Experts, community organizers, researchers, and educators around the globe, TensorFlow has come to define its category. \nToday, TensorFlow is the most-used machine learning platform, adopted by millions of developers. It\u2019s the 3rd most-starred software repository on GitHub (right behind Vue and React) and the most-downloaded machine learning package on PyPI. It has brought machine learning to the mobile ecosystem: TFLite now runs on four billion devices (maybe on yours, too!). TensorFlow has also brought machine learning to the Web: TensorFlow.js is now downloaded 170 thousand times weekly.\nAcross Google's product lineup, TensorFlow powers virtually all production machine learning, from Search, GMail, YouTube, Maps, Play, Ads, Photos, and many more. Beyond Google, at other Alphabet companies, TensorFlow and Keras enable the machine intelligence in Waymo's self-driving cars. \nIn the broader industry, TensorFlow powers machine learning systems at thousands of companies, including most of the largest machine learning users in the world \u2013 Apple, ByteDance, Netflix, Tencent, Twitter, and countless more. And in the research world, every month, Google Scholar is indexing over 3,000 new scientific publications that mention TensorFlow or Keras.\nToday, our user base and developer ecosystem are larger than ever, and growing!\nWe see the growth of TensorFlow not just as an achievement to celebrate, but as an opportunity to go further and deliver more value for the machine learning community.\nOur goal is to provide the best machine learning platform on the planet. Software that will become a new superpower in the toolbox of every developer. Software that will turn machine learning from a niche craft into an industry as mature as web development.\nTo achieve this, we listen to the needs of our users, anticipate new industry trends, iterate on our APIs, and work to make it increasingly easy for you to innovate at scale. In the same way that TensorFlow originally helped the rise of deep learning, we want to continue to facilitate the evolution of machine learning by giving you the platform that lets you push the boundaries of what's possible. Machine learning is evolving rapidly, and so is TensorFlow.\nToday, we're excited to announce we've started working on the next iteration of TensorFlow that will enable the next decade of machine learning development. We are building on TensorFlow's class-leading capabilities, and focusing on four pillars.\nFour pillars of TensorFlow\nFast and scalable\nXLA Compilation. We are focusing on XLA compilation and aim to make most model training and inference workflows faster on GPU and CPU, building on XLA\u2019s performance wins on TPU. We intend for XLA to become the industry-standard deep learning compiler, and we\u2019ve opened it up to open-source collaboration as part of the OpenXLA initiative.\nDistributed computing. We are investing in DTensor, a new API for large-scale model parallelism. DTensor unlocks the future of ultra-large model training and deployment and allows you to develop your model as if you were training on a single device, even while using multiple clients. DTensor will be unified with the tf.distribute API, allowing for flexible model and data parallelism.\nPerformance optimization. Besides compilation, we are also further investing in algorithmic performance optimization techniques such as mixed-precision and reduced-precision computation, which can deliver considerable speed ups on GPUs and TPUs.\nApplied ML\nNew tools for CV and NLP. We are investing in our ecosystem for applied ML, in particular via the KerasCV and KerasNLP packages which offer modular and composable components for applied CV and NLP use cases, including a large array of state-of-the-art pretrained models.\nProduction grade solutions. We are expanding the TF Model Garden (GitHub) to cover a broad spectrum of ML tasks and domains. The Model Garden provides end-to-end production-grade modeling solutions. It has many reproducible canonical state-of-the-art (SOTA) model implementations for Computer Vision and Natural Language Processing (NLP). It also offers a training codebase to allow you to quickly run machine learning experiments using these models and export to standard TF serving formats.\nDeveloper resources. We are adding more code examples, guides, and documentation for popular and emerging applied ML use cases. We aim to increasingly reduce the barrier to entry of ML and turn it into a tool in the hands of every developer.\nReady to deploy\nEasier exporting. We are making it even easier to export to mobile (Android or iOS), edge (microcontrollers), server backends, or JavaScript. Exporting your model to TFLite and TF.js and optimizing its inference performance will be as easy as a call to `model.export()`.\nC++ API for applications. We are developing a public TF2 C++ API for native server-side inference as part of a C++ application.\nDeploy JAX models. We are making it easier for you to deploy models developed using JAX with TensorFlow Serving, and to mobile and the web with TensorFlow Lite and TensorFlow.js. \nSimplicity\nNumPy API. As the field of ML expanded over the last few years TensorFlow\u2019s API surface also increased, not always in ways that are consistent or simple to understand. We are working actively on consolidating and simplifying these APIs. For example, we will be adopting the NumPy API standard for numerics. \nEasier debugging. A framework isn't just its API surface, it's also its debugging experience. We aim at minimizing the time-to-solution for developing any applied ML system by focusing on better debugging capabilities.\nThe future of TensorFlow will be 100% backwards-compatible\nWe want TensorFlow to serve as a bedrock foundation for the machine learning industry to build upon. We see API stability as our most important feature. As an engineer who relies on TensorFlow as part of their product, as a builder of a TensorFlow ecosystem package, you should be able to upgrade to the latest TensorFlow version and immediately start benefiting from its new features and performance improvements \u2013 without fear that your existing codebase might break. As such, we commit to full backwards compatibility from TensorFlow 2 to the next version \u2013 your TensorFlow 2 code will run as-is. There will be no conversion script to run, no manual changes to apply.\nTimeline\nWe plan to release a preview of the new TensorFlow capabilities in Q2 2023 and will release the production version later in the year. We will publish regular updates on our progress in the meantime. You can follow our progress via the TensorFlow blog, and on the TensorFlow YouTube channel.\nYour feedback is welcome\nWe want to hear from you! For questions or feedback, please reach out via the TensorFlow forum.",
    "link": "https://blog.tensorflow.org/2022/10/building-the-future-of-tensorflow.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhTtFXWao9sbX4P__SCJhLChXRdTkVur0sJTnQp5K0MtVR7U0-l3j5Yrpx41U2YSh4N671c-Wn7dJ68xim8cGAiexDI3IOdEV_vHpMsWdsZxSYU3TUpAzNEIVOOOV3O-wxa4caTUT2VwVTTy-R6GlGji1H4lhewb9WC5nmRCzN8Ofe7yF1NW6ArPI7Q/s1600/Tensorflow-septmber-update-social%20%281%29.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRgwmbW2Tp7Q_Y3GTKSmkfboFNHuUizDaYh1uLrQSCZarfkvs1mK7OodevH9l318Ls8ddEmKNPewlpVhMJzKvtpktP6TeKniEEMAzgRrHq-D-kIEsoQnZyvc7n4pUVsn1RkFF066dnujQZ1htprWST0uSJftVZxQyc2Qm8aijQTMhrtJlj7rrEc6s7/s1600/Tensorflow-septmber-update-header%20%282%29.png"
    ],
    "time": "2023/12/12 00:58:43"
  },
  {
    "title": "Optimizing TF, XLA and JAX for LLM Training on NVIDIA GPUs",
    "content": "Posted by Douglas Yarrington (Google TPgM), James Rubin (Google PM), Neal Vaidya (NVIDIA TME), Jay Rodge (NVIDIA PMM)\nTogether, NVIDIA and Google are delighted to announce new milestones and plans to optimize TensorFlow and JAX for the Ampere and recently announced Hopper GPU architectures by leveraging the power of XLA: a performant, flexible and extensible ML compiler built by Google. We will deepen our ongoing collaboration with dedicated engineering teams focused on delivering improved performance in currently available A100 GPUs. NVIDIA and Google will also jointly support unique features in the recently announced H100 GPU, including the Transformer Engine with support for hardware-accelerated 8-bit floating-point (FP8) data types and the transformer library.\n\nWe are announcing improved performance in TensorFlow, new NVIDIA GPU-specific features in XLA and the first release of JAX for multi-node, multi-GPU training, which will significantly improve large language model (LLM) training. We expect the Hopper architecture to be especially popular for LLMs.\nNVIDIA H100 Tensor Core GPU\nXLA for GPU\nGoogle delivers high performance with LLMs on NVIDIA GPUs because of a notable technology, XLA, which supports all leading ML frameworks, such as TensorFlow, JAX, and PyTorch. Over 90% of Google\u2019s ML compilations - across research and production, happen on XLA. These span the gamut of ML use cases, from ultra-large scale model training at DeepMind and Google Research, to optimized deployments across our products, to edge inferencing at Waymo.\n\nXLA\u2019s deep feature set accelerates large language model performance and is solving most large model challenges seen in the industry today. For example, a feature unique to XLA, SPMD, automates most of the work needed to partition models across multiple cores and devices, making large model training significantly more scalable and performant. XLA can also automatically recognize and select the most optimal hand-written library implementation for your target backend, like cuDNN for CUDA chipsets. Otherwise, XLA can natively generate optimized code for performant execution.\n\nWe\u2019ve been collaborating with NVIDIA on several exciting features and integrations that will further optimize LLMs for GPUs. We recently enabled collectives such as all-reduce to run in parallel to compute. This has resulted in a significant reduction in end to end latency for customers. Furthermore, we enabled support for bfloat16, which has resulted in compute gains of 4.5x over 32 bit floating point while retaining the same dynamic range of values.\n\nOur joint efforts mean that XLA integrates even more deeply with NVIDIA\u2019s AI tools and can better leverage NVIDIA\u2019s suite of AI hardware optimized libraries. In Q1 2023, we will release a XLA-cuDNN Graph API integration, which provides customers with optimized fusion of convolution/matmul operations and multi-headed attention in transformers for improved use of memory and faster GPU kernel execution. As a result, overheads drop significantly and performance improves notably.\nTensorFlow for GPU\nTensorFlow recently released distributed tensors (or DTensors) to enable Tensor storage across devices like NVIDIA GPUs while allowing programs to manipulate them seamlessly. The goal of DTensor is to make parallelizing large-scale TensorFlow models across multiple devices easy, understandable, and fast. DTensors are a drop-in replacement for local TensorFlow tensors and scale well to large clusters. In addition, the DTensor project improves the underlying TensorFlow execution and communication primitives, and they are available for use today!\n\nWe are also collaborating with NVIDIA on several exciting new features in TensorFlow that leverage GPUs, including supporting the new FP8 datatype which should yield a significant improvement in training times for transformer models, when using the Hopper H100 GPU.\nJAX for GPU\nGoogle seeks to empower every developer with purpose-built tools for every step of the ML workflow. That includes TensorFlow for robust, production-ready models and JAX with highly optimized capabilities for cutting-edge research. We are pleased to announce the unique collaboration between NVIDIA and Google engineering teams to enhance TensorFlow and JAX for large deep-learning models, like LLMs. Both frameworks fully embrace NVIDIA A100 GPUs, and will support the recently-announced H100 GPUs in the future.\n\nOne of the key advantages of JAX is the ease of achieving superior hardware utilization with industry-leading FLOPs across the accelerators. Through our collaboration with NVIDIA, we are translating these advantages to GPU using some XLA compiler magic. Specifically, we are leveraging XLA for operator fusion, improving GSPMD for GPU to support generalized data and model parallelism and optimizing for cross-host NVLink.\nFuture Plans\nNVIDIA and Google are pleased with all the progress shared in this post, and are excited to hear from community members about their experience using TensorFlow and JAX, by leveraging the power of XLA for Ampere (A100) and Hopper (H100) GPUs.\n\nCheck out the release notes for more information. To stay up to date, you can read the TensorFlow blog, follow twitter.com/tensorflow, or subscribe to youtube.com/tensorflow. If you\u2019ve built something you\u2019d like to share, please submit it for our Community Spotlight at goo.gle/TFCS. For feedback, please file an issue on GitHub or post to the TensorFlow Forum.\n\nTensorFlow is also available in the NVIDIA GPU Cloud (NGC) as a docker container that contains a validated set of libraries that enable and optimize GPU performance, with JAX NGC container coming soon later this year.\n\n\nThank you!\n\nContributors: Frederic Bastien (NVIDIA), Abhishek Ratna (Google), Sean Lee (NVIDIA), Nathan Luehr (NVIDIA), Ayan Moitra (NVIDIA), Yash Katariya (Google), Peter Hawkins (Google), Skye Wanderman-Milne (Google), David Majnemer (Google), Stephan Herhut (Google), George Karpanov (Google), Mahmoud Soliman (NVIDIA), Yuan Lin (NVIDIA), Vartika Singh (NVIDIA), Vinod Grover (NVIDIA), Pooya Jannaty (NVIDIA), Paresh Kharya (NVIDIA), Santosh Bhavani (NVIDIA)",
    "link": "https://blog.tensorflow.org/2022/09/optimizing-tf-xla-and-jax-for-llm-training-on-nvidia-gpus.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhu7jxiuANJm2mKsE_-8WP2eOKqivo7VW7p_Qv7bhW4UqtRjaKJMx9-VbQUsqrPSI3SSB-GxHw1xb3b3qs9su4TaPgvJ9Y9ki5q3MnTFnB1epDGpnCMjozcWZ-z2kJWz5qc4bnxRZKoYIbBI60hourWcCf3aeYQQJxVsAuzzsoFo6eAkxCrD3u4Tqou/s16000/NVIDIA%20H100.jpeg"
    ],
    "time": "2023/12/11 00:58:43"
  },
  {
    "title": "September Machine Learning Updates",
    "content": "Posted by the TensorFlow team\nOn September 14, at the Google Developers Summit in Shanghai, China, members of Google\u2019s open-source ML teams will be on stage to talk about updates to our growing ecosystem, and we\u2019d love to share them here with you.\nMediaPipe Studio\nWe recognize that creating and productionizing custom on-device ML solutions can be challenging, so we\u2019re reinventing how you develop them by leveraging simple-to-use abstraction APIs and no-code GUIs. We\u2019re excited to give you a sneak peek at MediaPipe Studio, our low-code and no-code solution that gets you from data to modeling to deployment on Android or iOS with native code integration libraries that make it easy to build ML-powered apps.\n\nGeneral Availability of TensorFlow Lite in Google Play Services\nWe recently launched the general availability of TensorFlow Lite in Google Play services. With this, the TensorFlow Lite runtime is automatically managed and updated by Google Play services, meaning you no longer need to ship it as part of your application. Your apps get smaller, and you get regular updates in the background, so your users will always have the latest version. This is nice for you as an app developer, because your user will get updates and bug fixes to the framework automatically, reducing the burden on you to provide them. And TensorFlow Lite in Google Play Services is production ready, already running over 100 billion daily inferences.\nTensor Projects\nAt Google, we are creating a world-class family of ML tools across all hardware and device types. Because we are committed to building tools that are fit for purpose, from cutting-edge research to tried-and-true planet-scale deployments, we are sharing our vision of an open ML ecosystem of the future: Tensor Projects.\n\nTensor Projects is an ecosystem of ML technologies and platforms that bring together Google\u2019s ML tools, and organize efforts across our world-class engineering and research teams. It creates a space and a promise of continued innovation and support to enable researchers, developers, MLOps, and business teams to build responsible and cutting edge ML, from novel model development to scaled production ML in any data center or on any device.\n\nThese tools, like TensorFlow, Keras, JAX, and MediaPipe Studio, will work well independently, with each other, and/or with other industry-leading tools and standards. We want to give you full flexibility and choice to build powerful, performant infrastructure for all of your ML use cases. And it\u2019s just the beginning. Tensor Projects will evolve and grow as ML continues to advance. Watch the summary video here:\n   Updates to Tensorflow.org\nWe have an updated experience on tensorflow.org for new or advanced users to easily find resources. You can quickly identify the right TensorFlow tool for your task, explore pre-built artifacts for faster model creation, find ideas and inspiration, get involved in the community, discover quick start guides for common scenarios and much more.\nPyTorch Foundation\nWe believe in the power of choice for ML developers and continue to invest resources to make it easy to train, deploy and manage models. Our investment intends to bring machine learning to every developer's toolbox and covers a broad spectrum of offerings: from TensorFlow and Keras, which provide free and open source offerings to millions of developers, allowing them to succeed with ML, and to JAX, which empowers researchers across Alphabet.\n\nAdditionally, in the spirit of openness, we support PyTorch developers with Cloud TPU using XLA. To continue to help all developers succeed with Google Cloud, and to better position Google to make meaningful contributions to the community, we\u2019re delighted to announce our role as a founding member of the newly formed PyTorch Foundation. As a member of the board, we will deepen our open source investment to deliver on the Foundation's mission to drive the adoption of AI and ML through open source platforms.\n\nThank you for reading! To stay up to date, you can read the TensorFlow blog, follow twitter.com/tensorflow, or subscribe to youtube.com/tensorflow.",
    "link": "https://blog.tensorflow.org/2022/09/september-machine-learning-updates.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjuxz1I7SepliJzFyFPXOW70EnGZkQP3nOou0UaioYRMaHTCxOcDQKKk_gDteq8xuPeVBmk-Iuqf2B2UolyKQxvOdBXv1skX01lj8aso2qoxo3TbUz1eeatH35brLe4u02qbL9OL3a5hfSzhpAUYHvHwciktzBaFsWYnTwGtZVDzVIyUwBlzkFmgdhk/s1600/Tensorflow-septmber-update-social.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmXp_vCLj44AbG_Qpkdz0-9KiYiLOB6SuqXNxNRo1GanyP4dkzdGzv3yjVlA8J7ctpQBW4GqwO1s48fiolwzM49-3nK1UHKJBbPhKfLHYNuNn15aGubUifmvANVDXjCkwRFgQOBibpGNF1Rf5W59fQe1Di5hlb5tFqLDzaSmPWWXmvIib4s_6zKPN6/s1600/Tensorflow-septmber-update-header.png"
    ],
    "time": "2023/12/11 00:58:43"
  },
  {
    "title": "Announcing TensorFlow Official Build Collaborators",
    "content": "Posted by Rostam Dinyari, Nitin Srinivasan, Douglas Yarrington and Rishika Sinha of the TensorFlow team\nStarting with TensorFlow 2.10, we are excited to announce our collaboration with Intel, AWS, ARM, and Linaro to develop official TensorFlow builds. This means that when you pip install TensorFlow on Windows Native and Linux Aarch64 hosts, you will receive a build of TensorFlow that has been reviewed and vetted by these platform experts. This happens transparently, and there are no changes to your workflow . We've updated the pip install scripts so it's automatic for you.\nOfficial builds are TensorFlow releases that follow rigorous functional and performance testing standards Google engineers and our collaborators publish with each release, which we align with our published support expectations under the SIG Build forum. Collaborators monitor the builds daily and publish artifacts to the community in coordination with the overall TensorFlow release schedule.\n\nFor the majority of use cases, there will be no changes to the behavior of pip install or pip uninstall TensorFlow. However, for Windows Native and Linux Aarch64 based systems an additional pip uninstall step may be needed. You can find details about install, uninstall and other best practices on tensorflow.org/install/pip.\n\nOver time, we expect the number of collaborators to expand but for now we want to share with you the progress we have made together to release increasingly performant and robust builds for these important platforms. You can learn more about each of the collaborations below.\nIntel Collaboration\nWe are pleased to share that Intel has joined the 3P Official Build program to take ownership over Windows Native CPU builds. This will include responsibility for managing both nightly and final production releases. We and Intel do not expect this to disrupt end user experiences; users simply install TensorFlow as usual and the Intel produced Python binary artifacts (wheel files) will be correctly installed.\nAWS, ARM and Linaro Collaboration\nWe are especially pleased to announce the availability of official builds for ARM Aarch64, specifically tuned for AWS Graviton instances. Together, the experts at Linaro have supported Google, AWS and ARM to ensure a highly performant version of TensorFlow is available on the emerging class of Aarch64 devices.\nNext steps\nThese changes should be transparent for most users. You can learn more at tensorflow.org/install.",
    "link": "https://blog.tensorflow.org/2022/09/announcing-tensorflow-official-build-collaborators.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgwYbl0yCfhpq9TY7E-8Pnn1c3a8dm_K5z7JPm56Hz3RpzQ2spN4rIkiTQiZzSEF8ZgS6wjtpplVuBmyHbk7CCXi843sO7jl5DpxIaT4Jt5pSTLsAVWB7LFaPDjCYjuUDEJqC2uInko31ft2ap-uKt5PjQ0LM6TTrX7-phltjU4GYxpwqan-4rws9t0/s1600/tensorflow-content-moderation-using-machine-learning-a-dual-approach-01.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgwYbl0yCfhpq9TY7E-8Pnn1c3a8dm_K5z7JPm56Hz3RpzQ2spN4rIkiTQiZzSEF8ZgS6wjtpplVuBmyHbk7CCXi843sO7jl5DpxIaT4Jt5pSTLsAVWB7LFaPDjCYjuUDEJqC2uInko31ft2ap-uKt5PjQ0LM6TTrX7-phltjU4GYxpwqan-4rws9t0/w681-h203/tensorflow-content-moderation-using-machine-learning-a-dual-approach-01.png"
    ],
    "time": "2023/12/11 00:58:43"
  },
  {
    "title": "What\u2019s new in TensorFlow 2.10?",
    "content": "Posted by the TensorFlow Team\nTensorFlow 2.10 has been released! Highlights of this release include user-friendly features in Keras to help you develop transformers, deterministic and stateless initializers, updates to the optimizers API, and new tools to help you load audio data. We\u2019ve also made performance enhancements with oneDNN, expanded GPU support on Windows, and more. This release also marks TensorFlow Decision Forests 1.0! Read on to learn more.\nKeras\nExpanded, unified mask support for Keras attention layers\nStarting from TensorFlow 2.10, mask handling for Keras attention layers, such as tf.keras.layers.Attention, tf.keras.layers.AdditiveAttention, and tf.keras.layers.MultiHeadAttention have been expanded and unified. In particular, we've added two features:\nCausal attention: All three layers now support a use_causal_mask argument to call (Attention and AdditiveAttention used to take a causal argument to __init__).\nImplicit masking: Keras Attention, AdditiveAttention, and MultiHeadAttention layers now support implicit masking  (set mask_zero=True in tf.keras.layers.Embedding).\nCombined, this simplifies the implementation of any Transformer-style model since getting the masking right is often a tricky part.\nA basic Transformer self-attention block can now be written as:\nimport tensorflow as tf\n\nembedding = tf.keras.layers.Embedding(\n    input_dim=10,\n    output_dim=3,\n    mask_zero=True) # Infer a correct padding mask.\n\n# Instantiate a Keras multi-head attention (MHA) layer,\n# a layer normalization layer, and an `Add` layer object.\nmha = tf.keras.layers.MultiHeadAttention(key_dim=4, num_heads=1)\nlayernorm = tf.keras.layers.LayerNormalization()\nadd = tf.keras.layers.Add()\n\n# Test input.\nx = tf.constant([[1, 2, 3, 4, 5, 0, 0, 0, 0],\n                 [1, 2, 1, 0, 0, 0, 0, 0, 0]])\n# The embedding layer sets the mask.\nx = embedding(x)\n\n# The MHA layer uses and propagates the mask.\na = mha(query=x, key=x, value=x, use_causal_mask=True)\nx = add([x, a]) # The `Add` layer propagates the mask.\nx = layernorm(x)\n\n# The mask made it through all layers.\nprint(x._keras_mask)\nAnd here's the output: \n> tf.Tensor(\n> [[ True  True  True  True  True False False False False]\n>  [ True  True  True False False False False False False]], shape=(2, > 9), dtype=bool)\nTry out the new Keras Optimizers API\nIn the previous release, Tensorflow 2.9, we published a new version of the Keras Optimizer API, in tf.keras.optimizers.experimental, which will replace the current tf.keras.optimizers namespace in TensorFlow 2.11. To prepare for the upcoming formal switch of the optimizer namespace to the new API, we've also exported all of the current Keras optimizers under tf.keras.optimizers.legacy in TensorFlow 2.10.\nMost users won't be affected by this change, but please check the API doc to see if any API used in your workflow has changed. If you decide to keep using the old optimizer, please explicitly change your optimizer to corresponding tf.keras.optimizers.legacy.Optimizer.\nYou can also find more details about new Keras Optimizers in this article.\nDeterministic and Stateless Keras initializers\nIn TensorFlow 2.10, we've made Keras initializers (the tf.keras.initializers API) stateless and deterministic, built on top of stateless TF random ops. Starting in TensorFlow 2.10, both seeded and unseeded Keras initializers will always generate the same values every time they are called (for a given variable shape). The stateless initializer enables Keras to support new features such as multi-client model training with DTensor.\ninit = tf.keras.initializers.RandomNormal()\na = init((3, 2))\nb = init((3, 2))\n# a == b\n\ninit_2 = tf.keras.initializers.RandomNormal(seed=1)\nc = init_2((3, 2))\nd = init_2((3, 2))\n# c == d\n# a != c\n\ninit_3 = tf.keras.initializers.RandomNormal(seed=1)\ne = init_3((3, 2))\n# e == c\n\ninit_4 = tf.keras.initializers.RandomNormal()\nf = init_4((3, 2))\n# f != a\nFor unseeded initializers (seed=None), a random seed will be created and assigned at initializer creation (different initializer instances get different seeds). An unseeded initializer will raise a warning if it is reused (called) multiple times. This is because it would produce the same values each time, which may not be intended.\nBackupAndRestore checkpoints with step level granularity\nIn the previous release, Tensorflow 2.9, the tf.keras.callbacks.BackupAndRestore Keras callback would backup the model and training state at epoch boundaries. In Tensorflow 2.10, the callback can also backup the model every N training steps. However, keep in mind that when BackupAndRestore is used with tf.distribute.MultiWorkerMirroredStrategy, the distributed dataset iterator state will be reinitialized and won't be restored when restoring the model. More information and code examples can be found in the migrate the fault tolerance mechanism guide.\nEasily generate an audio classification dataset from a directory of audio files\nYou can now use a new utility, tf.keras.utils.audio_dataset_from_directory, to easily generate audio classification datasets from directories of .wav files. Just sort your audio files into one different directory per file class, and a single line of code will get you a labeled tf.data.Dataset you can pass to a Keras model. You can find an example here.\nThe EinsumDense layer is no longer experimental\nThe einsum function is the swiss army knife of linear algebra. It can efficiently and explicitly describe a wide variety of operations. The tf.keras.layers.EinsumDense layer brings some of that power to Keras.\nOperations like einsum, einops.rearrange, and the EinsumDense layer operate based on a string \"equation\" that describes the axes of the inputs and outputs. For EinsumDense the equation lists the axes of the input argument, the axes of the weights, and the axes of the output. A basic Dense layer can be written as: \ndense = keras.layers.Dense(units=10, activation='relu')\ndense = keras.layers.EinsumDense('...i, ij -> \u2026j', output_shape=(10,), activation='relu')\nNotes:\n...i - This only works on the last axis of the input, that axis is called i.\nij - The weights are a matrix with shape (ij).\n...j - The result sums out the i axis and leaves j.\nFor example, here is a stack of 5 Dense layers with 10 units each:\ndense = keras.layers.EinsumDense('...i, nij -> \u2026nj', output_shape=(5,10))\nHere is a stack of Dense layers, where each one operates on a different input vector:\ndense = keras.layers.EinsumDense('...ni, nij -> \u2026nj', output_shape=(5,10))\nHere is a stack of Dense layers where each one operates on each input vector independently:\ndense = keras.layers.EinsumDense('...ni, mij -> \u2026nmj', output_shape=(None, 5,10))\nPerformance and collaborations\nImproved aarch64 CPU performance: ACL/oneDNN integration\nWe have worked with Arm, AWS, and Linaro to integrate Compute Library for the Arm\u00ae Architecture (ACL) with TensorFlow through oneDNN to accelerate performance on aarch64 CPUs. Starting with TensorFlow 2.10, you can try these experimental optimizations by setting the environment variable TF_ENABLE_ONEDNN_OPTS=1 before running your TensorFlow program.\nThere may be slightly different numerical results due to different computation and floating-point round-off approaches. If this causes issues for you, turn the optimizations off by setting TF_ENABLE_ONEDNN_OPTS=0 before running your program.\nTo verify that the optimizations are on, look for a message beginning with \u201coneDNN custom operations are on\u201d in your program log. We welcome feedback on GitHub and the TensorFlow Forum.\nExpanded GPU support on Windows\nTensorFlow can now leverage a wider range of GPUs on Windows through the TensorFlow-DirectML plug-in. To enable model training on DirectX 12-capable GPUs from vendors such as AMD, Intel, NVIDIA, and Qualcomm, install the plug-in alongside standard TensorFlow CPU packages on native Windows or WSL2. The preview package currently supports a limited number of basic machine learning models, with a goal to increase model coverage in the future. You can view the open-source code and leave feedback at the TensorFlow-DirectML GitHub repository.\nNew features in tf.data\nCreate tf.data Dataset from lists of elements\nTensorflow 2.10 introduces a convenient new experimental API tf.data.experimental.from_list which creates a tf.data.Dataset comprising the given list of elements. The returned dataset will produce the items in the list one by one. The functionality is identical to tf.data.Dataset.from_tensor_slices when elements are scalars, but different when elements have structure.\nConsider the following example:\ndataset = tf.data.experimental.from_list([(1, 'a'), (2, 'b'), (3, 'c')])\nlist(dataset.as_numpy_iterator())\n[(1, 'a'), (2, 'b'), (3, 'c')]\nIn contrast, to get the same output with `from_tensor_slices`, the data needs to be reorganized:\ndataset = tf.data.Dataset.from_tensor_slices(([1, 2, 3], ['a', 'b', 'c']))\nlist(dataset.as_numpy_iterator())\n[(1, 'a'), (2, 'b'), (3, 'c')]\nUnlike the from_tensor_slices method, from_list supports non-rectangular input (achieving the same with from_tensor_slices requires the use of ragged tensors).\nSharing tf.data service with concurrent trainers\nIf you run multiple trainers concurrently using the same training data, it could save resources to cache the data in one tf.data service cluster and share the cluster with the trainers. For example, if you use Vizier to tune hyperparameters, the Vizier jobs can run concurrently and share one tf.data service cluster.\nTo enable this feature, each trainer needs to generate a unique trainer ID, and you pass the trainer ID to tf.data.experimental.service.distribute. Once a job has consumed the data, the data remains in the cache and is re-used by jobs with different trainer_ids. Requests with the same trainer_id do not re-use data. For example:\ndataset = expensive_computation()\ndataset = dataset.apply(tf.data.experimental.service.distribute(\nprocessing_mode=tf.data.experimental.service.ShardingPolicy.OFF,\nservice=FLAGS.tf_data_service_address,\njob_name=\"job\",\ncross_trainer_cache=data_service_ops.CrossTrainerCache(\ntrainer_id=trainer_id())))\n tf.data service uses a sliding-window cache to store shared data. When one trainer consumes data, the data remains in the cache. When other trainers need data, they can get data from the cache instead of repeating the expensive computation. The cache has a bounded size, so some workers may not read the full dataset. To ensure all the trainers get sufficient training data, we require the input dataset to be infinite. This can be achieved, for example, by repeating the dataset and performing random augmentation on the training instances.\nTensorFlow Decision Forests 1.0\nIn conjunction with the release of Tensorflow 2.10, Tensorflow Decision Forests (TF-DF) reaches version 1.0. With this milestone we want to communicate more broadly that Tensorflow Decision Forests has become a more stable and mature library. We\u2019ve improved our documentation and established more comprehensive testing to make sure that TF-DF is ready for professional environments.\nThe new release of TF-DF also offers a first look at the Javascript and Go APIs for inference of TF-DF models. While these APIs are still in beta, we are actively looking for feedback for them. TF-DF 1.0 improves performance of oblique splits. Oblique splits allow decision trees to express more complex patterns by conditioning on multiple features at the same time \u2013 learn more in our Decision Forests class on developers.google.com. Benchmarks and real-world observations show that oblique splits outperform classical axis-aligned splits on the majority of datasets. Finally, the new release includes our latest bug fixes.\nNext steps\nCheck out the release notes for more information. To stay up to date, you can read the TensorFlow blog, follow twitter.com/tensorflow, or subscribe to youtube.com/tensorflow. If you\u2019ve built something you\u2019d like to share, please submit it for our Community Spotlight at goo.gle/TFCS. For feedback, please file an issue on GitHub or post to the TensorFlow Forum. Thank you!",
    "link": "https://blog.tensorflow.org/2022/09/whats-new-in-tensorflow-210.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjTTtwrXCFQjLdf78AdU4J7sn3IQFwfTogF8puW-H3Qh7INjONpdJDVJTELyUMCr2d43T2iNrPeaJS2iyiVlmjIkNGnChvjXKg2Z57iRMN9LbfIgudDYbGTDEs-GVw-aejUijXUGQ5LD8IDEkJQCuIPcFqkJh5UGZGOR0N3VgCBsIC0Z9fgSSf3aKMb/s1600/tensorflow-a-progress-update-header.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjTTtwrXCFQjLdf78AdU4J7sn3IQFwfTogF8puW-H3Qh7INjONpdJDVJTELyUMCr2d43T2iNrPeaJS2iyiVlmjIkNGnChvjXKg2Z57iRMN9LbfIgudDYbGTDEs-GVw-aejUijXUGQ5LD8IDEkJQCuIPcFqkJh5UGZGOR0N3VgCBsIC0Z9fgSSf3aKMb/w640-h190/tensorflow-a-progress-update-header.png",
      "https://lh4.googleusercontent.com/MBb3xZmHn8lNlvOXeRMTAlhxuGXavE0giJJub4RVVGqjE4guBVauJ063TLziq-3shQWyCJNYk1lNDCdu9fRq2MN8f4m-8IRUXdhzyEMMer0takDzOtQMGvGoOaU2ysc16uVqZCIZyqjLBC6MC9Km1Bk3MvPAoQiZl9_dQVr9MQpGfqXdiFybj51nN_fvkXJSLyBLZ24DeSx1s2pz1uKrqlEfXsdrEWslMw"
    ],
    "time": "2023/12/11 00:58:43"
  },
  {
    "title": "Bringing Machine Learning to every developer\u2019s toolbox",
    "content": "Posted by Laurence Moroney and Josh Gordon for the TensorFlow team\nWith the release of the recent Stack Overflow Developer Survey, we\u2019re delighted to see the growth of TensorFlow as the most-used ML tool, being adopted by 3 million software developers to enhance their products and solutions using Machine Learning. And we\u2019re only getting started \u2013 the survey showed that TensorFlow was the most wanted framework amongst developers, with an estimated 4 million developers wanting to adopt it in the near future.\nTensorFlow is now being downloaded over 18M times per month and has amassed 166k stars on GitHub \u2013 more than any other ML framework. Within Google, it powers virtually all AI production workflows, including Search, Ads, YouTube, GMail, Maps, Play, Photos, and many more. It also powers production systems at many of the largest companies in the world \u2013 Apple, Netflix, Stripe, Tencent, Uber, Roche, LinkedIn, Twitter, Baidu, Orange, LVMH, and countless others. And every month, over 3,000 new scientific publications that mention TensorFlow or Keras are being indexed by Google Scholar, including important applied science like the CANDLE research into understanding cancer.\nWe continue to grow the family of products and open source services that make up the Google AI/ML ecosystem. In recent years, we learned that a single universal framework could not work for all scenarios \u2013 in particular, the needs of production and cutting edge research are often in conflict. So we created JAX, a minimalistic API for distributed numerical computing to power the next era of scientific computing research. JAX is excellent for pushing new frontiers: reaching new scales of parallelism, advancing new algorithms and architectures, and developing new compilers and systems. The adoption of JAX by researchers has been exciting, and advances such as AlphaFold and Imagen underscore this.\nIn this new multi-framework world, TensorFlow is our answer to the needs of applied ML developers \u2013 engineers who need to build and deploy reliable, stable, performant ML systems, at any scale, and for any platform. Our vision is to create a cohesive ecosystem where researchers and engineers can leverage components that work together regardless of the framework where they originated. We've already made strides towards JAX and TensorFlow interoperability, in particular via jax2tf. Researchers who develop JAX models will be able to bring them to production via the tools of the TensorFlow platform.\nGoing forward, we intend to continue to develop TensorFlow as the best-in-class platform for applied ML, side-by-side with JAX to push the boundaries of ML research. We will continue to invest in both ML frameworks to drive forward research and applications for our millions of users.\nThere\u2019s lots of great stuff baking that we can\u2019t wait to share with you, so watch this blog for more details!\nPS: Interested in working on any of our AI and ML frameworks? We're hiring.",
    "link": "https://blog.tensorflow.org/2022/06/%20bringing-machine-learning-to-every-developers-toolbox.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglKMff39bQMNWFnXhr8C3F8ICwnNyKup_rQBU6X5v9fAtlyfFQ8GdltFJ1yvkx-alW7fRBwUrQIBPWNYQwRnQt6DmA-GHJAIDIPmLrqUZuMtZHGqiTklYbKPCzFkYGKUi0vqJDSsZH4xYqL2g4uPYsCZhk4JQuepVbMV60Q93VeHiP38ELbOEKgAfh/s16000/TF%20Logo%20(1080%20%C3%97%20400%20px).png"
    ],
    "time": "2023/12/11 00:58:43"
  },
  {
    "title": "Adding Quantization-aware Training and Pruning to the TensorFlow Model Garden",
    "content": "Posted by Jaehong Kim, Rino Lee, and Fan Yang, Software Engineers\nThe TensorFlow model optimization toolkit (TFMOT) provides modern optimization techniques such as quantization aware training (QAT) and pruning. Since the introduction of TFMOT, we have been continuously improving its usability and coverage. Today, we are excited to announce that we are extending the TFMOT model coverage to popular computer vision models in the TensorFlow Model Garden.\nTo do so, we added 8-bit QAT API support for subclassed models and custom layers, and Pruning API support. You can use these new features in the model garden, and when developing your own models as well. With this, we have showcased applying QAT and pruning to several canonical computer vision models, while accelerating the model development cycle significantly.\nIn this article, we will describe the technical challenges we encountered to apply QAT and pruning to the subclass models and custom layers. And show the optimized results to show the benefits from optimization techniques.\nNew support for Model Garden models\nQuantization\nWe have resolved a few technical challenges to support subclassed models and simplified the process of applying QAT API. All the new changes have already been taken care of by TFMOT and Model Garden to save users from knowing all technical details. The final user-facing API to apply QAT on a computer vision model in Model Garden is quite straightforward. By applying a few configuration changes, you can enable QAT to finetune a pre-trained model and obtain a deployable on-device model in just a few hours. There is minimal to no code change at all. Here we will talk about those challenges and how we addressed them.\nThe previous QAT API assumed that the model only contained built-in layers. To support nested functional models, we apply the QAT method to different parts of the model individually. For example, to apply QAT to an image classification model (M) in the Model Garden that consists of two sub modules: the backbone network (B) and the classification head (C). Here B is a nested model within M, and C is a layer. Both B and C only contain built-in layers. Instead of directly quantizing the entire classification model M, we quantize the backbone B and classification head C individually. First, we apply QAT to backbone B only. Then we connect the quantized backbone B to its corresponding classification head C to form a new classification model, and annotate C to be quantized. Finally, we quantize the entire new model, which effectively applies QAT to the annotated classification head C.\nWhen the backbone network also contains custom layers rather than built-in layers, we add quantized versions of those custom layers first. For example, if the backbone network (B) or the classification head (C) of the classification model (M) also contain a custom layer called MyLayer, we create its QAT counterpart called MyLayerQuantized and wrap any built-in layers within it by a quantize wrapper API. We do this recursively if there are any nested custom layers, until all built-in layers are properly wrapped.\nThe remaining part after applying quantize is loading the weights from the original model because the QAT-applied model contains more parameters due to additional quantization parameters. Our current solution is variable name filtering. We have added a logic to load the weights from the original model to filtered weight from the QAT-applied model to support fine-tuning from pre-trained models.\nPruning\nAlong with QAT, we provide two Model garden models with pruning, which is another in-training model optimization technique of MOT. Pruning sparsifies (forces a fixed portion of elements to zero) the given model\u2019s weights during training for computation and storage efficiency.\nUsers can easily set pruning parameters in Model Garden configs. For better pruned model quality, starting pruning from a pre-trained dense model and careful tuning pruning schedule over training steps are well-known techniques. Both are available in Model Garden Pruning configs.\nThis work also provides an example of nested functional layer support in pruning. The way we used here using get_prunable_weight() is also applicable to any other Keras models with custom layers.\nWith the provided two Model Garden Pruning configs, users can quickly demonstrate pruning to ResNet50 and MobileNetV2 models for image classification. Understanding the practical usage of Pruning API and the pruning process by monitoring tensorboard are also another takeaways of this work.\nExamples and Results\nWe support two tasks, image classification and semantic segmentation. Specifically, for QAT in image classification, we support the common MobileNet family, including MobileNetV2, MobileNetV3 (large), Multi-Hardware MobileNet (AVG), and ResNet (through quantization on common building blocks such as InvertedBottleneckBlockQuantized and BottleneckBlockQuantized). For QAT in semantic segmentation, we support MobileNetV2 backbone with DeepLab V3/V3+. For Pruning in image classification we support MobileNetV2 and ResNet. Please refer to the documentations of QAT and pruning for more details.\nCreate QAT Models using Model Garden\nUsing QAT with Model Garden is simple and straightforward. First, we train a floating point model following the standard process of training models using Model Garden. After training converges, we take the best checkpoint as our starting point to apply QAT, analogous to a finetuning stage. Soon, we will obtain a model that is more quantization friendly. Such model then can be converted to a TFLite model for on-device deployment.\nFor image classification, we evaluate the top-1 accuracy on the ImageNet validation set. As shown in Table 1, QAT model consistently outperforms PTQ model by a large margin, which achieves comparable latency. Notably, on models where PTQ fails to produce reasonable results (MobileNetV3), QAT is still capable of generating a strong quantized model with negligible accuracy drop.\nTable 1. Accuracy and latency comparison of supported models for ImageNet classification. Latency is measured on a Samsung Galaxy S21 using 1-thread CPU. FP32 refers to the unquantized floating point TFLite model. PTQ INT8 refers to full integer post-training quantization. QAT INT8 refers to the quantized QAT model.\nmodel\nreso-\nlution\n\nTFLite Model\nTop-1 accuracy\nTop-1 accuracy (FP32)\nTop-1 accuracy (PTQ INT8)\nTop-1 accuracy (QAT INT8)\nLatency (FP32, ms/img)\nLatency (PTQ\nINT8, ms/img)\nLatency (QAT INT8, ms/img)\nResNet50\n224x224\n76.7\n76.7\n76.4\n77.2\n184.01\n48.73\n64.49\nMobileNet V2\n224x224\n72.8\n72.8\n72.4\n72.8\n16.74\n6.85\n6.84\nMobileNet V3 Large\n224x224\n75.1\n75.1\n34.5*\n74.4\n13.32\n6.43\n6.85\nMobileNet Multi-HW AVG\n224x224\n75.3\n75.2\n73.5\n75.1\n20.97\n7.73\n7.73\n* PTQ fails to quantize MobileNet V3 properly due to hard-swish activation, thus leading to low accuracy.\nWe have a similar observation on semantic segmentation: PTQ introduces 1.3 mIoU drop, compared to FP32 model, while QAT model minimizes the drop to just 0.7 and maintains comparable latency. On average, we expect QAT will only introduce 0.5 top-1 accuracy drop for image classification and less than 1 mIoU drop for semantic segmentation.\nTable 2. Accuracy and latency comparison of a MobileNet v2 + DeepLab v3 on Pascal VOC segmentation. Latency is measured on a Samsung Galaxy S21 using 1-thread CPU. FP32 refers to the unquantized floating point TFLite model. PTQ INT8 refers to full integer post-training quantization. QAT INT8 refers to the quantized QAT model.\nmodel\nreso-\nlution\n\nTFLite Model\nmIoU\nmIoU (FP32)\nmIoU (PTQ\nINT8)\nmIoU (QAT INT8)\nLatency (FP32, ms/img)\nLatency (PTQ\nINT8, ms/img)\nLatency (QAT INT8, ms/img)\nMobileNet v2 + DeepLab v3\n512x512\n75.27\n75.30\n73.95\n74.68\n136.60\n60.94\n55.53\nPruning Models in Model Garden\nWe support ResNet50 and MobileNet V2 for image classification. Pretrained dense models for each task are generated using the Model Garden training configs. The pruned model can be converted to the TFLite model. By simply setting a flag for sparsity in TFLite conversion, one can get a benefit of model size reduction through sparse data format.\nFor image classification, we again evaluate the top-1 accuracy on the ImageNet validation set, as well as the size of converted TFLite models. As sparsity level increases, the model size becomes more compact but accuracy degrades. The accuracy impact in high sparsity is more severe in parameter-efficient models like MobileNetV2.\nTable 3. Accuracy and model size comparison of ResNet-50 and MobileNet v2 for ImageNet classification. Model size is measured by disk usage of saved TFLite models. Dense refers to the unpruned TFLite model, and 50% sparsity refers to the TFLite model with all prunable layers\u2019 weights randomly pruned 50% of their elements.\nModel\nResolution\nTop-1 Accuracy (Dense)\nTop-1 Accuracy (50% sparsity)\nTop-1 Accuracy (80% sparsity)\nTFLite Model size (Dense)\nTFLite Model size (Mb, 50% sparsity)\nTFLite Model size (Mb, 80% sparsity)\nMobileNet V2\n224x224\n72.768%\n71.334%\n61.378%\n13.36 Mb\n9.74 Mb\n4.00 Mb\nResNet50\n224x224\n76.704%\n76.61%\n75.508%\n97.44 Mb\n70.34 Mb\n28.35 Mb\nConclusions\nWe have presented an extension to TFMOT that offers QAT and pruning support for computer vision models in Model Garden. We highlight the ease of use and outstanding trade-offs about maintaining accuracy while keeping low latency or small model size.\nWhile we believe this is a simple and user-friendly solution to enable QAT and pruning, we know this is just the beginning of streamlined works to provide even better usability.\nCurrently, supported tasks are limited to image classification and semantic segmentation. We will continue to add more support to other tasks, such as object detection and instance segmentation. We will also add more models, such as transformer based models, and improve the usability of TFMOT and Model Garden\u2019s API. Thanks for your interest in this work.\nAcknowledgements\nWe would like to thank everyone who contributed to this work, including Model Garden, Model Optimization, and our collaborators from Research. Special thanks to David Rim (emeritus), Ethan Kim (emeritus) from the Model Optimization team; Abdullah Rashwan, Xianzhi Du, Yeqing Li, Jaeyoun Kim, Jing Li from the Model Garden team; Yuqi Li from the on-device ML team.",
    "link": "https://blog.tensorflow.org/2022/06/Adding-Quantization-aware-Training-and-Pruning-to-the-TensorFlow-Model-Garden.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiiUG8GI1S4QmJU2L4Ei5QG_ByJZD2eZsU86h7yfmsjoRqpXrekX6lDsUHsx5_JiilOLk8RgOBRSDuabwUgFmzx5VXRelmk1HWurMgjTu_hSorImB71bdN-wbMIebYjdC_YSDl7CAKeKoWQIq5E9Km-a7dbbjZ-OgFCFOs6_gF_QA_RFQ-l8w_53JPu/s1600/image1.png"
    ],
    "time": "2023/12/11 00:58:43"
  },
  {
    "title": "New documentation on tensorflow.org",
    "content": "Posted by the TensorFlow team\nAs Google I/O took place, we published a lot of exciting new docs on tensorflow.org, including updates to model parallelism and model remediation, TensorFlow Lite, and the TensorFlow Model Garden. Let's take a look at what new things you can learn about!\nCounterfactual Logit Pairing\nThe Responsible AI team added a new model remediation technique as part of their Model Remediation library. The TensorFlow Model Remediation library provides training-time techniques to intervene on the model such as changing the model itself by introducing or altering model objectives. Originally, model remediation launched with its first technique, MinDiff, which minimizes the difference in performance between two slices of data.\nNew at I/O is Counterfactual Logit Pairing (CLP). This is a technique that seeks to ensure that a model\u2019s prediction doesn\u2019t change when a sensitive attribute referenced in an example is either removed or replaced. For example, in a toxicity classifier, examples such as \"I am a man\" and \"I am a lesbian\" should be equal and not classified as toxic.\nCheck out the basic tutorial, the Keras tutorial, and the API reference.\nModel parallelism: DTensor\nDTensor provides a global programming model that allows developers to operate on tensors globally while managing distribution across devices. DTensor distributes the program and tensors according to the sharding directives through a procedure called Single program, multiple data (SPMD) expansion.\nBy decoupling the overall application from sharding directives, DTensor enables running the same application on a single device, multiple devices, or even multiple clients, while preserving its global semantics. If you remember Mesh TensorFlow from TF1, DTensor can address the same issue that Mesh addressed: training models that may be larger than a single core.\nWith TensorFlow 2.9, we made DTensor, that had been in nightly builds, visible on tensorflow.org. Although DTensor is experimental, you're welcome to try it out. Check out the DTensor Guide, the DTensor Keras Tutorial, and the API reference.\nNew in TensorFlow Lite\nWe made some big changes to the TensorFlow Lite site, including to the getting started docs.\nDeveloper Journeys\nFirst off, we now organize the developer journeys by platform (Android, iOS, and other edge devices) to make it easier to get started with your platform. Android gained a new learning roadmap and quickstart. We also earlier added a guide to the new beta for TensorFlow Lite in Google Play services. These quickstarts include examples in both Kotlin and Java, and upgrade our example code to CameraX, as recommended by our colleagues in Android developer relations!\nIf you want to immediately run an Android sample, one can now be imported directly from Android studio. When starting a new project, choose: New Project > Import Sample... and look for Artificial Intelligence > TensorFlow Lite in Play Services image classification example application. This is the sample that can help you find your mug...or other objects:\nModel Maker\nThe TensorFlow Lite Model Maker library simplifies the process of training a TensorFlow Lite model using custom datasets. It uses transfer learning to reduce the amount of training data required and reduce training time, and comes pre-built with seven common tasks including image classification, object detection, and text search.\nWe added a new tutorial for text search. This type of model lets you take a text query and search for the most related entries in a text dataset, such as a database of web pages. On mobile, you might use this for auto reply or semantic document search.\nWe also published the full Python library reference.\nTF Lite model page\nFinding the right model for your use case can sometimes be confusing. We've written more guidance on how to choose the right model for your task, and what to consider to make that decision.You can also find links to models for common use cases.\nModel Garden: State of the art models ready to go\nThe TensorFlow Model Garden provides implementations of many state-of-the-art machine learning (ML) models for vision and natural language processing (NLP), as well as workflow tools to let you quickly configure and run those models on standard datasets. The Model Garden covers both vision and text tasks, and a flexible training loop library called Orbit. Models come with pre-built configs to train to state-of-the-art, as well as many useful specialized ops.\nWe're just getting started documenting all the great things you can do with the Model Garden. Your first stops should be the overview, lists of available models, and the image classification tutorial.\nOther exciting things!\nDon't miss the crown-of-thorns starfish detector! Find your own COTS on real images from the Great Barrier reef. See the video, read the blog post, and try out the model in Colab yourself.\nAlso, there is a new tutorial on TensorFlow compression, which does lossy compression using neural networks. This example uses something like an autoencoder to compress and decompress MNIST.\nAnd, of course, don't miss all the great I/O talks you can watch on YouTube. Thank you!",
    "link": "https://blog.tensorflow.org/2022/06/new-documentation-on-tensorfloworg.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgAojM5Hs8D3sh_arXYrTDG6wOCUAJeF3atUeHVLyoHRh5GTBxaSimWiMmRc2-ot6rfY5R7aE-_axl51X9CyCqQDst4FyW55HlZ7MR6SG3qjpET9KaZdOz0hhii5li4l1CtgeoSQ7uGrzXVAJ98ZZ7uBt-PZ71fzCKI3FNKgUuE7iR1-fk8ohnZnzbI/s1600/output_1iFcAD0WF78p_3.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEitL9xenSIzgEVUPOUhPMlczODfKKR08sF-icqlPfB4QmCD5dMaP4aQifb18ZDU5E2PtkuU6yonEbTXVIurMmGCWIJjyw0Upz_UD6rKeBB0tygva8DoOVCKuOSUAnImPbpoz8r114ew1Ejhuk-mFfDZNSZXkbfMWuMF3rgr-qiNIjjEdM2kcUy0Cho6/s1600/app_gif.gif",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgAojM5Hs8D3sh_arXYrTDG6wOCUAJeF3atUeHVLyoHRh5GTBxaSimWiMmRc2-ot6rfY5R7aE-_axl51X9CyCqQDst4FyW55HlZ7MR6SG3qjpET9KaZdOz0hhii5li4l1CtgeoSQ7uGrzXVAJ98ZZ7uBt-PZ71fzCKI3FNKgUuE7iR1-fk8ohnZnzbI/s1600/output_1iFcAD0WF78p_3.png"
    ],
    "time": "2023/12/11 00:58:43"
  },
  {
    "title": "What's new in TensorFlow 2.9?",
    "content": "Posted by Goldie Gadde and Douglas Yarrington for the TensorFlow team\nTensorFlow 2.9 has been released! Highlights include performance improvements with oneDNN, and the release of DTensor, a new API for model distribution that can be used to seamlessly move from data parallelism to model parallelism\nWe\u2019ve also made improvements to the core library, including Eigen and tf.function unification, deterministic behavior, and new support for Windows' WSL2. Finally, we\u2019re releasing new experimental APIs for tf.function retracing and Keras Optimizers. Let's take a look at these new and improved features.\nImproved CPU performance: oneDNN by default\nWe have worked with Intel to integrate the oneDNN performance library with TensorFlow to achieve top performance on Intel CPUs. Since TensorFlow 2.5, TensorFlow has had experimental support for oneDNN, which could provide up to a 4x performance improvement. In TensorFlow 2.9, we are turning on oneDNN optimizations by default on Linux x86 packages and for CPUs with neural-network-focused hardware features such as AVX512_VNNI, AVX512_BF16, AMX, and others, which are found on Intel Cascade Lake and newer CPUs.\nUsers running TensorFlow with oneDNN optimizations enabled might observe slightly different numerical results from when the optimizations are off. This is because floating-point round-off approaches and order differ, and can create slight errors. If this causes issues for you, turn the optimizations off by setting TF_ENABLE_ONEDNN_OPTS=0 before running your TensorFlow programs. To enable or re-enable them, set TF_ENABLE_ONEDNN_OPTS=1 before running your TensorFlow program. To verify that the optimizations are on, look for a message beginning with \"oneDNN custom operations are on\" in your program log. We welcome feedback on GitHub and the TensorFlow Forum.\nModel parallelism with DTensor\nDTensor is a new TensorFlow API for distributed model processing that allows models to seamlessly move from data parallelism to single program multiple data (SPMD) based model parallelism, including spatial partitioning. This means you have tools to easily train models where the model weights or inputs are so large they don\u2019t fit on a single device. (If you are familiar with Mesh TensorFlow in TF1, DTensor serves a similar purpose.)\nDTensor is designed with the following principles at its core:\nA device-agnostic API: This allows the same model code to be used on CPU, GPU, or TPU, including models partitioned across device types.\nMulti-client execution: Removes the coordinator and leaves each task to drive its locally attached devices, allowing scaling a model with no impact to startup time.\nA global perspective vs. per-replica: Traditionally with TensorFlow, distributed model code is written around replicas, but with DTensor, model code is written from the global perspective and per replica code is generated and run by the DTensor runtime. Among other things, this means no uncertainty about whether batch normalization is happening at the global level or the per replica level.\nWe have developed several introductory tutorials on DTensor, from DTensor concepts to training DTensor ML models with Keras:\nDTensor Concepts\nDistributed ML with DTensors\nUsing DTensors with Keras\nTraceType for tf.function\nWe have revamped the way tf.function retraces to make it simpler, predictable, and configurable.\nAll arguments of tf.function are assigned a tf.types.experimental.TraceType. Custom user classes can declare a TraceType using the Tracing Protocol (tf.types.experimental.SupportsTracingProtocol).\nThe TraceType system makes it easy to understand retracing rules. For example, subtyping rules indicate what type of arguments can be used with particular function traces. Subtyping also explains how different specific shapes are joined into a generic shape that is their supertype, to reduce the number of traces for a function.\nTo learn more, see the new APIs for tf.types.experimental.TraceType, tf.types.experimental.SupportsTracingProtocol, and the reduce_retracing parameter of tf.function.\nSupport for WSL2\nThe Windows Subsystem for Linux lets developers run a Linux environment directly on Windows, without the overhead of a traditional virtual machine or dual boot setup. TensorFlow now supports WSL2 out of the box, including GPU acceleration. Please see the documentation for more details about the requirements and how to install WSL2 on Windows.\nDeterministic behavior\nThe API tf.config.experimental.enable_op_determinism makes TensorFlow ops deterministic.\nDeterminism means that if you run an op multiple times with the same inputs, the op returns the exact same outputs every time. This is useful for debugging models, and if you train your model from scratch several times with determinism, your model weights will be the same every time. Normally, many ops are non-deterministic due to the use of threads within ops which can add floating-point numbers in a nondeterministic order.\nTensorFlow 2.8 introduced an API to make ops deterministic, and TensorFlow 2.9 improved determinism performance in tf.data in some cases. If you want your TensorFlow models to run deterministically, just add the following to the start of your program:\n```\ntf.keras.utils.set_random_seed(1)\ntf.config.experimental.enable_op_determinism()\n```\nThe first line sets the random seed for Python, NumPy, and TensorFlow, which is necessary for determinism. The second line makes each TensorFlow op deterministic. Note that determinism in general comes at the expense of lower performance and so your model may run slower when op determinism is enabled.\nOptimized Training with Keras\nIn TensorFlow 2.9, we are releasing a new experimental version of the Keras Optimizer API, tf.keras.optimizers.experimental. The API provides a more unified and expanded catalog of built-in optimizers which can be more easily customized and extended.\nIn a future release, tf.keras.optimizers.experimental.Optimizer (and subclasses) will replace tf.keras.optimizers.Optimizer (and subclasses), which means that workflows using the legacy Keras optimizer will automatically switch to the new optimizer. The current (legacy) tf.keras.optimizers.* API will still be accessible via tf.keras.optimizers.legacy.*, such as tf.keras.optimizers.legacy.Adam.\nHere are some highlights of the new optimizer class:\nIncrementally faster training for some models.\nEasier to write customized optimizers.\nBuilt-in support for moving average of model weights (\"Polyak averaging\").\nFor most users, you will need to take no action. But, if you have an advanced workflow falling into the following cases, please make corresponding changes:\nUse Case 1: You implement a customized optimizer based on the Keras optimizer\nFor these works, please first check if it is possible to change your dependency to tf.keras.optimizers.experimental.Optimizer. If for any reason you decide to stay with the old optimizer (we discourage it), then you can change your optimizer to tf.keras.optimizers.legacy.Optimizer to avoid being automatically switched to the new optimizer in a later TensorFlow version.\nUse Case 2: Your work depends on third-party Keras-based optimizers (such as tensorflow_addons)\nYour work should run successfully as long as the library continues to support the specific optimizer. However, if the library maintainers fail to take actions to accommodate the Keras optimizer change, your work would error out. So please stay tuned with the third-party library\u2019s announcement, and file a bug to Keras team if your work is broken due to optimizer malfunction.\nUse Case 3: Your work is based on TF1\nFirst of all, please try migrating to TF2. It is worth it, and may be easier than you think! If for any reason migration is not going to happen soon, then please replace your tf.keras.optimizers.XXX to tf.keras.optimizers.legacy.XXX to avoid being automatically switched to the new optimizer.\nUse Case 4: Your work has customized gradient aggregation logic\nUsually this means you are doing gradients aggregation outside the optimizer, and calling apply_gradients() with experimental_aggregate_gradients=False. We changed the argument name, so please change your optimizer to tf.keras.optimizers.experimental.Optimizer and set skip_gradients_aggregation=True. If it errors out after making this change, please file a bug to Keras team.\nUse Case 5: Your work has direct calls to deprecated optimizer public APIs\nPlease check if your method call has a match here. change your optimizer to tf.keras.optimizers.experimental.Optimizer. If for any reason you want to keep using the old optimizer, change your optimizer to tf.keras.optimizers.legacy.Optimizer.\nNext steps\nCheck out the release notes for more information. To stay up to date, you can read the TensorFlow blog, follow twitter.com/tensorflow, or subscribe to youtube.com/tensorflow. If you\u2019ve built something you\u2019d like to share, please submit it for our Community Spotlight at goo.gle/TFCS. For feedback, please file an issue on GitHub or post to the TensorFlow Forum. Thank you!",
    "link": "https://blog.tensorflow.org/2022/05/whats-new-in-tensorflow-29.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgYIMKKNGUkhWyF3gL1qLpn7cQ3iQu5GleP-36OHbTJwn90YdRUk8vGTefc9ctjwPcJhyBbFlprd581nDsWEKfMAeAo9xuX8zKfxti8Fvl2f2v69Qmvt695cCJY1dfVPbMIlfWqMFKMEyBCgIaRLXypYCrHlob-OiAb0mvVbhmBEt65-agfmRWDMuaI/s1600/image1.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgYIMKKNGUkhWyF3gL1qLpn7cQ3iQu5GleP-36OHbTJwn90YdRUk8vGTefc9ctjwPcJhyBbFlprd581nDsWEKfMAeAo9xuX8zKfxti8Fvl2f2v69Qmvt695cCJY1dfVPbMIlfWqMFKMEyBCgIaRLXypYCrHlob-OiAb0mvVbhmBEt65-agfmRWDMuaI/s1600/image1.png"
    ],
    "time": "2023/12/11 00:58:43"
  },
  {
    "title": "How to migrate from BoostedTrees Estimators to TensorFlow Decision Forests",
    "content": "Posted by Mathieu Guillame-Bert and Josh Gordon for the TensorFlow team\nDecision forest models like random forests and gradient boosted trees are often the most effective tools available for working with tabular data. They provide many advantages over neural networks, including being easier to configure, and faster to train. Using trees greatly reduces the amount of code required to prepare your dataset, as they natively handle numeric, categorical, and missing features. And they often give good results out-of-the-box, with interpretable properties.\nAlthough we usually think of TensorFlow as a library to train neural networks, a popular use case at Google is to use TensorFlow to create decision forests.\nAn animation of a decision tree classifying data.\nThis article provides a migration guide if you were previously creating tree-based models using tf.estimator.BoostedTrees, which was introduced in 2019. The Estimator API took care of much of the complexity of working with models in production, including distributed training and serialization. However, it is no longer recommended for new code.\nIf you are starting a new project, we recommend that you use TensorFlow Decision Forests (TF-DF). This library provides state-of-the-art algorithms for training, serving and interpreting decision forest models, with many benefits over the previous approach, notably regarding quality, speed, and ease of use.\nTo start, here are equivalent examples using the Estimator API and TF-DF to create a boosted tree model.\nPreviously, this is how you would train a gradient boosted tree models with tf.estimator.BoostedTrees (no longer recommended)\nimport tensorflow as tf\n\n# Dataset generators\ndef make_dataset_fn(dataset_path):\n    def make_dataset():\n        data = ... # read dataset\n        return tf.data.Dataset.from_tensor_slices(...data...).repeat(10).batch(64)\n    return make_dataset\n\n# List the possible values for the feature \"f_2\".\nf_2_dictionary = [\"NA\", \"red\", \"blue\", \"green\"]\n\n# The feature columns define the input features of the model.\nfeature_columns = [\n    tf.feature_column.numeric_column(\"f_1\"),\n    tf.feature_column.indicator_column(\n       tf.feature_column.categorical_column_with_vocabulary_list(\"f_2\",\n         f_2_dictionary,\n         # A special value \"missing\" is used to represent missing values.\n         default_value=0)\n       ),\n    ]\n\n# Configure the estimator\nestimator = boosted_trees.BoostedTreesClassifier(\n          n_trees=1000,\n          feature_columns=feature_columns,\n          n_classes=3,\n          # Rule of thumb proposed in the BoostedTreesClassifier documentation.\n          n_batches_per_layer=max(2, int(len(train_df) / 2 / FLAGS.batch_size)),\n      )\n\n# Stop the training is the validation loss stop decreasing.\nearly_stopping_hook = early_stopping.stop_if_no_decrease_hook(\n      estimator,\n      metric_name=\"loss\",\n      max_steps_without_decrease=100,\n      min_steps=50)\n\ntf.estimator.train_and_evaluate(\n      estimator,\n      train_spec=tf.estimator.TrainSpec(\n          make_dataset_fn(train_path),\n          hooks=[\n              # Early stopping needs a CheckpointSaverHook.\n              tf.train.CheckpointSaverHook(\n                  checkpoint_dir=input_config.raw.temp_dir, save_steps=500),\n              early_stopping_hook,\n          ]),\n      eval_spec=tf.estimator.EvalSpec(make_dataset_fn(valid_path)))\nHow to train the same model using TensorFlow Decision Forests\nimport tensorflow_decision_forests as tfdf\n\n# Load the datasets\n# This code is similar to the estimator.\ndef make_dataset(dataset_path):\n    data = ... # read dataset\n    return tf.data.Dataset.from_tensor_slices(...data...).batch(64)\n\ntrain_dataset = make_dataset(train_path)\nvalid_dataset = make_dataset(valid_path)\n\n# List the input features of the model.\nfeatures = [\n  tfdf.keras.FeatureUsage(\"f_1\", keras.FeatureSemantic.NUMERICAL),\n  tfdf.keras.FeatureUsage(\"f_2\", keras.FeatureSemantic.CATEGORICAL),\n]\n\nmodel = tfdf.keras.GradientBoostedTreesModel(\n  task = tfdf.keras.Task.CLASSIFICATION,\n  num_trees=1000,\n  features=features,\n  exclude_non_specified_features=True)\n\nmodel.fit(train_dataset, valid_dataset)\n\n# Export the model to a SavedModel.\nmodel.save(\"project/model\")\nRemarks\nWhile not explicit in this example, early stopping is automatically enabled and configured.\nThe dictionary of the \"f_2\" features is automatically built and optimized (e.g. rare values are merged into an out-of-vocabulary item).\nThe number of classes (3 in this example) is automatically determined from the dataset.\nThe batch size (64 in this example), has no impact on the model training. Larger values are often preferable as it makes reading the dataset more efficient.\nTF-DF is all about ease of use, and the previous example can be further simplified and improved, as shown next.\nHow to train a TensorFlow Decision Forests (recommended solution)\nimport tensorflow_decision_forests as tfdf\nimport pandas as pd\n\n# Pandas dataset can be used easily with pd_dataframe_to_tf_dataset.\ntrain_df = pd.read_csv(\"project/train.csv\")\n\n# Convert the Pandas dataframe into a TensorFlow dataset.\ntrain_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_df, label=\"my_label\")\n\nmodel = tfdf.keras.GradientBoostedTreeModel(num_trees=1000)\nmodel.fit(train_dataset)\nRemarks\nWe did not specify the semantics (e.g. numerical, or categorical) of the features. In this case, the semantics will be automatically inferred.\nWe also didn\u2019t list which input features to use. In this case, all the columns (except for the label) will be used. The list and semantics of the input feature is visible in the training logs, or with the model inspector API.\nWe did not specify any validation dataset. Each algorithm will optionally extract a validation dataset from the training examples as best for the algorithm. For example, by default, GradientBoostedTreeModel uses 10% of the training data for validation if no validation dataset is provided.\nNow, let\u2019s look at a couple differences between the Estimator API and TF-DF.\nDifferences between the Estimator API and TF-DF\nType of algorithms\nTF-DF is a collection of decision forest algorithms. This includes (but is not limited to) the Gradient Boosted Trees available with the Estimator API. Notably, TF-DF also supports Random Forest (great for nosy datasets) and a CART implementation (great for model interpretation).\nIn addition, for each of those algorithms, TF-DF includes many variations found in the literature and validated experimentally [1, 2, 3].\nExact vs approximate splits\nThe TF1 GBT Estimator is an approximated tree learning algorithm. Informally, the Estimator builds trees by only considering a random subset of examples and a random subset of the conditions at each step.\nBy default, TF-DF is an exact tree training algorithm. Informally, TF-DF considers all the training examples and all the possible splits at each step. This is a more common and often better performing solution.\nWhile sometimes faster on larger datasets (>10B examples x features), the estimator approximation are often less accurate (as more trees need to be grown to reach the same quality). In a small dataset (<100M examples x features), the form of approximated training implemented in the Estimator can even be slower than exact training.\nTF-DF also supports various types of \"approximated\" tree training. The recommended approach is to use exact training, and optionally test approximated training on large datasets.\nInference\nThe Estimator runs model inference using the top-down tree routing algorithm. TF-DF uses an extension of the QuickScorer algorithm.\nWhile both algorithms return the exact same results, the top-down algorithm is less efficient because of exceeding branching predictions and cache misses. TF-DF inference is generally 10x faster on the same model.\nFor latency critical applications TF-DF offers a C++ API. It provides often ~1\u00b5s/example/core inference time. This is often a 50x-1000x speed-up over TF SavedModel inference (especially on small batches).\nMulti-head models\nThe Estimator supports multi-head models (a model that outputs multiple predictions). TF-DF (currently) does not support multi-head models directly, however, using the Keras Functional API, multiple TF-DF models trained in parallel can be assembled into a multi-head model.\nLearning more\nYou can learn more about TensorFlow Decision Forests by visiting the website. If you\u2019re new to this library, the beginner example is a good place to start. Experienced TensorFlow users can visit this guide for important details about the difference between using decision forests and neural networks in TensorFlow, including how to configure your training pipeline, and tips on Dataset I/O. You can also see Migrate from Estimator to Keras APIs for more info on migrating from Estimators to Keras in general.",
    "link": "https://blog.tensorflow.org/2022/04/how-to-migrate-from-boostedtrees.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiW0Cq5zZzPAlNtIN1Fmb_Xxw-r_0c5_60QzOrJYVTJcvFxKjte2zA6kwQxgWbXOBKizFIWVneiaa2KnpP0cNdTWHxy5d3OApFMIO2WnojCRgrdnqVihqZW5DqztA3GkKuY-fv7Anwn8PpDfwPuCaNYjbaRwASdviQT2eei2jAf6xlF2xOEYohCaYEJ/s1600/image1.gif",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiW0Cq5zZzPAlNtIN1Fmb_Xxw-r_0c5_60QzOrJYVTJcvFxKjte2zA6kwQxgWbXOBKizFIWVneiaa2KnpP0cNdTWHxy5d3OApFMIO2WnojCRgrdnqVihqZW5DqztA3GkKuY-fv7Anwn8PpDfwPuCaNYjbaRwASdviQT2eei2jAf6xlF2xOEYohCaYEJ/s1600/image1.gif"
    ],
    "time": "2023/12/11 00:58:43"
  },
  {
    "title": "Boost your model's accuracy using self-supervised learning with TensorFlow Similarity",
    "content": "Posted by Elie Bursztein and Owen Vallis, Google\nTensorFlow similarity now supports key self-supervised learning algorithms to help you boost your model\u2019s accuracy when you don\u2019t have a lot of labeled data.\nBasic Self-Supervised Training.\nOften when training a new machine learning classifier, we have a lot more unlabeled data, such as photos, than labeled examples. Self-supervised learning techniques aim at leveraging those unlabeled data to learn useful data representations to boost classifier accuracy via a pre-training phase on those unlabeled examples. The ability to tap into abundant unlabeled data can significantly improve model accuracy in some cases.\nPerhaps the most well known example of successful self-supervised training are transformer models, such as BERT, that learn meaningful language representations by pre-training on very large quantities of text, e.g., wikipedia or the web.\nSelf-supervised learning can be applied to any type of data and at various data scales. For example, if you have only a few hundred labeled images, using self-supervised learning can boost your model accuracy by pre-training on a medium sized dataset such as ImageNet. For example, SimCLR uses the ImageNet ILSVRC-2012 dataset for training the representations and then evaluates the transfer learning performance on 12 other image datasets such as CIFAR, Oxford-IIIT Pets, Food-101, and others. Self-supervised learning works at larger scales as well, where pre-training on billions of examples improves accuracy as well, including text transformer and vision transformer.\nHigh level overview of how self-supervised learning works for images.\nAt its core, self-supervised learning works by contrasting two augmented \u201cviews\u201d of the same example. The model objective is to maximize the similarity between these views to learn representations that are useful for down-stream tasks, such as training a supervised classifier. In practice, after pre-training on a large corpus of unlabeled images, training an image classifier is done by adding a single softmax dense layer on top of the frozen pre-trained representation and training as usual using a small number of labeled examples.\nExamples of pairs of augmented views on CIFAR10 from the hello world notebook.\nTensorFlow Similarity currently provides three key approaches for learning self-supervised representations: SimCLR, SimSiam, Barlow Twins, that work out of the box. TensorFlow Similarity also provides all the necessary components to implement additional forms of unsupervised learning. These include, callbacks, metrics, and data samplers.\nYou can start to explore how to leverage a self-supervised learning hello world notebook that demonstrates how to double the accuracy on CIFAR10.",
    "link": "https://blog.tensorflow.org/2022/02/boost-your-models-accuracy.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/a/AVvXsEhum2ibImcxULbNEe0BUQvpXCimvnu-YjatMfm7KH_xVn6j9eZicbN6_EqqMXyf4ocYRz1F9o2OrZiPtlhgpDsF-c9Ab3CgcxS8llFHpvHa1crr5tWJ3Svz7vWQEc8Iyv79ceBUKq1luIhzhDRV_bEdcuEkreHj5MMfUPHb9qv68YiqoYYcC5HRgj0Y",
      "https://blogger.googleusercontent.com/img/a/AVvXsEhum2ibImcxULbNEe0BUQvpXCimvnu-YjatMfm7KH_xVn6j9eZicbN6_EqqMXyf4ocYRz1F9o2OrZiPtlhgpDsF-c9Ab3CgcxS8llFHpvHa1crr5tWJ3Svz7vWQEc8Iyv79ceBUKq1luIhzhDRV_bEdcuEkreHj5MMfUPHb9qv68YiqoYYcC5HRgj0Y",
      "https://blogger.googleusercontent.com/img/a/AVvXsEjSCAHCDIDck2Nk8RXmYVZeM_OKpJVjPfVFKrDBTFU2kozI0Pms-B3_JyQ5c4JDYGdj6Nr4INh8vQRCwbTwrfFssNu696HK7fYKzIkK0vDd47UKZkLjMeWk4xG28pMA1A7C87bQXKaDTVB97WCoYGr69uzPp8vSTwo7v4fZuRVEk42UzTeRSk-ZLSYT",
      "https://blogger.googleusercontent.com/img/a/AVvXsEjbwQUkOwIAIzjDFAUWTI4LbnJR_QA24eH16DceXGCCs0vsKLtEXvAbuDRt8Cc3_JQtlRsUM4yJqwTAYu-NWYjnSOz-_EY-2IpIN2Pg1jSxQPi7_fRkBH6JfxrM3jPYo_wUuhoHXlATU3y9SZOf1cZImMLSI8jhXbPcTFTPxmc--FvyrNjtbP7g7W6k"
    ],
    "time": "2023/12/11 00:58:43"
  },
  {
    "title": "TFRT: A Progress Update",
    "content": "Posted by Mingsheng Hong, TFRT Tech Lead/Manager & Eric Johnson, TFRT Product Manager\nRoughly two years ago, we announced an ambitious new Machine Learning (ML) runtime effort called TFRT (short for TensorFlow Runtime). We simultaneously provided a deep dive of the initial technical design and open-sourced its codebase.\nDriven by trends in the ML ecosystem \u2013 larger and bigger models, ML being deployed to more diverse execution environments, and the need to keep up with continued research and modeling innovations \u2013 TFRT was started with the following set of goals in mind:\nDeliver faster and cheaper execution for ML models\nEnable more flexible deployment\nProvide more modular and extensible infrastructure to facilitate innovations in ML infra and modeling\nIn this post, we share our progress to date, the experiences and lessons we\u2019ve learned over the past two years of development, as well as what you can expect going forward.\nProgress to Date\nThe last two years of development have largely been focused on implementing and validating our ambitious ideas by enabling Google\u2019s most important internal workloads for users such as Ads and Search. To date, we have deployed TFRT broadly inside Google on a variety of training and inference workloads, and obtained great results.\nTechnical Lessons\nHow have we been able to achieve the above? Here are some interesting technical lessons that we learned, beyond what was in the original design:\nFirst, async support is important for some of the key workloads (e.g. overlapping compute and I/O, and driving heterogeneous devices), while fast sync execution is critical for many other workloads, including small, \u201cembedded\u201d ML models.\nWe spent a lot of effort in designing and refining AsyncValue, a key low level abstraction in TFRT, which allows the host runtime to asynchronously drive devices, as well as invoking kernels. This led to improved device utilization due to the ability to overlap more computation and communication across hosts and devices. For example, we were able to successfully run bulk inference of an 80B-parameter model on one TPU chip with good performance by splitting the model into multiple stages and using TFRT to overlap variable transfer of the next stage with TPU computation of the current stage.\nOn the other hand, small CPU models that are embedded in application servers, invoked within the application process instead of via RPC/REST calls, remain critical for some of Google\u2019s business workloads from users like Ads. For these models, the async-first internal design of TFRT initially caused a performance and resource regression. We worked with the Ads team to successfully address it, by extending the TFRT design with a synchronous interpreter, as well as an experimental memory planning optimization, to avoid heap allocation during kernel execution. We are working on productizing this extension.\nThis diagram below showcases the impact of the resulting TFRT design over a benchmark, as compared to \u201cCurrent TF\u201d which ran the old runtime before TFRT\u2019s deployment. This benchmark focused on executing a tiny CPU model, where a large number of small matmuls executed sequentially. Notably, the optimized execution in TFRT (265 ns) is approaching the optimal baseline we set up (204 ns), via hand-written C++ code without any ML runtime overhead.\nSecond, while faster runtime execution is critical, optimizing the input program to reduce execution complexity is important as well.\nNote that while compiler-based graph optimization should be performed when TF SavedModel is saved to the disk whenever possible, there are also important inference-time compiler optimizations that can only be performed with the knowledge of being in an inference context (e.g. when training variables remain constant).\nAs we were onboarding ML models onto TFRT, we had the chance to examine some of the models in depth, and identified new ways of rewriting and simplifying the program, before its execution. The simplified program, along with a faster execution of each kernel in the graph program, led to a nice compounding effect in the reduction of the execution latency and resource cost.\nFor example, in the left hand side graph program below, we were able to hoist the scalar op normalization computation (e.g. divide a float value by the max value of its domain), identical across all 18 input scalars, above the \u201cconcat\u201d op, therefore enabling vectorized execution of the normalization, over a concatenated 1D float tensor.\nWhile it is possible to perform this optimization at model training time as well, the compiler+runtime used to produce the trained model did not include this optimization.\nIn addition, we also find it critical to hoist computation from model execution time to load time whenever possible (e.g. const folding).\nThird, cost-based execution is not just for SQL queries.\nWe developed a simple compile-time cost model (analogous to SQL query optimizer\u2019s cost model) for TF op kernels, and applied cost-based optimization for ML model execution (see stream analysis), and achieved a better load balancing of the kernel execution across a set of threadpool threads. In contrast, TF1 has a runtime-based cost model, in which each operation's runtime cost is profiled and used to guide that operation\u2019s scheduling. In TFRT, we moved the cost analysis to compile-time, thus removing runtime cost. Also, our compiler approach allows the entire computational graph to be analyzed, thereby resulting in scheduling decisions that are optimal at a more global scope.\nSee this tech talk for more similarities between data and ML infra.\nLooking Ahead\nWhile we\u2019ve certainly made some strong progress, especially with respect to our first goal \u2013 faster and cheaper execution \u2013 we admittedly still have work to do on enabling a more modular design and enabling more flexible deployments via hardware integration.\nIn terms of modularity, with the initial integration successes such as JAX\u2019s adoption of TFRT device runtimes (e.g. CPU), we will continue to explore how TFRT could support workloads beyond just TensorFlow. We expect some of the TFRT components will also benefit the PyTorch/XLA workloads going forward.\nMoreover, while we have successfully integrated CPU and TPU (with upcoming integration into Cloud TPU), the two most important device types at Google for ML computation, with NVIDIA GPU also in progress.\nWith respect to training workload, TFRT has been used as building blocks for Google's large scale distributed training framework which are currently in active development.\nAs we look to the future, our organization has been exploring its integration with Pixel\u2019s hardware SOC devices such as Google Tensor. In addition, due to TFRT\u2019s proven success for Google\u2019s internal workloads, it is also being integrated into new venues such as GCP\u2019s Vertex AI and Waymo.\nSpecial Thanks\nThe TFRT team has really enjoyed working on this new, ambitious infrastructure project. It has often felt like bootstrapping a new startup. With that in mind, we would like to give a huge shout out to everyone who has advised, contributed to and supported TFRT through this incredible 2-year journey:\n(alphabetically) Adi Agrawal, Andrew Bernard, Andrew Leaver, Andy Selle, Ayush Dubey, Bangda Zhou, Bramandia Ramadhana, Catherine Payne, Ce Zheng, Chiachen Chou, Chao Xie, Christina Sorokin, Chuanhao Zhuge, Dan Hurt, Dong Lin, Eugene Zhulenev, Ewa Matejska, Hadi Hashemi, Haoliang Zhang, HanBin Yoon, Haoyu Zhang, Hongmin Fan, Jacques Pienaar, Jeff Dean, Jeremy Lau, Jordan Soyke, Jing Dong, Juanli Shen, Kemal El Moujahid, Kuangyuan Chen, Mehdi Amini, Ning Niu, Peter Gavin, Phil Sun, Pulkit Bhuwalka, Qiao Zhang, Raziel Alvarez, Russell Power, Sanjoy Das, Shengqi Zhu, Smit Hinsu, Tatiana Shpeisman, Tianrun Li, Tim Davis, Tom Black, Victor Akabutu, Vilobh Meshram, Xiao Yu, Xiaodan Song, Yiming Zhang, YC Ling, Youlong Chen, and Zhuoran Liu.\nWe would like to give special thanks to Chris Lattner for his initial technical leadership in bootstrapping this project, Martin Wicke for his support in TFRT throughout the first year, Alex Zaks for his support in TFRT during the second year and seeing through the impactful landing for Google\u2019s ML serving workloads.",
    "link": "https://blog.tensorflow.org/2022/02/tfrt-progress-update.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/a/AVvXsEioxLwPYbbp4dtHj1gVVgUJ3PkqQ7GtRr0VS-DLtuwoUnOWf99F8Cl8lQ2cZpTVLnY-fd6M22hltTa9kzgrlE0tLmt0JVpdNEcz3nv9wNBLEcyPakq0nscIkCimcwi2HwYo0osTOltdpe0bJA1VlQVIgiUz1cJLqfLybKBykoOxQVGhlC9FqYwtVdRB",
      "https://blogger.googleusercontent.com/img/a/AVvXsEioxLwPYbbp4dtHj1gVVgUJ3PkqQ7GtRr0VS-DLtuwoUnOWf99F8Cl8lQ2cZpTVLnY-fd6M22hltTa9kzgrlE0tLmt0JVpdNEcz3nv9wNBLEcyPakq0nscIkCimcwi2HwYo0osTOltdpe0bJA1VlQVIgiUz1cJLqfLybKBykoOxQVGhlC9FqYwtVdRB",
      "https://blogger.googleusercontent.com/img/a/AVvXsEiYzf8oM9wdeTBAKSiHs_p9s2ydVqfMLAEEudvXbrDai3L3N_PUgA0s5XKd1fWKQW44HpxkDU44Y0yLkg7MlST4yTDmbYwsBh5Bww9fWkQBF2ymarCzRO47pKQ6xRQYsvbRbAZ9TBNSZ3UpE_CRH_gqRVpT4N-nxBINANkI2Se4-EdQrdxLazihhq_U",
      "https://blogger.googleusercontent.com/img/a/AVvXsEj-v7lebv0sfEmQhiVODb9IsOueYkFfUJkVdG4EeJOxG3VtymcOIzVwXldIUOpuAlFC1P1nDbuklJospFgJoYqwKh9jYuUSmNJhMA1eLEeX-3sVxUULKaiQymckXtcB8YsVAejT_gZJTVd2R3cFWoD0mXXtjbb4vAtDyrvuI5maH30ERGQvLecZUyvS"
    ],
    "time": "2023/12/11 00:58:43"
  },
  {
    "title": "An Introduction to Keras Preprocessing Layers",
    "content": "Posted by Matthew Watson, Keras Developer\nDetermining the right feature representation for your data can be one of the trickiest parts of building a model. Imagine you are working with categorical input features such as names of colors. You could one-hot encode the feature so each color gets a 1 in a specific index ('red' = [0, 0, 1, 0, 0]), or you could embed the feature so each color maps to a unique trainable vector ('red' = [0.1, 0.2, 0.5, -0.2]). Larger category spaces might do better with an embedding, and smaller spaces as a one-hot encoding, but the answer is not clear cut. It will require experimentation on your specific dataset.\nIdeally, we would like updates to our feature representation and updates to our model architecture to happen in a tight iterative loop, applying new transformations to our data while changing our model architecture. In practice, feature preprocessing and model building are usually handled by entirely different libraries, frameworks, or languages. This can slow the process of experimentation.\nOn the Keras team, we recently released Keras Preprocessing Layers, a set of Keras layers aimed at making preprocessing data fit more naturally into model development workflows. In this post we are going to use the layers to build a simple sentiment classification model with the imdb movie review dataset. The goal will be to show how preprocessing can be flexibly developed and applied. To start, we can import tensorflow and download the training data.\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\ntrain_ds = tfds.load('imdb_reviews', split='train', as_supervised=True).batch(32)\nKeras preprocessing layers can handle a wide range of input, including structured data, images, and text. In this case, we will be working with raw text, so we will use the TextVectorization layer.\nBy default, the TextVectorization layer will process text in three phases:\nFirst, remove punctuation and lower cases the input.\nNext, split text into lists of individual string words.\nFinally, map strings to numeric outputs using a vocabulary of known words.\nA simple approach we can try here is a multi-hot encoding, where we only consider the presence or absence of terms in the review. For example, say a layer vocabulary is ['movie', 'good', 'bad'], and a review read 'This movie was bad.'. We would encode this as [1, 0, 1], where movie (the first vocab term) and bad (the last vocab term) are present.\ntext_vectorizer = tf.keras.layers.TextVectorization(\n     output_mode='multi_hot', max_tokens=2500)\nfeatures = train_ds.map(lambda x, y: x)\ntext_vectorizer.adapt(features)\nAbove, we create a TextVectorization layer with multi-hot output, and do two things to set the layer\u2019s state. First, we map over our training dataset and discard the integer label indicating a positive or negative review. This gives us a dataset containing only the review text. Next, we adapt() the layer over this dataset, which causes the layer to learn a vocabulary of the most frequent terms in all documents, capped at a max of 2500.\nAdapt is a utility function on all stateful preprocessing layers, which allows layers to set their internal state from input data. Calling adapt is always optional. For TextVectorization, we could instead supply a precomputed vocabulary on layer construction, and skip the adapt step.\nWe can now train a simple linear model on top of this multi-hot encoding. We will define two functions: preprocess, which converts raw input data to the representation we want for our model, and forward_pass, which applies the trainable layers.\ndef preprocess(x):\n  return text_vectorizer(x)\n\ndef forward_pass(x):\n  return tf.keras.layers.Dense(1)(x)  # Linear model\n\ninputs = tf.keras.Input(shape=(1,), dtype='string')\noutputs = forward_pass(preprocess(inputs))\nmodel = tf.keras.Model(inputs, outputs)\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True))\nmodel.fit(train_ds, epochs=5)\nThat\u2019s it for an end-to-end training example, and already enough for 85% accuracy. You can find complete code for this example at the bottom of this post.\nLet\u2019s experiment with a new feature. Our multi-hot encoding does not contain any notion of review length, so we can try adding a feature for normalized string length. Preprocessing layers can be mixed with TensorFlow ops and custom layers as desired. Here we can combine the tf.strings.length function with the Normalization layer, which will scale the input to have 0 mean and 1 variance. We have only updated code up to the preprocess function below, but we will show the rest of training for clarity.\n# This layer will scale our review length feature to mean 0 variance 1.\nnormalizer = tf.keras.layers.Normalization(axis=None)\nnormalizer.adapt(features.map(lambda x: tf.strings.length(x)))\n\ndef preprocess(x):\n  multi_hot_terms = text_vectorizer(x)\n  normalized_length = normalizer(tf.strings.length(x))\n  # Combine the multi-hot encoding with review length.\n  return tf.keras.layers.concatenate((multi_hot_terms, normalized_length))\n\ndef forward_pass(x):\n  return tf.keras.layers.Dense(1)(x)  # Linear model.\n\ninputs = tf.keras.Input(shape=(1,), dtype='string')\noutputs = forward_pass(preprocess(inputs))\nmodel = tf.keras.Model(inputs, outputs)\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True))\nmodel.fit(train_ds, epochs=5)\nAbove, we create the normalization layer and adapt it to our input. Within the preprocess function, we simply concatenate our multi-hot encoding and length features together. We learn a linear model over the union of the two feature representations.\nThe last change we can make is to speed up training. We have one major opportunity to improve our training throughput. Right now, every training step, we spend some time on the CPU performing string operations (which cannot run on an accelerator), followed by calculating a loss function and gradients on a GPU.\nWith all computation in a single model, we will first preprocess each batch on the CPU and then update parameter weights on the GPU. This leaves gaps in our GPU usage.\nThis gap in accelerator usage is totally unnecessary! Preprocessing is distinct from the actual forward pass of our model. The preprocessing doesn't use any of the parameters being trained. It\u2019s a static transformation that we could precompute.\nTo speed things up, we would like to prefetch our preprocessed batches, so that each time we are training on one batch we are preprocessing the next. This is easy to do with the tf.data library, which was built for uses like this. The only major change we need to make is to split our monolithic keras.Model into two: one for preprocessing and one for training. This is easy with Keras\u2019 functional API.\ninputs = tf.keras.Input(shape=(1,), dtype=\"string\")\npreprocessed_inputs = preprocess(inputs)\noutputs = forward_pass(preprocessed_inputs)\n\n# The first model will only apply preprocessing.\npreprocessing_model = tf.keras.Model(inputs, preprocessed_inputs)\n# The second model will only apply the forward pass.\ntraining_model = tf.keras.Model(preprocessed_inputs, outputs)\ntraining_model.compile(\n    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True))\n\n# Apply preprocessing asynchronously with tf.data.\n# It is important to call prefetch and remember the AUTOTUNE options.\npreprocessed_ds = train_ds.map(\n    lambda x, y: (preprocessing_model(x), y),\n    num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n\n# Now the GPU can focus on the training part of the model.\ntraining_model.fit(preprocessed_ds, epochs=5)\nIn the above example, we pass a single keras.Input through our preprocess and forward_pass functions, but define two separate models over the transformed inputs. This slices our single graph of operations into two. Another valid option would be to only make a training model, and call the preprocess function directly when we map over our dataset. In this case, the keras.Input would need to reflect the type and shape of the preprocessed features rather than the raw strings.\nUsing tf.data to prefetch batches cuts our train step time by over 30%! Our compute time now looks more like the following:\nWith tf.data, we are now precomputing each preprocessed batch before the GPU needs it. This significantly speeds up training.\nWe could even go a step further than this, and use tf.data to cache our preprocessed dataset in memory or on disk. We would simply add a .cache() call directly before the call to prefetch. In this way, we could entirely skip computing our preprocessing batches after the first epoch of training.\nAfter training, we can rejoin our split model into a single model during inference. This allows us to save a model that can directly handle raw input data.\ninputs = preprocessing_model.input\noutputs = training_model(preprocessing_model(inputs))\ninference_model = tf.keras.Model(inputs, outputs)\ninference_model.predict(\n    tf.constant([\"Terrible, no good, trash.\", \"I loved this movie!\"]))\nKeras preprocessing layers aim to provide a flexible and expressive way to build data preprocessing pipelines. Prebuilt layers can be mixed and matched with custom layers and other tensorflow functions. Preprocessing can be split from training and applied efficiently with tf.data, and joined later for inference. We hope they allow for more natural and efficient iterations on feature representation in your models.\nTo play around with the code from this post in a Colab, you can follow this link. To see a wide range of tasks you can do with preprocessing layers, see the Quick Recipes section of our preprocessing guide. You can also check out our complete tutorials for basic text classification, image data augmentation, and structured data classification.",
    "link": "https://blog.tensorflow.org/2021/11/an-introduction-to-keras-preprocessing.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/a/AVvXsEjmnnyx1Otv5UXKzl07IKRgmoqCouX5GwJop_2wpijzId9XqUMRKM9GxDlztu8pFDqs9YVzT7bh_DbVT-4SV27vAGgAU-LB88LNFmvg_yCDszdFWpWsl_ENaZJq0y1UqeKwAZIMxX2uQyNoPd2Qo821PeoGH8Ga5BEo13M5BrsGgmxc5Jcj8L0cGW6a",
      "https://blogger.googleusercontent.com/img/a/AVvXsEjmnnyx1Otv5UXKzl07IKRgmoqCouX5GwJop_2wpijzId9XqUMRKM9GxDlztu8pFDqs9YVzT7bh_DbVT-4SV27vAGgAU-LB88LNFmvg_yCDszdFWpWsl_ENaZJq0y1UqeKwAZIMxX2uQyNoPd2Qo821PeoGH8Ga5BEo13M5BrsGgmxc5Jcj8L0cGW6a",
      "https://blogger.googleusercontent.com/img/a/AVvXsEh32GHsR2wj3SmvDSpBULe43ExBpMn_1dnHl3mlB2l64EihYx3UrD1crKwEe7xC77Dk7LNHOfGeybCq6G2LTFsUK0vSXFKhBxxVzbygC7vGf4LXbVTw7d8GEMOQ8Eue6n7M4zSEutVRMruRv2Howmmk5xPU4kemorcKRsdBS_BBCCTjMwwBzZ_yMLAF",
      "https://blogger.googleusercontent.com/img/a/AVvXsEjCNU_y8qIY_YZ81eYKbgqV8qJV-RM5hbwMKW-OBJWBC3Ywgh-lGpsnj9BEu8z-7PqC9zqQrIR4YKVobiBCqr6ggGH-t5xS0pCJ6NsjCuC88g_ap23h62H5CZjeo9z3msV9ZW9-DGvSI6-uvxpowGJUgzXxkVVSdAkSI4RGAbnFc1O1O81mlY7kZW2e"
    ],
    "time": "2023/12/11 00:58:43"
  },
  {
    "title": "Introducing TensorFlow Graph Neural Networks",
    "content": "Posted by Sibon Li, Jan Pfeifer and Bryan Perozzi and Douglas Yarrington\nToday, we are excited to release TensorFlow Graph Neural Networks (GNNs), a library designed to make it easy to work with graph structured data using TensorFlow. We have used an earlier version of this library in production at Google in a variety of contexts (for example, spam and anomaly detection, traffic estimation, YouTube content labeling) and as a component in our scalable graph mining pipelines. In particular, given the myriad types of data at Google, our library was designed with heterogeneous graphs in mind. We are releasing this library with the intention to encourage collaborations with researchers in industry.\nWhy use GNNs?\nGraphs are all around us, in the real world and in our engineered systems. A set of objects, places, or people and the connections between them is generally describable as a graph. More often than not, the data we see in machine learning problems is structured or relational, and thus can also be described with a graph. And while fundamental research on GNNs is perhaps decades old, recent advances in the capabilities of modern GNNs have led to advances in domains as varied as traffic prediction, rumor and fake news detection, modeling disease spread, physics simulations, and understanding why molecules smell.\nGraphs can model the relationships between many different types of data, including web pages (left), social connections (center), or molecules (right).\nA graph represents the relations (edges) between a collection of entities (nodes or vertices). We can characterize each node, edge, or the entire graph, and thereby store information in each of these pieces of the graph. Additionally, we can ascribe directionality to edges to describe information or traffic flow, for example.\nGNNs can be used to answer questions about multiple characteristics of these graphs. By working at the graph level, we try to predict characteristics of the entire graph. We can identify the presence of certain \u201cshapes,\u201d like circles in a graph that might represent sub-molecules or perhaps close social relationships. GNNs can be used on node-level tasks, to classify the nodes of a graph, and predict partitions and affinity in a graph similar to image classification or segmentation. Finally, we can use GNNs at the edge level to discover connections between entities, perhaps using GNNs to \u201cprune\u201d edges to identify the state of objects in a scene.\nStructure\nTF-GNN provides building blocks for implementing GNN models in TensorFlow. Beyond the modeling APIs, our library also provides extensive tooling around the difficult task of working with graph data: a Tensor-based graph data structure, a data handling pipeline, and some example models for users to quickly onboard.\nThe various components of TF-GNN that make up the workflow.\nThe initial release of the TF-GNN library contains a number of utilities and features for use by beginners and experienced users alike, including:\nA high-level Keras-style API to create GNN models that can easily be composed with other types of models. GNNs are often used in combination with ranking, deep-retrieval (dual-encoders) or mixed with other types of models (image, text, etc.)\nGNN API for heterogeneous graphs. Many of the graph problems we approach at Google and in the real world contain different types of nodes and edges. Hence we chose to provide an easy way to model this.\nA well-defined schema to declare the topology of a graph, and tools to validate it. This schema describes the shape of its training data and serves to guide other tools.\nA GraphTensor composite tensor type which holds graph data, can be batched, and has graph manipulation routines available.\nA library of operations on the GraphTensor structure:\nVarious efficient broadcast and pooling operations on nodes and edges, and related tools.\nA library of standard baked convolutions, that can be easily extended by ML engineers/researchers.\nA high-level API for product engineers to quickly build GNN models without necessarily worrying about its details.\nAn encoding of graph-shaped training data on disk, as well as a library used to parse this data into a data structure from which your model can extract the various features.\nExample usage\nIn the example below, we build a model using the TF-GNN Keras API to recommend movies to a user based on what they watched and genres that they liked.\nWe use the ConvGNNBuilder method to specify the type of edge and node configuration, namely to use WeightedSumConvolution (defined below) for edges. And for each pass through the GNN, we will update the node values through a Dense interconnected layer:\n    import tensorflow as tf\n    import tensorflow_gnn as tfgnn\n\n    # Model hyper-parameters:\n    h_dims = {'user': 256, 'movie': 64, 'genre': 128}\n    \n    # Model builder initialization:\n    gnn = tfgnn.keras.ConvGNNBuilder(\n      lambda edge_set_name: WeightedSumConvolution(),\n      lambda node_set_name: tfgnn.keras.layers.NextStateFromConcat(\n         tf.keras.layers.Dense(h_dims[node_set_name]))\n    )\n    \n    # Two rounds of message passing to target node sets:\n    model = tf.keras.models.Sequential([\n        gnn.Convolve({'genre'}),  # sends messages from movie to genre\n        gnn.Convolve({'user'}),  # sends messages from movie and genre to users\n        tfgnn.keras.layers.Readout(node_set_name=\"user\"),\n        tf.keras.layers.Dense(1)\n    ])\nThe code above works great, but sometimes we may want to use a more powerful custom model architecture for our GNNs. For example, in our previous use case, we might want to specify that certain movies or genres hold more weight when we give our recommendation. In the following snippet, we define a more advanced GNN with custom graph convolutions, in this case with weighted edges. We define the WeightedSumConvolution class to pool edge values as a sum of weights across all edges:\nclass WeightedSumConvolution(tf.keras.layers.Layer):\n  \"\"\"Weighted sum of source nodes states.\"\"\"\n\n  def call(self, graph: tfgnn.GraphTensor,\n           edge_set_name: tfgnn.EdgeSetName) -> tfgnn.Field:\n    messages = tfgnn.broadcast_node_to_edges(\n        graph,\n        edge_set_name,\n        tfgnn.SOURCE,\n        feature_name=tfgnn.DEFAULT_STATE_NAME)\n    weights = graph.edge_sets[edge_set_name]['weight']\n    weighted_messages = tf.expand_dims(weights, -1) * messages\n    pooled_messages = tfgnn.pool_edges_to_node(\n        graph,\n        edge_set_name,\n        tfgnn.TARGET,\n        reduce_type='sum',\n        feature_value=weighted_messages)\n    return pooled_messages\nNote that even though the convolution was written with only the source and target nodes in mind, TF-GNN makes sure it\u2019s applicable and works on heterogeneous graphs (with various types of nodes and edges) seamlessly.\nNext steps\nYou can check out the TF-GNN GitHub repo for more information. To stay up to date, you can read the TensorFlow blog, join the TensorFlow Forum at discuss.tensorflow.org, follow twitter.com/tensorflow, or subscribe to youtube.com/tensorflow. If you\u2019ve built something you\u2019d like to share, please submit it for our Community Spotlight at goo.gle/TFCS. For feedback, please file an issue on GitHub. Thank you!\nAcknowledgments\nThe work described here was a research collaboration between Oleksandr Ferludin, Martin Blais, Jan Pfeifer, Arno Eigenwillig, Dustin Zelle, Bryan Perozzi and Da-Cheng Juan of Google, and Sibon Li, Alvaro Sanchez-Gonzalez, Peter Battaglia, Kevin Villela, Jennifer She and David Wong of DeepMind.",
    "link": "https://blog.tensorflow.org/2021/11/introducing-tensorflow-gnn.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/a/AVvXsEhCZhxqPJb0_Qa4DD8pRS65AmohKJL49y9kMRGj1tADlHxteSKQjUovAZohPop-ej9dfw-Z4_vKyq4aS8Smvro2aSDWmLq3LzwEmfICqTf3ipmSPiWAjix1fSddgyLoajK2y387-pi1r_aRiE3FEH0L_hu-CEboaAloznbVw9aroBzJPmJWfAuhzVGu",
      "https://blogger.googleusercontent.com/img/a/AVvXsEhCZhxqPJb0_Qa4DD8pRS65AmohKJL49y9kMRGj1tADlHxteSKQjUovAZohPop-ej9dfw-Z4_vKyq4aS8Smvro2aSDWmLq3LzwEmfICqTf3ipmSPiWAjix1fSddgyLoajK2y387-pi1r_aRiE3FEH0L_hu-CEboaAloznbVw9aroBzJPmJWfAuhzVGu",
      "https://blogger.googleusercontent.com/img/a/AVvXsEgD62mrhRCGU6l88bnlBEHLcv_NdlcHc2fWhctuLkQ5Uu_6u8WsJKOsZ8US-jTM7Wu3ddpsenADjlit2bFq7hNYGMdr53UzOV5zj7t9yM0wtQl0_eIK4fiJk4MjldeR7WmyOqYZjz83eZ5JjLoxkdqGb2Vz5DCjZvS89obW5-TgnEGt5K6LQLXoVwel",
      "https://blogger.googleusercontent.com/img/a/AVvXsEjnEMa3dJZyuF0XfjVb2AXEQs74mWENe4s4JRWICLi4gGpvCi_pm88uqOnL-w8B24EraW5cDYxI1ivrFkod1ebj55E17VMPMnTpgnueDexFZjC_Y1RUfiuH8JbKxQreIx3yMHzacbaeI6R1jviBiPvHZLT0veIrrPwZ3FoF54THxEtcwYUS4-iEwX1I"
    ],
    "time": "2023/12/10 00:58:43"
  },
  {
    "title": "What's new in TensorFlow 2.7?",
    "content": "Posted by Goldie Gadde and Josh Gordon for the TensorFlow team\nTensorFlow 2.7 is here! This release improves usability with clearer error messages, simplified stack traces, and adds new tools and documentation for users migrating to TF2.\nImproved Debugging Experience\nThe process of debugging your code is a fundamental part of the user experience of a machine learning framework. In this release, we've considerably improved the TensorFlow debugging experience to make it more productive and more enjoyable, via three major changes: simplified stack traces, displaying additional context information in errors that originate from custom Keras layers, and a wide-ranging audit of all error messages in Keras and TensorFlow.\nSimplified stack traces\nTensorFlow is now filtering by default the stack traces displayed upon error to hide any frame that originates from TensorFlow-internal code, and keep the information focused on what matters to you: your own code. This makes stack traces simpler and shorter, and it makes it easier to understand and fix the problems in your code.\nIf you're actually debugging the TensorFlow codebase itself (for instance, because you're preparing a PR for TensorFlow), you can turn off the filtering mechanism by calling tf.debugging.disable_traceback_filtering().\nAutomatic context injection for Keras layer exceptions\nOne of the most common use cases for writing low-level code is creating custom Keras layers, so we wanted to make debugging your layers as easy and productive as possible. The first thing you do when you're debugging a layer is to print the shapes and dtypes of its inputs, as well the value of its training and mask arguments. We now add this information automatically to all stack traces that originate from custom Keras layers.\nSee the effect of stack trace filtering and call context information display in practice in the image below:\nSimplified stack traces in TensorFlow 2.7\nAudit and improve all error messages in the TensorFlow and Keras codebases\nLastly, we've audited every error message in the Keras and TensorFlow codebases (thousands of error locations!) and improved them to make sure they follow UX best practices. A good error message should tell you what the framework expected, what you did that didn't match the framework's expectations, and should provide tips to fix the problem.\nImprove tf.function error messages\nWe have improved two common types of tf.function error messages: runtime error messages and \"Graph\" tensor error messages, by including tracebacks pointing to the error source in the user code. For other vague and inaccurate tf.function error messages, we also updated them to be more clear and accurate.\nFor the runtime error message caused by the user code\n@tf.function\ndef f():\n l = tf.range(tf.random.uniform((), minval=1, maxval=10, dtype=tf.int32))\n return l[20]\nA summary of the old error message looks like\n# \u2026 Python stack trace of the function call \u2026\n\nInvalidArgumentError:  slice index 20 of dimension 0 out of bounds.\n         [[node strided_slice (defined at <'ipython-input-8-250c76a76c0e'>:5) ]] [Op:__inference_f_75]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node strided_slice:\n range (defined at <ipython-input-8-250c76a76c0e >':4)\n\nFunction call stack:\nf\nA summary of the new error message looks like\n# \u2026 Python stack trace of the function call \u2026\n\nInvalidArgumentError:  slice index 20 of dimension 0 out of bounds.\n         [[node strided_slice\n (defined at <ipython-input-3-250c76a76c0e>:5)\n]] [Op:__inference_f_15]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node strided_slice:\nIn[0] range (defined at <ipython-input-3-250c76a76c0e>:4)       \nIn[1] strided_slice/stack:      \nIn[2] strided_slice/stack_1:    \nIn[3] strided_slice/stack_2:\n\nOperation defined at: (most recent call last)\n# \u2026 Stack trace of the error within the function \u2026\n>>>   File \"<ipython-input-3-250c76a76c0e>\", line 7, in <module>\n>>>     f()\n>>> \n>>>   File \"<ipython-input-3-250c76a76c0e>\", line 5, in f\n>>>     return l[20]\n>>> \nThe main difference is runtime errors raised while executing a tf.function now include a stack trace which shows the source of the error, in the user\u2019s code.\n# \u2026 Original error message and information \u2026\n# \u2026 More stack frames \u2026\n>>>   File \"<ipython-input-3-250c76a76c0e>\", line 7, in <module>\n>>>     f()\n>>> \n>>>   File \"<ipython-input-3-250c76a76c0e>\", line 5, in f\n>>>     return l[20]\n>>> \nFor the \u201cGraph\u201d tensor error messages caused by the following user code\nx = None\n\n@tf.function\ndef leaky_function(a):\n global x\n x = a + 1  # Bad - leaks local tensor\n return a + 2\n\n@tf.function\ndef captures_leaked_tensor(b):\n b += x\n return b\n\nleaky_function(tf.constant(1))\ncaptures_leaked_tensor(tf.constant(2))\nA summary of the old error message looks like\n# \u2026 Python stack trace of the function call \u2026\n\nTypeError: An op outside of the function building code is being passed\na \"Graph\" tensor. It is possible to have Graph tensors\nleak out of the function building context by including a\ntf.init_scope in your function building code.\nFor example, the following function will fail:\n  @tf.function\n  def has_init_scope():\n    my_constant = tf.constant(1.)\n    with tf.init_scope():\n      added = my_constant * 2\nThe graph tensor has name: add:0\nA summary of the new error message looks like\n# \u2026 Python stack trace of the function call \u2026\n\nTypeError: Originated from a graph execution error.\n\nThe graph execution error is detected at a node built at (most recent call last):\n# \u2026 Stack trace of the error within the function \u2026\n>>>  File <ipython-input-5-95ca3a98778f>, line 6, in leaky_function\n# \u2026 More stack trace of the error within the function \u2026\n\nError detected in node 'add' defined at: File \"<ipython-input-5-95ca3a98778f>\", line 6, in leaky_function\n\nTypeError: tf.Graph captured an external symbolic tensor. The symbolic tensor 'add:0' created by node 'add' is captured by the tf.Graph being executed as an input. But a tf.Graph is not allowed to take symbolic tensors from another graph as its inputs. Make sure all captured inputs of the executing tf.Graph are not symbolic tensors. Use return values, explicit Python locals or TensorFlow collections to access it. Please see https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values for more information.\nThe main difference is errors for attempting to capture a tensor that was leaked from an unreachable graph now include a stack trace which shows where the tensor was created in the user\u2019s code:\n# \u2026 Original error message and information \u2026\n# \u2026 More stack frames \u2026\n>>>  File <ipython-input-5-95ca3a98778f>, line 6, in leaky_function\n\nError detected in node 'add' defined at: File \"<ipython-input-5-95ca3a98778f>\", line 6, in leaky_function\n\nTypeError: tf.Graph captured an external symbolic tensor. The symbolic tensor 'add:0' created by node 'add' is captured by the tf.Graph being executed as an input. But a tf.Graph is not allowed to take symbolic tensors from another graph as its inputs. Make sure all captured inputs of the executing tf.Graph are not symbolic tensors. Use return values, explicit Python locals or TensorFlow collections to access it. Please see https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values for more information.\nIntroducing tf.experimental.ExtensionType\nUser-defined types can make your projects more readable, modular, maintainable. TensorFlow 2.7.0 introduces the ExtensionType API, which can be used to create user-defined object-oriented types that work seamlessly with TensorFlow's APIs. Extension types are a great way to track and organize the tensors used by complex models. Extension types can also be used to define new tensor-like types, which specialize or extend the basic concept of \"Tensor.\" To create an extension type, simply define a Python class with tf.experimental.ExtensionType as its base, and use type annotations to specify the type for each field:\nclass TensorGraph(tf.experimental.ExtensionType):\n  \"\"\"A collection of labeled nodes connected by weighted edges.\"\"\"\n  edge_weights: tf.Tensor                      # shape=[num_nodes, num_nodes]\n  node_labels: typing.Mapping[str, tf.Tensor]  # shape=[num_nodes]; dtype=any\n\nclass MaskedTensor(tf.experimental.ExtensionType):\n  \"\"\"A tensor paired with a boolean mask, indicating which values are valid.\"\"\"\n  values: tf.Tensor\n  mask: tf.Tensor       # shape=values.shape; false for missing/invalid values.\n\nclass CSRSparseMatrix(tf.experimental.ExtensionType):\n  \"\"\"Compressed sparse row matrix (https://en.wikipedia.org/wiki/Sparse_matrix).\"\"\"\n  values: tf.Tensor     # shape=[num_nonzero]; dtype=any\n  col_index: tf.Tensor  # shape=[num_nonzero]; dtype=int64\n  row_index: tf.Tensor  # shape=[num_rows+1]; dtype=int64\nThe ExtensionType base class adds a constructor and special methods based on the field type annotations (similar to typing.NamedTuple and @dataclasses.dataclass from the standard Python library). You can optionally customize the type by overriding these defaults, or adding new methods, properties, or subclasses.\nExtension types are supported by the following TensorFlow APIs:\nKeras: Extension types can be used as inputs and outputs for Keras Models and Layers.\nDataset: Extension types can be included in Datasets, and returned by dataset Iterators.\nTensorFlow hub: Extension types can be used as inputs and outputs for tf.hub modules.\nSavedModel: Extension types can be used as inputs and outputs for SavedModel functions.\ntf.function: Extension types can be used as arguments and return values for functions wrapped with the @tf.function decorator.\ncontrol flow: Extension types can be used by control flow operations, such as tf.cond and tf.while_loop. This includes control flow operations added by autograph.\ntf.py_function: Extension types can be used as arguments and return values for the func argument to tf.py_function.\nTensor ops: Extension types can be extended to support most TensorFlow ops that accept Tensor inputs (e.g., tf.matmul, tf.gather, and tf.reduce_sum), using dispatch decorators.\ndistribution strategy: Extension types can be used as per-replica values.\nFor more information about extension types, see the Extension Type guide.\nNote: The tf.experimental prefix indicates that this is a new API, and we would like to collect feedback from real-world usage; barring any unforeseen design issues, we plan to migrate ExtensionType out of the experimental package in accordance with the TF experimental policy.\nTF2 Migration made easier!\nTo support users interested in migrating their workloads from TF1 to TF2, we have created a new Migrate to TF2 tab on the TensorFlow website, which includes updated guides and completely new documentation with concrete, runnable examples in Colab.\nA new shim tool has been added which dramatically eases migration of variable_scope-based models to TF2. It is expected to enable most TF1 users to run existing model architectures as-is (or with only minor adjustments) in TF2 pipelines without having to rewrite your modeling code. You can learn more about it in the model mapping guide.\nNew community contributed models on TensorFlow Hub\nSince the last TensorFlow release, the community really came together to make many new models available on TensorFlow Hub. Now you can find models like MLP-Mixer, Vision Transformers, Wav2Vec2, RoBERTa, ConvMixer, DistillBERT, YoloV5 and many more. All of these models are ready to use via TensorFlow Hub. You can learn more about publishing your models here.\nNext steps\nCheck out the release notes for more information. To stay up to date, you can read the TensorFlow blog, follow twitter.com/tensorflow, or subscribe to youtube.com/tensorflow. If you\u2019ve built something you\u2019d like to share, please submit it for our Community Spotlight at goo.gle/TFCS. For feedback, please file an issue on GitHub or post to the TensorFlow Forum. Thank you!",
    "link": "https://blog.tensorflow.org/2021/11/whats-new-in-tensorflow-27.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/a/AVvXsEjhvq3skq8o7UfnWmKelk18VfYYQXLeKSIjdGWbxb6hMPIHVyjp_sHCRtpXPG3tGTSNszGJv68CBNzNUecHWmYBQqAvh38TjQR1eIlzQ80zEl6wcTsq4zitrjwvF2YKD_PqpQMDauTEuxAgDT78WdGY_Ah_ALER7wytvXzWAwLVKrzE0akmKjVaeiNK",
      "https://blogger.googleusercontent.com/img/a/AVvXsEinZC9BtI7eGeeCKCPjXq-tP8pF_lgNRIo7k43X1q3Tfmh8hQjDpLFiNdA0DbEEe26o8pMKLRvAOC27baPIrn6Y9AGV-Wb02nHTxklndGdtMQyE_fvSBLjm9aV5hiO0QUWTAOxwBB5GEW-3qdurUK1BtANPtUHmgyO86Trq_tTzEu-zq2h-WDiNjRGe",
      "https://blogger.googleusercontent.com/img/a/AVvXsEjhvq3skq8o7UfnWmKelk18VfYYQXLeKSIjdGWbxb6hMPIHVyjp_sHCRtpXPG3tGTSNszGJv68CBNzNUecHWmYBQqAvh38TjQR1eIlzQ80zEl6wcTsq4zitrjwvF2YKD_PqpQMDauTEuxAgDT78WdGY_Ah_ALER7wytvXzWAwLVKrzE0akmKjVaeiNK"
    ],
    "time": "2023/12/10 00:58:43"
  },
  {
    "title": "Introducing TensorFlow Similarity",
    "content": "Posted by Elie Bursztein and Owen Vallis, Google\nToday we are releasing the first version of TensorFlow Similarity, a python package designed to make it easy and fast to train similarity models using TensorFlow.\nExamples of nearest neighbor searches performed on the embeddings generated by a similarity model trained on the Oxford IIIT Pet Dataset\nThe ability to search for related items has many real world applications, from finding similar looking clothes, to identifying the song that is currently playing, to helping rescue missing pets. More generally, being able to quickly retrieve related items is a vital part of many core information systems such as multimedia searches, recommender systems, and clustering pipelines.\nSimilarity models learn to output embeddings that project items in a metric space where similar items are close together and far from dissimilar ones\nUnder the hood, many of these systems are powered by deep learning models that are trained using contrastive learning. Contrastive learning teaches the model to learn an embedding space in which similar examples are close while dissimilar ones are far apart, e.g., images belonging to the same class are pulled together, while distinct classes are pushed apart from each other. In our example, all the images from the same animal breed are pulled together while different breeds are pushed apart from each other.\nOxford-IIIT Pet dataset visualization using the Tensorflow Similarity projector\nWhen applied to an entire dataset, contrastive losses allow a model to learn how to project items into the embedding space such that the distances between embeddings are representative of how similar the input examples are. At the end of training you end up with a well clustered space where the distance between similar items is small and the distance between dissimilar items is large. For example, as visible above, training a similarity model on the Oxford-IIIT Pet dataset leads to meaningful clusters where similar looking breeds are close-by and cats and dogs are clearly separated.\nFinding related items involve computing the query image embedding, performing an ANN search to find similar items and fetching similar items metadata including the images bytes.\nOnce the model is trained, we build an index that contains the embeddings of the various items we want to make searchable. Then at query time, TensorFlow Similarity leverages Fast Approximate Nearest Neighbor search (ANN) to retrieve the closest matching items from the index in sub-linear time. This fast look up leverages the fact that TensorFlow Similarity learns a metric embedding space where the distance between embedded points is a function of a valid distance metric. These distance metrics satisfy the triangle inequality, making the space amenable to Approximate Nearest Neighbor search and leading to high retrieval accuracy.\nOther approaches, such as using model feature extraction, require the use of an exact nearest neighbor search to find related items and may not be as accurate as a trained similarity model. This prevents those methods scaling as performing an exact search requires a quadratic time in the size of the search index. In contrast, TensorFlow Similarity\u2019s built-in Approximate Nearest Neighbor indexing system, which relies on the NMSLIB, makes it possible to search over millions of indexed items, retrieving the top-K similar matches within a fraction of second.\nBeside accuracy and retrieval speed, the other major advantage of similarity models is that they allow you to add an unlimited new number of classes to the index without having to retrain. Instead you only need to compute the embeddings for representative items of the new classes and add them to the index. This ability to dynamically add new classes is particularly useful when tackling problems where the number of distinct items is unknown ahead of time, constantly changing, or is extremely large. An example of this would be enabling users to discover newly released music that is similar to songs they have liked in the past.\nTensorFlow Similarity provides all the necessary components to make similarity training evaluation and querying intuitive and easy. In particular, as illustrated below, TensorFlow Similarity introduces the SimilarityModel(), a new Keras model that natively supports embedding indexing and querying. This allows you to perform end-to-end training and evaluation quickly and efficiently..\nA minimal example that trains, indexes and searches on MNIST data can be written in less than 20 lines of code:\nfrom tensorflow.keras import layers\n\n# Embedding output layer with L2 norm\nfrom tensorflow_similarity.layers import MetricEmbedding \n# Specialized metric loss\nfrom tensorflow_similarity.losses import MultiSimilarityLoss \n# Sub classed keras Model with support for indexing\nfrom tensorflow_similarity.models import SimilarityModel\n# Data sampler that pulls datasets directly from tf dataset catalog\nfrom tensorflow_similarity.samplers import TFDatasetMultiShotMemorySampler\n# Nearest neighbor visualizer\nfrom tensorflow_similarity.visualization import viz_neigbors_imgs\n\n\n# Data sampler that generates balanced batches from MNIST dataset\nsampler = TFDatasetMultiShotMemorySampler(dataset_name='mnist', classes_per_batch=10)\n\n# Build a Similarity model using standard Keras layers\ninputs = layers.Input(shape=(28, 28, 1))\nx = layers.Rescaling(1/255)(inputs)\nx = layers.Conv2D(64, 3, activation='relu')(x)\nx = layers.Flatten()(x)\nx = layers.Dense(64, activation='relu')(x)\noutputs = MetricEmbedding(64)(x)\n\n# Build a specialized Similarity model\nmodel = SimilarityModel(inputs, outputs)\n\n# Train Similarity model using contrastive loss\nmodel.compile('adam', loss=MultiSimilarityLoss())\nmodel.fit(sampler, epochs=5)\n\n# Index 100 embedded MNIST examples to make them searchable\nsx, sy = sampler.get_slice(0,100)\nmodel.index(x=sx, y=sy, data=sx)\n\n# Find the top 5 most similar indexed MNIST examples for a given example\nqx, qy = sampler.get_slice(3713, 1)\nnns = model.single_lookup(qx[0])\n\n# Visualize the query example and its top 5 neighbors\nviz_neigbors_imgs(qx[0], qy[0], nns)\nEven though the code snippet above uses a sub-optimal model, it still yields good matching results where the nearest neighbors clearly looks like the queried digit as visible in the screenshot below:\nThis initial release focuses on providing all the necessary components to help you build contrastive learning based similarity models, such as losses, indexing, batch samplers, metrics, and tutorials. TF Similarity also makes it easy to work with the Keras APIs and use the existing Keras Architectures. Moving forward, we plan to build on this solid foundation to support semi-supervised and self-supervised methods such as BYOL, SWAV, and SimCLR.\nYou can start experimenting with TF Similarity right away by heading to the Hello World tutorial. For more information you can check out the project Github.",
    "link": "https://blog.tensorflow.org/2021/09/introducing-tensorflow-similarity.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-eQvYkk8aoCk/YTqG_fflJEI/AAAAAAAAEfQ/iczbXhhdTZsfpdVoMcNUwHnd61wOv9tigCLcBGAsYHQ/s0/TF%2Bsimilarity%2Btwitter.jpg",
      "https://1.bp.blogspot.com/-QTSZQKVJNO4/YTufdQr2FII/AAAAAAAAEfY/ccaIGeBka5EBdoHO5SUgBBY7kfTwsRpcwCLcBGAsYHQ/s0/tensorflow-similarity-output-visualization.jpg",
      "https://1.bp.blogspot.com/-liPhVjhtUZs/YTuhf0rlVgI/AAAAAAAAEfg/KhByF0kNDYACktWZ6r505yz8FS7J7OYCACLcBGAsYHQ/s0/similarity-learning-overview.png",
      "https://1.bp.blogspot.com/-eQvYkk8aoCk/YTqG_fflJEI/AAAAAAAAEfQ/iczbXhhdTZsfpdVoMcNUwHnd61wOv9tigCLcBGAsYHQ/s0/TF%2Bsimilarity%2Btwitter.jpg",
      "https://1.bp.blogspot.com/-K7w8UP6f9iY/YTuiqovSFUI/AAAAAAAAEfo/CYXeMifX8gwQm5iY3_XtpccQo_aUnYySQCLcBGAsYHQ/s0/tensorflow-similarity-overview.png",
      "https://1.bp.blogspot.com/-MR20zQ_bQss/YTujnFD-LpI/AAAAAAAAEfw/qypb1ENTIec64-wk-iJN62An1ERzgLcdwCLcBGAsYHQ/s0/image2.png"
    ],
    "time": "2023/12/10 00:58:43"
  },
  {
    "title": "Easy Machine Learning for On-Device Audio",
    "content": "Posted by Luiz GUStavo Martins, Developer Advocate\nAt Google I/O, we shared a set of tutorials to help you use machine learning on audio. In this blog post you'll find resources to help you develop and customize an audio classification model for your app, and a couple of real world examples for inspiration.\nMachine learning for audio\nSound and audio are sometimes used interchangeably, but they have a key difference. Sound is in essence what you can hear while audio is the sound's electronic representation. That's why we usually use the term audio when talking about machine learning.\nMachine Learning for audio can be used to:\nUnderstand speech\nUnderstand musical instruments\nClassify events (which bird is that?)\nDetect pitch\nGenerate music\nIn this post we will focus on audio classification of events, a common scenario in practice with many real world applications like NOAA creating a humpback whale acoustic detector, and the Zoological Society of London using audio recognition to protect wildlife.\nA number of classification models are available for you to try right now on TensorFlow Hub (YAMNet, Whale detection).\nAudio recognition can also run completely on-device. For example, Android has a sound notifications feature that provides push notification for important sounds around you. It can also detect which music is playing, or even help with an ML-powered audio recorder app that can transcribe conversations on-device.\nHaving the models is only the beginning. Now you might ask:\nHow do I use them on my app?\nHow do I customize them for my audio use case?\nDeploying machine learning models on-device\nImagine you have an audio classification model ready, such as a pretrained one from TF-Hub, how would you use this in a mobile app? To help you integrate audio classification into your app we created the TensorFlow Lite Task Library. The Audio Classifier component was released and you only need a couple of lines of code to add audio classification to your application:\n// Initialization\nval classifier = AudioClassifier.createFromFile(this, modelPath)\n\n// Start recording\nval record = classifier.createAudioRecord()\nrecord.startRecording()\n\n// Load latest audio samples\nval tensor = classifier.createInputTensorAudio()\ntensor.load(record);\n\n// Run inference\nval output = classifier.classify(tensor)\nThe library takes care of loading the model to memory, to create the audio recorder with the proper model specifications (sample rate, bit rate) and the classification method to get the model's inference results. Here you can find a full sample to get some inspiration.\nCustomizing the models\nWhat if you need to recognize audio events that are not in the set provided by the pretrained models? Or if you need to specialize them to fewer classes? In these situations, you need to fine tune the model using a technique called Transfer Learning.\nThis is a very popular process and you don't need to be an expert on machine learning to be able to do it. You can use Model Maker to help you with this.\nspec = audio_classifier.YamNetSpec()\ndata = audio_classifier.DataLoader.from_folder(spec, DATA_DIR)\n \ntrain_data, validation_data = data.split(0.8)\nmodel = audio_classifier.create(train_data, spec, validation_data)\n \nmodel.export(models_path)\nYou can find complete code here. The output model can be directly loaded by the Task Library. And Model Maker can customize models not only for audio but also for image, text and recommendation system\nSummary\nMachine learning for audio is an exciting field and with many possibilities, enabling many new features. Doing ML on-device is getting easier and faster with tools like TensorFlow Lite Task Library and customization can be done without expertise in the field with Model Maker.\nYou can learn more about it on our new On-Device Machine Learning website (the audio path is here). You'll find tutorials, codelabs and lots of resources on how to do not only audio related tasks but also for image (classification, object detection) and text (classification, entity extraction, question and answer)\nYou can share with us what you build by adding #TensorFlow on your social network post with your project, or submit it for the TensorFlow community spotlight program. And if you have any questions, you can ask them on discuss.tensorflow.org.",
    "link": "https://blog.tensorflow.org/2021/09/easy-machine-learning-for-on-device-audio.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-4FSri3A9wEY/YTe78NC-vMI/AAAAAAAAEeQ/H7C5WqUoOp85IDBUfE95s9flShL79IwEACLcBGAsYHQ/s0/final_60e5ccb2d87a380071412c80_917378.gif"
    ],
    "time": "2023/12/10 00:58:43"
  },
  {
    "title": "Build fast, sparse on-device models with the new TF MOT Pruning API",
    "content": "Posted by Yunlu Li and Artsiom Ablavatski\nIntroduction\nPruning is one of the core optimization techniques provided in the TensorFlow Model Optimization Toolkit (TF MOT). Not only does it help to significantly reduce model size, but it can also be used to accelerate CPU inference on mobile and web. With modern compute intensive models, the area of pruning as a model optimization technique has drawn significant attention, demonstrating that dense networks can be easily pruned (i.e. a fraction of the weights set to zero) with negligible quality degradation. Today, we are excited to announce a set of updates to TF MOT Pruning API that simplify pruning and enable developers to build sparse models for fast on-device inference.\nUpdates to TF MOT\nTensorFlow has long standing support for neural network pruning via TensorFlow Model Optimization Toolkit (TF MOT) Pruning API. The API, featured in 2019, introduced essential primitives for pruning, and enabled researchers throughout the world with new optimization techniques. Today we are happy to announce experimental updates to the API that further advance model pruning. We are releasing tools that simplify the control of pruning and enable latency reduction for on-device inference.\nThe TF MOT Pruning API has extensive functionality that provides the user with tools for model manipulation:\nprune_low_magnitude function applies PruneLowMagnitude wrapper to every layer in the model\nPruneLowMagnitude wrapper handles low-level pruning logic\nPruningSchedule controls when pruning is applied\nPruningSummaries callback logs the pruning progress\nThese abstractions allow to control almost any aspect of model pruning (i.e. how to prune (PruneLowMagnitude), when to prune (PruningSchedule) and how to track the progress of the pruning (PruningSummaries) with the exception of what to prune, i.e. where PruneLowMagnitude wrapper is applied. We are happy to release an extension of TF MOT PruningPolicy, a class that controls which parts of the model the PruneLowMagnitude wrapper is applied to. The instance of PruningPolicy is used as an argument in the prune_low_magnitude function and provides the following functionalities:\nControls where the pruning wrapper should be applied on per-layer basis through the allow_pruning function\nChecks that the whole model supports pruning via ensure_model_supports_pruning function\nPruningPolicy is an abstract interface, and it can have many implementations depending on the particular application. For latency improvements on CPU via XNNPACK, the concrete implementation PruneForLatencyOnXNNPack applies the pruning wrapper only to the parts of the model that can be accelerated via sparse on-device inference while leaving the rest of the network untouched. Such selective pruning allows an application to maintain model quality while targeting parts of the model that can be accelerated by sparsity.\nThe below example showcases the PruneForLatencyOnXNNPack policy in action on\nMobileNetV2 (the full example is available in a recently introduced colab):\nimport tensorflow as tf\nimport tensorflow_model_optimization as tfmot\nprune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n\n# See the implementation of the function below.\nmodel = load_model_for_pruning()\n\nmodel_for_pruning = prune_low_magnitude(\n    model, pruning_policy=tfmot.sparsity.keras.PruneForLatencyOnXNNPack())\nIn order to comply with the constraints of XNNPACK sparse inference the Keras implementation of MobileNetV2 model requires slight modification of the padding for the first convolution operation:\ndef load_model_for_pruning():\n  input_tensor = tf.keras.layers.Input((224, 224, 3))\n  input_tensor = tf.keras.layers.ZeroPadding2D(1)(input_tensor)\n  model = tf.keras.applications.MobileNetV2(input_tensor=input_tensor)\n\n  def clone_fn(layer):\n    if layer.name == 'Conv1':\n      # The default padding `SAME` for the first convolution is incompatible\n      # with XNNPACK sparse inference.\n      layer.padding = 'valid'\n      # We ask the model to rebuild since we've changed the padding parameter.\n      layer.built = False\n    return layer\n\n  return tf.keras.models.clone_model(model, clone_function=clone_fn)\nThe PruneForLatencyOnXNNPack policy applies the pruning wrapper only to convolutions with 1x1 kernel size since only these layers can be accelerated on CPU by as much as 2x using XNNPACK. The rest of the layers are left untouched allowing the network to recover after quality degradation at the pruning step. Also, the policy verifies that the model is amenable to being pruned by using the ensure_model_supports_pruning method. Once the sparse model has been trained and converted, we recommend using the TensorFlow Lite benchmark utility in debug mode to confirm that the final model is compatible with XNNPack\u2019s sparse inference backend.\nWe hope that this newly introduced experimental API will be useful in practice and we will continue to improve its stability and flexibility in the future.\nCompression and Latency Improvements\nModel compression is another major benefit of applying pruning to a model. Using a smart compression format allows efficient storage of model weights which leads to a significant size reduction.\nTFLite adopted the TACO format to encode sparse tensors. Compared to widely used formats like CSR and CSC, the TACO format has several advantages:\nIt supports flexible traversal order to store a tensor in row-major or column-major formats easily.\nIt supports multi-dimensional sparse tensors like the 4-D filter of a convolution op.\nIt can represent block structure as the inner dimension of the tensor (example of a 4x4 tensor with 2x2 inner block structure).\nWe also adapted the format to use flexible data types for the metadata storing the indices of non-zero elements. This reduces the storage overhead for small tensors, or tensors with compact data types like int8_t.\nIn order to realize size reductions in practice during the model conversion, the tf.lite.Optimize.EXPERIMENTAL_SPARSITY optimization needs to be applied. This optimization handles examining the model for sparse tensors and converting them to an efficient storage format. It also works seamlessly with quantization and you can combine them to achieve more aggressive model compresion. The full example of such a conversion is shown below:\n# Remove the pruning wrappers from the model. \nmodel = tfmot.sparsity.keras.strip_pruning(model)\n\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\n# We apply float16 quantization together with sparsity optimization that\n# compactly stores pruned weights.\nconverter.optimizations = [\n    tf.lite.Optimize.EXPERIMENTAL_SPARSITY,  # Enables size reduction optimization. \n    tf.lite.Optimize.DEFAULT  # Enables quantization at conversion.\n]\nconverter.target_spec.supported_types = [tf.float16]\ntflite_buffer = converter.convert()\nAfter applying the tf.lite.Optimize.EXPERIMENTAL_SPARSITY optimization together with PruneForLatencyOnXNNPack pruning policy, a ~2x size reduction can be achieved as is demonstrated in Figure 1:\nFigure 1. Ablation study of MobileNetV2 model size (float32 and float16 types) with different sparsity levels using PruneForLatencyOnXNNPack pruning policy. Only the 1x1 convolutional layers are pruned and the rest of the layers are left dense.\nIn addition to size reduction, pruning can provide inference acceleration on CPU via XNNPACK. Using the PruneForLatencyOnXNNPack pruning policy, we\u2019ve conducted an ablation study of CPU inference latency for a MobileNetV2 model on Pixel 4 using TensorFlow Lite benchmark with the use_xnnpack option enabled:\nFigure 2. Ablation study of CPU inference speed of MobileNetV2 model with different sparsity levels on a Pixel 4 device.\nThis study in Figure 2 demonstrates 1.7x latency improvement when running on mobile devices using XNNPACK. The strategies for training the sparse MobileNetV2 model together with hyperparameters and pre-trained checkpoints are described in Elsen et al.\nPruning techniques & tips\nPruning aware training is a key step in model optimization. Many hyperparameters are involved in training and some of them like the pruning schedule and learning rate can have a dramatic impact on the final quality of the model. Though many strategies have been proposed, a simple yet effective 3-steps strategy (see Table 1) achieves strong performance for the majority of our use cases. The strategy builds on top of the well-proven approach from Zhu & Gupta and produces good results without extensive re-training:\nStep\nLearning rate\nDuration\nNotes\n1. Pre-training or\nusing pre-trained weights (optional)\nThe same as for the regular dense network: starting from high value (possibly with warm-up) and ending with low value\nThe same as for the regular dense network\nPaired with weight decay regularization this step helps the model to push unimportant weights towards 0 for pruning in the next step\n2. Iterative pruning\nConstant, mean of the learning rate values for the regular training\n30 to 100 epochs\nIterative pruning step during which weights become sparse\n3. Fine-tuning\nThe same as at the first stage but without warm up stage\nThe same as at the first stage\nHelps to mitigate quality degradation after the pruning step\n3-step schedule for training the sparse model\nThe strategy inevitably leads to a substantial increase (~3x) in the training time. However, paired with the PolynomialDecay pruning schedule, this 3-step strategy achieves limited or no quality degradation with significantly pruned (>70%) neural networks.\nPruned models in MediaPipe\nTogether with the updates to the TF MOT Pruning API, we are happy to release pruned models for some of the MediaPipe solutions. The released models include pose and face detectors as well as a pruned hand tracking model. All of these models have been trained with the newly introduced functionality using the 3-steps pruning strategy. Compared with dense baselines the released pruned models have demonstrated significant model size reduction as well as superior performance when running on CPU via XNNPACK. Quality-wise the pruned models achieve similar metrics including in the evaluation on our fairness datasets (see model cards for details). Side-by-side demos of the solutions are shown below:\nFigure 3. Comparison of dense (left) and sparse (right) models in the end-to-end examples of face (top) and pose (bottom) detection\nPruning for GPU\nWhile exploiting sparsity on GPUs can be challenging, recent work has made progress in improving the performance of sparse operations on these platforms. There is momentum for adding first-class support for sparse matrices and sparse operations in popular frameworks, and state-of-the-art GPUs have recently added hardware acceleration for some forms of structured sparsity. Going forward, improvements in software and hardware support for sparsity in both training and inference will be a key contributor to progress in the field.\nFuture directions\nTF MOT offers a variety of model optimization methods, many of which have proven to be essential for efficient on-device model inference. We will continue to expand the TF MOT Pruning API with algorithms beyond low magnitude pruning, and also investigate the combination of pruning and quantization techniques to achieve even better results for on-device inference. Stay tuned!\nAcknowledgments\nHuge thanks to all who worked on this project: Karthik Raveendran, Ethan Kim, Marat Dukhan, Trevor Gale, Utku Evci, Erich Elsen, Frank Barchard, Yury Kartynnik, Valentin Bazarevsky, Matsvei Zhdanovich, Juhyun Lee, Chuo-Ling Chang, Ming Guang Yong, Jared Duke and Matthias Grundmann.",
    "link": "https://blog.tensorflow.org/2021/07/build-fast-sparse-on-device-models-with-tf-mot-pruning-api.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-p5PaqqF4Wm0/YPXC8oIptHI/AAAAAAAAEX8/afoNl6LXZ6oYzbSZzjoQLqCHXjmDtxfGgCLcBGAsYHQ/s0/float_32.jpeg",
      "https://1.bp.blogspot.com/-p5PaqqF4Wm0/YPXC8oIptHI/AAAAAAAAEX8/afoNl6LXZ6oYzbSZzjoQLqCHXjmDtxfGgCLcBGAsYHQ/s0/float_32.jpeg",
      "https://1.bp.blogspot.com/-j-ljdH43lUk/YPXDeFkfPjI/AAAAAAAAEYE/9L9LjM57r0Ms73PmLi9GhVuso8DGkr6OwCLcBGAsYHQ/s0/imageLikeEmbed%2B%25282%2529.png",
      "https://1.bp.blogspot.com/-_gDTrK8m-vo/YPXK_JX8IrI/AAAAAAAAEYM/pKxrXKLMw_gMaIru45aI6bNIqRkSvsMdwCLcBGAsYHQ/s0/motion%2Btrack%2Bgif.gif",
      "https://1.bp.blogspot.com/-9gZXSpoWVMI/YPXLcHOMvmI/AAAAAAAAEYU/wdfYEzLIRB8eujAQiBSMRgbQr3IRenxhACLcBGAsYHQ/s0/motion%2Btrack%2Bgif%2B2.gif"
    ],
    "time": "2023/12/10 00:58:43"
  },
  {
    "title": "TensorFlow Hub for Real World Impact",
    "content": "Posted by Luiz GUStavo Martins and Elizabeth Kemp on behalf of the TensorFlow Hub team\nAs a developer, when you\u2019re faced with a challenge, you might think: \u201cHow can I solve this?\", or, \"What is the right tool for the job?\u201d For a growing number of situations, machine learning can help! ML can solve many tasks that would be very challenging using classical programming, for example: detecting objects in images, classifying sound events, or understanding text.\nBut training machine learning models may take a lot of time, use large amounts of data, require deep expertise in the field, and be resource intensive. What if instead of starting from scratch, someone has already solved the same problem you have? Or at least solved a very similar problem that could give you a good starting point? This is where TensorFlow Hub can help you!\nTensorFlow Hub is an open repository of pre-trained machine learning models ready for fine-tuning and deployable anywhere, from servers to mobile devices, microcontrollers and browsers.\nDevelopers are using models available from TF Hub to solve real world problems across many domains, and at Google I/O 2021 we highlighted some examples of developers changing the world using models from TensorFlow Hub.\nIn this article, we\u2019ll cover these use cases as well, with links so you can check them out.\nImages\nImage classification is one of the most popular use cases for machine learning. The development of this field helped the whole of machine learning by showing very good results and pushing the boundaries of research.\nTensorFlow Hub has many models for the image problem domain for tasks like image classification, object detection, image segmentation, pose detection, style transfer and many others.\nMany of the available models have a visualizer, like the one below, right on their documentation page, enabling you to try the model without any code or downloading anything.\nTFHub makes Transfer Learning simpler and easier to experiment with many state of the art models like MobilenetV3, EfficientNet V2 to find the best one for your data. A real world use case can be seen in this CropNet tutorial to create the best model possible to detect diseases in cassava leaves and deploy it on device for use in the field.\nText\nUnderstanding text has always been a very challenging task for computers because of all the context that is necessary, and the large number of words and phrases. Many state of the art Natural Language Processing (NLP) models are available on TensorFlow Hub and ready for you to use.\nOne example is the BERT family of models. Using them from TFHub is easier than ever. Aside from the base BERT model, there are more advanced versions and in many languages ready to be used like you can see here in Making BERT Easier with Preprocessing Models From TensorFlow Hub.\nOne good example is the MuRIL model that is a multilingual BERT model trained on 17 Indian languages used by developers to solve local NLP challenges in India.\nAn animation of the preprocessing model that makes it easy for you to input text into BERT.\nDevelopers can also use the TF Hub spam detection model for detecting spam comments in online forums. The model is available for TF.js and TFLite for running in the browser and on-device.\nAudio\nTF Hub has many audio models that you can use on desktop, for on-device inference on mobile devices, or on the web. There are also audio embedding models that can be used with transfer learning which you can adapt to your own data.\nDevelopers are using audio classification to understand what's happening on a forest (How ZSL uses ML to classify gunshots to protect wildlife) or inside the ocean (Acoustic Detection of Humpback Whales Using a Convolutional Neural Network) or even closer to us, understanding what is happening in your own home (Important household sounds become more accessible).\nVideo\nVideo processing is increasingly important and TensorFlow Hub also has models for this domain like the MoViNet collection that can do video classification or the I3D for action recognition.\nTFHub also has tutorials for Action recognition, Video Interpolation and Text-to-video retrieval.\nSummary\nReusing code is usually better than re-writing it. The same applies to machine learning models. If you can use a pre-trained model for a task, it can save you time, resources, and help you make an impact in the world. TensorFlow Hub has thousands of models available for you to deploy or customize to your task with transfer learning.\nIf you want to know more about how to use TensorFlow Hub and find great tutorials, check out the documentation at tensorflow.org/hub. To find models for your own real world impact, search on tfhub.dev.\nLet us know what you build and also share with the community. You can talk to the team on the TensorFlow Forum and find a community that is eager to help!",
    "link": "https://blog.tensorflow.org/2021/07/tensorflow-hub-for-real-world-impact.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-PBE41Sm9dWc/YN9Exu2Os1I/AAAAAAAAEXE/B8gxq7GR1WQ9qTmiiYzPfzap2Q83Pet2QCLcBGAsYHQ/s0/TF%2BBLOG%2Bfor%2Breal%2Bworld%2Bimpact.gif",
      "https://1.bp.blogspot.com/-PBE41Sm9dWc/YN9Exu2Os1I/AAAAAAAAEXE/B8gxq7GR1WQ9qTmiiYzPfzap2Q83Pet2QCLcBGAsYHQ/s0/TF%2BBLOG%2Bfor%2Breal%2Bworld%2Bimpact.gif",
      "https://1.bp.blogspot.com/-RvrHAhJ1T6A/YOXOSS6SEGI/AAAAAAAAEXc/VLdOExrgmSwk7EvAk8-3BNVb8W99nvfQQCLcBGAsYHQ/s0/final_60e5ccb2d87a380071412c80_917378.gif",
      "https://1.bp.blogspot.com/-Dta-HU0NF_4/YN9QnnJN3cI/AAAAAAAAEXU/8UAhBGGTMmsyl9PB9g3tf_TWTrghFpixgCLcBGAsYHQ/s0/TF%2BBLOG%2Bfor%2Breal%2Bworld%2Bimpact%2B3.gif"
    ],
    "time": "2023/12/10 00:58:43"
  },
  {
    "title": "PluggableDevice: Device Plugins for TensorFlow",
    "content": "Posted by Penporn Koanantakool and Pankaj Kanwar.\nAs the number of accelerators (GPUs, TPUs) in the ML ecosystem has exploded, there has been a strong need for seamless integration of new accelerators with TensorFlow. In this post, we introduce the PluggableDevice architecture which offers a plugin mechanism for registering devices with TensorFlow without the need to make changes in TensorFlow code.\nThis PluggableDevice architecture has been designed & developed collaboratively within the TensorFlow community. It leverages the work done for Modular TensorFlow, and is built using the StreamExecutor C API. The PluggableDevice mechanism is available in TF 2.5.\nThe need for Seamless integration\nPrior to this, any integration of a new device required changes to the core TensorFlow. This was not scalable because of several issues, for example:\nComplex build dependencies and compiler toolchains. Onboarding a new compiler is nontrivial and adds to the technical complexity of the product.\nSlow development time. Changes need code reviews from the TensorFlow team, which can take time. Added technical complexity also adds to the development and testing time for new features.\nCombinatorial number of build configurations to test for. The changes made for a particular device might affect other devices or other components of TensorFlow. Each new device could increase the number of test configurations in a multiplicative manner.\nEasy to break. The lack of a contract via a well defined API means that it\u2019s easier to break a particular device.\nWhat is PluggableDevice?\nThe PluggableDevice mechanism requires no device-specific changes in the TensorFlow code. It relies on C APIs to communicate with the TensorFlow binary in a stable manner. Plug-in developers maintain separate code repositories and distribution packages for their plugins and are responsible for testing their devices. This way, TensorFlow\u2019s build dependencies, toolchains, and test process are not affected. The integration is also less brittle since only changes to the C APIs or PluggableDevice components could affect the code.\nThe PluggableDevice mechanism has four main components:\nPluggableDevice type: A new device type in TensorFlow which allows device registration from plug-in packages. It takes priority over native devices during the device placement phase.\nCustom operations and kernels: Plug-ins register their own operations and kernels to TensorFlow through the Kernel and Op Registration C API.\nDevice execution and memory management: TensorFlow manages plug-in devices through the StreamExecutor C API.\nCustom graph optimization pass: Plug-ins can register one custom graph optimization pass, which will be run after all standard Grappler passes, through the Graph Optimization C API.\nHow a device plug-in interacts with TensorFlow.\nUsing PluggableDevice\nTo be able to use a particular device, like one would a native device in TensorFlow, users only have to install the device plug-in package for that device. The following code snippet shows how the plugin for a new device, say Awesome Processing Unit (APU), would be installed and used. For simplicity, let this APU plug-in only have one custom kernel for ReLU.\n$ pip install tensorflow-apu-0.0.1-cp36-cp36m-linux_x86_64.whl\n\u2026\nSuccessfully installed tensorflow-apu-0.0.1\n$ python\nPython 3.6.9 (default, Oct  8 2020, 12:12:24) \n[GCC 8.4.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import tensorflow as tf  # TensorFlow registers PluggableDevices here\n>>> tf.config.list_physical_devices()\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:APU:0', device_type='APU')]\n\n>>> a = tf.random.normal(shape=[5], dtype=tf.float32)  # Runs on CPU\n>>> b =  tf.nn.relu(a)  # Runs on APU\n \n>>> with tf.device(\"/APU:0\"):  # Users can also use 'with tf.device' syntax\n...  c = tf.nn.relu(a)  # Runs on APU\n \n>>> @tf.function  # Defining a tf.function\n... def run():\n...    d = tf.random.uniform(shape=[100], dtype=tf.float32)  # Runs on CPU\n...    e = tf.nn.relu(d)  # Runs on APU\n>>> run()  # PluggableDevices also work with tf.function and graph mode.\nUpcoming PluggableDevices\nWe are excited to announce that Intel will be one of our first partners to release a PluggableDevice. Intel has made significant contributions to this effort, submitting over 3 RFCs implementing the overall mechanism. They will release an Intel extension for TensorFlow plugin package to bring Intel XPU to TensorFlow for AI workload acceleration. We also expect other partners to take advantage of PluggableDevice and release additional plug-ins.\nWe will publish a detailed tutorial on how to develop a PluggableDevice plug-in for partners who might be interested in leveraging this infrastructure. For questions on the PluggableDevices, engineers can post questions directly on the RFC PRs [1, 2, 3, 4, 5, 6], or on the TensorFlow Forum with the tag pluggable_device.\n\nContributors: Jianhui Li (Intel), Zhoulong Jiang (Intel), Yiqiang Li (Intel), Anna Revinskaya (Google), Yi Situ (Google), Eric Lin (Intel), AG Ramesh (Intel), Sophie Chen (Intel), Yang Sheng (Intel), Rasmus Larsen (Google), Eugene Zhulenev (Google), Jose Baiocchi Paredes (Google), Saurabh Saxena (Google), Kulin Seth (Apple), Chai Chaoweeraprasit (Microsoft), Patrice Vignola (Microsoft), Gunhan Gulsoy (Google), Russell Power (Google)",
    "link": "https://blog.tensorflow.org/2021/06/pluggabledevice-device-plugins-for-TensorFlow.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-kMJ38lnfsXY/YL5HjgcyjUI/AAAAAAAAESs/g01X-Kd-IxIS2lbYIFBD1FjvPnoOdZkYgCLcBGAsYHQ/s0/2021-05-02_PluggableDevice_Diagram%2B%25284%2529.jpeg",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiUgeT1xjk1ED1-YkvM6dGjhcZn88lMQ6ZOJBdCRFW2ROiElFMO_PWrc5gRJXnGnMqLj4VRgYplBVMHzgfQGdiYC9T4s5EyxxCS9zMcO8UKnTqGarF5VApUUGwoeyYxU6Km5L8mn7VvQzY0jkIfG8gmi8toQXYihK62ak8ekCg6MdasS6Bsd-L0Rr1t/s1600/2021-05-22_PluggableDevice-diagram.png"
    ],
    "time": "2023/12/10 00:58:43"
  },
  {
    "title": "New Courses: Machine Learning Engineering for Production",
    "content": "Posted by Robert Crowe and Jocelyn Becker\nHave you mastered the art of building and training ML models, and are now ready to use them in a production deployment for a product or service? If so, we have a new set of courses to get you going. Built as a collaboration between the TensorFlow team, Andrew Ng, and deeplearning.ai, the new set of courses are launching as a specialization on Coursera: The Machine Learning Engineering for Production (MLOps) specialization.\nThe new specialization builds on the foundational knowledge taught in the popular specialization, DeepLearning.AI TensorFlow Developer Professional Certificate, that teaches how to build machine learning models with TensorFlow. The new MLOps specialization kicks off with an introductory course taught by Andrew Ng, followed by courses taught by Robert Crowe and Laurence Moroney that dive into the details of getting your models out to users.\nEvery lesson comes with plenty of hands-on exercises that give you practice at preparing your data, and training and deploying models.\nBy the end of the specialization, you'll be ready to design and deploy an ML production system end-to-end. You'll understand project scoping, data needs, modeling strategies, and deployment requirements. You\u2019ll know how to optimize your data, models, and infrastructure to manage costs. You'll know how to validate the integrity of your data to get it ready for production use, and then prototype, develop, and deploy your machine learning models, monitor the outcomes, and update the datasets and retrain the models continuously.\nYou'll learn how to implement feature engineering, transformation, and selection with TFX as well as how to use analytics to address model fairness and explainability issues, and how to mitigate bottlenecks. You'll also explore different scenarios and case studies of ML in practice, from personalization systems to automated vehicles.\nYou'll learn how processing requirements are different in deployment than in training\nYou'll learn about different tools and platforms for deploying your machine learning systems.\nA common use of ML in production is personalization systems for product recommendations.\nA cutting edge use of ML in practice is to guide automated vehicles.\nDespite the growing recognition of AI/ML as a crucial pillar of digital transformation, successful ML deployments are a bottleneck for getting value from AI. For example, 72% of a cohort of organizations that began AI pilots before 2019 haven't deployed even a single application in production. A survey by Algorithmia of the state of enterprise machine learning found that 55% of companies surveyed haven't deployed an ML model.\nModels don\u2019t make it into production and if they do, they break because they fail to adapt to changes in the environment. Deloitte identified lack of talent and integration issues as factors that can stall or derail AI initiatives. This is where ML engineering and MLOps are essential. ML engineering provides a superset of the discipline of software engineering that handles the unique complexities of the practical applications of ML. MLOps is a methodology for ML engineering that unifies ML system development (the ML element) with ML system operations (the Ops element).\nUnfortunately, job candidates with ML engineering and MLOps skills are relatively hard to find and expensive to hire. Our new MLOps specialization teaches a broad range of many of the skills necessary to work in this field, and will help prepare developers for current and future workplace challenges. We believe that this is a valuable contribution to the ML community, and we\u2019re excited to make it available.\nEnroll today to develop your machine learning engineering skills, and learn how to roll out your ML models to benefit your company and your users.",
    "link": "https://blog.tensorflow.org/2021/06/mlep-courses.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-ZRe5dZBzdTc/YK2YuyyhzZI/AAAAAAAAEP0/Wl76cO-7C7cvv5rvLCBfS18O5MRwijBowCLcBGAsYHQ/s0/CAIP.jpg",
      "https://1.bp.blogspot.com/-ZRe5dZBzdTc/YK2YuyyhzZI/AAAAAAAAEP0/Wl76cO-7C7cvv5rvLCBfS18O5MRwijBowCLcBGAsYHQ/s0/CAIP.jpg",
      "https://1.bp.blogspot.com/-AQkRN8h3Xus/YK2U0IK0yuI/AAAAAAAAEPU/hxOnPczgM94naEocvz-vRl2RshyhVC52QCLcBGAsYHQ/s0/Images%2Bfor%2BMLEP%2Bblog%2Bpost.jpg",
      "https://1.bp.blogspot.com/-QssmUIWAUtI/YK2VUgbNO0I/AAAAAAAAEPc/j9CAutPuBiEHoTHhLGSoEATguGTxoIciQCLcBGAsYHQ/s0/Images%2Bfor%2BMLEP%2Bblog%2Bpost%2B%25281%2529.jpg",
      "https://1.bp.blogspot.com/-EfWjQMdVez4/YK2XCeTPuXI/AAAAAAAAEPk/Vs24kTGesu0BEVYVRLA1tzE7oa9L8m2UgCLcBGAsYHQ/s0/Images%2Bfor%2BMLEP%2Bblog%2Bpost%2B%25282%2529.jpg",
      "https://1.bp.blogspot.com/-vAXAxXTo9J0/YK2XbLvS5_I/AAAAAAAAEPs/UA78gH_CtGkcHIG_DyKD2pL-6ahhbiaVgCLcBGAsYHQ/s0/Images%2Bfor%2BMLEP%2Bblog%2Bpost%2B%25283%2529.jpg"
    ],
    "time": "2023/12/10 00:58:43"
  },
  {
    "title": "Introducing TensorFlow Decision Forests",
    "content": "Posted by Mathieu Guillame-Bert, Sebastian Bruch, Josh Gordon, Jan Pfeifer\nWe are happy to open source TensorFlow Decision Forests (TF-DF). TF-DF is a collection of production-ready state-of-the-art algorithms for training, serving and interpreting decision forest models (including random forests and gradient boosted trees). You can now use these models for classification, regression and ranking tasks - with the flexibility and composability of the TensorFlow and Keras.\nRandom Forests are a popular type of decision forest model. Here, you can see a forest of trees classifying an example by voting on the outcome.\nAbout decision forests\nDecision forests are a family of machine learning algorithms with quality and speed competitive with (and often favorable to) neural networks, especially when you\u2019re working with tabular data. They\u2019re built from many decision trees, which makes them easy to use and understand - and you can take advantage of a plethora of interpretability tools and techniques that already exist today.\nTF-DF brings this class of models along with a suite of tailored tools to TensorFlow users:\nBeginners will find it easier to develop and explain decision forest models. There is no need to explicitly list or pre-process input features (as decision forests can naturally handle numeric and categorical attributes), specify an architecture (for example, by trying different combinations of layers like you would in a neural network), or worry about models diverging. Once your model is trained, you can plot it directly or analyse it with easy to interpret statistics.\nAdvanced users will benefit from models with very fast inference time (sub-microseconds per example in many cases). And, this library offers a great deal of composability for model experimentation and research. In particular, it is easy to combine neural networks and decision forests.\nIf you\u2019re already using decision forests outside of TensorFlow, here\u2019s a little of what TF-DF offers:\nIt provides a slew of state-of-the-art Decision Forest training and serving algorithms such as random forests, gradient-boosted trees, CART, (Lambda)MART, DART, Extra Trees, greedy global growth, oblique trees, one-side-sampling, categorical-set learning, random categorical learning, out-of-bag evaluation and feature importance, and structural feature importance.\nThis library can serve as a bridge to the rich TensorFlow ecosystem by making it easier for you to integrate tree-based models with various TensorFlow tools, libraries, and platforms such as TFX.\nAnd for users new to neural networks, you can use decision forests as an easy way to get started with TensorFlow, and continue to explore neural networks from there.\nCode example\nA good example is worth a thousand words. So in this blog post, we will show how easy it is to train a model with TensorFlow Decision Forests. More examples are available on the TF-DF website and GitHub page. You may also watch our talk at Google I/O 2021 .\nTraining a model\nLet's start with a minimal example where we train a random forest model on the tabular Palmer's Penguins dataset. The objective is to predict the species of an animal from its characteristics. The dataset contains both numerical and categorical features and is stored as a csv file.\nThree examples from the Palmer's Penguins dataset.\nLet's train a model:\n# Install TensorFlow Decision Forests\n!pip install tensorflow_decision_forests\n\n# Load TensorFlow Decision Forests\nimport tensorflow_decision_forests as tfdf\n\n# Load the training dataset using pandas\nimport pandas\ntrain_df = pandas.read_csv(\"penguins_train.csv\")\n\n# Convert the pandas dataframe into a TensorFlow dataset\ntrain_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_df, label=\"species\")\n\n# Train the model\nmodel = tfdf.keras.RandomForestModel()\nmodel.fit(train_ds)\nObserve that nowhere in the code did we provide input features or hyperparameters. That means, TensorFlow Decision Forests will automatically detect the input features from this dataset and use default values for all hyperparameters.\nEvaluating a model\nNow, let's evaluate the quality of our model:\n# Load the testing dataset\ntest_df = pandas.read_csv(\"penguins_test.csv\")\n\n# Convert it to a TensorFlow dataset\ntest_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_df, label=\"species\")\n\n# Evaluate the model\nmodel.compile(metrics=[\"accuracy\"])\nprint(model.evaluate(test_ds))\n# >> 0.979311\n# Note: Cross-validation would be more suited on this small dataset.\n# See also the \"Out-of-bag evaluation\" below.\n\n# Export the model to a TensorFlow SavedModel\nmodel.save(\"project/my_first_model\")\nEasy, right? And a default RandomForest model with default hyperparameters provides a quick and good baseline for most problems. Decision forests in general will train quickly for small and medium sized problems, require less hyperparameter tuning compared to many other types of models, and will often provide strong results.\nInterpreting a model\nNow that you have looked at the accuracy of the trained model, let\u2019s consider its interpretability. Interpretability is important if you wish to understand and explain the phenomenon being modeled, debug a model, or begin to trust its decisions. As noted above, we have provided a number of tools to interpret trained models, beginning with plots.\ntfdf.model_plotter.plot_model_in_colab(model, tree_idx=0)\nYou can visually follow the tree structure. In this tree, the first decision is based on the bill length. Penguins with bills longer than 42.2mm are likely to be the blue (Gentoo) or green (Chinstrap) species, while the ones with shorter bills are likely to be of the red specy (Adelie).\nFor the first group, the tree then asks about the flipper length. Penguins with flippers longer than 206.5mm are likely to be of the green species (Chinstrap), while the remaining are likely to be of the blue species (Gentoo).\nModel statistics are complementary additions to plots. Example statistics include:\nHow many times is each feature used?\nHow fast did the model train (in number of trees and time)?\nHow are the nodes distributed in the tree structure (for example, what is the length of most branches?)\nThese and answers to more such inquiries are included in the model summary and accessible in the model inspector.\n# Print all the available information about the model\nmodel.summary()\n>> Input Features (7):\n>>   bill_depth_mm\n>>   bill_length_mm\n>>   body_mass_g\n>>   ...\n>> Variable Importance:\n>>   1.    \"bill_length_mm\" 653.000000 ################\n>>   ...\n>> Out-of-bag evaluation: accuracy:0.964602 logloss:0.102378\n>> Number of trees: 300\n>> Total number of nodes: 4170\n>>   ...\n\n# Get feature importance as a array\nmodel.make_inspector().variable_importances()[\"MEAN_DECREASE_IN_ACCURACY\"]\n>> [(\"flipper_length_mm\", 0.149),\n>>      (\"bill_length_mm\", 0.096),\n>>      (\"bill_depth_mm\", 0.025),\n>>      (\"body_mass_g\", 0.018),\n>>      (\"island\", 0.012)]\nIn the example above, the model was trained with default hyperparameter values. This is a good first solution, but \"tuning\" the hyper-parameters can often further improve the quality of the model. That can be done as in the following:\n# List all the other available learning algorithms\ntfdf.keras.get_all_models()\n>> [tensorflow_decision_forests.keras.RandomForestModel,\n>>  tensorflow_decision_forests.keras.GradientBoostedTreesModel,\n>>  tensorflow_decision_forests.keras.CartModel]\n\n# Display the hyper-parameters of the Gradient Boosted Trees model \n? tfdf.keras.GradientBoostedTreesModel\n>> A GBT (Gradient Boosted [Decision] Tree) is a set of shallow decision trees trained sequentially. Each tree is trained to predict and then \"correct\" for the errors of the previously trained trees (more precisely each tree predicts the gradient of the loss relative to the model output)..\n   ...\n   Attributes:\n     num_trees: num_trees: Maximum number of decision trees. The effective number of trained trees can be smaller if early stopping is enabled. Default: 300.\n     max_depth: Maximum depth of the tree. `max_depth=1` means that all trees will be roots. Negative values are ignored. Default: 6.\n   ...\n\n# Create another model with specified hyper-parameters\nmodel = tfdf.keras.GradientBoostedTreesModel(\n    num_trees=500,\n    growing_strategy=\"BEST_FIRST_GLOBAL\",\n    max_depth=8,\n    split_axis=\"SPARSE_OBLIQUE\",\n    )\n\n# Evaluate the model\nmodel.compile(metrics=[\"accuracy\"])\nprint(model.evaluate(test_ds))\n# >> 0.986851\nNext steps\nWe hope you enjoyed reading this short demonstration of TensorFlow Decision Forests, and that you are as excited to use it and contribute to it as we are to develop it.\nWith TensorFlow Decision Forests, you can now train state-of-the-art Decision Forests models with maximum speed and quality and with minimal effort in TensorFlow. And if you feel adventurous, you can now combine decision forests and neural networks together to create new types of hybrid models.\nIf you would like to learn more about the TensorFlow Decision Forests library, we have put together a number of resources and recommend the following:\nYou can find the complete code from this article in the beginner colab notebook, and also check out this intermediate notebook and this advanced one.\nYou can watch this video from Google I/O.\nYou can check out and star the TensorFlow Decision Forests project on GitHub.\nExisting TensorFlow users will benefit from reading our migration guide if you\u2019re interested in adding decision forests to your existing workflow.\nAdvanced users may want to follow the Yggdrasil Decision ForestsYggdrasil Decision Forest GitHub project, the c++ engine powering TensorFlow Decision Forests, where the algorithms are implemented.\nIf you have any questions, please ask them on the discuss.tensorflow.org using the tag \u201cTFDF\u201d and we\u2019ll do our best to help. Thanks again.",
    "link": "https://blog.tensorflow.org/2021/05/introducing-tensorflow-decision-forests.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-Ax59WK4DE8w/YK6o9bt_9jI/AAAAAAAAEQA/9KbBf9cdL6kOFkJnU39aUn4m8ydThPenwCLcBGAsYHQ/s0/Random%2BForest%2B03.gif",
      "https://1.bp.blogspot.com/-rb9shTXLnJg/YK6tTC-iCEI/AAAAAAAAEQI/XPADFRziT-8VIV1rF43XWETzDXrs5Pp5wCLcBGAsYHQ/s0/pasted%2Bimage%2B0%2B%252812%2529.png",
      "https://1.bp.blogspot.com/-ABwicWSnu7k/YK6uUFo6jMI/AAAAAAAAEQQ/mthT_PGLwLg0shDOu3UsB6bwZTDT_xcLgCLcBGAsYHQ/s0/pasted%2Bimage%2B0%2B%252813%2529.png"
    ],
    "time": "2023/12/10 00:58:43"
  },
  {
    "title": "Run Your First Multi-Worker TensorFlow Training Job With GCP AI Platform",
    "content": "Posted by Nikita Namjoshi, Machine Learning Solutions Engineer\nWhen a single machine is not enough, it\u2019s time to train and iterate faster with TensorFlow\u2019s MultiWorkerMirroredStrategy. In this tutorial-style article you\u2019ll learn how to launch a multi-worker training job on Google Cloud Platform (GCP) using AI Platform Training. You\u2019ll also learn the basics of how TensorFlow distributes data and implements synchronous data parallelism across multiple machines. While this article focuses on a managed solution on GCP, you can also do all of this entirely in open-source on your own hardware.\nOverview of Distributed Training\nIf you have a single GPU, TensorFlow will use this accelerator to speed up model training with no extra work on your part. However, if you want to get an additional boost from using multiple GPUs on a single machine or multiple machines (each with potentially multiple GPUs), then you\u2019ll need to use tf.distribute, which is TensorFlow\u2019s library for running a computation across multiple devices.\nThe simplest way to get started with distributed training is a single machine with multiple GPU devices. A TensorFlow distribution strategy from the tf.distribute module will manage the coordination of data distribution and gradient updates across all of the GPUs. If you want to learn more about training in this scenario, check out the previous post on distributed training basics.\nIf you\u2019ve mastered single host training and are looking to scale even further, then adding multiple machines to your cluster can help you get an even greater performance boost. You can make use of a cluster of machines that are CPU only, or that each have one or more GPUs.\nThere are many ways to do multi-worker training on GCP. In this article we\u2019ll use AI Platform Training, as it\u2019s the quickest way to launch a distributed training job and has additional features that make it very easy to include as part of your production pipeline. To use this managed service, you\u2019ll need to add a bit of extra code to your program and set up a config file that is specific to AI Platform. However; you will not have to endure the pains of GPU driver installation or cluster management, which can be very challenging in a distributed scenario.\nMulti-Worker Cluster Configuration\nThe tf.distribute module currently provides two strategies for multi-worker training. In TensorFlow 2.5, ParameterServerStrategy is experimental, and MultiWorkerMirroredStrategy is a stable API.\nLike its single-worker counterpart, MirroredStrategy, MultiWorkerMirroredStrategy is a synchronous data parallelism strategy that you can use with only a few code changes.\nHowever, unlike MirroredStrategy, for a multi-worker setup TensorFlow needs to know which machines are part of your cluster. This is generally specified with the environment variable TF_CONFIG.\nos.environ[\"TF_CONFIG\"] = json.dumps({\n   \"cluster\": {\n       \"chief\": [\"host1:port\"],\n       \"worker\": [\"host2:port\", \"host3:port\"],\n   },\n  \"task\": {\"type\": \"worker\", \"index\": 1}\n})\nIn this simple TF_CONFIG example, the \u201ccluster\u201d key contains a dictionary with the internal IPs and ports of all the machines. In MultiWorkerMirroredStrategy, all machines are designated as workers, which are the physical machines on which the replicated computation is executed. In addition to each machine being a worker, there needs to be one worker that takes on some extra work such as saving checkpoints and writing summary files to TensorBoard. This machine is known as the chief (or by its deprecated name master).\nAfter you\u2019ve added your machines to the cluster key, the next step is to set the \u201ctask\u201d. This specifies the task type and task index of the current machine, which is an index into the cluster dictionary. The cluster key should be the same on each machine, but the task keys will be different.\nConveniently, when using AI Platform Training, the TF_CONFIG environment variable is set for you on each machine in your cluster so you don\u2019t need to worry about this set up!\nHowever, if you were trying to run a multi-worker job with, for example, 3 instances on Google Compute Engine, you would need to set this environment variable on each machine as shown below. For the machines that are not the chief, the TF_CONFIG looks the same except the task index increments by 1.\nMachine 1 (Chief)\nos.environ[\"TF_CONFIG\"] = json.dumps({\n   \"cluster\": {\n       \"chief\": [\"host1:port\"],\n       \"worker\": [\"host2:port\", \"host3:port\"],\n   },\n  \"task\": {\"type\": \"chief\", \"index\": 0}\n})\nMachine 2\nos.environ[\"TF_CONFIG\"] = json.dumps({\n   \"cluster\": {\n       \"chief\": [\"host1:port\"],\n       \"worker\": [\"host2:port\", \"host3:port\"],\n   },\n  \"task\": {\"type\": \"worker\", \"index\": 0}\n})\nMachine 3\nos.environ[\"TF_CONFIG\"] = json.dumps({\n   \"cluster\": {\n       \"chief\": [\"host1:port\"],\n       \"worker\": [\"host2:port\", \"host3:port\"],\n   },\n  \"task\": {\"type\": \"worker\", \"index\": 1}\n})\nSetting this environment variable is fairly easy to do when you have only a few machines in your cluster; however, once you start scaling up, you don\u2019t want to be assigning this variable to each machine manually. As mentioned earlier, one of the many benefits of using AI Platform is that this coordination happens automatically. The only configuration you have to provide is the number of machines in your cluster, and the number and type of GPUs per machine. We\u2019ll do this step in a later section.\nSet up the Distribution Strategy\nIn this Colab notebook, you\u2019ll find the code to train a ResNet50 architecture on the Cassava dataset. In the following sections, we\u2019ll review the new code that needs to be added to our program in order to do distributed training on multiple machines.\nAs with any strategy in the tf.distribute module, step one is to instantiate the strategy.\nstrategy = tf.distribute.MultiWorkerMirroredStrategy()\nNote that there is a limitation where the instance of MultiWorkerMirroredStrategy needs to be created at the beginning of the program. Code that may create ops should be placed after the strategy is instantiated.\nNext, you wrap the creation of your model variables within the strategy\u2019s scope. This crucial step tells TensorFlow which variables should be mirrored across the replicas.\nwith strategy.scope():\n  model = create_model()\n  model.compile(\n    loss='sparse_categorical_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(0.0001),\n    metrics=['accuracy'])\nLastly, you\u2019ll need to scale your batch size by the number of replicas in your cluster. This ensures that each replica processes the same number of examples on each step.\nper_replica_batch_size = 64\nglobal_batch_size = per_replica_batch_size * strategy.num_replicas_in_sync\nIf you\u2019ve used MirroredStrategy before, then the previous steps should be familiar. The main difference when moving from synchronous data parallelism on one machine to many is that the gradients at the end of each step now need to be synchronized across all GPUs in a machine and across all machines in the cluster. This additional step of synchronizing across the machines increases the overhead of distribution.\nIn TensorFlow, the multi-worker all-reduce communication is achieved via CollectiveOps. You don\u2019t need to know much detail to execute a successful and performant training job, but at a high level, a collective op is a single op in the TensorFlow graph that can automatically choose an all-reduce algorithm according to factors such as hardware, network topology, and tensor sizes.\nDataset Sharding\nIn the single worker case, at each step your dataset is divided up across the replicas on your machine. This data splitting process becomes slightly more complicated in the multi-worker case. The data now also needs to be sharded, meaning that each worker is assigned a subset of the entire dataset. Therefore, at each step a global batch size of non overlapping dataset elements will be processed by each worker. This sharding happens automatically with tf.data.experimental.AutoShardPolicy.\nBy default, TensorFlow will first attempt to shard your data by FILE. This means that if your data exists across multiple files, each worker will process different file(s) and split the corresponding data amongst the replicas. FILE is the default autoshard policy because MultiWorkerMirroredStrategy works best for use cases with very large datasets, which are likely to not be in a single file. However, this option can lead to idle workers if the number of files is not divisible by the number of workers, or if some files are substantially longer than others.\nIf your data is not stored in multiple files, then the AutoShardPolicy will fall back to DATA, meaning that TensorFlow will autoshard the elements across all the workers. This guards against the potential idle worker scenario, but the downside is that the entire dataset will be read on each worker. You can read more about the different policies and see examples in the Distributed Input guide.\nIf you don\u2019t want to use the default AUTO policy, you can set the desired AutoShardPolicy with the following code:\noptions = tf.data.Options()\noptions.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\ntrain_data = train_data.with_options(options)\nSave Your Model\nSaving your model is slightly more complicated in the multi-worker case because the destination needs to be different for each of the workers. The chief worker will save to the desired model directory, while the other workers will save the model to temporary directories. It\u2019s important that these temporary directories are unique in order to prevent multiple workers from writing to the same location. Saving can contain collective ops, so all workers must save and not just the chief.\nThe following is boilerplate code that implements the intended saving logic, as well as some cleanup to delete the temporary directories once the training has completed. Note that the model_path is the name of the Google Cloud Storage (GCS) bucket where your model will be saved at the end of training.\nmodel_path = {gs://path_to_your_gcs_bucket}\n \n# Note that with MultiWorkerMirroredStrategy,\n# the program is run on every worker.\ndef _is_chief(task_type, task_id):\n # Note: there are two possible `TF_CONFIG` configurations.\n #   1) In addition to `worker` tasks, a `chief` task type is used.  \n #      The implementation demonstrated here is for this case.\n #   2) Only `worker` task type is used; in this case, worker 0 is\n #      regarded as the chief. In this case, this function\n #      should be modified to\n #      return (task_type == 'worker' and task_id == 0) or task_type is None\n  return task_type == 'chief'\n \n \ndef _get_temp_dir(dirpath, task_id):\n base_dirpath = 'workertemp_' + str(task_id)\n temp_dir = os.path.join(dirpath, base_dirpath)\n tf.io.gfile.makedirs(temp_dir)\n return temp_dir\n \ndef write_filepath(filepath, task_type, task_id):\n dirpath = os.path.dirname(filepath)\n base = os.path.basename(filepath)\n if not _is_chief(task_type, task_id):\n   dirpath = _get_temp_dir(dirpath, task_id)\n return os.path.join(dirpath, base)\n \n# Determine type and task of the machine from\n# the strategy cluster resolver\ntask_type, task_id = (strategy.cluster_resolver.task_type,\n                     strategy.cluster_resolver.task_id)\n \n# Based on the type and task, write to the desired model path\nwrite_model_path = write_filepath(model_path, task_type, task_id)\nmodel.save(write_model_path)\nEverything we\u2019ve covered about setting up the distribution strategy, sharding data, and saving models applies whether you\u2019re training on GCP, your own hardware, or another cloud platform.\nPrepare code for AI Platform\nThe basic prerequisites for using AI Platform are that you need to have a GCP project with billing enabled, the AI Platform APIs enabled, and sufficient AI Platform quota. If any of these steps are a mystery to you, refer to the previous post to get up to speed on GCP basics.\nIf you\u2019re already familiar with training on AI Platform with a single node, then you\u2019ll likely breeze through this section. We\u2019ll take the pieces we walked through in the previous section, and do a bit of rearranging to match AI Platform Training convention. All of the code can be found in this Github repo, but we\u2019ll walk through it in detail in this section.\nBy AI Platform convention, training code is arranged according to the diagram below. The task.py file contains the code that executes your training job. The example in this tutorial also includes a model.py file, which has the Keras functional API code for the model. For more complex production applications you\u2019ll likely have additional util.py or setup.py files, and you can see where those fit in the hierarchy below.\nModel code\nThe model.py file can be found in Github here. You can see that this file just has the code for building the ResNet50 model architecture.\nTask code\nThe task.py file can be found in Github here. This file contains the main function, which will execute the training job and save the model.\ndef main():\n args = get_args()\n strategy = tf.distribute.MultiWorkerMirroredStrategy()\n global_batch_size = PER_REPLICA_BATCH_SIZE * strategy.num_replicas_in_sync\n train_data, number_of_classes = create_dataset(global_batch_size)\n \n with strategy.scope():\n   model = create_model(number_of_classes)\n \n model.fit(train_data, epochs=args.epochs)\n \n # Determine type and task of the machine from\n # the strategy cluster resolver\n task_type, task_id = (strategy.cluster_resolver.task_type,\n                       strategy.cluster_resolver.task_id)\n \n # Based on the type and task, write to the desired model path\n write_model_path = write_filepath(args.job_dir, task_type, task_id)\n model.save(write_model_path)\nIn this simple example, the data preprocessing happens directly in the task.py file, but in reality for more complicated data processing you would probably want to split out this code into a separate data.py file that you can import into task.py (for example if your preprocessing includes parsing TFRecord files).\nWe explicitly set the AutoShardPolicy to DATA in this case because the Cassava dataset is not downloaded as multiple files. However, if we did not set the policy to DATA, the default AUTO policy would kick in and the end result would be the same.\noptions = tf.data.Options()\noptions.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\ntrain_data = train_data.with_options(options)\nThe task.py file also parses any command line arguments we need. In this simple example, the epochs are passed in via the command line. Additionally, we need to parse the argument job-dir, which is the GCS bucket where our model will be stored.\ndef get_args():\n '''Parses args.'''\n parser = argparse.ArgumentParser()\n parser.add_argument(\n     '--epochs',\n     required=True,\n     type=int,\n     help='number training epochs')\n parser.add_argument(\n     '--job-dir',\n     required=True,\n     type=str,\n     help='bucket to save model')\n args = parser.parse_args()\n return args\nLastly, the task.py file contains our boilerplate code for saving the model. For a production example, you probably would want to add this boilerplate to a util.py file, but again for this simple example we\u2019ll keep everything in one file.\nCustom Container Set up\nAI Platform provides standard runtimes for you to execute your training job. While these runtimes might work for your use case, more specialized needs require a custom container. In this section, we\u2019ll walk through how to set up your container image and push it to Google Container Registry (GCR).\nWrite Your Dockerfile\nThe following Dockerfile specifies the base image, using the TensorFlow 2.5 Enterprise GPU Deep Learning Container. Using the TensorFlow Enterprise image as our base image provides a useful design pattern for developing on GCP. TensorFlow Enterprise is a distribution of TensorFlow that is optimized for GCP. You can use TensorFlow Enterprise with AI Platform Notebooks, the Deep Learning VMs, and AI Platform Training, providing a seamless transition between different environments.\nThe code in our trainer directory is copied to the Docker image, and our entry point is the task.py script, which we will run as a module.\n# Specifies base image and tag\nFROM gcr.io/deeplearning-platform-release/tf2-gpu.2-5\nWORKDIR /root\n \n# Copies the trainer code to the docker image.\nCOPY trainer/ /root/trainer/\n \n# Sets up the entry point to invoke the trainer.\nENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]\nPush Your Dockerfile to GCR\nNext, we\u2019ll set up some useful environment variables. You can select any name of your choosing for IMAGE_REPO_NAME and IMAGE_TAG. If you have not already set up the Google Cloud SDK, you can follow the steps here, as you\u2019ll need to use the gcloud tool to push your container and kick off the training job.\nexport PROJECT_ID=$(gcloud config list project --format \"value(core.project)\")\nexport IMAGE_REPO_NAME={your_repo_name}\nexport IMAGE_TAG={your_image_tag}\nexport IMAGE_URI=gcr.io/$PROJECT_ID/$IMAGE_REPO_NAME:$IMAGE_TAG\nNext, you\u2019ll build your Dockerfile.\ndocker build -f Dockerfile -t $IMAGE_URI ./\nLastly, you can push your image to GCR.\ngcloud auth configure-docker\ndocker push $IMAGE_URI\nIf you navigate to the GCR page in the GCP console UI, you should see your newly pushed image.\nConfigure Your Cluster\nThe final step before we can kick off our training job is to set up the cluster. AI Platform offers a set of predefined cluster specifications called scale tiers, but we\u2019ll need to provide our own cluster setup for distributed training.\nIn the following config.yaml file, we\u2019ve designated one master (equivalent to chief) and one worker. Each machine has one NVIDIA T4 Tensor Core GPU. For both machines, you\u2019ll also need to specify the imageUri as the image you pushed to GCR in the previous step.\ntrainingInput:\n scaleTier: CUSTOM\n masterType: n1-standard-8\n masterConfig:\n   acceleratorConfig:\n     count: 1\n     type: NVIDIA_TESLA_T4\n   imageUri: gcr.io/{path/to/image}:{tag}\n useChiefInTfConfig: true\n workerType: n1-standard-8\n workerCount: 1\n workerConfig:\n   acceleratorConfig:\n     count: 1\n     type: NVIDIA_TESLA_T4\n   imageUri: gcr.io/{path/to/image}:{tag}\nIn case you\u2019re wondering what the useChiefInTfConfig flag does, TensorFlow uses the terminology \u201cChief\u201d and AI Platform uses the terminology \u201cMaster\u201d, so this flag will manage that discrepancy. You don\u2019t need to worry about the details (although you will see an error message if you forget to set this flag!).\nFeel free to experiment with this configuration by adding machines, adding GPUs, or removing all GPUs and training with CPUs only. You can see the supported regions and GPU types here for AI Platform, so just make sure your project has sufficient quota for whatever configuration you choose.\nLaunch Your Training Job\nYou can launch your training job easily with the following command:\ngcloud ai-platform jobs submit training {job_name} \\ \n   --region europe-west2 \\\n   --config config.yaml \\\n   --job-dir gs://{gcs_bucket/model_dir} -- \\\n   --epochs 5 \\\nIn the command above, you\u2019ll need to give your job a name. In addition to passing in the region, you\u2019ll need to define job-dir, which is the directory in your GCS bucket where you want your saved model file to be stored after training completes.\n\nThe empty -- flag marks the end of the gcloud specific flags and the start of the args that you want to pass to your application (in this case, this is just the epochs).\n\nAfter executing the training command, you should see the following message.\nYou can navigate to the AI Platform UI in the GCP console and track the status of your job.\nYou\u2019ll notice that your job will take around ten minutes to launch. This overhead might seem huge in our simple example where it doesn\u2019t even take ten minutes to train on a single GPU. However, this overhead will be amortized for large jobs.\nWhen the job completes training, you\u2019ll see a green check mark next to the job. You can then click the Model location URI and you\u2019ll find your saved_model.pb file.\nWhat\u2019s Next\nYou now know the basics of launching a multi-worker training job on GCP. You also know the core concepts of MultiWorkerMirroredStrategy. To take your skills to the next level, try leveraging AI Platform\u2019s hyperparameter tuning feature for your next training job (in open-source, you can use Keras Tuner), or using TFRecord files as your input data. You can also try out Parameter Server Strategy if you\u2019d like to explore asynchronous training in TensorFlow. Happy distributed training!",
    "link": "https://blog.tensorflow.org/2021/05/run-your-first-multi-worker-tensorflow-training-job-with-gcp.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-Nvz9HIVpC1Y/YK1vdFEOZeI/AAAAAAAAEPE/DtvmfGyTJxQaRLGB8WU6geB32hWfBuj0QCLcBGAsYHQ/s0/TensorFlow-SavedModelSIGs-Social-v2%2B%25281%2529.png",
      "https://1.bp.blogspot.com/-Nvz9HIVpC1Y/YK1vdFEOZeI/AAAAAAAAEPE/DtvmfGyTJxQaRLGB8WU6geB32hWfBuj0QCLcBGAsYHQ/s0/TensorFlow-SavedModelSIGs-Social-v2%2B%25281%2529.png",
      "https://1.bp.blogspot.com/-1R77nq3ezTQ/YKwkCFT2nZI/AAAAAAAAEOs/xIocNfkMNbQYOZmfyGhT9RJv9rhpYNUvQCLcBGAsYHQ/s0/Screen%2BShot%2B2020-12-03%2Bat%2B4.02.39%2BPM.png",
      "https://1.bp.blogspot.com/-QXhHTJvsYRA/YKwl9Dk8PdI/AAAAAAAAEO0/yCTWtZKY4qMgR7YecDaHdU90UugjOvIAgCLcBGAsYHQ/s0/Screen%2BShot%2B2021-03-22%2Bat%2B12.37.47%2BPM.png",
      "https://1.bp.blogspot.com/-R5DYCkjWoUY/YKwmP_2aBUI/AAAAAAAAEO8/rlvzSeeWjMosf-6Ulz7_PHKYAWZfGPSLACLcBGAsYHQ/s0/Screen%2BShot%2B2021-03-22%2Bat%2B1.27.34%2BPM.png"
    ],
    "time": "2023/12/10 00:58:43"
  },
  {
    "title": "Recap of TensorFlow at Google I/O 2021",
    "content": "Posted by the TensorFlow team\nThanks to everyone who joined our virtual I/O 2021 livestream! While we couldn\u2019t meet in person, we hope we were able to make the event more accessible than ever. In this article, we\u2019re recapping a few of the updates we shared during the keynote. You can watch the keynote below, and you can find recordings of every talk on the TensorFlow YouTube channel. Here\u2019s a summary of a few announcements by product area (and there\u2019s more in the videos, so be sure to check them out, too).\nTensorFlow for Mobile and Web\nThe TensorFlow Lite runtime will be bundled with Google Play services\nLet\u2019s start with the announcement that the TensorFlow Lite runtime is going to be bundled with Google Play services, meaning you don\u2019t need to distribute it with your app. This can greatly reduce your app\u2019s bundle size. Now you can distribute your model without needing to worry about the runtime. You can sign up for an early access program today, and we expect a full rollout later this year.\nYou can now run TensorFlow Lite models on the web\nAll your TensorFlow Lite models can now directly be run on the web in the browser with the new TFLite Web APIs that are unified with TensorFlow.js. This task-based API supports running all TFLite Task Library models for image classification, objection detection, image segmentation, and many NLP problems. It also supports running arbitrary, custom TFLite models with easy, intuitive TensorFlow.js compatible APIs. With this option, you can unify your mobile and web ML development with a single stack.\nA new On-Device Machine Learning site\nWe understand that the most effective developer path to reach Android, the Web and iOS isn\u2019t always the most obvious. That\u2019s why we created a new On-Device Machine Learning site to help you navigate your options, from turnkey to custom models, from cross platform mobile, to in-browser. It includes pathways to take you from an idea to a deployed app, with all the steps in between.\nPerformance profiling\nWhen it comes to performance, we\u2019re also working on additional tooling for Android developers. TensorFlow Lite includes built-in support for Systrace, integrating seamlessly with perfetto for Android 10.\nAnd perf improvements aren\u2019t limited to Android \u2013 for iOS developers TensorFlow Lite comes with built-in support for signpost-based profiling. When you build your app with the trace option enabled, you can run the Xcode profiler to see the signpost events, letting you dive deeper, and seeing all the way down to individual ops during execution.\nTFX\nTFX 1.0: Production ML at Enterprise-scale\nMoving your ML models from prototype to production requires lots of infrastructure. Google created TFX because we needed a strong framework for our ML products and services, and then we open-sourced it so that others can use it too. It includes support for training models for mobile and web applications, as well as server-based applications.\nAfter a successful beta with many partners, today we\u2019re announcing TFX 1.0 \u2014 ready today for production ML at enterprise-scale. TFX includes all of the things an enterprise-ready framework needs, including enterprise-grade support, security patches, bug fixes, and guaranteed backward compatibility for the entire 1.X release cycle. It also includes strong support for running on Google Cloud and support for mobile, web, and NLP applications.\nIf you\u2019re ready for production ML, TFX is ready for you. Visit the TFX site to learn more.\nResponsible AI\nWe\u2019re also sharing a number of new tools to help you keep Responsible AI top of mind in everything that you do when developing with ML.\nKnow Your Data\nKnow Your Data (KYD) is a new tool to help ML researchers and product teams understand rich datasets (images and text) with the goal of improving data and model quality, as well as surfacing and mitigating fairness and bias issues. Try the interactive demo at the link above to learn more.\n\nPeople + AI Guidebook 2.0\nAs you create AI solutions, building with a people centric approach is a key to doing it responsibly, and we\u2019re delighted to announce the People + AI Guidebook 2.0. This update is designed to help you put best practices and guidance for people-centric AI into practice with a lot of new resources including code, design patterns and much more!\nAlso check out our Responsible AI Toolkit to help you integrate Responsible AI practices into your ML workflow using TensorFlow.\nDecision forests in Keras\nNew support for random forests and gradient boosted trees\nThere\u2019s more to ML than neural networks. Starting with TensorFlow 2.5, you can easily train powerful decision forest models (including favorites like random forests and gradient boosted trees) using familiar Keras APIs. There\u2019s support for many state-of-the-art algorithms for training, serving and interpreting models for classification, regression and ranking tasks. And you can serve your decision forests using TF Serving, just like any other model trained with TensorFlow. Check out the tutorials here, and the video from this session.\nTensorFlow Lite for Microcontrollers\nA new pre-flashed board, experiments, and a challenge\nTensorFlow Lite for Microcontrollers is designed to help you run ML models on microcontrollers and other devices with only a few kilobytes of memory. You can now purchase pre-flashed Arduino boards that will connect via Bluetooth and your browser. And you can use these to try out new Experiments With Google that let you make gestures and even create your own classifiers and run custom TensorFlow models. If you\u2019re interested in challenges, we\u2019re also running a new TensorFlow Lite for Microcontrollers challenge, you can check it out here. And also be sure to check out the TinyML workshop video in the next steps below.\nGoogle Cloud\nVertex AI: A new managed ML platform on Google Cloud\nAn ML model is only valuable if you can actually put it into production. And as you know, it can be challenging to productionize efficiently and at scale. That\u2019s why Google Cloud is releasing Vertex AI, a new managed machine learning platform to help you accelerate experimentation and deployment of AI models. Vertex AI has tools that span every stage of the developer workflow, from data labeling, to working with notebooks and models, to prediction tools and continuous monitoring - all unified into one UI. While many of these offerings may be familiar to you, what really distinguishes Vertex AI is the introduction of new MLOps features. You can now manage your models with confidence using our MLOps tools such as Vertex Pipelines and Vertex Feature Store, to remove the complexity of robust self-service model maintenance and repeatability.\nTensorFlow Cloud: Transition from local model building to distributed training on the Cloud\nTensorFlow Cloud provides APIs that ease the transition from local model building and debugging to distributed training and hyperparameter tuning on Google Cloud. From inside a Colab or Kaggle Notebook or a local script file, you can send your model for tuning or training on Cloud directly, without needing to use the Cloud Console. We recently added a new site and new features, check it out if you\u2019re interested in learning more.\nCommunity\nA new TensorFlow Forum\nWe created a new TensorFlow Forum for you to ask questions and connect with the community. It\u2019s a place for developers, contributors, and users to engage with each other and the TensorFlow team. Create your account and join the conversation at discuss.tensorflow.org.\nFind all the talks here\nThis is just a small part of what was shared at Google I/O 2021. You can find all of the TensorFlow sessions in this playlist, and for your convenience here are direct links to each of the sessions also:\nML Recap Video\nWhat\u2019s new in Machine Learning (Keynote)\nMachine learning for next gen web apps with TensorFlow.js\nDoes your app use machine learning? Make it a product with TFX\nOptimize your TensorFlow Lite models\nML Kit: turnkey APIs to use on-device ML in mobile apps\nTensorFlow Hub for real world impact\nModern Keras design patterns\nBuilding with the Responsible AI Toolkit\nDecision forests in TensorFlow\nCross platform computer vision made easy with Model Maker\nCoral: Expanding the edge AI landscape\nSpotting and solving everyday problems with machine learning\nEasily deploy TF Lite models to the web (Demo)\nTrain TensorFlow models at cloud scale with TensorFlow Cloud (Demo)\nBeyond evaluation: Improving fairness with Model Remediation (Demo)\nTo learn more about TensorFlow, you check out tensorflow.org, read other articles on the blog, follow us on social media, and subscribe to our YouTube Channel, or join a TensorFlow User Group near you.",
    "link": "https://blog.tensorflow.org/2021/05/recap-of-tensorflow-at-google-io-2021.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-kLzhlIx8-LE/YKhDm_4gCYI/AAAAAAAAEOk/9d1QYu9xN1A38fAkxxv5JgYcibyrPr_DwCLcBGAsYHQ/s0/tensorflow-recap-google-io-2021-header.png",
      "https://1.bp.blogspot.com/-kLzhlIx8-LE/YKhDm_4gCYI/AAAAAAAAEOk/9d1QYu9xN1A38fAkxxv5JgYcibyrPr_DwCLcBGAsYHQ/s0/tensorflow-recap-google-io-2021-header.png",
      "https://1.bp.blogspot.com/-wi7E_28vzho/YKbsrMyv_2I/AAAAAAAAEOE/F23ECQ1RE10eIMHm7Cok_8CFZ_x5RYlnACLcBGAsYHQ/s0/gif%2B1.gif",
      "https://1.bp.blogspot.com/-x2w6C0d0-ow/YKbumpBBs2I/AAAAAAAAEOM/OPo0gF7rURIs9OfIgTi9YEu1NxZOcigrgCLcBGAsYHQ/s0/Kyd%2Bcapture%2Bbackground.png",
      "https://1.bp.blogspot.com/-hWynPEPM10I/YKb09SBT7JI/AAAAAAAAEOU/MMVGoazqwNQyxOfw9dGfWgLnZ-llUw6cgCLcBGAsYHQ/s0/chip.png",
      "https://1.bp.blogspot.com/-HLsufT_7qRk/YKb1XE5FxFI/AAAAAAAAEOc/7FZcDI8Klcst2hOLzI0HgNCPMxi56ttFACLcBGAsYHQ/s0/Untitled%2B%25284%2529.png"
    ],
    "time": "2023/12/10 00:58:43"
  },
  {
    "title": "How-to Write a Python Fuzzer for TensorFlow",
    "content": "Posted by Laura Pak\nFuzz testing is a process of testing APIs with generated data. Fuzzing ensures that code will not break on the negative path, generating randomized inputs that try to cover every branch of code. A popular choice is to pair fuzzers with sanitizers, which are tools that check for illegal conditions and thus flag the bugs triggered by the fuzzers\u2019 inputs.\nIn this way, fuzzing can find:\nBuffer overflows\nMemory leaks\nDeadlocks\nInfinite recursion\nRound-trip consistency failures\nUncaught exceptions\nAnd more.\nThe best way to fuzz to have your fuzz tests running continuously. The more a test runs, the more inputs can be generated and tested against. In this article, you\u2019ll learn how to add a Python fuzzer to TensorFlow.\nThe technical how-to\nTensorFlow Python fuzzers run via OSS-Fuzz, the continuous fuzzing service for open source projects.\nFor Python fuzzers, OSS-Fuzz uses Atheris, a coverage-guided Python fuzzing engine. Atheris is based on the fuzzing engine libFuzzer, and it can be used with the dynamic memory error detector Address Sanitizer or the fast undefined behavior detector, Undefined Behavior Sanitizer. Atheris dependencies will be pre-installed on OSS-Fuzz base Docker images.\nHere is a barebones example of a Python fuzzer for TF. The runtime will call TestCode with different random data.\nimport sys\nimport atheris_no_libfuzzer as atheris\n\ndef TestCode(data):\n  DoSomethingWith(data)\n\ndef main():\n  atheris.Setup(sys.argv, TestCode, enable_python_coverage=True)\n  atheris.Fuzz()\nIn the tensorflow repo, in the directory with the other fuzzers, add your own Python fuzzer like above. In TestCode, pick a TensorFlow API that you want to fuzz. In constant_fuzz.py, that API is tf.constant. That fuzzer simply passes data to the chosen API to see if it breaks. No need for code that catches the breakage; OSS-Fuzz will detect and report the bug.\nSometimes an API needs more structured data than just one input. TensorFlow has a Python class called FuzzingHelper that allows you to generate random int lists, a random bool, etc. See an example of its use in sparseCountSparseOutput_fuzz.py, a fuzzer that checks for uncaught exceptions in the API tf.raw_ops.SparseCountSparseOutput.\nTo build and run, your fuzzer needs a fuzzing target of type tf_py_fuzz_target, defined in tf_fuzzing.bzl. Here is an example fuzz target, with more examples here.\ntf_py_fuzz_target(\n    name = \"fuzz_target_name\",\n    srcs = [\"your_fuzzer.py\"],\n    tags = [\"notap\"],  # Important: include to run in OSS.\n)\nTesting your fuzzer with Docker\nMake sure that your fuzzer builds in OSS-Fuzz with Docker.\nFirst install Docker. In your terminal, run command docker image prune to remove any dangling images.\nClone oss-fuzz from Github. The project for a Python TF fuzzer, tensorflow-py, contains a build.sh file to be executed in the Docker container defined in the Dockerfile. Build.sh defines how to build binaries for fuzz targets in tensorflow-py. Specifically, it builds all the Python fuzzers found in $SRC/tensorflow/tensorflow, including your new fuzzer!\nInside oss-fuzz, run the following commands:\npython infra/helper.py shell tensorflow\nexport FUZZING_LANGUAGE=python\ncompile\nThe command compile will run build.sh, which will attempt to build your new fuzzer.\nThe results\nOnce your fuzzer is up and running, you can search this dashboard for your fuzzer to see what vulnerabilities your fuzzer has uncovered.\nConclusion\nFuzzing is an exciting way to test software from the unhappy path. Whether you want to dabble in security or gain a deeper understanding of TensorFlow\u2019s internals, we hope this post gives you a good place to start.",
    "link": "https://blog.tensorflow.org/2021/04/how-to-write-python-fuzzer-for-tensorflow.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-GPL832zwIqQ/YGTdocf9-wI/AAAAAAAAEHc/u173aSQbyhMeMS1en2fjrGx10_Yj76SowCLcBGAsYHQ/s0/TensorFlow-HowtoWritePythonFuzzer-Social.png",
      "https://1.bp.blogspot.com/-GPL832zwIqQ/YGTdocf9-wI/AAAAAAAAEHc/u173aSQbyhMeMS1en2fjrGx10_Yj76SowCLcBGAsYHQ/s0/TensorFlow-HowtoWritePythonFuzzer-Social.png"
    ],
    "time": "2023/12/09 00:58:43"
  },
  {
    "title": "TensorFlow Quantum turns one year old",
    "content": "Posted by Michael Broughton, Alan Ho, Masoud Mohseni\nLast year we announced TensorFlow Quantum (TFQ) at the 2020 TensorFlow developer summit and on the Google AI Blog. Bringing all of the tools and features that TensorFlow has to offer to the world of quantum computing has led to some great research success stories. In this post, we would like to look back on what\u2019s happened in the last year involving TensorFlow Quantum and how far it\u2019s come. We also discuss the future of quantum computing and machine learning in TensorFlow Quantum.\nSince the release of TensorFlow Quantum, we\u2019ve been happy to see increasing use of the library in the academic world as well as inside Alphabet, in particular the Quantum AI team at Google. There have been many research articles published in the last year that made use of TensorFlow Quantum in quantum machine learning or hybrid quantum-classical models, including discriminative models and generative models. With the cross pollination of ideas between the two fields, we are also seeing advanced learning algorithms from classical machine learning being reimagined such as quantum reinforcement learning, layerwise, and neural architecture search. We leverage the scalability and tooling of TensorFlow to run numerical experiments with large numbers of qubits and gates to more faithfully discover algorithms that will be practical in the future.\nHere are a few papers published using TFQ if you\u2019d like to check them out:\nPower of data in quantum machine learning (Google, Discrimitivate, 30 qubits)\nAbsence of Barren Plateaus in Quantum Convolutional Neural Networks (LANL, Discriminative, 26 qubits)\nQuantum Hamiltonian-Based Models and the Variational Quantum Thermalizer Algorithm (Alphabet X, Generative, 16 qubits)\nEntanglement Diagnostics for Efficient Quantum Computation (Princeton / Tel-Aviv U., Generative, 20 qubits)\nLayerwise learning for quantum neural networks (VW / Google, Discriminative, 18 qubits)\nDifferentiable Quantum Architecture Search (Tencent, Neural Architecture Search, 8 qubits)\nIn our recent publication to quantify the computational advantage of quantum machine learning, experiments were conducted at PetaFLOP/s throughput scales, which is nothing new for classical machine learning, but represents a huge leap forward in the scale seen in quantum machine learning experiments before TensorFlow Quantum came along. We are very excited for the future that quantum computing and machine learning have together and we are happy to see TensorFlow Quantum having such a positive impact already.\nThe academic world isn\u2019t the only place machine learning and quantum computing have been able to come together. Over the past year members of the TensorFlow Quantum team helped out in supporting the artistic works of Refik Anadol Studios\u2019 \u201cQuantum memories\u201d piece. This combines the random circuit sample data from the 2019 beyond classical experiment and adoptions of StyleGAN to create some truly magnificent works of art\nQuantum memories installation at the NGV (image used with permission).\nNext steps\nWe will soon be releasing TensorFlow Quantum 0.5.0, with more support for distributed workloads as well as lots of new quantum centric features and some small performance boosts. Looking forward, we hope that these features will enable our users to continue to push the boundaries of complexity and scale in quantum computing and machine learning and eventually help lead to groundbreaking quantum computing experiments (not just simulations). Our ultimate goal when we released TensorFlow Quantum was to have it aid in the search for quantum advantage in the field of machine learning. In time, it is our hope to see the world reach that goal, with the help of the continued hard work and dedication of the QML research community. Quantum machine learning is still a very young field and there\u2019s still a long way to go before this happens, but over the past year we\u2019ve seen the community make amazing strides in many different areas and we can\u2019t wait to see what you will accomplish in the years to come.",
    "link": "https://blog.tensorflow.org/2021/03/tensorflow-quantum-turns-one-year-old.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-UVJcN2nd85k/YFNdy_WDdJI/AAAAAAAAEHM/ah_Pk2sk7UMYJmSyy0OAXOadq2HoxD69QCLcBGAsYHQ/s0/pasted%2Bimage%2B0%2B%252844%2529.png",
      "https://1.bp.blogspot.com/-UVJcN2nd85k/YFNdy_WDdJI/AAAAAAAAEHM/ah_Pk2sk7UMYJmSyy0OAXOadq2HoxD69QCLcBGAsYHQ/s0/pasted%2Bimage%2B0%2B%252844%2529.png"
    ],
    "time": "2023/12/09 00:58:43"
  },
  {
    "title": "A Tour of SavedModel Signatures",
    "content": "Posted by Daniel Ellis, TensorFlow Engineer\nNote: This blog post is aimed at TensorFlow developers who want to learn the details of how graphs and models are stored. If you are new to TensorFlow, you should check out the TensorFlow Basics guides before reading this article.\nTensorFlow can run models without the original Python objects, as demonstrated by TensorFlow Serving and TensorFlow Lite, or when you download a trained model from TensorFlow Hub.\nModels and layers can be loaded from this representation without actually making an instance of the Python class that created it. This is desired in situations where you do not have (or want) a Python interpreter, such as serving at scale or on an edge device, or in situations where the original Python code is not available.\nSaved models are represented by two separate, but equally important, parts: the graph, which describes the fixed computation described in code, and the weights, which are the dynamic parameters you trained during training. If you aren't already familiar with this and @tf.function, you should check out the Introduction to graphs and functions guide as well as the section on saving in the modules, layers, and models guide.\nFrom a code standpoint, functions decorated with @tf.function create a Python callable; in the documentation we refer to these as polymorphic functions, as they are Python callables that can take a variety argument signatures. Each time you call a @tf.function with a new argument signature, TensorFlow traces out a new graph just for that set of arguments. This new graph is then added as a \"concrete function\" to the callable. Thus, a saved model can be one or more subgraphs, each with a different signature.\nA SavedModel is what you get when you call tf.saved_model.save(). Saved models are stored as a directory on disk. The file, saved_model.pb,within that directory, is a protocol buffer describing the functional tf.Graph.\nIn this blog post, we'll take a look inside this protobuf and see how function signature serialization and deserialization works under the hood. After reading this, you'll have a greater appreciation for what functions and signatures before, which can help you load, modify, or optimize saved models.\nBackground\nThere are a total of five places inputs to functions are defined in the saved model protobuf. It can be tough to understand and remember what each of these does. This post intends to inventory each of these definitions and what they\u2019re used for. It also goes through a basic example illustrating what a simple model looks like after serialization.\nThe actual APIs you use will always be carefully versioned (as they have been since 2016), and the models themselves will conform to the version compatibility guide. However, the material in this document lays out a snapshot of the existing state of things. Any links to code will include point-in-time revisions so as not to drift out of date. As with all non-documented implementation details, these details are subject to change in the future.\nWe\u2019ll occasionally use the term \u201csignatures\u201d to talk about the general concept of describing function inputs (e.g. in the title of this document). In this sense, we will be referring not just to TensorFlow\u2019s specific concept of signatures, but all of the ways TensorFlow defines and validates inputs to functions. Context should make the meaning clear.\nWhat This Is Not About\nThis document is not intended to describe how signatures or functions work from a user perspective. It is intended for TensorFlow developers working on the internals of TensorFlow. Likewise, this document does not make a statement of the way things \u201cshould\u201d be. It aims to simply document the way things are.\nOverview of Signature Definitions\nThere are five protos that store definitions of function inputs in one manner or another. Their names and code locations, as well as their paths within the saved model proto, are as follows:\nProto messages, and their location in SavedModel\nFunctionDef: meta_graphs -> graph_def -> library -> function\nSignatureDef: meta_graphs -> signature_def\nSavedFunction: meta_graphs -> object_graph_def -> nodes -> kind -> function\nSavedBareConcreteFunction: meta_graphs -> object_graph_def -> nodes -> kind -> bare_concrete_function\nSavedConcreteFunction: meta_graphs -> object_graph_def -> concrete_functions\nFunctionDef\nOf the five definitions discussed in this document, FunctionsDefs are the most core to execution. When loading a saved model, these function definitions are registered in the function library of the runtime and used to create ConcreteFunctions. These functions can then be executed via PartitionedCall or TFE_Py_Execute.\nThis is where the actual nodes describing execution are defined, as well as what the inputs and outputs to the function are.\nSignatureDef\nSignatureDefs are generated from signatures passed into @tf.function. We do not save the signature\u2019s TensorSpecs directly, however. Instead, when saving, we call the underlying function using the TensorSpecs in order to generate a concrete function. From there, we inspect the generated concrete function to get the inputs and outputs, storing them on the SignatureDef.\nOn the loading side,SignatureDefs are essentially ignored. They are primarily used in v1 or C++, where the developer loading the model can inspect the returned SignatureDef protos directly. This allows them to use their desired signature name to lookup the placeholder and output names needed for execution.\nThese input and output names can then be passed into feeds and fetches when calling Session.run in TensorFlow V1 code.\nSavedFunction\nSavedFunction is one of the many types of SavedObjects in the nodes list of the ObjectGraphDef. SavedFunctions are restored into a RestoredFunctions at load time. Like all nodes in this list, they are then attached to the returned model via the hierarchy defined by the children ObjectReference field.\nSavedFunction\u2019s main purpose is polymorphism. SavedFunctions support polymorphism by specifying a number of concrete function names defined in the function library above (via FunctionDef). At call time, we iterate through the concrete function names to find the first whose signature matches. If we find a match, we call it; if not, we throw an exception.\nThere is one more bit of complexity. When a RestoredFunction is called with a particular set of arguments, a new concrete function is created whose sole purpose is to call the matching concrete function. This is done using restored_function_body under the hood and is where the logic lives to find the appropriate concrete function.\nThis is invisible in the SavedModel proto, but these extra concrete functions are registered at call time in the runtime\u2019s function library just as the other function library functions are.\nThe second purpose of SavedFunction is to update the FunctionSpec of all associated ConcreteFunctions using the FunctionSpec stored on the SavedFunction. This function spec is used at call time to\nvalidate passed in structured arguments, and\nconvert structured arguments into flat ones needed for calling the underlying concrete function.\nSavedBareConcreteFunction\nSimilar to SavedFunctions, SavedBareConcreteFunctions are used to update a\nspecific concrete function\u2019s arguments and function spec. This is done here. Unlike SavedFunctions, they only reference a single specific concrete function.\nIn practice, SavedBareConcreteFunctions are commonly attached to and accessed via the signatures map (i.e. the signatures attribute on the loaded object). The underlying concrete functions they modify, in this case, are signature_wrapper functions. This wrapping is done to format the output in the way v1 expects (i.e. a dictionary of tensors). Similar to restored_function_body concrete functions, and other than restructuring the output, these concrete functions do nothing but call their associated concrete functions.\nSavedConcreteFunction\nSavedConcreteFunction objects are not SavedObjectGraph nodes. They are stored in a map directly on the SavedObjectGraph. These objects reference a specific, already-registered concrete function -- the key in the map is that concrete function\u2019s registered name.\nThese objects serve two purposes. The first is handling function \"captures\" via\nthe bound_inputs field. Captured variables are those a function reads or modifies that were not explicitly passed in when calling into the function. Since functions in the function library do not have a concept of captured variables, any variables used by the function must be passed in as an argument. bound_inputs stores a list of node IDs that should be passed in to the underlying ConcreteFunction when called. We set this up here.\nThe second purpose, and similar to SavedFunction and SavedBareConcreteFunction, is modifying the existing concrete function\u2019s FuncGraph structured inputs and outputs. This also is used for argument validation. The setup for this is done here.\nExample Walkthrough\nA simple example may help illustrate all of this with more clarity. Let\u2019s make a basic model and take a look at the subsequent generated proto to get a better feel for what\u2019s going on.\nBasic Model\nclass ExampleModel(tf.Module):\n\n  @tf.function(input_signature=[tf.TensorSpec(shape=(), dtype=tf.float32)])\n  def capture_fn(self, x):\n    if not hasattr(self, 'weight'):\n      self.weight = tf.Variable(5.0, name='weight')\n    self.weight.assign_add(x * self.weight)\n    return self.weight\n\n  @tf.function\n  def polymorphic_fn(self, x):\n    return tf.constant(3.0) * x\n\nmodel = ExampleModel()\nmodel.polymorphic_fn(tf.constant(4.0))\nmodel.polymorphic_fn(tf.constant([1.0, 2.0, 3.0]))\ntf.saved_model.save(\n    model, \"/tmp/example-model\", signatures={'capture_fn': model.capture_fn})\nThis model contains the basis for most of the complexity we\u2019ll need to fully explore the intricacies of saving and signatures. This will allow us to look at functions with and without signatures, with and without captures, and with and without polymorphism.\nFunction with Captures\nLet\u2019s start by looking at our function with captures, capture_fn. We can see we have a concrete function defined in the function library, as expected:\nA FunctionDef located in FunctionDefLibrary of MetaGraphDef.graph_def\nNote the expected float input, \"x\", as well as the additional captured argument, \"mul_readvariableop_resource\". Since this function has a capture, we should see a variable being referenced in the bound_inputs field of one of our SavedConcreteFunctions:\nA SavedConcreteFunction located in the concrete_functions map of the ObjectGraphDef\nIndeed, we can see bound_inputs refers to node 1, which is a SavedVariable with the name and dtype we expect:\nA SavedVariable located in ObjectGraphDef.nodes\nNote that we also are storing on canonicalized_input_signature additional data that will be used to modify the concrete function. The key of this object, \"__inference_capture_fn_59\", is the same name as the concrete function registered in our function library.\nSince we\u2019ve specified a signature, we should also see a SavedBareConcreteFunction:\nA SavedBareConcreteFunction located in ObjectGraphDef.nodes\nAs discussed above, we use the function spec and argument information to modify the underlying concrete function. But what\u2019s up with the \"__inference_signature_wrapper_68\" name? And how does this fit in with the rest of the code?\nFirst, note that this is the fifth (5) node in the node list. This will come up again shortly.\nLet\u2019s start by looking at the nodes list. If we start at the first node in the nodes list, we\u2019ll see a \"signatures\" node attached as a child:\nA SavedUserObject located in ObjectGraphDef.nodes\nIf we look at node 2, we\u2019ll see this node is a signature map that references one final node: node 5, our BareConcreteSavedFunction.\nA SavedUserObject located in ObjectGraphDef.nodes\nThus, when we access this function via model.signatures[\"capture_fn\"], we will actually be calling into this intermediate signature wrapper function first.\nAnd what does that function, \"__inference_signature_wrapper_68\", look like?\nA FunctionDef located in FunctionDefLibrary of MetaGraphDef.graph_def\nIt takes the arguments we expect, and makes a call out to\u2026 \"__inference_capture_fn_59\", our original function! Just as we expect.\nBut wait\u2026 what happens if we don\u2019t access our function via model.signatures[\"capture_fn\"]? After all, we should be able to call it directly via model.capture_fn.\nNotice above, we had a child on the top level object named \"capture_fn\" with a node_id of 3. If we look at node 3, we\u2019ll see a SavedFunction object that references our original concrete function with no signature wrapper intermediary:\nA SavedFunction located in ObjectGraphDef.nodes\nAgain, the function spec is used to modify the function spec of our concrete function, \"__inference_capture_fn_59\". Notice also that concrete_functions here is a list. We only have one item right now, but this will come up again when we take a look at our polymorphic function example.\nNow, we\u2019ve fully mapped essentially everything needed for execution of this function, but we have one last thing to look at: SignatureDef. We\u2019ve defined a signature, so we expect a SignatureDef to be defined:\nA SignatureDef located in the MetaObjectGraph.signature_def map\nThis is very important for loading in v1 and C++ for serving. Note those funky names: \"capture_fn_x:0\" and \"StatefulPartitionedCall:0\". To call this function in v1, we need a way to map our nice argument names to the actual graph placeholder names for passing in as feeds and fetches (and doing validation, if we wish). Looking at this SignatureDef allows us to do just that.\nPolymorphic Functions\nWe\u2019re not quite done yet. Let\u2019s take a look at our polymorphic function. We won\u2019t repeat everything, since a lot of it is the same. We won\u2019t have any signature wrapper functions or signature defs, since we skipped the signature on this one. Let\u2019s look at what\u2019s different.\nA FunctionDef located in FunctionDefLibrary of MetaGraphDef.graph_def\nFor one, we now have two concrete functions registered in the function library, each with slightly different input shapes.\nWe also have two SavedConcreteFunction modifiers:\nTwo SavedConcreteFunctions located in the concrete_functions map of the ObjectGraphDef\nAnd finally, we can see our SavedFunction references two underlying concrete functions instead of one:\nA SavedFunction located in ObjectGraphDef.nodes\nThe function spec here will be attached to both of these concrete functions at load time. When we call our SavedFunction, it will use the arguments we pass in to find the correct concrete function and execute it.\nNext Steps\nYou should now be an expert on how functions and their signatures are saved at a code level. Remember, what's described in this blog post is how the code works right now. For updated code and examples in the future, see the official documentation on tensorflow.org.\nSpeaking of documentation, if you want a fast introduction to the basic APIs for saved models, you should introductory articles on how the APIs for functions and modules are traced and saved. For experts, don't miss this detailed guide on SavedModel itself, as well as a complete discussion of autograph.\nAnd finally, if you do any exciting or useful protobuf surgery, share with us on Twitter. Thanks for reading this far!",
    "link": "https://blog.tensorflow.org/2021/03/a-tour-of-savedmodel-signatures.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-Cd9xtmKVWZk/YD65kLEyq8I/AAAAAAAAEGY/oIria9iDaoMGgVq7VIghTxMYUieH6ynxgCLcBGAsYHQ/s0/TensorFlow-SavedModelSIGs-Social-v2.png",
      "https://1.bp.blogspot.com/-pDhxTr7XWlU/YD2C_VobFuI/AAAAAAAAEE4/SNRHJucbLxALKYRa4U8lxABsfNNSgtL3wCLcBGAsYHQ/s0/image%2B1.png",
      "https://1.bp.blogspot.com/-7jvR81DAvh0/YD2Es00bEgI/AAAAAAAAEFA/DmuZy4T1bfge3TLkhk34XR7n4b9Xz7m7ACLcBGAsYHQ/s0/image%2B2.png",
      "https://1.bp.blogspot.com/-qsNGXS2dIyM/YD2Fws-VXAI/AAAAAAAAEFI/h68Rq9yx_dM6TdB-Wj_0TGxURSlS0BErgCLcBGAsYHQ/s0/image%2B4.png",
      "https://1.bp.blogspot.com/-Wt7A79EVv5M/YD2Goqg0iDI/AAAAAAAAEFQ/-22-q5d0h0wd3aQb2t3h8UpNebA13HRpQCLcBGAsYHQ/s0/image%2B5.png",
      "https://1.bp.blogspot.com/-ZbS0nip-jH4/YD2Hhx5T5YI/AAAAAAAAEFY/sfwoT4_9_f0Ds6XRhV8tkqOdfNFXcDw8QCLcBGAsYHQ/s0/image%2B6.png",
      "https://1.bp.blogspot.com/-h6miDxtJmqw/YD2IsDbHt5I/AAAAAAAAEFg/2NxQoReg91EmnRcvwgtLxJuP7sCaTr6xgCLcBGAsYHQ/s0/image%2B7.png",
      "https://1.bp.blogspot.com/-9OL3chFygz4/YD2JpR0aaLI/AAAAAAAAEFo/QUGRPEFU8E8kc_Th1Mc0KDd1eeP_ySVDgCLcBGAsYHQ/s0/image%2B8.png",
      "https://1.bp.blogspot.com/-7X7M-LgFjjQ/YD2LTIhQbtI/AAAAAAAAEFw/qLSrMEZFOY4SBQr_XOPcKSFXv_sqCrswQCLcBGAsYHQ/s0/image%2B9.png",
      "https://1.bp.blogspot.com/-S-zEoTxT-G8/YD2MG56qw2I/AAAAAAAAEF4/becvt0tbAqITbPU4dZtttcvTSCTxFr_PQCLcBGAsYHQ/s0/image%2B10.png",
      "https://1.bp.blogspot.com/-4P83mxGfOFA/YD-9_DMOmMI/AAAAAAAAEGg/cAgUE4Si200l-9xwk62SuCc8hIqdYyAhgCLcBGAsYHQ/s0/polymorphic.png",
      "https://1.bp.blogspot.com/-tvLBh1Jj_Do/YD-_WFOmDlI/AAAAAAAAEGw/qwcA6P5rGxAUXPtjT99WodoxgKVDG5VtACLcBGAsYHQ/s0/saved-concrete-functions.png",
      "https://1.bp.blogspot.com/-zV1TuJt3QWk/YD--3KGudLI/AAAAAAAAEGo/AU1tksOAj_kZJKRqPebj1TOOuSqijxF2ACLcBGAsYHQ/s0/saved-function.png"
    ],
    "time": "2023/12/09 00:58:43"
  },
  {
    "title": "Transfer Learning for Audio Data with YAMNet",
    "content": "Posted by Luiz GUStavo Martins, Developer Advocate\nTransfer learning is a popular machine learning technique, in which you train a new model by reusing information learned by a previous model. Most common applications of transfer learning are for the vision domain, to train accurate image classifiers, or object detectors, using a small amount of data -- or for text, where pre-trained text embeddings or language models like BERT are used to improve on natural language understanding tasks like sentiment analysis or question answering. In this article, you'll learn how to use transfer learning for a new and important type of data: audio, to build a sound classifier.\nThere are many important use cases of audio classification, including to protect wildlife, to detect whales and even to fight against illegal deforestation.\nWith YAMNet, you can create a customized audio classifier in a few easy steps:\nPrepare and use a public audio dataset\nExtract the embeddings from the audio files using YAMNet\nCreate a simple two layer classifier and train it.\nSave and test the final model\nYou can follow the code here in this tutorial.\nThe YAMNet model\nYAMNet (\"Yet another Audio Mobilenet Network\") is a pretrained model that predicts 521 audio events based on the AudioSet corpus.\nThis model is available on TensorFlow Hub including the TFLite and TF.js versions, for running the model on mobile and the web. The code can be found on their repository.\nThe model has 3 outputs:\nClass scores that you'd use for inference\nEmbeddings, which are the important part for transfer learning\nLog Mel Spectrograms to provide a visualization of the input signal\nThe model takes a waveform represented as 16 kHz samples in the range [-1.0, 1.0], frames it in windows of 0.96 seconds and hop of 0.48 seconds, and then runs the core of the model to extract the embeddings on a batch of these frames.\nThe 0.96 seconds windows hopping over a waveform\nAs an example, trying the model with this audio file [link] will give you these results:\nYour browser does not support the audio element.\nThe first graph is the waveform. The second graph is the log-mel spectrogram. The third graph shows the class probability predictions per frame of audio, where darker is more likely.\nThe ESC-50 dataset\nTo do transfer learning with the model, you'll use the Dataset for Environmental Sound Classification, or ESC-50 for short. This is a collection of 2000 environmental audio recordings from 50 classes. Each recording is 5 seconds long and they came originally from the Freesound project.\nThe ESC-50 has the classes Dog and Cat that you'll need.\nThe dataset has two important components: the audio files and a metadata csv file with the metadata about every audio file.\nThe columns in the metadata csv file contains information that will be used to train the model:\nFilename gives the name of the .wav audio file\nCategory is the human-readable class name for the numeric target id\nTarget is the unique numeric id of the category\nFold ensures that clips originating from the same initial source are always contained in the same group. This is important to avoid cross-contamination when splitting the data into train, validation and test sets and for cross-validation.\nFor more detailed information you can read the original ESC paper.\nWorking with the dataset\nTo load the dataset, you'll start from the metadata file and load it using the Pandas method read_csv.\nWith the loaded dataframe, the next steps are to filter by the classes that will be used, in this case: Dogs and Cats.\nNext step would be to load the audio files to start the process but if there are too many audio files, just loading all of them to memory can be prohibitive and lead to out of memory issues. The best solution is to lazily load the audio files when needed. TensorFlow can help do this easily with tf.data.Dataset and the map method.\nLet's create the Dataset from the the previous created pandas dataframe and apply the load_wav method to all the files:\nfilenames = filtered_pd['filename']\ntargets = filtered_pd['target']\nfolds = filtered_pd['fold']\n \nmain_ds = tf.data.Dataset.from_tensor_slices((filenames, targets, folds))\nmain_ds = main_ds.map(load_wav_for_map)\nHere, no audio file was loaded to memory yet since the mapping wasn't evaluated. For example, if you request a size of the dataset for example (len(list(train_ds.as_numpy_iterator()))\n), that would make the map function to be evaluated and load all the files.\nThe same technique will be used to extract all the features (embeddings) from each audio file.\nExtracting the audio embeddings\nHere you are going to load the YAMNet model from TensorFlow Hub. All you need is the model's handle, and call the load method from the tensorflow_hub library.\nyamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\nyamnet_model = hub.load(yamnet_model_handle)\nThis will load the model to memory ready to be used.\nFor each audio file, you'll extract the embeddings using the YAMNet model. For each audio file, YAMNet is executed. The embeddings output is paired with the same label and folder from the audio file.\ndef extract_embedding(wav_data, label, fold):\n  ''' run YAMNet to extract embedding from the wav data '''\n  scores, embeddings, spectrogram = yamnet_model(wav_data)\n  num_embeddings = tf.shape(embeddings)[0]\n  return (embeddings,\n            tf.repeat(label, num_embeddings), \n            tf.repeat(fold, num_embeddings))\n\nmain_ds = main_ds.map(extract_embedding).unbatch()\nThese embeddings will be the input for the classification model. From the model's documentation, you can read that for a given audio file, it will frame the waveform into sliding windows of length 0.96 seconds and hop 0.48 seconds, and then run the core of the model. So, in summary, for each 0.48 seconds, the model will output one embedding array with 1024 float values. This part is also done using map(), so again, lazy evaluation and that's why it executes so fast.\nThe final dataset contains the three used columns: embedding, label and fold.\nThe last dataset operation is to split into train, validation and test datasets. To do so the filter() method and use the fold field (an integer between 1 and 5) as criteria.\ncached_ds = main_ds.cache()\ntrain_ds = cached_ds.filter(lambda embedding, label, fold: fold < 4)\nval_ds = cached_ds.filter(lambda embedding, label, fold: fold == 4)\ntest_ds = cached_ds.filter(lambda embedding, label, fold: fold == 5)\nTraining the Classifier\nWith the YAMNet embedding vectors and the label, the next step is to train a classifier that learns what's a dog's sound and what is a cat's sound.\nThe classifier model is very simple with just two dense layers, but as you'll see this is enough for the amount of data used.\nmy_model = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=(1024), dtype=tf.float32, name='input_embedding'),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dense(len(my_classes))            \n])\nSaving the final model\nThe model that was trained works and has good accuracy but the input it expects is not an audio waveform but an embedding array. To address this problem, the final model will combine YAMNet as the input layer and the model just trained. This way, the final model will accept a waveform and output the class:\ninput_segment = tf.keras.layers.Input(shape=(), dtype=tf.float32,\n                                       name='audio')\nembedding_extraction_layer = hub.KerasLayer('https://tfhub.dev/google/yamnet/1', trainable=False)\nscores, embeddings, spectrogram = embedding_extraction_layer(input_segment)\nserving_outputs = my_model(embeddings_output)\nserving_outputs = ReduceMeanLayer(axis=0, name='classifier')(serving_outputs)\nserving_model = tf.keras.Model(input_segment, serving_outputs)\nserving_model.save(saved_model_path, include_optimizer=False)\nTo try the reloaded model, you can use the same way it was used earlier in the colab:\nreloaded_model = tf.saved_model.load(saved_model_path)\nreloaded_results = reloaded_model(testing_wav_data)\ncat_or_dog = my_classes[tf.argmax(reloaded_results)]\nThis model can also be used with TensorFlow Serving with the 'serving_default'\nserving_results =  reloaded_model.signatures['serving_default'](testing_wav_data)\ncat_or_dog = my_classes[tf.argmax(serving_results['classifier'])]\nIn this post, you learned how to use the YAMNet model for transfer learning to recognize audio of dogs and cats from the ESC-50 dataset.\nCheck out the YAMNet model on tfhub.dev and the tutorial on tensorflow.org. You can apply this technique to your own dataset, or to other classes in the ESC-50 dataset.\nWe would love to know what you can build with this! Share your project with us on social media by using the hashtag #TFHub.\nAcknowledgements\nWe\u2019d like to thank a number of colleagues for their contribution to this work: Dan Ellis, Manoj Plakal and Eduardo Fonseca for an amazing YAMNet model and support with the colab and multiple reviews.\nMark Daoust and Elizabeth Kemp have greatly improved the presentation of the material in this post and the associated tutorial.",
    "link": "https://blog.tensorflow.org/2021/03/transfer-learning-for-audio-data-with-yamnet.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-CLyq7ilQIow/YDawZXp_NiI/AAAAAAAAEEg/vVa58jb24Fkw-LZPsezB_qMdnvndOYuzwCLcBGAsYHQ/s0/yamnet_animation%2B%25282%2529.gif",
      "https://1.bp.blogspot.com/-CLyq7ilQIow/YDawZXp_NiI/AAAAAAAAEEg/vVa58jb24Fkw-LZPsezB_qMdnvndOYuzwCLcBGAsYHQ/s0/yamnet_animation%2B%25282%2529.gif",
      "https://1.bp.blogspot.com/-EsE02ujNqfY/YDaxFZNZBYI/AAAAAAAAEEo/N3xC1fYXvHkdrNq8VHjoVChU6DA4-YxZgCLcBGAsYHQ/s0/image%2B2.png"
    ],
    "time": "2023/12/09 00:58:43"
  },
  {
    "title": "Variational Inference with Joint Distributions in TensorFlow Probability",
    "content": "Posted by Emily Fertig, Joshua V. Dillon, Wynn Vonnegut, Dave Moore, and the TensorFlow Probability team\nIn this post, we introduce new tools for variational inference with joint distributions in TensorFlow Probability, and show how to use them to estimate Bayesian credible intervals for weights in a regression model.\nOverview\nVariational Inference (VI) casts approximate Bayesian inference as an optimization problem and seeks a 'surrogate' posterior distribution that minimizes the KL divergence with the true posterior. Gradient-based VI is often faster than MCMC methods, composes naturally with optimization of model parameters, and provides a lower bound on model evidence that can be used directly for model comparison, convergence diagnosis, and composable inference.\nTensorFlow Probability (TFP) offers tools for fast, flexible, and scalable VI that fit naturally into the TFP stack. These tools enable the construction of surrogate posteriors with covariance structures induced by linear transformations or normalizing flows.\nVI can be used to estimate Bayesian credible intervals for parameters of a regression model to estimate the effects of various treatments or observed features on an outcome of interest. Credible intervals bound the values of an unobserved parameter with a certain probability, according to the posterior distribution of the parameter conditioned on observed data and given an assumption on the parameter's prior distribution.\nIn this post, we demonstrate how to use VI to obtain credible intervals for parameters of a Bayesian linear regression model for radon levels measured in homes (using Gelman et al.'s (2007) Radon dataset; see similar examples in Stan). We demonstrate how TFP JointDistributions combine with bijectors to build and fit two types of expressive surrogate posteriors:\na standard Normal distribution transformed by a block matrix. The matrix may reflect independence among some components of the posterior and dependence among others, relaxing the assumption of a mean-field or full-covariance posterior.\na more complex, higher-capacity inverse autoregressive flow.\nThe surrogate posteriors are trained and compared with results from a mean-field surrogate posterior baseline. These plots show credible intervals for four model parameters obtained with the three VI surrogate posteriors, as you\u2019ll learn about below, as well as Hamiltonian Monte Carlo (HMC) for comparison.\nYou can follow along and see all the details in this Google Colab.\nExample: Bayesian hierarchical linear regression on Radon measurements\nRadon is a radioactive gas that enters homes through contact points with the ground. It is a carcinogen that is the primary cause of lung cancer in non-smokers. Radon levels vary greatly from household to household.\nThe EPA did a study of radon levels in 80,000 houses. Two important predictors are:\nFloor on which the measurement was taken (radon higher in basements)\nCounty uranium level (positive correlation with radon levels)\nPredicting radon levels in houses grouped by county is a classic problem in Bayesian hierarchical modeling, introduced by Gelman and Hill (2006). We are interested in credible intervals for the effect of location (county) on the radon level of houses in Minnesota. In order to isolate this effect, the effects of floor and uranium level are also included in the model. Additionally, we will incorporate a contextual effect corresponding to the mean floor on which the measurement was taken, by county, so that if there is variation among counties of the floor on which the measurements were taken, this is not attributed to the county effect.\nThe regression model is specified as follows:\nin which i indexes the observations and countyi is the county in which the ith observation was taken.\nWe use a county-level random effect to capture geographical variation. The parameters uranium_weight and county_floor_weight are modeled probabilistically, and floor_weight and the constant bias are deterministic. These modeling choices are largely arbitrary, and are made for the purpose of demonstrating VI on a probabilistic model of reasonable complexity. For a more thorough discussion of multilevel modeling with fixed and random effects in TFP, using the radon dataset, see Multilevel Modeling Primer and Fitting Generalized Linear Mixed-effects Models Using Variational Inference.\nThe full code for this example is available on Github.\nVariables are defined for the deterministic parameters and the Normal distribution scale parameters, with the latter constrained to be positive.\nimport tensorflow as tf\nimport tensorflow_probability as tfp\ntfd = tfp.distributions\ntfb = tfp.bijectors\n\nfloor_weight = tf.Variable(0.)\nbias = tf.Variable(0.)\nlog_radon_scale = tfp.util.TransformedVariable(1., tfb.Exp())\ncounty_effect_scale = tfp.util.TransformedVariable(1., tfb.Exp())\nWe specify the probabilistic graphical model for the regression as a TFP JointDistribution.\n@tfd.JointDistributionCoroutineAutoBatched\ndef model():\n uranium_weight = yield tfd.Normal(0., scale=1., name='uranium_weight')\n county_floor_weight = yield tfd.Normal(\n     0., scale=1., name='county_floor_weight')\n county_effect = yield tfd.Sample(\n     tfd.Normal(0., scale=county_effect_scale),\n     sample_shape=[num_counties], name='county_effect')\n yield tfd.Normal(\n     loc=(log_uranium * uranium_weight\n          + floor_of_house * floor_weight\n          + floor_by_county * county_floor_weight\n          + tf.gather(county_effect, county, axis=-1)\n          + bias),\n     scale=log_radon_scale[..., tf.newaxis],\n     name='log_radon')\nWe pin log_radon to the observed radon data to model the unnormalized posterior.\ntarget_model = model.experimental_pin(log_radon=log_radon)\nQuick summary of Bayesian Variational Inference\nSuppose we have the following generative process, where \ud835\udf03 represents random parameters (uranium_weight, county_floor_weight, and county_effect in the regression model) and \u03c9 represents deterministic parameters (floor_weight, log_radon_scale, county_effect_scale, and bias). The x\ud835\udc56 are features (log_uranium, floor_of_house, and floor_by_county) and the \ud835\udc66\ud835\udc56 are target values (log_radon) for \ud835\udc56 = 1...n observed data points:\nVI is then characterized by:\n(Technically we're assuming q is absolutely continuous with respect to r. See also, Jensen's inequality.)\nSince the bound holds for all q, it is obviously tightest for:\nRegarding terminology, we call\nq* the \"surrogate posterior,\" and,\nQ the \"surrogate family.\"\n\u03c9* represents the maximum-likelihood values of the deterministic parameters on the VI loss. See this survey for more information on variational inference.\nExpressive surrogate posteriors\nNext we estimate the posterior distributions of the parameters using VI with two different types of surrogate posteriors:\nA constrained multivariate Normal distribution, with covariance structure induced by a blockwise matrix transformation.\nA multivariate standard Normal distribution transformed by an Inverse Autoregressive Flow, which is then split and restructured to match the support of the posterior.\nMultivariate Normal surrogate posterior\nTo build this surrogate posterior, a trainable linear operator is used to induce correlation among the components of the posterior.\nWe begin by constructing a base distribution with vector-valued standard Normal components, with sizes equal to the sizes of the corresponding prior components. The components are vector-valued so they can be transformed by the linear operator.\nflat_event_size = tf.nest.map_structure(\n     tf.reduce_prod,\n     tf.nest.flatten(target_model.event_shape_tensor()))\nbase_standard_dist = tfd.JointDistributionSequential(\n     [tfd.Sample(tfd.Normal(loc=0., scale=1.), s)\n      for s in flat_event_size])\nTo this distribution, we apply a trainable blockwise lower-triangular linear operator to induce correlation in the posterior. Within the linear operator, a trainable full-matrix block represents full covariance between two components of the posterior, while a block of zeros (or None) expresses independence. Blocks on the diagonal are either lower-triangular or diagonal matrices, so that the entire block structure represents a lower-triangular matrix.\nApplying this bijector to the base distribution results in a multivariate Normal distribution with mean 0 and (Cholesky-factored) covariance equal to the lower-triangular block matrix.\noperators = (\n  (tf.linalg.LinearOperatorDiag,),  # Variance of uranium weight (scalar).\n  (tf.linalg.LinearOperatorFullMatrix,  # Covariance between uranium and floor-by-county weights.\n   tf.linalg.LinearOperatorDiag),  # Variance of floor-by-county weight (scalar).\n  (None,  # Independence between uranium weight and county effects.\n   None,  # Independence between floor-by-county and county effects.\n   tf.linalg.LinearOperatorDiag)  # Independence among the 85 county effects.\n)\nblock_tril_linop = (\n  tfp.experimental.vi.util.build_trainable_linear_operator_block(\n      operators, flat_event_size))\nscale_bijector = tfb.ScaleMatvecLinearOperatorBlock(block_tril_linop)\nFinally, we allow the mean to take nonzero values by applying trainable Shift bijectors.\nloc_bijector = tfb.JointMap(\n   tf.nest.map_structure(\n       lambda s: tfb.Shift(\n           tf.Variable(tf.random.uniform(\n               (s,), minval=-2., maxval=2., dtype=tf.float32))),\n       flat_event_size))\nThe resulting multivariate Normal distribution, obtained by transforming the standard Normal distribution with the scale and location bijectors, must be reshaped and restructured to match the prior, and finally constrained to the support of the prior.\nreshape_bijector = tfb.JointMap(\n   tf.nest.map_structure(tfb.Reshape, flat_event_shape))\nunflatten_bijector = tfb.Restructure(\n       tf.nest.pack_sequence_as(\n           event_shape, range(len(flat_event_shape))))\nevent_space_bijector = target_model.experimental_default_event_space_bijector()\nNow, put it all together -- chain the trainable bijectors together and apply them to the base standard Normal distribution to construct the surrogate posterior.\nsurrogate_posterior = tfd.TransformedDistribution(\n   base_standard_dist,\n   bijector = tfb.Chain([  # Chained bijectors are applied in reverse order.\n        event_space_bijector,  # Constrain to the support of the prior.\n        unflatten_bijector,  # Pack components into the event_shape structure.\n        reshape_bijector,  # Reshape the vector-valued components.\n        loc_bijector,  # Allow for nonzero mean.\n        scale_bijector  # Apply the block matrix transformation.\n      ]))\nTrain the multivariate Normal surrogate posterior.\noptimizer = tf.optimizers.Adam(learning_rate=1e-2)\n@tf.function(jit_compile=True)\ndef run_vi():\n return tfp.vi.fit_surrogate_posterior(\n   target_model.unnormalized_log_prob,\n   surrogate_posterior,\n   optimizer=optimizer,\n   num_steps=10**4,\n   sample_size=16)\nmvn_loss = run_vi()\nmvn_samples = surrogate_posterior.sample(1000)\nSince the trained surrogate posterior is a TFP distribution, we can take samples from it and process them to produce posterior credible intervals for the parameters.\nThe box-and-whiskers plots below show 50% and 95% credible intervals for the county effect of the two largest counties and the regression weights on soil uranium measurements and mean floor by county. The posterior credible intervals for county effects indicate that location in St. Louis county is associated with lower radon levels, after accounting for other variables, and that the effect of location in Hennepin county is near neutral.\nPosterior credible intervals on the regression weights show that higher levels of soil uranium are associated with higher radon levels, and counties where measurements were taken on higher floors (likely because the house didn't have a basement) tend to have higher levels of radon, which could relate to soil properties and their effect on the type of structures built.\nThe (deterministic) coefficient of floor is -0.7, indicating that lower floors have higher radon levels, as expected.\nInverse autoregressive flow surrogate posterior\nInverse autoregressive flows (IAFs) are normalizing flows that use neural networks to capture complex, nonlinear dependencies among components of the distribution. Next we build an IAF surrogate posterior to see whether this higher-capacity, more flexible model outperforms the constrained multivariate Normal.\nWe begin by building a standard Normal distribution with vector event shape, of length equal to the total number of degrees of freedom in the posterior.\nbase_distribution = tfd.Sample(\n   tfd.Normal(loc=0., scale=1.),\n   sample_shape=[tf.reduce_sum(flat_event_size)])\nA trainable IAF transforms the Normal distribution.\nnum_iafs = 2\niaf_bijectors = [\n   tfb.Invert(tfb.MaskedAutoregressiveFlow(\n       shift_and_log_scale_fn=tfb.AutoregressiveNetwork(\n           params=2,\n           hidden_units=[256, 256],\n           activation='relu')))\n   for _ in range(num_iafs)\n]\nThe IAF bijectors are chained together with other bijectors to build a surrogate posterior with the same event shape and support as the prior.\niaf_surrogate_posterior = tfd.TransformedDistribution(\n   base_distribution,\n   bijector=tfb.Chain([\n        event_space_bijector,  # Constrain to the support of the prior.\n        unflatten_bijector,  # Pack components into the event_shape structure.\n        reshape_bijector,  # Reshape the vector-valued components.\n        tfb.Split(flat_event_size),  # Split into parts, same size as prior.\n   ] + iaf_bijectors))  # Apply a flow model.\nLike the multivariate Normal surrogate posterior, the IAF surrogate posterior is trained using tfp.vi.fit_surrogate_posterior. The credible intervals for the IAF surrogate posterior appear similar to those of the constrained multivariate Normal.\nMean-field surrogate posterior\nVI surrogate posteriors are often assumed to be mean-field (independent) Normal distributions, with trainable means and variances, that are constrained to the support of the prior with a bijective transformation. We define a mean-field surrogate posterior in addition to the two more expressive surrogate posteriors, using the same general formula as the multivariate Normal surrogate posterior. Instead of a blockwise lower triangular linear operator, we use a blockwise diagonal linear operator, in which each block is diagonal:\noperators = (\n  tf.linalg.LinearOperatorDiag,\n  tf.linalg.LinearOperatorDiag,\n  tf.linalg.LinearOperatorDiag,\n)\nblock_diag_linop = (\n   tfp.experimental.vi.util.build_trainable_linear_operator_block(\n       operators, flat_event_size))\nIn this case, the mean field surrogate posterior gives similar results to the more expressive surrogate posteriors, indicating that this simpler model may be adequate for the inference task. As a \u201cground truth\u201d, we also take samples with Hamiltonian Monte Carlo (see the Colab for the full example). All three surrogate posteriors produced credible intervals that are visually similar to the HMC samples, though sometimes under-dispersed due to the effect of the ELBO loss, as is common in VI.\nConclusion\nIn this post, we built VI surrogate posteriors using joint distributions and multipart bijectors, and fit them to estimate credible intervals for weights in a regression model on the radon dataset. For this simple model, more expressive surrogate posteriors appeared to perform similarly to a mean-field surrogate posterior. The tools we demonstrated, however, can be used to build a wide range of flexible surrogate posteriors suitable for more complex models.\nCheck out our code, documentation, and further examples on the TFP home page.",
    "link": "https://blog.tensorflow.org/2021/02/variational-inference-with-joint-distributions-in-tensorflow-probability.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-Oc13cqtwBQk/YC1KwqOA1EI/AAAAAAAAEEE/S1XhP6Ui-M4XVPeWvaB26MBSDrOo4mxFwCLcBGAsYHQ/s0/figure_1.jpeg",
      "https://1.bp.blogspot.com/-QhgWLdhPPhU/YCxbrwKdg7I/AAAAAAAAEDg/n4yLbNxRJPc2L0t35Pu6C2BDGpST_mhVwCLcBGAsYHQ/s0/figure_1%2Breal%2Bfinal.png",
      "https://1.bp.blogspot.com/-T_t2lZ8mPf8/YCxADkpkUZI/AAAAAAAAEBI/DUhtulVGtbYGQWD69fcBk2uQFypa9FHJwCLcBGAsYHQ/s0/vi_blog_eq_1.png",
      "https://1.bp.blogspot.com/-ACsqtU7csO0/YCxBgwOCuDI/AAAAAAAAEBU/dhfAN2OWq_4ILLQ-79ibqyGXVnG08VyLgCLcBGAsYHQ/s0/vi_blog_eq_2.png",
      "https://1.bp.blogspot.com/-pizBaOjYSxg/YC_4fdemT3I/AAAAAAAAEEU/hejXqykuveUsL1AdrJijGi-2HQrMwGfyACLcBGAsYHQ/s0/vi_blog_eq_3%2B%25281%2529.png",
      "https://1.bp.blogspot.com/-_zWE35CNqB4/YCxCd76sNqI/AAAAAAAAEBk/0cay5UiSfZ4wSNX7ZU7r6KDnd5x9VUrYACLcBGAsYHQ/s0/vi_blog_eq_4.png",
      "https://1.bp.blogspot.com/-lZN-65uCGqM/YCxb8W6_NQI/AAAAAAAAEDo/DMBQFeY5iJQd3oRvcFd-pMhM47CjIrhEgCLcBGAsYHQ/s0/figure_2%2Breal%2Bfinal.png",
      "https://1.bp.blogspot.com/-6x8eZsJYx78/YCxcYip2v8I/AAAAAAAAED0/angqUgtwOfgIyeixmhCxG-rTO6NyFmlhgCLcBGAsYHQ/s0/figure_3%2Breal%2Bfinal.png",
      "https://1.bp.blogspot.com/-_7h4vuisl7I/YCcIKMldIyI/AAAAAAAAEA8/Qr4iiOgMufg6GHZre-79s63MjHjvxtphQCLcBGAsYHQ/s0/image%2B8.png"
    ],
    "time": "2023/12/09 00:58:43"
  },
  {
    "title": "Leveraging TensorFlow-TensorRT integration for Low latency Inference",
    "content": "Posted by Jonathan Dekhtiar (NVIDIA), Bixia Zheng (Google), Shashank Verma (NVIDIA), Chetan Tekur (NVIDIA)\n\nTensorFlow-TensorRT (TF-TRT) is an integration of TensorFlow and TensorRT that leverages inference optimization on NVIDIA GPUs within the TensorFlow ecosystem. It provides a simple API that delivers substantial performance gains on NVIDIA GPUs with minimal effort. The integration allows for leveraging of the optimizations that are possible in TensorRT while providing a fallback to native TensorFlow when it encounters segments of the model that are not supported by TensorRT.\nIn our previous blog on TF-TRT integration, we covered the workflow for TensorFlow 1.13 and earlier releases. This blog will introduce TensorRT integration in TensorFlow 2.x, and demonstrate a sample workflow with the latest API. Even if you are new to this integration, this blog contains all the information you need to get started. Using the TensorRT integration has shown to improve performance by 2.4X compared to native TensorFlow inference on Nvidia T4 GPUs.\nTF-TRT Integration\nWhen TF-TRT is enabled, in the first step, the trained model is parsed in order to partition the graph into TensorRT-supported subgraphs and unsupported subgraphs. Then each TensorRT-supported subgraph is wrapped in a single special TensorFlow operation (TRTEngineOp). In the second step, for each TRTEngineOp node, an optimized TensorRT engine is built. The TensorRT-unsupported subgraphs remain untouched and are handled by the TensorFlow runtime. This is illustrated in Figure 1.\nTF-TRT allows for leveraging TensorFlow's flexibility while also taking advantage of the optimizations that can be applied to the TensorRT supported subgraphs. Only portions of the graph are optimized and executed with TensorRT, and TensorFlow executes the remaining graph.\nIn the inference example shown in Figure 1, TensorFlow executes the Reshape Op and the Cast Op. Then TensorFlow passes the execution of the TRTEngineOp_0, the pre-built TensorRT engine, to TensorRT runtime.\nFigure 1: An example of graph partitioning and building TRT engine in TF-TRT\nWorkflow\nIn this section, we will take a look at the typical TF-TRT workflow using an example.\nFigure 2: Workflow diagram when performing inference in TensorFlow only, and in TensorFlow-TensorRT using a converted SavedModel\nFigure 2 shows a standard inference workflow in native TensorFlow and contrasts it with the TF-TRT workflow. The SavedModel format contains all the information required to share or deploy a trained model. In native TensorFlow, the workflow typically involves loading the saved model and running inference using TensorFlow runtime. In TF-TRT, there are a few additional steps involved, including applying TensorRT optimizations to the TensorRT supported subgraphs of the model, and optionally pre-building the TensorRT engines.\nFirst, we create an object to hold the conversion parameters, including a precision mode. The precision mode is used to indicate the minimum precision (for example FP32, FP16 or INT8) that TF-TRT can use to implement the TensorFlow operations. Then we create a converter object which takes the conversion parameters and input from a saved model. Note that in TensorFlow 2.x, TF-TRT only supports models saved in the TensorFlow SavedModel format.\nNext, when we call the converter convert() method, TF-TRT will convert the graph by replacing TensorRT compatible portions of the graph with TRTEngineOps. For better performance at runtime, the converter build() method can be used for creating the TensorRT execution engine ahead of time. The build() method requires the input data shapes to be known before the optimized TensorRT execution engines are built. If input data shapes are not known then TensorRT execution engine can be built at runtime when the input data is available. The TensorRT execution engine should be built on a GPU of the same device type as the one on which inference will be executed as the building process is GPU specific. For example, an execution engine built for a Nvidia A100 GPU will not work on a Nvidia T4 GPU.\nFinally, the TF-TRT converted model can be saved to disk by calling the save method. The code corresponding to the workflow steps mentioned in this section are shown in the codeblock below:\nfrom tensorflow.python.compiler.tensorrt import trt_convert as trt\n\n# Conversion Parameters \nconversion_params = trt.TrtConversionParams(\n    precision_mode=trt.TrtPrecisionMode.<FP32 or FP16>)\n\nconverter = trt.TrtGraphConverterV2(\n    input_saved_model_dir=input_saved_model_dir,\n    conversion_params=conversion_params)\n\n# Converter method used to partition and optimize TensorRT compatible segments\nconverter.convert()\n\n# Optionally, build TensorRT engines before deployment to save time at runtime\n# Note that this is GPU specific, and as a rule of thumb, we recommend building at runtime\nconverter.build(input_fn=my_input_fn)\n\n# Save the model to the disk \nconverter.save(output_saved_model_dir)\nAs can be seen from the code example above, the build() method requires an input function corresponding to the shape of the input data. An example of an input function is shown below:\n# input_fn: a generator function that yields input data as a list or tuple,\n# which will be used to execute the converted signature to generate TensorRT\n# engines. Example:\ndef my_input_fn():\n    # Let's assume a network with 2 input tensors. We generate 3 sets\n    # of dummy input data:\n    input_shapes = [[(1, 16), (2, 16)], # min and max range for 1st input list\n                    [(2, 32), (4, 32)], # min and max range for 2nd list of two tensors\n                    [(4, 32), (8, 32)]] # 3rd input list\n    for shapes in input_shapes:\n        # return a list of input tensors\n        yield [np.zeros(x).astype(np.float32) for x in shapes]\nSupport for INT8\nCompared to FP32 and FP16, INT8 requires additional calibration data to determine the best quantization thresholds. When the precision mode in the conversion parameter is INT8, we need to provide an input function to the convert() method call. This input function is similar to the input function provided to the build() method. In addition, the calibration data generated by the input function passed to the convert() method should generate data that are statistically similar to the actual data seen during inference.\nfrom tensorflow.python.compiler.tensorrt import trt_convert as trt\n\nconversion_params = trt.TrtConversionParams(\n    precision_mode=trt.TrtPrecisionMode.INT8)\n\nconverter = trt.TrtGraphConverterV2(\n    input_saved_model_dir=input_saved_model_dir,\n    conversion_params=conversion_params)\n\n# requires some data for calibration\nconverter.convert(calibration_input_fn=my_input_fn)\n\n# Optionally build TensorRT engines before deployment.\n# Note that this is GPU specific, and as a rule of thumb we recommend building at runtime\nconverter.build(input_fn=my_input_fn)\n\nconverter.save(output_saved_model_dir)\nExample: ResNet-50\nThe rest of this blog will show the workflow of taking a TensorFlow 2.x ResNet-50 model, training it, saving it, optimizing it with TF-TRT and finally deploying it for inference. We will also compare inference throughputs using TensorFlow native vs TF-TRT in three precision modes, FP32, FP16, and INT8.\nPrerequisites for the example :\nUbuntu OS\nDocker: https://docs.docker.com/get-docker/\nThe latest available TensorFlow 2.x Container:\ndocker pull tensorflow/tensorflow:latest-gpu\nNVIDIA Container Toolkit: https://github.com/NVIDIA/NVIDIA-docker. This allows you to use NVIDIA GPUs in a docker container.\nNVIDIA Driver >= 450 installed on the host machine (at the time of writing, check the requirements of the latest tensorflow container). You can check which version is currently installed on your machine by running: nvidia-smi | grep \"Driver Version:\"\nTraining ResNet-50 using the TensorFlow 2.x container:\nFirst, the latest release of the ResNet-50 model needs to be downloaded from the TensorFlow github repository:\n# Adding the git remote and fetch the existing branches\n$ git clone --depth 1  https://github.com/tensorflow/models.git .\n\n# List the files and directories present in our working directory\n$ ls -al\n\nrwxrwxr-x  user user     4 KiB  Wed Sep 30 15:31:05 2020  ./\nrwxrwxr-x  user user     4 KiB  Wed Sep 30 15:30:45 2020  ../\nrw-rw-r--  user user   337 B    Wed Sep 30 15:31:05 2020  AUTHORS\nrw-rw-r--  user user  1015 B    Wed Sep 30 15:31:05 2020  CODEOWNERS\nrwxrwxr-x  user user     4 KiB  Wed Sep 30 15:31:05 2020  community/\nrw-rw-r--  user user   390 B    Wed Sep 30 15:31:05 2020  CONTRIBUTING.md\nrwxrwxr-x  user user     4 KiB  Wed Sep 30 15:31:15 2020  .git/\nrwxrwxr-x  user user     4 KiB  Wed Sep 30 15:31:05 2020  .github/\nrw-rw-r--  user user     1 KiB  Wed Sep 30 15:31:05 2020  .gitignore\nrw-rw-r--  user user     1 KiB  Wed Sep 30 15:31:05 2020  ISSUES.md\nrw-rw-r--  user user    11 KiB  Wed Sep 30 15:31:05 2020  LICENSE\nrwxrwxr-x  user user     4 KiB  Wed Sep 30 15:31:05 2020  official/\nrwxrwxr-x  user user     4 KiB  Wed Sep 30 15:31:05 2020  orbit/\nrw-rw-r--  user user     3 KiB  Wed Sep 30 15:31:05 2020  README.md\nrwxrwxr-x  user user     4 KiB  Wed Sep 30 15:31:06 2020  research/\nAs noted in the earlier section, for this example we will be using the latest TensorFlow container available in the Docker repository. The user does not need any additional installation steps as TensorRT integration is already included in the container. The steps to pull the container and launch it are as follows:\n$ docker pull tensorflow/tensorflow:latest-gpu\n\n# Please ensure that the  Nvidia Container Toolkit is installed  before running the following command\n$ docker run -it --rm \\\n   --gpus=\"all\" \\\n   --shm-size=2g --ulimit memlock=-1 --ulimit stack=67108864 \\\n   --workdir /workspace/ \\\n   -v \"$(pwd):/workspace/\" \\\n   -v \"</path/to/save/data/>:/data/\" \\  # This is the path that will hold the training data\n   tensorflow/tensorflow:latest-gpu\nFrom inside the container, we can then verify that we have access to the relevant files and the Nvidia GPU we would like to target:\n# Let's first test that we can access the ResNet-50 code that we previously downloaded\n$ ls -al\ndrwxrwxr-x  8 1000 1000  4096 Sep 30 22:31 .git\ndrwxrwxr-x  3 1000 1000  4096 Sep 30 22:31 .github\n-rw-rw-r--  1 1000 1000  1104 Sep 30 22:31 .gitignore\n-rw-rw-r--  1 1000 1000   337 Sep 30 22:31 AUTHORS\n-rw-rw-r--  1 1000 1000  1015 Sep 30 22:31 CODEOWNERS\n-rw-rw-r--  1 1000 1000   390 Sep 30 22:31 CONTRIBUTING.md\n-rw-rw-r--  1 1000 1000  1115 Sep 30 22:31 ISSUES.md\n-rw-rw-r--  1 1000 1000 11405 Sep 30 22:31 LICENSE\n-rw-rw-r--  1 1000 1000  3668 Sep 30 22:31 README.md\ndrwxrwxr-x  2 1000 1000  4096 Sep 30 22:31 community\ndrwxrwxr-x 12 1000 1000  4096 Sep 30 22:31 official\ndrwxrwxr-x  3 1000 1000  4096 Sep 30 22:31 orbit\ndrwxrwxr-x 23 1000 1000  4096 Sep 30 22:31 research\n\n# Let's verify we can see our GPUs:\n$ nvidia-smi\n\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 450.XX.XX    Driver Version: 450.XX.XX    CUDA Version: 11.X     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            On   | 00000000:1A:00.0 Off |                  Off |\n| 38%   52C    P8     14W / 70W |      1MiB / 16127MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\nWe can now start training ResNet-50. To avoid spending hours training a deep learning model, this article will use the smaller MNIST dataset. However, the workflow will not change with a more state-of-the-art dataset like ImageNet.\n# Install dependencies\n$ pip install tensorflow_datasets tensorflow_model_optimization\n\n# Download MNIST data and Train\n$ python -m \"official.vision.image_classification.mnist_main\" \\\n  --model_dir=./checkpoints \\\n  --data_dir=/data \\\n  --train_epochs=10 \\\n  --distribution_strategy=one_device \\\n  --num_gpus=1 \\\n  --download\n\n# Let\u2019s verify that we have the trained model saved on our machine.\n$ ls -al checkpoints/\n\n-rw-r--r-- 1 root root      87 Sep 30 22:34 checkpoint\n-rw-r--r-- 1 root root 6574829 Sep 30 22:34 model.ckpt-0001.data-00000-of-00001\n-rw-r--r-- 1 root root     819 Sep 30 22:34 model.ckpt-0001.index\n[...]\n-rw-r--r-- 1 root root 6574829 Sep 30 22:34 model.ckpt-0010.data-00000-of-00001\n-rw-r--r-- 1 root root     819 Sep 30 22:34 model.ckpt-0010.index\ndrwxr-xr-x 4 root root    4096 Sep 30 22:34 saved_model\ndrwxr-xr-x 3 root root    4096 Sep 30 22:34 train\ndrwxr-xr-x 2 root root    4096 Sep 30 22:34 validation\nObtaining a SavedModel to be used by TF-TRT\nAfter training, Google\u2019s ResNet-50 code exports the model in the SavedModel format at the following path: checkpoints/saved_model/.\nThe following sample code can be used as a reference in order to export your own trained model as a TensorFlow SavedModel.\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\ndef get_model():\n    # Create a simple model.\n    inputs = keras.Input(shape=(32,))\n    outputs = keras.layers.Dense(1)(inputs)\n    model = keras.Model(inputs, outputs)\n    model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n    return model\n\nmodel = get_model()\n\n# Train the model.\ntest_input = np.random.random((128, 32))\ntest_target = np.random.random((128, 1))\nmodel.fit(test_input, test_target)\n\n# Calling `save('my_model')` creates a SavedModel folder `my_model`.\nmodel.save(\"my_model\")\nWe can verify that the SavedModel generated by Google\u2019s ResNet-50 script is readable and correct:\n$ ls -al checkpoints/saved_model\n\ndrwxr-xr-x 2 root root   4096 Sep 30 22:49 assets\n-rw-r--r-- 1 root root 118217 Sep 30 22:49 saved_model.pb\ndrwxr-xr-x 2 root root   4096 Sep 30 22:49 variables\n\n$ saved_model_cli show --dir checkpoints/saved_model/ --tag_set serve --signature_def serving_default\n\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n\nThe given SavedModel SignatureDef contains the following input(s):\n  inputs['input_1'] tensor_info:\n      dtype: DT_FLOAT\n      shape: (-1, 28, 28, 1)\n      name: serving_default_input_1:0\nThe given SavedModel SignatureDef contains the following output(s):\n  outputs['dense_1'] tensor_info:\n      dtype: DT_FLOAT\n      shape: (-1, 10)\n      name: StatefulPartitionedCall:0\nMethod name is: tensorflow/serving/predict\nNow that we have verified that our SavedModel has been properly saved, we can proceed with loading it with TF-TRT for inference.\nInference\nResNet-50 Inference using TF-TRT\nIn this section, we will go over the steps for deploying the saved ResNet-50 model on the NVIDIA GPU using TF-TRT. As previously described, we first convert a SavedModel into a TF-TRT model using the convert method and then load the model.\n# Convert the SavedModel\nconverter = trt.TrtGraphConverterV2(input_saved_model_dir=path)\nconverter.convert()\n\n# Save the converted model\nconverter.save(converted_model_path)\n\n# Load converted model and infer\nmodel = tf.saved_model.load(converted_model_path)\nfunc = root.signatures['serving_default']\noutput = func(input_tensor)\nFor simplicity, we will use a script to perform inference (tf2_inference.py). We will download the script from github.com and put it in the working directory \u201c/workspace/\u201d of the same docker container as before. After this, we can execute the script:\n$ wget https://raw.githubusercontent.com/tensorflow/tensorrt/master/tftrt/blog_posts/Leveraging%20TensorFlow-TensorRT%20integration%20for%20Low%20latency%20Inference/tf2_inference.py\n\n$ ls\nAUTHORS     CONTRIBUTING.md  LICENSE    checkpoints  data      orbit     tf2_inference.py\nCODEOWNERS  ISSUES.md        README.md  community    official  research\n\n$ python tf2_inference.py --use_tftrt_model --precision fp16\n\n=========================================\nInference using: TF-TRT \u2026\nBatch size: 512\nPrecision:  fp16\n=========================================\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nTrtConversionParams(rewriter_config_template=None, max_workspace_size_bytes=8589934592, precision_mode='FP16', minimum_segment_size=3, is_dynamic_op=True, maximum_cached_engines=100, use_calibration=True, max_batch_size=512, allow_build_at_runtime=True)\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n\nProcessing step: 0100 ...\nProcessing step: 0200 ...\n[...]\nProcessing step: 9900 ...\nProcessing step: 10000 ...\n\nAverage step time: 2.1 msec\nAverage throughput: 244248 samples/sec\nSimilarly, we can run inference for INT8, and FP32\n$ python tf2_inference.py --use_tftrt_model --precision int8\n\n$ python tf2_inference.py --use_tftrt_model --precision fp32\nInference using native TensorFlow (GPU) FP32\nYou can also run the unmodified SavedModel without any TF-TRT acceleration.\n$ python tf2_inference.py --use_native_tensorflow\n\n=========================================\nInference using: Native TensorFlow \u2026\nBatch size: 512\n=========================================\n\nProcessing step: 0100 ...\nProcessing step: 0200 ...\n[...]\nProcessing step: 9900 ...\nProcessing step: 10000 ...\n\nAverage step time: 4.1 msec\nAverage throughput: 126328 samples/sec\nThis run was executed with a NVIDIA T4 GPU. The same workflow will work on any NVIDIA GPU.\nComparing Native Tensorflow 2.x performance vs TF-TRT for Inference\nMaking minimal code changes to take advantage of TF-TRT can result in a significant performance boost. For example, using the inference script in this blog, with a batch-size of 512 on an NVIDIA T4 GPU, we observe almost 2x speedup with TF-TRT FP16, and a 2.4x speedup with TF-TRT INT8 over native TensorFlow. The amount of speedup obtained may differ depending on various factors like the model used, the batch size, the size and format of images in the dataset, and any CPU bottlenecks.\nIn conclusion, in this blog we show the acceleration provided by TF-TRT. Additionally, with TF-TRT we can use the full TensorFlow Python API and interactive environments like Jupyter Notebooks or Google Colab.\nSupported Operators\nThe TF-TRT user guide lists operators that are supported in TensorRT-compatible subgraphs. Operators outside this list will be executed by the native TensorFlow runtime.\nWe encourage you to try it yourself and if you encounter problems, please open an issue here.",
    "link": "https://blog.tensorflow.org/2021/01/leveraging-tensorflow-tensorrt-integration.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-rtzsDBSoU-w/YBMWkrI01kI/AAAAAAAAD8U/zDWITPwHRKk6nWw8xKo9ApJLqrwY_hLxgCLcBGAsYHQ/s0/image_2%2B%25281%2529.jpeg",
      "https://1.bp.blogspot.com/-nXVrlJ6XcpE/YBCqtEOpeLI/AAAAAAAAD74/Gm24g8YPbF0nejOhQS86Kj7qQ-ybuqGHACLcBGAsYHQ/s0/image%2B1.png",
      "https://1.bp.blogspot.com/-z8gvj_OldMg/YBCu3Z37cHI/AAAAAAAAD8E/G2g6tjkkMI4t2T_WtwllYLg9iRrghqwtQCLcBGAsYHQ/s0/image%2B2.png"
    ],
    "time": "2023/12/09 00:58:43"
  },
  {
    "title": "What\u2019s new in TensorFlow 2.4?",
    "content": "Posted by Goldie Gadde and Nikita Namjoshi for the TensorFlow Team\n\nTF 2.4 is here! With increased support for distributed training and mixed precision, new NumPy frontend and tools for monitoring and diagnosing bottlenecks, this release is all about new features and enhancements for performance and scaling.\nNew Features in tf.distribute\nParameter Server Strategy\nIn 2.4, the tf.distribute module introduces experimental support for asynchronous training of models with ParameterServerStrategy and custom training loops. Like MultiWorkerMirroredStrategy, ParameterServerStrategy is a multi-worker data parallelism strategy; however, the gradient updates are asynchronous.\nA parameter server training cluster consists of workers and parameter servers. Variables are created on parameter servers and then read and updated by workers during each step. The reading and updating of variables happens independently across the workers without any synchronization. Because the workers do not depend on one another, this strategy has the benefit of worker fault tolerance and is useful if you use preemptible VMs.\nTo get started with this strategy, check out the Parameter Server Training tutorial. This tutorial shows you how to set up ParameterServerStrategy and define a training step, and explains how to use the ClusterCoordinator class to dispatch the execution of training steps to remote workers.\nMulti Worker Mirrored Strategy\nMultiWorkerMirroredStrategy has moved out of experimental and is now part of the stable API. Like its single worker counterpart, MirroredStrategy, MultiWorkerMirroredStrategy implements distributed training with synchronous data parallelism. However, as the name suggests, with MultiWorkerMirroredStrategy you can train across multiple machines, each with potentially multiple GPUs.\nIn synchronous training, each worker computes the forward and backward passes on different slices of the input data, and the gradients are aggregated before updating the model. For this aggregation, known as an all-reduce, MultiWorkerMirroredStrategy uses CollectiveOps to keep variables in sync. A collective op is a single op in the TensorFlow graph that can automatically choose an all-reduce algorithm in the TensorFlow runtime according to hardware, network topology, and tensor sizes.\nTo get started with MultiWorkerMirroredStrategy, check out the Multi-worker training with Keras tutorial, which has been updated with details on dataset sharding, saving/loading models trained with a distribution strategy, and failure recovery with the BackupAndRestore callback.\nIf you are new to distributed training and want to learn how to get started, or you\u2019re interested in distributed training on GCP, see this blog post for an introduction to the key concepts and steps.\nUpdates in Keras\nMixed Precision\nIn TensorFlow 2.4, the Keras mixed precision API has moved out of experimental and is now a stable API. Most TensorFlow models use the float32 dtype; however, there are lower-precision types such as float16 that use less memory. Mixed precision is the use of 16-bit and 32-bit floating point types in the same model for faster training. This API can improve model performance by 3x on GPUs and 60% on TPUs.\n\nTo make use of the mixed precision API, you must use Keras layers and optimizers, but it\u2019s not necessary to use other Keras classes such as models or losses. If you\u2019re curious to learn how to take advantage of this API for better performance, check out the Mixed Precision tutorial.\nOptimizers\nThis release includes refactoring the tf.keras.optimizers.Optimizer class, enabling users of model.fit or custom training loops to write training code that works with any optimizer. All built-in tf.keras.optimizer.Optimizer subclasses now accept gradient_transformers and gradient_aggregator arguments, allowing you to easily define custom gradient transformations.\nWith the refactor, you can now pass a loss tensor directly to Optimizer.minimize when writing custom training loops:\ntape = tf.GradientTape()\nwith tape:\n  y_pred = model(x, training=True)\n  loss = loss_fn(y_pred, y_true)\n\n# You can pass in the `tf.GradientTape` when using a loss `Tensor` as shown below.\n\noptimizer.minimize(loss, model.trainable_variables, tape=tape)\nThese changes are intended to make both Model.fit and custom training loops more agnostic to optimizer details, allowing you to write training code that works with any optimizer without modification.\nFunctional API model construction internal improvements\nLastly, TensorFlow 2.4 includes a major refactoring of the internals of the Keras Functional API, improving the memory consumption of functional model construction and simplifying triggering logic. This refactoring also ensures TensorFlowOpLayers behave predictably and work with CompositeTensor type signatures.\nIntroducing tf.experimental.numpy\nTensorFlow 2.4 introduces experimental support for a subset of NumPy APIs, available as tf.experimental.numpy. This module enables you to run NumPy code, accelerated by TensorFlow. Because it is built on top of TensorFlow, this API interoperates seamlessly with TensorFlow, allowing access to all of TensorFlow\u2019s APIs and providing optimized execution using compilation and auto-vectorization. For example, TensorFlow ND arrays can interoperate with NumPy functions, and similarly TensorFlow NumPy functions can accept inputs of different types including tf.Tensor and np.ndarray.\nimport tensorflow.experimental.numpy as tnp\n\n# Use NumPy code in input pipelines\n\ndataset = tf.data.Dataset.from_tensor_slices(\n    tnp.random.randn(1000, 1024)).map(\n    lambda z: z.clip(-1,1)).batch(100)\n\n# Compute gradients through NumPy code\n\ndef grad(x, wt):\n  with tf.GradientTape() as tape:\n    tape.watch(wt)\n    output = tnp.dot(x, wt)\n    output = tf.sigmoid(output)\n  return tape.gradient(tnp.sum(output), wt)\nYou can learn more about how to use this API in the NumPy API on TensorFlow guide.\nNew Profiler Tools\nMultiWorker Support in TensorFlow Profiler\nThe TensorFlow Profiler is a suite of tools you can use to measure the training performance and resource consumption of your TensorFlow models. The TensorFlow Profiler helps you understand the hardware resource consumption of the ops in your model, diagnose bottlenecks, and ultimately train faster.\nPreviously, the TensorFlow Profiler supported monitoring multi-GPU, single host training jobs. In 2.4 you can now profile MultiWorkerMirroredStrategy training jobs. For example, you can use the sampling mode API to perform on demand profiling and connect to the same server:port in use by MultiWorkerMirroredStrategy workers:\n# Start a profiler server before your model runs.\n\n\ntf.profiler.experimental.server.start(6009)\n\n# Model code goes here....\n \n# E.g. your worker IP addresses are 10.0.0.2, 10.0.0.3, 10.0.0.4, and you\n# would like to profile for a duration of 2 seconds. The profiling data will\n# be saved to the Google Cloud Storage path \u201cyour_tb_logdir\u201d.\n \ntf.profiler.experimental.client.trace(\n    'grpc://10.0.0.2:6009,grpc://10.0.0.3:6009,grpc://10.0.0.4:6009',\n    'gs://your_tb_logdir',\n    2000)\nAlternatively, you can use the TensorBoard profile plugin by providing the worker addresses to the Capture Profile tool.\nAfter profiling, you can use the new Pod Viewer tool to choose a training step and view its step-time category breakdown across all workers.\nFor more information on how to use the TensorFlow Profiler, check out the newly released GPU Performance Guide. This guide shows common scenarios you might encounter when you profile your model training job and provides a debugging workflow to help you get better performance, whether you\u2019re training with one GPU, multiple GPUs, or multiple machines.\nTFLite Profiler\nThe TFLite Profiler enables tracing TFLite internals in Android to identify performance bottlenecks. The TFLite Performance Measurement Guide shows you how to add trace events, enable TFLite tracing, and capture traces with both the Android Studio CPU Profiler and the System Tracing app.\nExample trace using the Android System Tracing app\nNew Features for GPU Support\nTensorFlow 2.4 runs with CUDA 11 and cuDNN 8, enabling support for the newly available NVIDIA Ampere GPU architecture. To learn more about CUDA 11 features, check out this NVIDIA developer blog.\nAdditionally, support for TensorFloat-32 on Ampere-based GPUs is enabled by default. TensorFloat-32, or `TF32` for short, is a math mode for NVIDIA Ampere GPUs that causes certain float32 ops, such as matrix multiplications and convolutions, to run much faster on Ampere GPUs but with reduced precision. To learn more , see the documentation for tf.config.experimental.enable_tensor_float_32_execution.\nNext steps\nCheck out the release notes for more information. To stay up to date, you can read the TensorFlow blog, follow twitter.com/tensorflow, or subscribe to youtube.com/tensorflow. If you\u2019ve built something you\u2019d like to share, please submit it for our Community Spotlight at goo.gle/TFCS. For feedback, please file an issue on GitHub. Thank you!",
    "link": "https://blog.tensorflow.org/2020/12/whats-new-in-tensorflow-24.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-9C6suSiauwc/X9ez34Ji_WI/AAAAAAAAD3o/H9xvIeoPMggI89LqM_rKEPadygA8NedBwCLcBGAsYHQ/s0/image_1%2B%25282%2529.jpeg",
      "https://1.bp.blogspot.com/-yk08j0aKXxA/X9P3JlhLycI/AAAAAAAAD20/BzIfkCwNm_4jQ_U7oDe1p2boDPogqKUEgCLcBGAsYHQ/s0/image%2B1.png",
      "https://1.bp.blogspot.com/-NiJR-DnATRk/X9P5f1p5J-I/AAAAAAAAD3A/zZANVp30_ZEq0QrtIiVj7OPsnCZZy6RRQCLcBGAsYHQ/s0/image%2B2.png",
      "https://1.bp.blogspot.com/-fajMxki2C3U/X9P6Fg4xtLI/AAAAAAAAD3I/8IBSFtJMoNkhOXhcS81kK-OWJqcP7d9IgCLcBGAsYHQ/s0/image%2B3.png"
    ],
    "time": "2023/12/09 00:58:43"
  },
  {
    "title": "Making BERT Easier with Preprocessing Models From TensorFlow Hub",
    "content": "Posted by Arno Eigenwillig, Software Engineer and Luiz GUStavo Martins, Developer Advocate\nBERT and other Transformer encoder architectures have been very successful in natural language processing (NLP) for computing vector-space representations of text, both in advancing the state of the art in academic benchmarks as well as in large-scale applications like Google Search. BERT has been available for TensorFlow since it was created, but originally relied on non-TensorFlow Python code to transform raw text into model inputs.\nToday, we are excited to announce a more streamlined approach to using BERT built entirely in TensorFlow. This solution makes both pre-trained encoders and the matching text preprocessing models available on TensorFlow Hub. BERT in TensorFlow can now be run on text inputs with just a few lines of code:\nAn animation of the preprocessing model that makes it easy for you to input text into BERT (described below).\n# Load BERT and the preprocessing model from TF Hub.\npreprocess = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1')\nencoder = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3')\n\n# Use BERT on a batch of raw text inputs.\ninput = preprocess(['Batch of inputs', 'TF Hub makes BERT easy!', 'More text.'])\npooled_output = encoder(input)[\"pooled_output\"]\nprint(pooled_output)\n\ntf.Tensor(\n[[-0.8384154  -0.26902363 -0.3839138  ... -0.3949695  -0.58442086  0.8058556 ]\n [-0.8223734  -0.2883956  -0.09359277 ... -0.13833837 -0.6251748   0.88950026]\n [-0.9045408  -0.37877116 -0.7714909  ... -0.5112085  -0.70791864  0.92950743]],\nshape=(3, 768), dtype=float32)\nThese encoder and preprocessing models have been built with TensorFlow Model Garden\u2019s NLP library and exported to TensorFlow Hub in the SavedModel format. Under the hood, preprocessing uses TensorFlow ops from the TF.text library to do the tokenization of input text \u2013 allowing you to build your own TensorFlow model that goes from raw text inputs to prediction outputs without Python in the loop. This accelerates the computation, removes boilerplate code, is less error prone, and enables the serialization of the full text-to-outputs model, making BERT easier to serve in production.\nTo show in more detail how these models can help you, we\u2019ve published two new tutorials:\nThe beginner tutorial solves a sentiment analysis task and doesn't need any special customization to achieve great model quality. It's the easiest way of using BERT and a preprocessing model.\nThe advanced tutorial solves NLP classification tasks from the GLUE benchmark, running on TPU. It also shows how to use the preprocessing model in situations where you need multi-segment input.\nChoosing a BERT model\nBERT models are pre-trained on a large corpus of text (for example, an archive of Wikipedia articles) using self-supervised tasks like predicting words in a sentence from the surrounding context. This type of training allows the model to learn a powerful representation of the semantics of the text without needing labeled data. However, it also takes a significant amount of computation to train \u2013 4 days on 16 TPUs (as reported in the 2018 BERT paper). Fortunately, after this expensive pre-training has been done once, we can efficiently reuse this rich representation for many different tasks.\nTensorFlow Hub offers a variety of BERT and BERT-like models:\nEight BERT models come with the trained weights released by the original BERT authors.\n24 Small BERTs have the same general architecture but fewer and/or smaller Transformer blocks, which lets you explore tradeoffs between speed, size and quality.\nALBERT: these are four different sizes of \"A Lite BERT\" that reduces model size (but not computation time) by sharing parameters between layers.\nThe 8 BERT Experts all have the same BERT architecture and size but offer a choice of different pre-training domains and intermediate fine-tuning tasks, to align more closely with the target task.\nElectra has the same architecture as BERT (in three different sizes), but gets pre-trained as a discriminator in a set-up that resembles a Generative Adversarial Network (GAN).\nBERT with Talking-Heads Attention and Gated GELU [base, large] has two improvements to the core of the Transformer architecture.\nLambert has been trained with the LAMB optimizer and several techniques from RoBERTa.\nMuRIL is Multilingual Representations for Indian Languages, pre-trained on 17 Indian languages (including English), and their transliterated counterparts.\nMobileBERT (english, multilingual), is a thin version of BERT, trained through distillation from a teacher BERT model on the Wikipedia, BooksCorpus.\n... and more to come.\nThese models are BERT encoders. The links above take you to their documentation on TF Hub, which refers to the right preprocessing model for use with each of them.\nWe encourage developers to visit these model pages to learn more about the different applications targeted by each model. Thanks to their common interface, it's easy to experiment and compare the performance of different encoders on your specific task by changing the URLs of the encoder model and its preprocessing.\nThe Preprocessing model\nFor each BERT encoder, there is a matching preprocessing model. It transforms raw text to the numeric input tensors expected by the encoder, using TensorFlow ops provided by the TF.text library. Unlike preprocessing with pure Python, these ops can become part of a TensorFlow model for serving directly from text inputs. Each preprocessing model from TF Hub is already configured with a vocabulary and its associated text normalization logic and needs no further set-up.\nWe\u2019ve already seen the simplest way of using the preprocessing model above. Let\u2019s look again more closely:\npreprocess = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1')\ninput = preprocess([\"This is an amazing movie!\"])\n \n{'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n  array([[ 101, 2023, 2003, 2019, 6429, 3185,  999,  102,    0,  ...]])>,\n 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n  array([[   1,    1,    1,    1,    1,    1,    1,    1,    0,  ...,]])>,\n 'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n  array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,  ...,]])>}\n \nCalling preprocess() like this transforms raw text inputs into a fixed-length input sequence for the BERT encoder. You can see that it consists of a tensor input_word_ids with numerical ids for each tokenized input, including start, end and padding tokens, plus two auxiliary tensors: an input_mask (that tells non-padding from padding tokens) and input_type_ids for each token (that can distinguish multiple text segments per input, which we will discuss below).\nThe same preprocessing SavedModel also offers a second, more fine-grained API, which supports putting one or two distinct text segments into one input sequence for the encoder. Let\u2019s look at a sentence entailment task, in which BERT is used to predict if a premise entails a hypothesis or not:\ntext_premises = [\"The fox jumped over the lazy dog.\",\n                 \"Good day.\"]\ntokenized_premises = preprocess.tokenize(text_premises)\n \n<tf.RaggedTensor\n  [[[1996], [4419], [5598], [2058], [1996], [13971], [3899], [1012]],\n  [[2204], [2154], [1012]]]>\n \n \ntext_hypotheses = [\"The dog was lazy.\",  # Entailed.\n                   \"Axe handle!\"]        # Not entailed.\ntokenized_hypotheses = preprocess.tokenize(text_hypotheses)\n \n<tf.RaggedTensor\n  [[[1996], [3899], [2001], [13971], [1012]],\n  [[12946], [5047], [999]]]>\nThe result of each tokenization is a RaggedTensor of numeric token ids, representing each of the text inputs in full. If some pairs of premise and hypothesis are too long to fit within the seq_length for BERT inputs in the next step, you can do additional preprocessing here, such as trimming the text segment or splitting it into multiple encoder inputs.\nThe tokenized input then gets packed into a fixed-length input sequence for the BERT encoder:\nencoder_inputs = preprocess.bert_pack_inputs(\n   [tokenized_premises, tokenized_hypotheses],\n   seq_length=18)  # Optional argument, defaults to 128.\n \n{'input_word_ids': <tf.Tensor: shape=(2, 18), dtype=int32, numpy=\n  array([[  101,  1996,  4419,  5598,  2058,  1996, 13971,  3899,  1012,\n            102,  1996,  3899,  2001, 13971,  1012,   102,     0,     0],\n         [  101,  2204,  2154,  1012,   102, 12946,  5047,   999,   102,\n              0,     0,     0,     0,     0,     0,     0,     0,     0]])>,\n 'input_mask': <tf.Tensor: shape=(2, 18), dtype=int32, numpy=\n  array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n         [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>,\n 'input_type_ids': <tf.Tensor: shape=(2, 18), dtype=int32, numpy=\n  array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n         [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>}\nThe result of packing is the already-familiar dict of input_word_ids, input_mask and input_type_ids (which are 0 and 1 for the first and second input, respectively). All outputs have a common seq_length (128 by default). Inputs that would exceed seq_length are truncated to approximately equal sizes during packing.\nAccelerating model training\nTensorFlow Hub provides BERT encoder and preprocessing models as separate pieces to enable accelerated training, especially on TPUs.\nTensor Processing Units (TPUs) are Google\u2019s custom-developed accelerator hardware that excel at large scale machine learning computations such as those required to fine-tune BERT. TPUs operate on dense Tensors and expect that variable-length data like strings has already been transformed into fixed-size Tensors by the host CPU.\nThe split between the BERT encoder model and its associated preprocessing model enables distributing the encoder fine-tuning computation to TPUs as part of model training, while the preprocessing model executes on the host CPU. The preprocessing computation can be run asynchronously on a dataset using tf.data.Dataset.map() with dense outputs ready to be consumed by the encoder model on the TPU. Asynchronous preprocessing like this can improve performance with other accelerators as well.\nOur advanced BERT tutorial can be run in a Colab runtime that uses a TPU worker and demonstrates this end-to-end.\nSummary\nUsing BERT and similar models in TensorFlow has just gotten simpler. TensorFlow Hub makes available a large collection of pre-trained BERT encoders and text preprocessing models that are easy to use in just a few lines of code.\nTake a look at our interactive beginner and advanced tutorials to learn more about how to use the models for sentence and sentence-pair classification. Let us know what you build with these new BERT models and tag your posts with #TFHub.\nAcknowledgements:\nWe\u2019d like to thank a number of colleagues for their contribution to this work.\nThe new preprocessing models have been created in collaboration with Chen Chen, Terry Huang, Mark Omernick and Rajagopal Ananthanarayanan.\nAdditional BERT models have been published to TF Hub on this occasion by Sebastian Ebert (Small BERTs), Le Hou and Hongkun Yu (Lambert, Talking Heads).\nMark Daoust, Josh Gordon and Elizabeth Kemp have greatly improved the presentation of the material in this post and the associated tutorials. Tom Small for the beautiful BERT animation.",
    "link": "https://blog.tensorflow.org/2020/12/making-bert-easier-with-preprocessing-models-from-tensorflow-hub.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-wsl0YirajxE/X8_UxEq9AKI/AAAAAAAAD2c/tHouCIa_Q8o7nChJIHRb6knLRhSsulUTQCLcBGAsYHQ/s0/tensorflow-editorial-BERT_social_02.png",
      "https://1.bp.blogspot.com/-IAqCwDBe_bg/YGNPb5HBN8I/AAAAAAAAEHU/hvAqCx4DWZoNHKMNjzwvD_cbRpIiBQelQCLcBGAsYHQ/s0/TF%2BHub%2B02.gif",
      "https://1.bp.blogspot.com/-wsl0YirajxE/X8_UxEq9AKI/AAAAAAAAD2c/tHouCIa_Q8o7nChJIHRb6knLRhSsulUTQCLcBGAsYHQ/s0/tensorflow-editorial-BERT_social_02.png"
    ],
    "time": "2023/12/09 00:58:43"
  },
  {
    "title": "Getting Started with Distributed TensorFlow on GCP",
    "content": "Posted by Nikita Namjoshi, Machine Learning Solutions Engineer\n\nFor many in the world of data science, distributed training can seem a daunting task. In addition to building and thoughtfully evaluating a high-quality ML model, you have to be aware of how to optimize your model for specific hardware and manage infrastructure. The latter skills are not often included in a data scientist\u2019s toolkit. However, with the help of managed services on the Google Cloud Platform (GCP), you can easily scale your model training job to multiple accelerators or even multiple machines, with no GPU expertise required.\nIn this tutorial-style article, you\u2019ll get hands-on experience with GCP data science tools and train a TensorFlow model across multiple GPUs. You\u2019ll also learn key terminology in the field of distributed training, such as data parallelism, synchronous training, and AllReduce.\nData parallelism is one of the concepts you will learn about in this article.\nWhy Distributed Training?\nEvery data scientist and machine learning engineer has experienced the agony of sitting and waiting for a model to train. Even if you have access to a GPU, with a large dataset it can take days for a large deep learning model to converge. Using the right hardware configuration can reduce training time to hours, or even minutes. And a shorter training time makes for faster iteration to reach your modeling goals.\nIf you have a GPU available, TensorFlow will use it automatically with no code changes required. Similarly, TensorFlow can make use of multiple CPU cores out of the box. However, if you want to train with two or more GPUs then you\u2019ll have to do a bit of extra work. This extra work is necessary because TensorFlow needs to know how to coordinate the training process across the multiple GPUs in your runtime. Fortunately, with the tf.distribute module, you have access to different distributed training strategies that you can easily incorporate into your program.\nWhen doing distributed training, it\u2019s important to be clear on the distinction between machines and devices. A device refers to a CPU or accelerator, such as GPUs or TPUs, on some machine that TensorFlow can run operations on. The focus in this article will be training with a single machine that has multiple GPU devices, but the tf.distribute.Strategy API also provides support for multi-worker training. In a multi-worker set up, the training is distributed across multiple machines. These machines can be CPU only, or have one or more GPU devices each.\nSingle GPU Training\nIn the following Colab notebook, you\u2019ll find the code to train a ResNet50 architecture on the Cassava dataset. If you execute the cells in the notebook and train the model, you\u2019ll notice that the number of steps taken in each epoch is 89, and each epoch takes around 100 seconds. Make note of these numbers; we will come back to them later.\nMulti-GPU Training\nYou can access a single GPU in colab, but your luck stops there if you want to use multiple GPUs. Moreover, while a Colab notebook is great for quick experimentation you\u2019ll likely want a more secure and reliable set up that offers you more control over your environment. For that, you can turn to the cloud.\nThere are many different ways to do distributed training on GCP. Picking the best option for your use case will likely involve different considerations if you are a student/researcher running experiments, versus an engineer at a company training models in a production workflow.\nIn this article you will use the GCP AI Platform Notebooks. This path provides an easy approach to distributed training and also gives you a chance to explore a managed notebook environment running on GCP. As an alternative, if you already have a local environment set up and are looking for a hassle free transition between your local and GCP environments, you can check out the TensorFlow Cloud library. TensorFlow Cloud can automate many of the steps described in this article; however, we will walk through the steps here so you can get a deeper understanding of the key concepts involved in distributed training.\nIn the following section, you\u2019ll learn how to modify the single GPU training code using the tf.distribute.Strategy API. The resulting code will be cloud platform agnostic so you could run it in a different environment without any changes. You can also run the same code on your own hardware.\nPrepare Code for Distributed Training\nThe first step in using the tf.distribute.Strategy API is to instantiate your strategy. In this tutorial, you will use MirroredStrategy, which is one of several distribution strategies available in TensorFlow.\nstrategy = tf.distribute.MirroredStrategy()\nNext, you need to wrap the creation of your model parameters within the scope of the strategy. This step is crucial because it tells MirroredStrategy which variables to mirror across your GPU devices.\nwith strategy.scope():\n   model = create_model()\n   model.compile(\n     loss='sparse_categorical_crossentropy',\n     optimizer=tf.keras.optimizers.Adam(0.0001),\n     metrics=['accuracy'])\nBefore we run the updated code, let\u2019s take a brief look at what will actually happen when we call model.fit and how training will differ now that we have added a strategy. For the sake of simplicity, imagine you have a simple linear model instead of the ResNet50 architecture. In TensorFlow, you can think of this simple model in terms of its computational graph.\nIn the image below, you can see that the matmul op takes in the X and W tensors, which are the training batch and weights respectively. The resulting tensor is then passed to the add op with the tensor b, which is the model\u2019s bias terms. The result of this op is Ypred, which is the model\u2019s predictions.\nWe want a way of executing this computational graph such that we can leverage two GPUs. There are multiple different ways we can achieve this. For example, you could put different layers of your model on different machines or devices, which is one flavor of model parallelism. Alternatively, you could distribute your dataset such that each device processes a portion of the input batch on each training step with the same model, which is known as data parallelism. Or you might do a combination of both. Data parallelism is the most common (and easiest) approach, and that\u2019s what we\u2019ll do here.\nThe next image shows an example of data parallelism. The input batch X is split in half, and one slice is sent to GPU 0 and the other to GPU 1. In this case, each GPU calculates the same ops but on different slices of the data.\nMirroredStrategy is a data parallelism strategy. So when we call model.fit, MirroredStrategy will make a copy (known as a replica) of the ResNet50 model on both of the GPUs. The CPU (host) is responsible for preparing the tf.data.Dataset batches and sending the data to the GPUs (devices).\nThe subsequent gradient updates will happen in a synchronous manner. This means that each worker device computes the forward and backward passes through the model on a different slice of the input data. The computed gradients from each of these slices are then aggregated across all of the devices and reduced (usually an average) in a process known as AllReduce. The optimizer then performs the parameter updates with these reduced gradients thereby keeping the devices in sync. Because each worker cannot proceed to the next training step until all the other workers have finished the current step, this gradient calculation becomes the main overhead in distributed training for synchronous strategies.\nWhile MirroredStrategy is a synchronous strategy, data parallelism strategies can also be asynchronous. In an asynchronous data parallelism strategy, each worker computes the gradients from a slice of the input data and makes updates to the parameters in an asynchronous fashion. Compared to synchronous strategies, asynchronous training has the benefit of fault tolerance because the workers are not dependent on one another, but can result in stale gradients. You can learn more about asynchronous training by experimenting with the TensorFlow Parameter Server Strategy.\nWith the two easy steps of instantiating MirroredStrategy, and then wrapping your model creation within the strategy scope, TensorFlow will do the heavy lifting of distributing your training job across your GPUs through data parallelism and synchronous gradient updates.\nThe last change you will want to make is to the batch size.\nBATCH_SIZE = 64 * strategy.num_replicas_in_sync\nRecall that in the single GPU case, the batch size was 64. This means that on each step of model training, 64 images were processed, and the number of resulting steps in each epoch was the total dataset size / batch size, which we noted previously as 89.\nWhen you do distributed training with the tf.distribute.Strategy API and tf.data, the batch size now refers to the global batch size. In other words, if you pass a batch size of 10, and you have two GPUs, then each machine will process 5 examples per step. In this case, 10 is known as the global batch size, and 5 as the per replica batch size. To make the most out of your GPUs, you will want to scale the batch size by the number of replicas, which is two in this case because there is one replica on each GPU.\nYou can make these code changes yourself, or simply use this other Colab notebook where the changes have been made already. Although MirroredStrategy is designed for a multi-GPU environment, you can actually run this notebook in Colab on a GPU runtime or a CPU runtime without error. TensorFlow will use a single GPU or multiple CPU cores out of the box anyway so you don\u2019t actually need a strategy, but this could come in handy for testing/experimentation purposes.\nSet up GCP Project\nNow that we\u2019ve made the necessary code changes, the next step is to set up the GCP environment. To do this you will need a GCP project with billing enabled.\nCreate your project in the UI\nCreate your billing account\nNext, you should enable the Cloud Compute Engine API. If you are working in a brand new project, then this process will likely also prompt you to connect the billing account you created. If you are using a GCP project that you have already worked with, then most likely the Compute Engine API will already be enabled.\nRequest Quota\nGoogle Cloud enforces quotas on resource usage to prevent abuse and accidental usage. If you need access to more of a particular resource than what is available by default, you\u2019ll have to request more quota. For this tutorial, we will use the NVIDIA T4 Tensor Core GPU. By default, you get access to one NVIDIA T4 Tensor Core GPU per location, but in order to do distributed training you\u2019ll need to request quota for an additional GPU in a location.\nIn the GCP console, scroll to the hamburger menu on the left side and navigate to IAM & Admin > Quotas\nOn the Quotas page you can add a service filter for the Compute Engine API. Note that if you have not enabled the Compute Engine API or enabled billing, you will not see Compute Engine API as a filter option, so be sure you have completed the earlier steps first.\nWhen you find the NVIDIA T4 GPUs resource in the list, go ahead and click on ALL QUOTAS for that row.\nOnce you\u2019ve made it to the Quota metric details page for NVIDIA T4 GPUs, select the Location: us-west1 and click edit quotas at the top of the page.\nIf you already have quota for a different type of GPU, or in a different location, you can easily use those instead. Just make sure you remember the GPU type and location as you will need to specify these parameters when setting up your AI Platform Notebook environment later. Additionally, if you prefer to follow along and just use a single GPU instead of requesting quota for two, you can do that as well. Your code will not be distributed, but you will still get the benefit of learning how to set your GCP environment.\n>\nFill in your contact details in the Quota changes menu and then set your New Limit to 2. Then click Done when you\u2019re finished.\nYou\u2019ll get a confirmation email first when you have submitted the request, and then when your request has been approved.\nCreate AI Platform Notebook Instance\nWhile you wait for quota approvals, the next step is to get set up with AI Platform Notebooks, which can be found using the same hamburger menu as before in the console and scrolling to Artificial Intelligence > AI Platform > Notebooks\nYou\u2019ll need to enable the API if this is your first time using the tool.\nAI Platform Notebooks is a managed service for doing data science work. This tool is ideal if you like developing in a notebook environment. You can easily add and remove GPUs without having to worry about GPU driver installation, and there are a number of instance images you can choose from depending on your use case so you don\u2019t need to hassle with setting up all the Python packages you need to get your job done.\nOnce the Notebooks API is enabled, the next step is to create your instance. You can do this by clicking the NEW INSTANCE button at the top of the page, and then selecting the TensorFlow Enterprise 2.3 image (or the most recent TensorFlow image if you\u2019re following along at a later date), with the 1 NVIDIA Tesla T4 option. TensorFlow Enterprise is a TensorFlow distribution optimized for GCP.\nClick ADVANCED OPTIONS at the bottom of the New notebook instance window, and then change the following fields:\nInstance name: give your instance a name\nRegion: us-west1\nGPU type: NVIDIA Tesla T4\nNumber of GPUs: 2\nCheck the Install NVIDIA GPU driver automatically for me box\nThen click CREATE. Note that if you have not yet been approved for the NVIDIA T4 GPU quota, you will get an error message when you click CREATE. So be sure you have received your approval message before completing this step. Additionally, if you plan to use a different GPU type or location other than NVIDIA T4 in us-west1, you will need to change these parameters when creating your notebook.\nYour instance will take a few minutes to launch, and when it\u2019s done you\u2019ll see the option to OPEN JUPYTERLAB appear in blue letters.\nNote that even after you\u2019ve created an AI Platform Notebook instance, you can change the hardware (for example adding or removing GPUs). Should you need to do this in the future, simply stop the instance and follow the steps here.\nTrain Multi-GPU Model on AI Platform Notebooks\nNow that your instance is set up, you can click on OPEN JUPYTERLAB.\nDownload the Colab Notebook as an .ipynb file, and upload it to your Jupyter Lab environment. When the file is uploaded go to the notebook and run the code.\nWhen you execute the model.fit cell, you should notice that the number of steps per epoch is now 45, which is half of what it was when using a single GPU. This is data parallelism in action. With a global batch size of 64 * 2, your CPU is sending batches of 64 images to each GPU. So while previously the model only saw 64 examples in a single step, it now sees 128 examples on each step and thus each epoch takes less time. Previously each epoch took around 100 seconds, and now each epoch takes around 60 seconds. You\u2019ll notice that adding a second GPU does not cut the time in half, as there is some overhead involved in synchronizing the gradients. The benefits will be more noticeable with a larger dataset (Cassava only has 5656 training images). Additionally, there are lots of techniques you can use to get even more benefit from that second GPU, such as making sure your input pipeline isn\u2019t a bottleneck. To learn more about making the most of your GPUs, see the TensorFlow Performance Debugging guide.\nLong Running Jobs on the DLVM\nSo far you\u2019ve learned how to use the GCP AI Platform Notebooks to run a simple distributed training job. The dataset we used was not very large, and the model achieved fairly high accuracy after only a few epochs. However, in reality your training job will probably run for a lot longer and you might not want to use a notebook.\nWhen you launch an AI Platform Notebook, it creates a Google Compute Engine (GCE) instance using the GCP Deep Learning VM Images. The Deep Learning VM images are Compute Engine virtual machine images optimized for data science and machine learning tasks. In our example we used the TensorFlow Enterprise 2.3 image, but there are many other options available.\nIn the console, you can use the menu to navigate to Compute Engine > VM instances\nAnd you should see an instance with the same name as the notebook you created earlier. Because this is a GCE instance, we can ssh into the machine and run the code there.\nInstall Google SDK\nInstalling the Google Cloud SDK will allow you to manage GCE resources in your project from your terminal. Follow the steps here to install the SDK and connect to your project.\nSSH into the VM\nOnce the SDK is installed and configured, you can use the following command in your terminal to ssh into your vm. Just be sure to change the instance name and project name.\ngcloud compute ssh {your-vm-name} --project={your-project-name}\nIf you run the command nvidia-smi on the vm, you\u2019ll see the two NVIDIA T4 Tensor Core GPUs we provisioned earlier.\nTo run the distributed training job, simply download the code from the Colab Notebook as a .py file, and use the following command from your local machine to copy it to your vm.\ngcloud compute scp --project {your-project-name} {local-path-to-py-file} {your-vm-name}:~/\nFinally, you can run the script on your vm with\npython dist_strat_blog_multi_gpu.py\nAnd you should see the output of your model training job\nIf Notebooks are your environment of choice, you can stick with the workflow we used in the previous section. But if you prefer to use vim or emacs, or if you want to run a long running job using Screen for example, you have the option to ssh into the vm from your terminal. Note that you can also launch a Deep Learning VM directly from the command line instead of using the AI Platform Notebooks UI like we did in this tutorial.\nWhen you\u2019re finished experimenting, do not forget to shut your instance down. You can do this by selecting the instance from the Notebook instances page, or GCE Instances page in the console UI and clicking STOP at the top of the window. Shutting down the instance is very important as you will be billed a few dollars for every hour that it is left running. You can easily stop your instance, then restart it when you want to run more experiments and all of your files will still be there.\nTake Your Distributed Training Skills to the Next Level\nIn this article you learned how to use MirroredStrategy, a synchronous data parallelism strategy, to distribute your TensorFlow training job across two GPUs on GCP. You now know the basic mechanics of how to set up your GCP environment and prepare your code, but there\u2019s a lot more to explore in the world of distributed training. For example, if you are interested in building a distributed training job into a production ML pipeline, check out the AI Platform Training service, which also allows you to configure a training job across multiple machines, each containing multiple GPUs. Or, you canbuild out a TFX pipeline and run it on the AI Platform Pipelines service.\nOn the tensorflow.org site you can check out the other strategies available with the tf.distribute.Strategy API in the overview guide, and also learn how to use a strategy with a custom training loop. For more advanced concepts, there\u2019s a guide on how data gets distributed, and a guide on how to do performance debugging with the TensorFlow Profiler to make sure you are maximizing utilization of your GPUs.",
    "link": "https://blog.tensorflow.org/2020/12/getting-started-with-distributed-tensorflow-on-gcp.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-NbZ_KxwIG4w/X8bVhQaX9JI/AAAAAAAADyQ/VieW1EnYcRgvVU9LoCO0NRkOx-jhTu71QCLcBGAsYHQ/s0/image%2B1.png",
      "https://1.bp.blogspot.com/-NbZ_KxwIG4w/X8bVhQaX9JI/AAAAAAAADyQ/VieW1EnYcRgvVU9LoCO0NRkOx-jhTu71QCLcBGAsYHQ/s0/image%2B1.png",
      "https://1.bp.blogspot.com/-A9DXH5XpTpc/X8bXVOZ1V4I/AAAAAAAADyc/wChIlRrNNhYOL0PWR-9uIjh4147JIFPHQCLcBGAsYHQ/s0/image%2B2.png",
      "https://1.bp.blogspot.com/-nkF3OUnznGc/X8bX0XxuXpI/AAAAAAAADyk/ZnyYWlYZ3mkfWnkfaoRxOKh4hMWofDRXACLcBGAsYHQ/s0/image%2B3.png",
      "https://1.bp.blogspot.com/-DMwYdm6VLLE/X8gctBWTWEI/AAAAAAAAD1A/e2_aMNUYqCAq7l51RFMSLp8oIllMZ1iAQCLcBGAsYHQ/s0/Screen%2BShot%2B2020-12-02%2Bat%2B2.57.30%2BPM.png",
      "https://1.bp.blogspot.com/-zZJYq68Xtb0/X8bZXl2J2gI/AAAAAAAADy4/q5NpfAu5p4MT21jlxbjz9QVTjJVuyUKdgCLcBGAsYHQ/s0/image%2B5.png",
      "https://1.bp.blogspot.com/-URRl4sF_cfU/X8bZ2u3zETI/AAAAAAAADzA/_2b9hBopStwtZAcM1TXdFevHIRPKtHzBwCLcBGAsYHQ/s0/image%2B6.png",
      "https://1.bp.blogspot.com/-MyuvIA5A-q8/X9PPPW6PRVI/AAAAAAAAD2o/sO2p2apCxvMbHS8bzwjq2GQp5nM-ZucfwCLcBGAsYHQ/s0/new%2Bimage.png",
      "https://1.bp.blogspot.com/-qyE57Wj0UOM/X8ba0ThxcsI/AAAAAAAADzQ/IPL0lEjYyNAcDE5RZ47uzz_SRVJNdtwSgCLcBGAsYHQ/s0/image%2B8.png",
      "https://1.bp.blogspot.com/-FExUg25TBE8/X8bcMzllBvI/AAAAAAAADzc/NKeHT0r3JXcRsnTUmiwJuvElJGrnXWkvACLcBGAsYHQ/s0/image%2B9.png",
      "https://1.bp.blogspot.com/-oKoXYcBGGbE/X8bdhyKUZTI/AAAAAAAADzw/tliPcztK21wTkc4W_T-7r0fYtaxBWlSEQCLcBGAsYHQ/s0/Screen%2BShot%2B2020-10-23%2Bat%2B12.51.34%2BPM.png",
      "https://1.bp.blogspot.com/-uUwzgKJu2fs/X8bdxOJ-faI/AAAAAAAADz0/N4cxsbnNcR0_hAk6yxii33wOp9jWiIh4wCLcBGAsYHQ/s0/image%2B11.png",
      "https://1.bp.blogspot.com/-N8fyH4t3qBk/X8becD0kmKI/AAAAAAAAD0A/oJj_hhFNZTI1hj3VPg1JiSqLZmg4PDsZgCLcBGAsYHQ/s0/Screen%2BShot%2B2020-11-06%2Bat%2B11.23.10%2BAM.png",
      "https://1.bp.blogspot.com/-o4PWUnnPrsU/X8bfKhPhaAI/AAAAAAAAD0I/zJs5JB10eWUBajfBTxp5PA3-Ibhu38VyQCLcBGAsYHQ/s0/image%2B13.png",
      "https://1.bp.blogspot.com/-h4AIw2F_BzU/X8biRZpoblI/AAAAAAAAD0g/lF9JZUrxIAAfRQbM9m6P2OXefDbuVotYwCLcBGAsYHQ/s0/Screen%2BShot%2B2020-10-26%2Bat%2B5.36.43%2BPM.png",
      "https://1.bp.blogspot.com/-BpR9ofUK7d0/X8bif48_VUI/AAAAAAAAD0k/xrc5sMxthNo-M4U6xTfieF39EX7msTk7wCLcBGAsYHQ/s0/Screen%2BShot%2B2020-10-26%2Bat%2B5.38.05%2BPM.png"
    ],
    "time": "2023/12/09 00:58:43"
  },
  {
    "title": "How Adobe used Web ML with TensorFlow.js to enhance Photoshop for web",
    "content": "Guest post by Joseph Hsieh (Principal Scientist, Project Lead at Adobe), Devin Fernandez (Director of Product Management, Adobe), and Jason Mayes (Web ML Lead, Google)\nIntroduction\nPhotoshop Web Beta  is a browser-based version of the popular desktop image editing software, Adobe Photoshop. This online tool offers a wide range of features and capabilities for editing, enhancing, and manipulating images, all through a web browser.\nIn this post, we will explore how Adobe plans to bring advanced ML features from desktop to web, such as the Object Selection tool. We will also look at how web-based machine learning in JavaScript can improve the performance and user experience of Photoshop Web Beta, and what we can expect in the future.\nChallenge\nPhotoshop has recently been made available on the web through WebAssembly in our first attempt to port our tooling to the browser. However, to bring advanced ML features such as the Object Selection Tool to Photoshop Web Beta, it currently adopts a cloud inference solution for object selection tasks which requires the user to be online, and to send data to the cloud service to perform the machine learning task. This means the web app cannot run offline, user privacy is not preserved, and there is an added latency and monetary cost to each call to the cloud as we need to run those models on our own hardware.\nWhen it comes to the Object Selection tool, relying on cloud inference can sometimes result in suboptimal performance due to network latency. To provide a better user experience, Adobe Photoshop Web Beta eliminates this latency by developing an on-device inference solution, resulting in faster predictions and a more responsive UI.\nTensorFlow.js is an open-source machine learning library from Google aimed at JavaScript developers that\u2019s able to run client side in the browser. It\u2019s the most mature option for web ML with comprehensive WebGL and WebAssembly backend operators support, and in the future, there will also be an option for a WebGPU backend to be used within the browser for faster performance as new web standards evolve. Adobe has collaborated with Google to bring TensorFlow.js to Photoshop Web Beta and enable advanced tasks such as object selection using ML running in the browser, the details of the collaboration are explained below.\nWhen we first started to convert to a web solution, we noticed that there were synchronization issues between WebAssembly (what our core ported Photoshop code was running in) and TensorFlow.js (for running the ML models in the browser). Essentially we needed to load and run the TensorFlow.js models synchronously instead of asynchronously to work with our WebAssembly port of Photoshop. One potential 3rd party solution was not an option due to its drawbacks \u2013 such as large code overhead size or unpredictable performance across devices. So, a new solution was required.\nTo tackle these challenges, first Google and Adobe collaborated to bring a proxying API to Emscripten - a 3rd party compiler toolchain that can compile to WebAssembly that uses LLVM to enable code written in C or C++ to run in browser and interact with JavaScript libraries. A Proxying API for Emscripten effectively resolves these issues that the 3rd party solution suffered and allows for seamless integration between Photoshop\u2019s Web Assembly implementation and the TensorFlow.js ML model running.\nNext, once communication between WebAssembly and TensorFlow.js was possible, Adobe ported key ML models such as the one used in object selection shown above to the TensorFlow.js format. The TensorFlow.js team aided in model optimization for such models by focusing on optimizing common ops models utilized such as the Conv2D operation to ensure the converted models ran as fast as possible in the browser.\nWith both cloud and on-device solutions now a possibility, Photoshop Web Beta can choose the optimal option for delivering the best user experience and deploy ML models accordingly. While on-device inference offers superior user interaction with low latency and privacy for frequently used tasks, not all ML models can run locally due to the limited memory per browser tab (currently around 4GB in Chrome). On the other hand, cloud inference can accommodate larger ML models for tasks where network latency may be acceptable, with the tradeoffs of less perceived privacy by the end user and the associated cost to host and execute such models on server side hardware.\nPerformance Improvement\nSince the Google team has improved TensorFlow.js hardware execution performance via its various supported backends (WebGL, WASM, Web GPU), it has resulted in models seeing anywhere from 30% to 200% performance improvements (especially for the larger models that tend to see the biggest gains), enabling close to real time performance right in the browser.\nLooking Ahead\nPhotoshop Web Beta's Select Subject and Object Selection tools demonstrate how machine learning can help enhance user workflow and experience. As web-based machine learning technology continues to evolve and TensorFlow.js backend support and efficiency continue to make performance gains, Photoshop Web Beta will be able to bring more advanced models to the edge on device in the browser, pushing the limits of what is possible and enabling even more advanced features to delight users.\nTry it out\nTry out Photoshop Web Beta right now for yourself at https://photoshop.adobe.com and see the power of machine learning in the browser that brings the best of Web ML (coming soon) and Cloud ML inference in action!\nAdobe offerings and trademarks belong to Adobe Inc and are not associated with Google.",
    "link": "https://blog.tensorflow.org/2023/03/how-adobe-used-web-ml-with-tensorflowjs-to-enhance-photoshop-for-web.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi249S4GMDkKS0NOavl_GELmKFme5HISmcgPkdr_gELKiROOLaEDAQcn-QxW8clAcx4hZt86gQhsHTfJ0mVyekcDoaa9bNfNMwvq-h6RPzOyP1OxK1J07NyM6UgAqF_SedAH4BuImMTqgFtdlBVe1Dr5YcM0l8bYplC5rfO_d3QajgQbLhuUX7_VG1j/s1600/image1.gif",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi249S4GMDkKS0NOavl_GELmKFme5HISmcgPkdr_gELKiROOLaEDAQcn-QxW8clAcx4hZt86gQhsHTfJ0mVyekcDoaa9bNfNMwvq-h6RPzOyP1OxK1J07NyM6UgAqF_SedAH4BuImMTqgFtdlBVe1Dr5YcM0l8bYplC5rfO_d3QajgQbLhuUX7_VG1j/s1600/image1.gif"
    ],
    "time": "2023/12/09 00:58:43"
  },
  {
    "title": "Content moderation using machine learning: the server-side part",
    "content": "Posted by Jen Person, Senior Developer Relations Engineer, TensorFlow\n\nWelcome to part 2 of my dual approach to content moderation! In this post, I show you how to implement content moderation using machine learning in a server-side environment. If you'd like to see how to implement this moderation client-side, check out part 1.\nRemind me: what are we doing here again?\nIn short, anonymity can create some distance between people in a way that allows them to say things they wouldn't say in person. That is to say, there are tons of trolls out there. And let's be honest: we've all typed something online we wouldn't actually say IRL at least once! Any website that takes public text input can benefit from some form of moderation. Client-side moderation has the benefit of instant feedback, but server-side moderation cannot be bypassed like client-side might, so I like to have both.\n\nThis project picks up where part 1 left off, but you can also start here with a fresh copy of the Firebase Text Moderation demo code. The website in the Firebase demo showcases content moderation through a basic guestbook using a server-side content moderation system implemented through a Realtime Database-triggered Cloud Function. This means that the guestbook data is stored in the Firebase Realtime Database, a NoSQL database. The Cloud Function is triggered whenever data is written to a certain area of the database. We can choose what code runs when that event is triggered. In our case, we will use the Text Toxicity Classifier model to determine if the text written to the database is inappropriate, and then remove it from the database if needed. With this model, you can evaluate text on different labels of unwanted content, including identity attacks, insults, and obscenity. You can try out the demo to see the classifier in action.\n\nIf you prefer to start at the end, you can follow along in a completed version of the project on GitHub.\nServer-side moderation\nThe Firebase text moderation example I used as my starting point doesn't include any machine learning. Instead, it checks for the presence of profanity from a list of words and then replaces them with asterisks using the bad-words npm package. I thought about blending this approach with machine learning (more on that later), but I decided to just wipe the slate clean and replace the code of the Cloud Function altogether. Start by navigating to the Cloud Functions folder of the Text Moderation example:\ncd text-moderation/functions\nOpen index.js and delete its contents. In index.js, add the following code:\nconst functions = require('firebase-functions');\nconst toxicity = require('@tensorflow-models/toxicity');\n\nexports.moderator = functions.database.ref('/messages/{messageId}').onCreate(async (snapshot, context) => {\n  const message = snapshot.val();\n\n  // Verify that the snapshot has a value\n  if (!message) { \n    return;\n  }\n  functions.logger.log('Retrieved message content: ', message);\n\n  // Run moderation checks on the message and delete if needed.\n  const moderateResult = await moderateMessage(message.text);\n  functions.logger.log(\n    'Message has been moderated. Does message violate rules? ',\n    moderateResult\n  );\n});\nThis code runs any time a message is added to the database. It gets the text of the message, and then passes it to a function called `moderateResult`. If you're interested in learning more about Cloud Functions and the Realtime Database, then check out the Firebase documentation.\nAdd the Text Toxicity Classifier model\nDepending on your development environment, you probably have some sort of error now since we haven't actually written a function called moderateMessage yet. Let's fix that. Below your Cloud Function trigger function, add the following code:\nexports.moderator = functions.database.ref('/messages/{messageId}').onCreate(async (snapshot, context) => {\n        //\u2026\n        // Your other function code is here.\n});\n\nasync function moderateMessage(message) {\n  const threshold = 0.9;\n\n  let model = await toxicity.load(threshold);\n\n  const messages = [message];\n\n  let predictions = await model.classify(messages);\n\n  for (let item of predictions) {\n    for (let i in item.results) {\n      if (item.results[i].match === true) {\n        return true;\n      }\n    }\n  }\n  return false;\n}\nThis function does the following:\nSets the threshold for the model to 0.9. The threshold of the model is the minimum prediction confidence you want to use to set the model's predictions to true or false--that is, how confident the model is that the text does or does not contain the given type of toxic content. The scale for the threshold is 0-1.0. In this case, I set the threshold to .9, which means the model will predict true or false if it is 90% confident in its findings.\nLoads the model, passing the threshold. Once loaded, it sets toxicity_model to the model` value.\nPuts the message into an array called messages, as an array is the object type that the classify function accepts.\nCalls classify on the messages array.\nIterates through the prediction results. predictions is an array of objects each representing a different language label. You may want to know about only specific labels rather than iterating through them all. For example, if your use case is a website for hosting the transcripts of rap battles, you probably don't want to detect and remove insults.\nChecks if the content is a match for that label. if the match value is true, then the model has detected the given type of unwanted language. If the unwanted language is detected, the function returns true. There's no need to keep checking the rest of the results, since the content has already been deemed inappropriate.\nIf the function iterates through all the results and no label match is set to true, then the function returns false \u2013 meaning no undesirable language was found. The match label can also be null. In that case, its value isn't true, so it's considered acceptable language. I will talk more about the null option in a future post.\nIf you completed part 1 of this tutorial, then these steps probably sound familiar. The server-side code is very similar to the client-side code. This is one of the things that I like about TensorFlow.js: it's often straightforward to transition code from the client to server and vice versa.\nComplete the Cloud Functions code\nBack in your Cloud Function, you now know that based on the code we wrote for moderateMessage, the value of moderateResult will be true or false: true if the message is considered toxic by the model, and false if it does not detect toxicity with certainty greater than 90%. Now add code to delete the message from the database if it is deemed toxic:\n  // Run moderation checks on the message and delete if needed.\n  const moderateResult = await moderateMessage(message.text);\n  functions.logger.log(\n    'Message has been moderated. Does message violate rules? ',\n    moderateResult\n  );\n\n  if (moderateResult === true) {\n    var modRef = snapshot.ref;\n    try {\n      await modRef.remove();\n    } catch (error) {\n      functions.logger.error('Remove failed: ' + error.message);\n    }\n  }\nThis code does the following:\nChecks if moderateResult is true, meaning that the message written to the guestbook is inappropriate.\nIf the value is true, it removes the data from the database using the remove function from the Realtime Database SDK.\nLogs an error if one occurs.\nDeploy the code\nTo deploy the Cloud Function, you can use the Firebase CLI. If you don't have it, you can install it using the following npm command:\nnpm install -g firebase-tools\nOnce installed, use the following command to log in:\nfirebase login\nRun this command to connect the app to your Firebase project:\nfirebase use --add\nFrom here, you can select your project in the list, connect Firebase to an existing Google Cloud project, or create a new Firebase project.\nOnce the project is configured, use the following command to deploy your Cloud Function:\nfirebase deploy\nOnce deployment is complete, the logs include the link to your hosted guestbook. Write some guestbook entries. If you followed part 1 of the blog, you will need to either delete the moderation code from the website and deploy again, or manually add guestbook entries to the Realtime Database in the Firebase console.\n\nYou can view your Cloud Functions logs in the Firebase console.\nBuilding on the example\nI have a bunch of ideas for ways to build on this example. Here are just a few. Let me know which ideas you would like to see me build, and share your suggestions as well! The best ideas come from collaboration.\n\nGet a queue\n\nI mentioned that the \"match\" value of a language label can be true, false, or null without going into detail on the significance of the null value. If the label is null, then the model cannot determine if the language is toxic within the given threshold. One way to limit the number of null values is to lower this threshold. For example, if you change the threshold value to 0.8, then the model will label the match value as true if it is at least 80% certain that the text contains language that fits the label. My website example assigns labels of value null the same as those labeled false, allowing that text through the filter. But since the model isn't sure if that text is appropriate, it's probably a good idea to get some eyes on it. You could add these posts to a queue for review, and then approve or deny them as needed. I said \"you\" here, but I guess I mean \"me\". If you think this would be an interesting use case to explore, let me know! I'm happy to write about it if it would be useful.\n\nWhat's in 'store\n\nThe Firebase moderation sample that I used as the foundation of my project uses Realtime Database. I prefer to use Firestore because of its structure, scalability, and security. Firestore's structure is well suited for implementing a queue because I could have a collection of posts to review within the collection of posts. If you'd like to see the website using Firestore, let me know.\nDon't just eliminate - moderate!\nOne of the things I like about the original Firebase moderation sample is that it sanitizes the text rather than just deleting the post. You could run text through the sanitizer before checking for toxic language through the text toxicity model. If the sanitized text is deemed appropriate, then it could overwrite the original text. If it still doesn't meet the standards of decent discourse, then you could still delete it. This might save some posts from otherwise being deleted.\nWhat's in a name?\nYou've probably noticed that my moderation functionality doesn't extend to the name field. This means that even a halfway-clever troll could easily get around the filter by cramming all of their expletives into that name field. That's a good point and I trust that you will use some type of moderation on all fields that users interact with. Perhaps you use an authentication method to identify users so they aren't provided a field for their name. Anyway, you get it: I didn't add moderation to the name field, but in a production environment, you definitely want moderation on all fields.\nBuild a better fit\nWhen you test out real-world text samples on your website, you might find that the text toxicity classifier model doesn't quite fit your needs. Since each social space is unique, there will be specific language that you are looking to include and exclude. You can address these needs by training the model on new data that you provide.\n\nIf you enjoyed this article and would like to learn more about TensorFlow.js, then there are a ton of things you can do:\nCheck out the TensorFlow.js edX course by Jason Mayes. If you are even remotely interested in using TensorFlow.js, I cannot recommend this enough. It might look like a lot at first, but the course is broken up into easy-to-follow manageable pieces.\nView all the TensorFlow.js pretrained models in the TFJS repository on GitHub.\nPlay around with TensorFlow.js projects on Glitch.",
    "link": "https://blog.tensorflow.org/2022/09/content-moderation-using-machine-learning-the-server-side-part.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgZ_JzGn-9CXVZpuyyrXbmdthLMMdql_3BLsvQKu5mkD0xrlA57ZYRtFhDVtpanBhS9tC_2NCRqsm46uJCrXWR5-Fgz8klSwKJ0AVMMdwgAhofeQ80ryOIoTz-ZdsU90vHjy1H6chshzhYYrDQOUriFB-k_vYAqPPq1NiZxx7Lu9uBWpCfpmpDWxFiH/s1600/tensorflow-content-moderation-using-machine-learning-the%20server-side-part-02.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhRXgc59QqLTXayrrbQbdn9X6ih338q09zrRlopkTgAr9ZgOPIgvdvOdQsrgax0lta-wQ_tTHw1NmUUPpbxrHdzPmEQlqe1mI1lOU8Vc7iBZSHhxFocsdusuw8biQZitqH1HvCbYeL--ycFkXlz8hMFWOIQlmlrnsBuQcb45bOSfbPa8cHUTn14_AHF/s1600/tensorflow-content-moderation-using-machine-learning-the%20server-side-part-01.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1m4XjRFV4Z5npUVUZGW9yME6SWET7Vj-P4EWQUV_GhQD0c5-ntz56oddfR5MEewonEnoPmVkgFFylMW-_uYn4zbyEOD2F5Uccxz1J9GciEp3_i4jIuaPn-8Z0G798apFzRgLIbAHDU8mAuIxoXWe2eb6nqhDIWaCdr946GHSTxjw-YQEKXsFx4wT1/s16000/Screen%20Shot%202022-09-07%20at%205.00.15%20PM.png"
    ],
    "time": "2023/12/09 00:58:43"
  },
  {
    "title": "JAX on the Web with TensorFlow.js",
    "content": "Posted by  Andreas Steiner and Marc van Zee, Google Research, Brain Team\nIntroduction\nIn this blog post we demonstrate how to convert and run Python-based JAX functions and Flax machine learning models in the browser using TensorFlow.js. We have produced three examples of JAX-to-TensorFlow.js conversion each with increasing complexity: \nA simple JAX function \nAn image classification Flax model trained on the MNIST dataset \nA full image/text Vision Transformer (ViT) demo, which was used for the Google AI blog post Locked-Image Tuning: Adding Language Understanding to Image Models (a preview of the demo is shown in Figure 1 below)\nFor each example, there are Google Colab notebooks you can use to try the JAX-to-TensorFlow.js conversion yourself.\nFigure 1. TensorFlow.js model matching user-provided text prompts to a precomputed image embedding (try it out yourself). See Example 3: LiT Demo below for implementation details.\n\nBackground: JAX and TensorFlow.js\nJAX is a NumPy-like library developed by Google Research for high performance computing. It uses XLA to compile programs optimized for GPUs and TPUs. Flax is a popular neural network library built on top of JAX. Researchers have been using JAX/Flax to train very large models with billions of parameters (such as PaLM for language understanding and generation, or Imagen for image generation), making full use of modern hardware. If you're new to JAX and Flax, start with this JAX 101 tutorial and this Flax Getting Started example.\n\nTensorFlow started as a library for ML towards the end of 2015 and has since become a rich ecosystem that includes tools for productionizing ML pipelines (TFX), data visualization (TensorBoard), deploying ML models to edge devices (TensorFlow Lite), and devices running on a web browser or any device capable of executing JavaScript (TensorFlow.js). Models developed in JAX or Flax can tap into this rich ecosystem by first converting such a model to the TensorFlow SavedModel format, and then using the same tooling as if they had been developed in TensorFlow natively.\n\nThis is now made even easier for TensorFlow.js through the new Python API \u2014 tfjs.converters.convert_jax() \u2014 which allows users to convert a JAX model written in Python to a web format (.json) directly, so that the model can be used in the browser with Tensorflow.js.\n\nTo learn how to perform JAX-to-TensorFlow.js conversion, check out the three examples below.\nExample 1: Converting a simple JAX function\nIn this introductory example, you\u2019ll convert a few simple JAX functions using converters.convert_jax().\n\nInternally, this function does the following:\nIt converts to the Tensorflow SavedModel format, which contains a complete TensorFlow program, including trained parameters (that is, tf.Variables) and computation.\nThen, it constructs a TensorFlow.js model from that SavedModel (refer to Figure 2 for more details).\nFigure 2. High-level visualization of the conversion steps inside converters.convert_jax(), which converts a JAX function to a Tensorflow.js model.\n\nTo convert a Flax model to TensorFlow.js, you need a few things:\nA function that runs the forward pass of the model.\nThe model parameters (this is usually a dict-like structure).\nA specification of the shapes and dtypes of the inputs to the function.\nThe following examples uses a single parameter weight and implements a function prod, which multiplies the input with the parameter (in a real example, params will contain the all weights of the modules used in the neural network):\n\ndef prod(params, xs):\n  return params['weight'] * xs\n\nLet's call this function with some values and verify the output makes sense:\n\nparams = {'weight': np.array([0.5, 1])}\n# This represents a batch of 3 inputs, each of length 2.\nxs = np.arange(6).reshape((3, 2))\nprod(params, xs)\n\nThis gives the following output, where each batch element is element-wise multiplied by [0.5, 1]:\n\n[[0. 1.]\n [1. 3.]\n [2. 5.]]\n\nNext, let's convert this to TensorFlow.js using convert_jax and use the helper function get_tfjs_predict_fn (which can be found in the Colab), allowing us to verify that the outputs for the JAX function and the web model match. (Note: this helper function will only work in Colab, as it uses some tooling to run the web model using Javascript.)\n\ntfjs.converters.convert_jax(\n    prod,\n    params, \n    input_signatures=[tf.TensorSpec((3, 2), tf.float32)],\n    model_dir=model_dir)\n\ntfjs_predict_fn = get_tfjs_predict_fn(model_dir)\ntfjs_predict_fn(xs)  # Same output as JAX.\n\nDynamic shapes are supported as usual in Tensorflow by passing the value None for the dynamic dimensions in input_signature. Additionally, one should pass the argument polymorphic_shapes specifying names for dynamic dimensions. Note that polymorphism is a term coming from type theory, but here we use it to mean that the function works for multiple related shapes, e.g., for multiple batch sizes. This is necessary for shape checking in the JAX function (see Colab for more examples, and here for more documentation on this notation).\n\ntfjs.converters.convert_jax(\n    prod,\n    params, \n    input_signatures=[tf.TensorSpec((None, 2), tf.float32)],\n    polymorphic_shapes=['(b, 2)')],\n    model_dir=model_dir)\n\ntfjs_predict_fn = get_tfjs_predict_fn(model_dir)\ntfjs_predict_fn(np.array([[1., 2.]]))  # Outputs: [[0.5, 2. ]]\nExample 2: MNIST Model\n\nLet's use the same conversion code snippet from before, but this time we'll use TensorFlow.js to run a real ML model. Flax provides a Colab example of an MNIST classifier that we'll use as a starting point.\n\nAfter cloning the repository, the model can be trained using:\n\ntrain_ds, test_ds = train.get_datasets()\nstate = train.train_and_evaluate(config, workdir=f'./workdir')\n\nThis yields a state.apply_fn that can be used to compute logits for input images. Note that the function expects the first argument to be the model weights state.params. Given a batch of input images shaped [batch_size, 28, 28, 1], this will produce the logits for the probability distribution over the ten labels for every model (shaped [batch_size, 10]).\n\nlogits = state.apply_fn({'params': state.params}, imgs)\n\nThe MNIST model's state.apply_fn() is then converted exactly the same way as in the previous section \u2013 after all, it's a pure function that takes params and images as inputs and returns logits:\n\ntfjs.converters.convert_jax(\n    state.apply_fn,\n    {'params': state.params},\n    input_signatures=[tf.TensorSpec((1, 28, 28, 1), tf.float32)],\n    model_dir=tfjs_model_dir,\n)\n\nOn the JavaScript side, you load the model asynchronously, showing a simple progress update in the status text, making sure to give some feedback while the model weights are transferred:\n\ntf.loadGraphModel(modelDir + '/model.json', {\n    onProgress: p => status.innerText = `loading model: ${Math.round(p*100)}%`\n})\n\nA minimal UI is loaded from this snippet, and in the callback function you call the TensorFlow.js model and output the predictions. The function parameter img is a Uint8Array of length 28*28, which is first converted to a TensorFlow.js tf.tensor, before computing the model outputs, and converting them to probabilities via the tf.softmax() function. The output values from the computation are then waited for synchronously by calling .dataSync(), and converted to JavaScript arrays before they're displayed.\n\nui.onUpdate(img => {\n  const imgs = tf.tensor(img).cast('float32').reshape([1, 28, 28, 1])\n  const logits = model.predict(imgs)\n  const preds = tf.softmax(logits)\n  const { values, indices } = tf.topk(preds, 10)\n\n  ui.showPreds([...values.dataSync()], [...indices.dataSync()]) \n})\n\nThe Colab then starts a webserver and tunnels the port so you can scan a QR code on a mobile phone and directly connect to the demo. Even though the training reports around 99.1% accuracy on the test set, you'll see that the model can easily be fooled with digits that are easy to recognize for the human eye, but hard for a model that has only seen digits from the MNIST dataset (Figure 3).\nFigure 3. Our model from the Colab with 99.1% accuracy on the MNIST test dataset is still surprisingly bad at recognizing hand-written digits. On the left, the model predicts all kinds of digits instead of \"one\". On the right side, the \"one\" is drawn more like the data from the training set.\nExample 3: LiT Demo\nWriting a more realistic application with a TensorFlow.js model is a bit more involved. This section goes through the main steps that were used to create the demo app from the Google AI blog post Locked-Image Tuning: Adding Language Understanding to Image Models. Refer to that post for technical details on the implementation of the ML model. Also make sure to check out the final LiT Demo.\nAdapting the model\nBefore starting to implement an ML demo, it's a good moment to think carefully about the different options and their respective strengths and weaknesses.\n\nAt a high level, you have two options: running the ML model on server-side infrastructure, or running the ML model on the edge (i.e. on the visiting user's device).\nRunning a model on a server has the advantage that it can use exactly the same framework / code that was used to develop the model. There are libraries like Streamlit or Gradio that make it very easy to quickly build interactive web apps around such centrally-hosted models. The servers running the model can be rather powerful, using lots of RAM and accelerators to run state-of-the-art ML models in near-real time, and such a website can be loaded even by the smallest mobile device.\nRunning the demo on-device puts a limit on the size of the model that you can use, but comes with convincing advantages:\nNo data is ever sent off the device, which is desirable both for privacy reasons and to bring down latency.\nFree scaling: For instance, a normal webserver (such as one running on GitHub Pages) can serve hundreds or thousands of users simultaneously free of charge. And running a powerful model on server-side infrastructure at this scale would be very expensive (massive compute is not cheap).\nThe model you use for the demo consists of two parts: an image encoder, and a text encoder (see Figure 4).\n\nFor computing image embeddings you use a large model, and for text embeddings\u2014a small model. To make the demo run faster and produce better results, the expensive image embeddings are pre-computed, so the Tensorflow.js model only needs to compute the text embeddings and then compare the image and text embeddings to compute similarities.\nFigure 4. Image/text models like LiT (or CLIP) consist of two encoders that can be used separately to create vector representations of images and texts. Usually both image and text encoders are of similar size (LiT-B16B model, left image). For the demo, we precompute image embeddings using a large image encoder, and then run inference on the text on-device using a tiny text encoder (LiT-L16Ti model, right image).\n\nFor the demo, we now get those powerful ViT-Large image representations for free, because we can precompute them for all demo images. This allows us to make for a compelling demo with a limited compute budget. In addition to the \"tiny\" text encoder, we have also prepared a \"small\" text encoder for the same image embeddings (LiT-L16S), which performs a bit better, but uses more bandwidth to download the model weights, and requires more GPU memory to run on-device. We have evaluated the different models with the code from this Colab:\n\n\nImage encoder\nText encoder\nZeroshot performance\nModel\nParams\nFLOPs\nParams\nFLOPs\nCIFAR-100\nImageNet\nLiT-B16B\n86M \n(344  MB)\n36B\n109M \n(436 MB)\n2.7B\n79.2%\n71.7%\nLiT-L16S  (\"small\" text encoder)\n303M \n(1.2 GB)\n123B\n28M \n(111 MB)\n0.7B\n75.8%\n60.7%\nLiT-L16Ti (\"tiny\" text encoder)\n303M \n(1.2 GB)\n123B\n9M \n(36 MB)\n0.2B\n73.2%\n53.4%\n\nNote though that the \"zeroshot performance\" should only be taken as a proxy. In the end, the model performance needs to be good enough for the demo, and in this case our manual testing showed that even the tiny text transformer was able to compute similarities good enough for the demo. Next, we tested the performance of the tiny and small text encoders using this TensorFlow.js benchmark tool on different platforms (using the \"custom model\" option, and benchmarking 5x16 tokens on the WebGL backend):\n\nLiT-L16T (\"tiny\" text encoder) - benchmark\nLiT-L16S (\"small\" text encoder) - benchmark\n\nLoad time\nWarmup\nAverage/10\nPeak memory\nLoad time\nWarmup\nAverage/10\nPeak memory\nMacBook Pro (Intel i7 2.6GHz / Radeon Pro 5300M)\n1.1s\n0.15s\n0.12s\n33.9 MB\n3.9s\n0.8s\n0.8s\n122 MB\niPad Air (4th gen)\n1.3s\n0.6s\n0.5s\n33.9 MB\n2.7s\n2.4s\n2.5s\n141 MB\nSamsung S21 G5 (cell phone)\n2.0s\n1.3s\n1.1s\n33.9 MB\n-\n-\n-\n-\n\nNote that the results for the model with the \"small\" text encoder are missing for \"Samsung S21 G5\" in the above table because the model did not fit into memory. In terms of performance, the model with the \"tiny\" text encoder produces results within approximately 0.1-1 seconds, which still feels quite responsive, even on the smallest platform tested.\nThe Lit-LiT web app \nPreparing the model for this application is a bit more complicated, because we need not only convert the text transformer model weights, but also a matching tokenizer, and the precomputed image embeddings. The Colab loads a LiT model and showcases how to use it, and then prepares contents needed by the web app:\nThe tiny/small text encoder converted to TensorFlow.js and the matching tokenizer vocabulary.\nImages in JPG format, as seen by the model (in particular, this means a fixed 224x224 pixel crop)\nPre-computed image embeddings (since the converted model will only be able to compute embeddings for the texts).\nA selection of example prompts for every image. The embeddings of these prompts are also precomputed to allow to show precomputed answers if the prompts are not modified.\nThese files are prepared inside the data/ directory and then downloaded as a ZIP file. This file can then be uploaded to a web hosting, from where it is loaded by the web app (for example on GitHub Pages: vision_transformer/lit/data).\n\nThe code for the entire client-side application is available on GitHub: https://github.com/google-research/big_vision/tree/main/big_vision/tools/lit_demo/.\n\nThe application is built using Lit web components. The main index.html declares the demo application:\n<lit-demo-app></lit-demo-app>\n\nThis web component is defined in lit-demo-app.ts in the src/components subdirectory, next to all the other web components (image carousel, model controls etc).\n\nFor the actual computation of image/text similarities, the component image-prompts.ts calls functions from the module src/lit_demo/compute.ts, which wraps all the TensorFlow.js specific code.\nexport class Model {\n  /** Tokenizes text. */\n  tokenize(texts: string[]): tf.Tensor { /* ... */ }\n  /** Computes text embeddings. */\n  embed(tokens: tf.Tensor): tf.Tensor {\n    return this.model!.execute({inputs: tokens}) as tf.Tensor;\n  }\n  /** Computes similarities texts / pre-computed image embeddings. */\n  computeSimilarities(texts: string[], imgidxs: number[]) {\n    const textEmbeddings = this.embed(this.tokenize(texts));\n    const imageEmbeddingsTransposed = tf.transpose(\n        tf.concat(imgidxs.map(idx => tf.slice(this.zimgs!, idx, 1))));\n    return tf.matMul(textEmbeddings, imageEmbeddingsTransposed);\n  }\n  /** Applies softmax to `computeSimilarities()`. */\n  computeProbabilities(texts: string[], imgidx: number): number[] {\n    const sims = this.computeSimilarities(texts, [imgidx]);\n    const row = tf.squeeze(tf.slice(tf.transpose(sims), 0, 1));\n    return [...tf.softmax(tf.mul(this.def!.temperature, row)).dataSync()];\n  }\n}\n\nThe parent directory of the data/ exported by the Colab above is referenced via the baseUrl in the file src/lit/constants.ts. By default it refers to the models from the official demo. When replacing the baseUrl with a different server, make sure to enable cross origin resource sharing.\n\nIn addition to the complete application, it's also possible to export the functional parts without the UI as a single JavaScript file that can be linked statically. See the file playground.html as an example, and refer to the instructions in README.md for how to compile the entire application or the functional part before deploying the application.\n\n<!-- Loads global symbol `lit`. -->\n<script src=\"exports_bin.js\"></script>\n<script>\nasync function demo() {\n  lit.setBaseUrl('https://google-research.github.io/vision_transformer/lit');\n  const model = new lit.Model('tiny');\n  await model.load();\n  console.log(model.computeProbabilities(['a dog', 'a cat'], /*imgIdx=*/1);\n}\ndemo();\n</script>\nConclusion\nIn this article you learned how to convert JAX functions and Flax models into the TensorFlow.js format that can be executed in a browser or on devices capable of running JavaScript.\n\nThe first example demonstrated how to convert a JAX function to a TensorFlow.js model, which can then be loaded in Colab for verification, or run on any device with a modern web browser \u2013 this is an exactly the same conversion that can be applied to more complex Flax models. The second example showed how to train an ML model in Colab, and test it interactively on a mobile phone.The third example provided a full template for running an on-device ML model (check out the live demo). We hope that this application can serve you as a good starting point for your own client-side demos using JAX models with TensorFlow.js. ",
    "link": "https://blog.tensorflow.org/2022/08/jax-on-web-with-tensorflowjs.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPgzXuw86PD1HNCCIhrah5h-OLyLNYYyTOnYclaxK5V-nxZvM4VD5qPlqm04cyUYPI8XMoP22aQ3rtz4TyySQIDNleKUDAnbf94PrHY_yXA4OX0LZyI_ljw0rXipCv5Y419LW_cmNt0w0lzep5GfJ5KzUaR23wsXqpewIRKiyRDFbO-2HffMx6p2LX/w660-h196/Tensorflow-Jax-on-the-Web-01.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgwpAmJdAP-fRNtCiHWHmS4154RxqEa4CSqGvw4hcTArM62yHJg2hXS5C87sMszpD7u8CEXZUkB8bsN1z5stRXIRc32DLJSRC_r4C6Gllbs4kYal_tYn7WXaRVrNssojVljHCzzGxbgsgoPvdhgdCpZeH2SUTq-PuNugAgLKgEfGAYqY09LkeKl8ueS/s1600/Tensorflow-Jax-on-the-Web-02.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4H94QsXfoepINTfAy2-gYFEuTiKMXJ6kOAM7Q6goa6IAC782zvnC3ShAIA149miSxf9QhIPKWnnOD69VQxPj9gvVNFdzVQUBi13r7XisyBC6qV8SRBJTG0F7VEoshLyp8dtAa01Uv2nj-fDFq4fUz7UWliIUKf7cXtzQoH-mvzwiffAPvTAzrlFay/w640-h262/lit_demo_cow.gif",
      "https://lh5.googleusercontent.com/vP0oQCOBaPjz6zRpw_eavL2gk6TLXU0d-5rlyhNZg4EnBRYi0Myi04XbHcZ6OO2KS0V0E1vet2-Z-AdueUUX9dBvOHXJhEGKjEqYy9vwAVng6yla5yuQJ85NODZGw2jQRRPCiD5pzbHFJLgQAHTEy4KGCNzD8vo1Q5xf88DEc-J6Ez8Hnr6sfnmmd9QAfJd7DaUivBsuqm0nmZvONVQs1UHVS4rmsPmDuA",
      "https://lh4.googleusercontent.com/9k8DmFKIJONGzpMwVgFHZNxqAssAyyr89zNZLC8CrsSwE2x6JW7PywaFPjuCtucwcVL52LWPLNaqsAL_TLTa9nEOvrR5f-Ni3QOFWic1ySCigrJt-pUrGKTckd3j6HJPukn-hZHyQzJNhm_LB0I-rASjVD9f5Gp_Drj2i85hGaHbA60QBFGJ88DD_SSTrOQzYI70eW2VdgGYLUTaCW1IaObXZPkEzZB3-A=s16000",
      "https://lh5.googleusercontent.com/vP0oQCOBaPjz6zRpw_eavL2gk6TLXU0d-5rlyhNZg4EnBRYi0Myi04XbHcZ6OO2KS0V0E1vet2-Z-AdueUUX9dBvOHXJhEGKjEqYy9vwAVng6yla5yuQJ85NODZGw2jQRRPCiD5pzbHFJLgQAHTEy4KGCNzD8vo1Q5xf88DEc-J6Ez8Hnr6sfnmmd9QAfJd7DaUivBsuqm0nmZvONVQs1UHVS4rmsPmDuA",
      "https://lh3.googleusercontent.com/jXraws2WolI4seZvfBq3YYjWuwC4rFeuRR9s_apRxjLrNj4Zxcn6R5StlF6aX_hCkKMzjeL0jsgkBHLLl9Y4lug3M5QNyKD5dZvMw7F5J2NkS86WMSGN2zJ1inS_biWNHtV9eM0A448RUzULdZwj6zjNFnm-4qJ6WQZPCVQGtEvuLb60XvVCSe8lCotI6g2995bDtUk3naPY8SZrt_Ls_Q1cTeavvu3gWQ=w640-h195",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgfVzg4q8D1JjL1fmHUBsA-MFcH5fcR2y7A2ApQv3Lv3dtzcPOFUhIffdK7xzGUMlr4_R61oDO9G0nh5ViI1fmAjl98yinxNferkoQJIW3fwDykQWQw_2Ez7j8vfxVr3TAOI8I4ny5hyw-OVn65JEiheOCr8vwjmNETG6c6XQ0AAHvM4GuJIVba8MMq/s16000/JAX%20BLog%205.png",
      "https://lh5.googleusercontent.com/vP0oQCOBaPjz6zRpw_eavL2gk6TLXU0d-5rlyhNZg4EnBRYi0Myi04XbHcZ6OO2KS0V0E1vet2-Z-AdueUUX9dBvOHXJhEGKjEqYy9vwAVng6yla5yuQJ85NODZGw2jQRRPCiD5pzbHFJLgQAHTEy4KGCNzD8vo1Q5xf88DEc-J6Ez8Hnr6sfnmmd9QAfJd7DaUivBsuqm0nmZvONVQs1UHVS4rmsPmDuA"
    ],
    "time": "2023/12/09 00:58:43"
  },
  {
    "title": "Content moderation using machine learning: a dual approach",
    "content": "Posted by Jen Person, Developer Advocate\nBeing kind: a perennial problem\nI've often wondered why anonymity drives people to say things that they'd never dare say in person, and it\u2019s unfortunate that comment sections for videos and articles are so often toxic! If you\u2019re interested in content moderation, you can use machine learning to help detect toxic posts which you consider for removal.\nML for web developers\nMachine learning is a powerful tool for all sorts of natural language-processing tasks, including translation, sentiment analysis, and predictive text. But perhaps it feels outside the scope of your work. After all, when you're building a website in JavaScript, you don't have time to collect and validate data, train a model using Python, and then implement some backend in Python on which to run said model. Not that there's anything wrong with Python\u2013it's just that, if you're a web developer, it's probably not your language of choice.\nFortunately, TensorFlow.js allows you to run your machine learning model on your website in everybody's favorite language: JavaScript. Furthermore, TensorFlow.js offers several pre-trained models for common use cases on the web. You can add the power of ML to your website in just a few lines of code! There is even a pre-trained model to help you moderate written content, which is what we're looking at today.\nThe text toxicity classifier ML model\nThere is an existing pretrained model that works well for content moderation: the TensorFlow.js text toxicity classifier model. With this model, you can evaluate text on different labels of unwanted content, including identity attacks, insults, and obscenity. You can try out the demo to see the classifier in action. I admit that I had a bit of fun testing out what sort of content would be flagged as harmful. For example:\nI recommend stopping here and playing around with the text toxicity classifier demo. It's a good idea to see what categories of text the model checks for and determine which ones you would want to filter from your own website. Besides, if you want to know what categories the above quote got flagged for, you'll have to go to the demo to read the headings.\nOnce you've hurled sufficient insults at the text toxicity classifier model, come back to this blog post to find out how to use it in your own code.\nA dual approach\nThis started as a single tutorial with client and server-side code, but it got a bit lengthy so I decided to split it up. Separating the tutorials also makes it easier to target the part that interests you if you just want to implement one part. In this post, I cover the implementation steps for client-side moderation with TensorFlow.js using a basic website. In part 2, I show how to implement the same model server-side using Cloud Functions for Firebase.\nClient-side moderation\nModerating content client-side provides a quicker feedback loop for your users, allowing you to stop harmful discourse before it starts. It can also potentially save on backend costs since inappropriate comments don't have to be written to the database, evaluated, and then subsequently removed.\nStarter code\nI used the Firebase text moderation example as the foundation of my demo website. It looks like this:\nKeep in mind TensorFlow.js doesn't require Firebase. You can use whatever hosting, database, and backend solutions that work best for your app's needs. I just tend to use Firebase because I'm pretty familiar with it already. And quite frankly, TensorFlow.js and Firebase work well together! The website in the Firebase demo showcases content moderation through a basic guestbook using a server-side content moderation system implemented through a Realtime Database-triggered Cloud Function. Don't worry if this sounds like a lot of jargon. I'll walk you through the specifics of what you need to know to use the TensorFlow.js model in your own code. That being said, if you want to build this specific example I made, it's helpful to take a look at the Firebase example on GitHub.\nIf you're building the example with me, clone the Cloud Functions samples repo. Then change to the directory of the text moderation app.\ncd text-moderation\nThis project requires you to have the Firebase CLI installed. If you don't have it, you can install it using the following npm command:\nnpm install -g firebase-tools\nOnce installed, use the following command to log in:\nfirebase login\nRun this command to connect the app to your Firebase project:\nfirebase use --add\nFrom here, you can select your project in the list, connect Firebase to an existing Google Cloud project, or create a new Firebase project. Once the project is configured, use the following command to deploy Realtime Database security rules and Firebase Hosting:\nfirebase deploy --only database,hosting\nThere is no need to deploy Cloud Functions at this time since we will be changing the sample code entirely.\nNote that the Firebase text moderation sample as written uses the Blaze (pay as you go) plan for Firebase. If you choose to follow this demo including the server-side component, your project might need to be upgraded from Spark to Blaze. If you have a billing account set on your project through Google Cloud, you are already upgraded and good to go! Most importantly, if you're not ready to upgrade your project, then do not deploy the Cloud Functions portion of the sample. You can still use the client-side moderation without Cloud Functions.\nTo implement client-side moderation in the sample, I added some code to the index.html and main.js files in the Firebase text moderation example. There are three main steps to implement when using a TensorFlow.js model: installing the required components, loading the model, and then running the prediction. Let's add the code for each of these steps.\nInstall the scripts\nAdd the required TensorFlow.js dependencies. I added the dependencies as script tags in the HTML, but you can use Node.js if you use a bundler/transpiler for your web app.\n<!--  index.html -->\n<!-- scripts for TensorFlow.js -->\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js\"> </script>\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow-models/toxicity\"></script>\nLoad the model\nAdd the following code to load the text toxicity model in the Guestbook() function. The Guestbook() function is part of the original Firebase sample. It initializes the Guestbook components and is called on page load.\n// main.js\n// Initializes the Guestbook.\nfunction Guestbook() {\n\n  // The minimum prediction confidence.\n  const threshold = 0.9;\n  // Load the model. Users optionally pass in a threshold and an array of\n  // labels to include.\n  toxicity.load(threshold).then(model => {\n    toxicity_model = model;\n  });\n//\u2026\nThe threshold of the model is the minimum prediction confidence you want to use to set the model's predictions to true or false--that is, how confident the model is that the text does or does not contain the given type of toxic content. The scale for the threshold is 0-1.0. In this case, I set the threshold to .9, which means the model will predict true or false if it is 90% confident in its findings. It is up to you to decide what threshold works for your use case. You may even want to try out the text toxicity classifier demo with some phrases that could come up on your website to determine how the model handles them.\ntoxicity.load loads the model, passing the threshold. Once loaded, it sets toxicity_model to the model value.\nRun the prediction\nAdd a checkContent function that runs the model predictions on messages upon clicking \"Add message\":\n// main.js\nGuestbook.checkContent = function(message) {\n  if (!toxicity_model) {\n    console.log('no model found');\n    return false;\n  }\n\n  const messages = [message];\n\n  return toxicity_model.classify(messages).then(predictions => {\n\n    for (let item of predictions) {\n      for (let i in item.results) {\n        console.log(item.results[i].match)\n        if (item.results[i].match === true) {\n          console.log('toxicity found');\n          return true;\n        }\n      }\n    }\n    console.log('no toxicity found');\n    return false;\n  });\n}\nThis function does the following:\nVerifies that the model load has completed. If toxicity_model has a value, then the load() function has finished loading the model.\nPuts the message into an array called messages, as an array is the object type that the classify function accepts.\nCalls classify on the messages array.\nIterates through the prediction results. predictions is an array of objects each representing a different language label. You may want to know about only specific labels rather than iterating through them all. For example, if your use case is a website for hosting the transcripts of rap battles, you probably don't want to detect and remove insults.\nChecks if the content is a match for that label. if the match value is true, then the model has detected the given type of unwanted language. If the unwanted language is detected, the function returns true. There's no need to keep checking the rest of the results, since the content has already been deemed inappropriate.\nIf the function iterates through all the results and no label match is set to true, then the function returns false \u2013 meaning no undesirable language was found. The match label can also be null. In that case, its value isn't true, so it's considered acceptable language. I will talk more about the null option in a future post.\nAdd a call to the checkContent in the saveMessage function:\n// main.js\n// Saves a new message on the Firebase DB.\nGuestbook.prototype.saveMessage = function(e) {\n  e.preventDefault();\n  if (!this.messageInput.value || !this.nameInput.value) { \n    return;\n  }\n\n  Guestbook.checkContent(this.messageInput.value).then((toxic) => {\n    if (toxic === true) {\n      // display a message to the user to be kind\n      Guestbook.displaySnackbar();\n      // clear the message field\n      Guestbook.resetMaterialTextfield(this.messageInput);\n      return;\n    }\n//\u2026\nAfter a couple quick checks for input values, the contents of the message box is passed to the checkContent function.\nIf the content passes this check, the message is written to the Realtime Database. If not, a snack bar displays reminding the message author to be kind. The snack bar isn't anything special, so I'm not going to include the code here. You can see it in the full example code, or implement a snack bar of your own.\nTry it out\nIf you've been following along in your own code, run this terminal command in your project folder to deploy the website:\nfirebase deploy \u2013only hosting\n\nYou can view the completed example code here.\nA message that's not acceptable gets rejected\nAn acceptable message gets published to the guestbook\n\nVerifying that this code was working properly was really uncomfortable. I had to come up with an insult that the model would deem inappropriate, and then keep writing it on the website. From my work computer. I know nobody could actually see it, but still. That was one of the stranger parts of my job, to be sure!\nNext steps\nUsing client-side moderation like this could catch most issues before they occur. But a clever user might open developer tools and try to find a way to write obscenities directly to the database, circumventing the content check. That's where server-side moderation comes in.\n\nIf you enjoyed this article and would like to learn more about TensorFlow.js, here are some things you can do:\nCheck out the TensorFlow.js edX course by Jason Mayes. If you are even remotely interested in using TensorFlow.js, I cannot recommend this enough. It might look like a lot at first, but the course is broken up into easy-to-follow manageable pieces. \nView all the TensorFlow.js pretrained models in the TFJS repository on GitHub. \nPlay around with TensorFlow.js projects on Glitch. \nTo see an example of ML image moderation on the web, try out Gant Laborde's NSFW TFJS image checker.",
    "link": "https://blog.tensorflow.org/2022/08/content-moderation-using-machine-learning-a-dual-approach.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEht0dlvklEzGfGe8X6EHy8fCHtBY4norDtdVs6JJOTEVRVqsU--EBwm6YWxC5c2iRWYdEHKxTnZWI7_3JetB8F3r3K2J3Ep7SMUF_3jlQEmoTuHcCHAz1yRkLUB_8jM2VksCE0w0xXt0ynPcR1ZJHS1LVs7VtJDmDes1AaJZw1tYkGxG_Lxir8lSn0s/s1600/Android-DeepLinksCrashCourse_Pt1_1024x512.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjzog6lDA2x_ZqZiRfK4-u1PfKvHiSzO4n_En9df6wT3FtJYdCRpGQjAz6Fi6NxMwgLXhM2WROOELVEzDrPXWTe0vyaLdZryWtgTH34fjZpAF_3sfqqK8dVqDrYlBvMop7RUFNjB50ZRLUIGyCmwvFcGEqZoY1tWu2RevJ0bAjAYj9bjD16B4yhYl4z/w686-h191/tensorflow-content-moderation-using-machine-learning-a-dual-approach-01.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiIuGhiE2jKW5V6XkKoGVmRQRgivcMN_guzKClgY133bEvkLH94m6L_ETzXupMWWHJWrKUgvygL52BlPZSdMJWAGn2JyzetFVa1D6sdxjAEQ5J3UXuWEiq1a5uNazaAFGoH9euRMkxTXZzUhlbcHpXDoXKke5u5-Gcu1YjP7pRAwNpcs1lmFtCA3eSt/s1600/TF%20Blog%201%20copy.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjSH4Omn4E0kp2CPMrT7XaHt9TNrkltPFlA-sEJhT8Du8cN9FUI_d2iO45Q4WbgfgV0tQU15y9rKkhcIp0-q4fs2jEQHyjNuWRPc0znnOLKUWp-sHt5Zd0bp8fMdj8hS2Jc3nzUwlPrRI-fg4WhlrzOwEjv-k_7FrF7-SvJ6ZS60CwH8XtikZcYOS3W/s1600/TF%20Blog%202%20copy.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjCudwB-q5bH0md-HHEcQ9ZAvMU5ABBEgQkjQx3rzkvvlpOw8WVA0uP_dmyjyE62W3V4_Hcd4CPhQirXyRwVJJCUJclvTnJb6dhtSHMi4v3T72h5M6UaNDKLnOZSU4xla_6BpuiHEFLL3seD-3Tq2hJRutrxjZJjkaZ5fXqFdie5KN1Saul2E3RWmGS/s1600/TF%20BLog%204.gif",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgghotkpgTBH5yNkpmpM_Ji0BfgqNSsexUQFTjyI4qPRD41gz_akoutZGscIuSewooRS0YcRPFVJik0hZ3fBvozlAChRElmIOgcwZW4z1b60aAKWWppvEIKl6zY1jQHUD89gMXTb9ZKwqt2begqJLSatmfkWOJqHeZbAbZPSeDTXPDUM63LH5pLSmCj/s16000/TF%20Blog%203.gif"
    ],
    "time": "2023/12/08 00:58:43"
  },
  {
    "title": "How Roboflow enables thousands of developers to use computer vision with TensorFlow.js",
    "content": "A guest post by Brad Dwyer, co-founder and CTO, Roboflow\nRoboflow lets developers build their own computer vision applications, from data preparation and model training to deployment and active learning. Through building our own applications, we learned firsthand how tedious it can be to train and deploy a computer vision model. That\u2019s why we launched Roboflow in January 2020 \u2013 we believe every developer should have computer vision available in their toolkit. Our mission is to remove any barriers that might prevent them from succeeding.\nOur end-to-end computer vision platform simplifies the process of collecting images, creating datasets, training models, and deploying them to production. Over 100,000 developers build with Roboflow\u2019s tools. TensorFlow.js makes up a core part of Roboflow's deployment stack that has now powered over 10,000 projects created by developers around the world.\nAs an early design decision, we decided that, in order to provide the best user experience, we needed to be able to run users' models directly in their web browser (along with our API, edge devices, and on-prem) instead of requiring a round-trip to our servers. The three primary concerns that motivated this decision were latency, bandwidth, and cost.\nFor example, Roboflow powers SpellTable's Codex feature which uses a computer vision model to identify Magic: The Gathering cards.\nFrom Twitter\nHow Roboflow Uses TensorFlow.js\nWhenever a user's model finishes training on Roboflow's backend, the model is converted and automatically converted to support sevel various deployment targets; one of those targets is TensorFlow.js. While TensorFlow.js is not the only way to deploy a computer vision model with Roboflow, some ways TensorFlow.js powers features within Roboflow include:\nroboflow.js\nroboflow.js is a JavaScript SDK developers can use to integrate their trained model into a web app or Node.js app. Check this video for a quick introduction:\nInference Server\nThe Roboflow Inference Server is a cross-platform microservice that enables developers to self-host and serve their model on-prem. (Note: while not all of Roboflow\u2019s inference servers are TFjs-based, it is one supported means of model deployment.)\nThe tfjs-node container runs via Docker and is GPU-accelerated on any machine with CUDA and a compatible NVIDIA graphics card, or using a CPU on any Linux, Mac, or Windows device.\nPreview\nPreview is an in-browser widget that lets developers seamlessly test their models on images, video, and webcam streams.\nLabel Assist\nLabel Assist is a model-assisted image labeling tool that lets developers use their previous model's predictions as the starting point for annotating additional images.\nOne way users leverage Label Assist is in-browser predictions:\nWhy We Chose TensorFlow.js\nOnce we had decided we needed to run in the browser, TensorFlow.js was a clear choice.\nBecause TFJS runs in our users' browsers and on their own compute, we are able to provide ML-powered features to our full user base of over 100,000 developers, including those on our free Public plan. That simply wouldn't be feasible if we had to spin up a fleet of cloud-hosted GPUs.\nBehind the Scenes\nTo implement roboflow.js with TensorFlow.js was relatively straightforward.\nWe had to change a couple of layers in our neural network to ensure all of our ops were supported on the runtimes we wanted to use, integrate the tfjs-converter into our training pipeline, and port our pre-processing and post-processing code to JavaScript from Python. From there, it was smooth sailing.\nOnce we'd built roboflow.js for our customers, we utilized it internally to power features like Preview, Label Assist, and one implementation of the Inference Server.\nTry it Out\nThe easiest way to try roboflow.js is by using Preview on Roboflow Universe, where we host over 7,000 pre-trained models that our users have shared. Any of these models can be readily built into your applications for things like seeing playing cards, counting surfers, reading license plates, and spotting bacteria under microscope, and more.\nOn the Deployment tab of any project with a trained model, you can drop a video or use your webcam to run inference right in your browser. To see a live in-browser example, give this community created mask detector a try by clicking the \u201cWebcam\u201d icon:\nTo train your own model for a custom use case, you can create a free Roboflow account to collect and label a dataset, then train and deploy it for use with roboflow.js in a single click. This enables you to use your model wherever you may need.\nAbout Roboflow\nRoboflow makes it easy for developers to use computer vision in their applications. Over 100,000 users have built with the company's end-to-end platform for image and video collection, organization, annotation, preprocessing, model training, and model deployment. Roboflow provides the tools for companies to improve their datasets and build more accurate computer vision models faster so their teams can focus on their domain problems without reinventing the wheel on vision infrastructure.\nBrowse datasets on Roboflow Universe\nGet started in the Roboflow documentation\nView all available Roboflow features",
    "link": "https://blog.tensorflow.org/2022/07/how-roboflow-enables-thousands-of-developers-to-use-computer-vision-with-TensorFlow.js.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhRIq1GZCW_M4yiPxRQqUfY9TAGkxrRZ_s3peedpES5VyFHkMOK-eVumBaW-pwoE_A3Q9IX_ar4Zp9HzDoQdxX4W4SgCXjgcKYt3BHjmbWyteaS-GegM9ya8OyzUKIouFq9mqxJcXclzwfsNrQMfcnEBN5ScGfIQuSgka_7kJk9IlEeb4cjqCzrPv0d/s1600/Roboflow%20blog%202.gif",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg3fH2YX7U79s6s7RHZJuPlYJcqkc8PB1mGwD5wPlmom2yh3q-3y3vAoaopZ0U-a7RfCAZNCk7KBKt1RDq2refpSuF50oj6vcg6mtEuQP7UwwgodufA2HKB0czwHW1SMSt1uVcsIOH2dfa2Rc6cOqzSR6pmC4YXJW_tHD5LND9j2UszTUYCkn-6kjlH/s1600/Roboflow%20blog%203.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwjHPQMr2knMeHlXX-bOFrx6XxXKKr-PzTZKVWqJ6ffj_VcuKY-CKta-3GjuXRQZx9MXAc-SHODMqnaNVRG4FcRRkBLgp2h6ZQ18AdE7x56poCzZaUfiJPN0hTuinPuC8nt8qcYMgNej1gjLwKkTm5oQJ52LOkBY0yr-cVXrN0CM4iXFRRXOVCIrfG/s1600/Roboflow%20blog%201.gif",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgSymprLcTpg7tM481QkcQ_gF3rXdwPyNmXehz3q_jq8dxR1_TE4dTi43ayquf9ngWPVEq7YLHJj61y_6BBN-DNreNMJpXAP9_J61FdZTa6haeL6cE-poQ2EwKBv7prPQByMJLZCxXVu4VMuo3bpkwE3F1qstHVKwVFf0CQjmL8VYixS_4a7nUSqLeE/s1600/Roboflow%20blog%205.gif"
    ],
    "time": "2023/12/08 00:58:43"
  },
  {
    "title": "OCR in the browser using TensorFlow.js",
    "content": "A guest post by Charles Gaillard, Mindee\nIntroduction\nOptical Character Recognition (OCR) refers to technologies capable of capturing text elements from images or documents and converting them into a machine-readable text format. If you want to learn more on that topic, this article is a good introduction.\nAt Mindee, we have developed an open-source Python-based OCR called DocTR, however we also wanted to deploy it in the browser to ensure that it was accessible to all developers - especially as ~70% developers choose to use JavaScript.\n\nWe managed to achieve this using the TensorFlow.js API, which resulted in a web demo that you can now try for yourself using images of your own.\nThe demo interface with a picture of 2 receipts being parsed by the OCR: 89 words were found here\nThis demo is designed to be very simple to use and run quickly on most computers, therefore we provided a single pretrained model that we trained with a small (512 x 512) input size to save memory. Images are resized to be squares, so it generalizes well to most of the documents which have an aspect ratio close to 1: cards, smaller receipts, tickets, A4, etc. For rectangles with a very high aspect ratio, segmentation results might not be as good because we don\u2019t preserve the aspect ratio (with padding) at the text detection step. It is optimized to work on documents with a significant word size (for example receipts, cards, etc). Keep in mind that these models have been designed to offer performance while running in the browser. Hence, performance might not be optimal on documents that have a very small writing size vs the size of the document or images with a very high aspect ratio.\nDive into the architecture\nOCR models can be divided into 2 parts: A detection model and a text recognition model. In DocTR, the detection model is a CNN (convolutional neural network) which segments the input image to find text areas, then text boxes are cropped around each detected word and sent to a recognition model. The second model is a convolutional recurrent neural network (CRNN), which extracts features from word-images and then decodes the sequence of letters on the image with recurrent layers (LSTM).\nGlobal architecture of the OCR model used in this Demo\nDetection model\nWe have different architectures implemented in DocTR, but we chose a very light one for use on the client side as device hardware can change from person to person. Here we used a mobilenetV2 backbone with a DB (Differentiable Binarization) head. The implementation details can be found in the DocTR Github. We trained this model with an input size of (512, 512, 3) to decrease latency and memory usage. We have a private dataset composed of 130,000 annotated documents that was used to train this model.\nRecognition model\nThe recognition model we used is also our lighter architecture: a CRNN (convolutional recurrent neural network) with a mobilenetV2 backbone. More information on this architecture can be found here. It is basically composed of the first half of the mobilenetV2 layers to extract features and it is followed by 2 bi-LSTMs to decode visual features as character sequences (words). It uses the CTC loss, introduced by Alex Graves, to decode a sequence efficiently. We have an input size of (32, 128, 3) for word images in this model, and we use padding to preserve the aspect ratio of crops. It is trained on our private dataset, composed of 11 millions text boxes extracted from different documents. This dataset has a wide variety of fonts, since it is composed of documents which come from many different data sources. We used data augmentation so that it generalizes well on different fonts, backgrounds, and renderings. It should also give decent results on handwritten text as long as it is human-readable.\nModel conversion & code implementation\nAs our model was originally implemented using TensorFlow, Python conversion was required to run the resulting models in the web browser at scale. To do this we exported a tensorflow SavedModel for each Python model trained and used the tensorflowjs_converter command line tool to quickly convert our saved models to the TensorFlow.js JSON format required for execution in the browser.\nThe resulting converted models were then integrated into our React.js front end application that powered the user interface of the demo. More precisely, we used MUI to design the components of the interface for our in-house front-end SDK react-mindee-js (which provides computer vision tools) and OpenCV.js for the detection model post processing. This post processing step took the raw binarized segmentation map and converted it to a list of polygons with OpenCV.js functions. We could then crop those boxes from the source image to finally obtain word images ready to be sent to the recognition model.\nSpeed & performance\nWe had to manage the tradeoff between speed and performance efficiently. OCR models are quite slow because you have 2 tasks (text areas segmentation + words recognition) that can't be parallelized, so we had to use lightweight models to ensure speedy execution on most devices.\n\nOn an modern computer with an RTX 2060 and an i7 9th Gen, the detection task takes around 750 milliseconds per image, and the recognition model around 170 milliseconds per batch of 32 crops (words) with the WebGL backend, benchmarked with the TensorFlow.js benchmarking tool.\nWrapping up the 2 models and the vision operations (detection post processing), the end-to-end OCR runs in less than 2 seconds on small documents (less than 100 words) and the prediction time can only take a few seconds more to run on very dense documents with a lot of words.\nA screenshot of the demo interface with a very dense old A4 document being parsed by the OCR: 738 words are identified.\nConclusion\nThis demo powered by TensorFlow.js is a way to give access to an online, relatively quick and robust document OCR to almost everyone, which is one of the first of its kind powered by TensorFlow.js entirely in the browser.\n\nAs we are executing the model on the client side, exact performance will vary depending on the hardware of the device it is run on. However the goal here is more to demonstrate that even complex and state-of-the-art deep learning models can be deployed in the browser and run on almost every machine in an efficient manner that can be very useful, especially for potentially sensitive document information, where you do not want to send the document to the cloud for analysis.\nWe are excited to offer this solution for all to use, and keen to follow the future of the Web ML industry, where things will no doubt get faster with time as new web standards like WebGPU become mainstream and enabled by default on modern web browsers.",
    "link": "https://blog.tensorflow.org/2022/06/ocr-in-browser-using-tensorflowjs.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUZjFVuG9BSKB8i2rF2n83tL9sVWXegW2Lu7ia6nSt-L8FWcpzEkP2rBFKMcRCkz5vXW7Ma4dvqgB6xL-AWhFP6HCPZ11Kj_rXyMj7KvARsj2T4POi5RlNuMn7XrhleBAww-TeLdoF3uvv0KSt32TqKaJfP8-gkl3HWLioEYI9jZrjsqlfjjWwrecq/s1600/image3.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhMI_POfn1KTijChup8uEzoTHu_f-Iej0qEkVEt-HXSo68zYZbDwK3HpO-y1uVk5Oqd0xzzX6fvR4xsYQ3zRsLAo4JdgbywdDqLCfcJyL_GtNmDT4Vx0jceb34a9BrkCmBa9y6_uKhUwPZBaLLyRYqIaalYRdoSPI4GhK3sHV13-Mp2MDm7g15_gnkw/s1600/image3.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh3BTkF3r714C-R0T3mok-qCUgGYjbVTe896k2TGPc11BGFmi0IX2NSvYnXR8YIpo2ipJrmgWQCf_lB1vpjHqnmsrqtnZKY5OmK-rBq0mUEdv5mnn01yqF8WbueGwybfVpCkcTQH5DbW2iPJqyWVPVZGxTNpkEdSPaRpDZ4kSGdqedJCzpe6-ArkCuu/s1600/image2.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjYe67hhAS-6v1JTtluOekvu72YGChcu0wc3OQlAIFlhSIOZrSWS_vqYMppePEWeuEi5q4xOBLoO3BqSar4PA0dvY9NWCDTY90S0jDhM5CbECshmTlUBjOT8SA_1m4CFxXTM048bbh1FGEa_2eSffkbBEo3GW2pAUdaN0qHcEWeOvH6NRi39_ZDhzZ-/s1600/image1.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjYe67hhAS-6v1JTtluOekvu72YGChcu0wc3OQlAIFlhSIOZrSWS_vqYMppePEWeuEi5q4xOBLoO3BqSar4PA0dvY9NWCDTY90S0jDhM5CbECshmTlUBjOT8SA_1m4CFxXTM048bbh1FGEa_2eSffkbBEo3GW2pAUdaN0qHcEWeOvH6NRi39_ZDhzZ-/s1600/image1.png"
    ],
    "time": "2023/12/08 00:58:43"
  },
  {
    "title": "Real-time SKU detection in the browser using TensorFlow.js",
    "content": "Posted by Hugo Zanini, Data Product Manager\nLast year, I published an article on how to train custom object detection in the browser using TensorFlow.js. This received lots of interest from developers from all over the world who tried to apply the solution to their personal or business projects.While answering reader\u2019s questions on my first article, I noticed a few difficulties in adapting our solution to large datasets, and deploying the resulting model in production using the new version of TensorFlow.js.\nTherefore, the goal of this article is to share a solution for a well-known problem in the consumer packaged goods (CPG) industry: real-time and offline SKU detection using TensorFlow.js.\nOffline SKU detection running in real time on a smartphone using TensorFlow.js\nThe problem\nItems consumed frequently by consumers (foods, beverages, household products, etc) require an extensive routine of replenishment and placement of those products at their point of sale (supermarkets, convenience stores, etc).\nOver the past few years, researchers have shown repeatedly that about two-thirds of purchase decisions are made after customers enter the store. One of the biggest challenges for consumer goods companies is to guarantee the availability and correct placement of their product in-stores.\nAt stores, teams organize the shelves based on marketing strategies, and manage the level of products in the stores. The people working on these activities may count the number of SKUs of each brand in a store to estimate product stocks and market share, and help to shape marketing strategies.\nThese estimations though are very time-consuming. Taking a photo and using an algorithm to count the SKUs on the shelves to calculate a brand\u2019s market share could be a good solution.\nTo use an approach like that, the detection should run in real-time such that as soon as you point a phone camera to the shelf, the algorithm recognizes the brands and calculates the market shares. And, as the internet inside the stores is generally limited, the detection should work offline.\nExample workflow\nThis post is going to show how to implement the real-time and offline image recognition solution to identify generic SKUs using the SKU110K dataset and the MobileNetV2 network.\nDue to the lack of a public dataset with labeled SKUs of different brands, we\u2019re going to create a generic algorithm, but all the instructions can be applied in a multiclass problem.\nAs with every machine learning flow, the project will be divided into four steps, as follows:\nObject Detection Model Production Pipeline\nPreparing the data\nThe first step to training a good model is to gather good data. As mentioned before, this solution is going to use a dataset of SKUs in different scenarios. The purpose of SKU110K was to create a benchmark for models capable of recognizing objects in densely packed scenes.\nThe dataset is provided in the Pascal VOC format and has to be converted to tf.record. The script to do the conversion is available here and the tf.record version of the dataset is also available in my project repository. As mentioned before, SKU110K is a large and very challenging dataset to work with. It contains many objects, often looking similar or even identical, positioned in close proximity.\nNumber of Images Average items per image Number of object classes Average classes per image\n11762 147.4 110712 86\nview raw\nproperties.csv hosted with \u2764 by GitHub\nTo work with this dataset, the neural network chosen has to be very effective in recognizing patterns and be small enough to run in real-time in TensorFlow.js.\nChoosing the model\nThere are a variety of neural networks capable of solving the SKU detection problem. But, the architectures that easily achieve a high level of precision are very dense and don't have reasonable inference times when converted to TensorFlow.js to run in real-time.\nBecause of that, the approach here is going to be to focus on optimizing a mid-level neural network to achieve reasonable precision working on densely packed scenes and run the inferences in real-time. Analyzing the TensorFlow 2.0 Detection Model Zoo, the challenge will be to try to solve the problem using the lighter single-shot model available: SSD MobileNet v2 320x320 which seems to fit the criteria required. The architecture is proven to be able to recognize up to 90 classes and can be trained to identify different SKUs.\nTraining the model\nWith a good dataset and the model selected, it\u2019s time to think about the training process. TensorFlow 2.0 provides an Object Detection API that makes it easy to construct, train, and deploy object detection models. In this project, we\u2019re going to use this API and train the model using a Google Colaboratory Notebook. The remainder of this section explains how to set up the environment, the model selection, and training. If you want to jump straight to the Colab Notebook, click here.\nSetting up the environment\nCreate a new Google Colab notebook and select a GPU as the hardware accelerator:\nRuntime > Change runtime type > Hardware accelerator: GPU\nClone, install, and test the TensorFlow Object Detection API:\nNext, download and extract the dataset using the following commands:\nViewer requires iframe.\nview raw\ndownload_dataset.ipynb hosted with \u2764 by GitHub\nSetting up the training pipeline\nWe\u2019re ready to configure the training pipeline. TensorFlow 2.0 provides pre-trained weights for the SSD Mobilenet v2 320x320 on the COCO 2017 Dataset, and they are going to be downloaded using the following commands:\nViewer requires iframe.\nview raw\nweights-download.ipynb hosted with \u2764 by GitHub\nThe downloaded weights were pre-trained on the COCO 2017 Dataset, but the focus here is to train the model to recognize one class so these weights are going to be used only to initialize the network\u200a\u2014\u200athis technique is known as transfer learning, and it\u2019s commonly used to speed up the learning process.\nThe last step is to set up the hyperparameters on the configuration file that is going to be used during the training. Choosing the best hyperparameters is a task that requires some experimentation and, consequently, computational resources.\nI took a standard configuration of MobileNetV2 parameters from the TensorFlow Models Config Repository and performed a sequence of experiments (thanks Google Developers for the free resources) to optimize the model to work with densely packed scenes on the SKU110K dataset. Download the configuration and check the parameters using the code below.\nViewer requires iframe.\nview raw\ndownload_config.ipynb hosted with \u2764 by GitHub\n# SSD with Mobilenet v2\n# Trained on COCO17, initialized from Imagenet classification checkpoint\n# Train on TPU-8\n#\n# Achieves 22.2 mAP on COCO17 Val\nmodel {\n  ssd {\n    inplace_batchnorm_update: true\n    freeze_batchnorm: false\n    num_classes: 1\n    box_coder {\n      faster_rcnn_box_coder {\n        y_scale: 10.0\n        x_scale: 10.0\n        height_scale: 5.0\n        width_scale: 5.0\n      }\n    }\n    matcher {\n      argmax_matcher {\n        matched_threshold: 0.5\n        unmatched_threshold: 0.5\n        ignore_thresholds: false\n        negatives_lower_than_unmatched: true\n        force_match_for_each_row: true\n        use_matmul_gather: true\n      }\n    }\n    similarity_calculator {\n      iou_similarity {\n      }\n    }\n    encode_background_as_zeros: true\n    anchor_generator {\n      ssd_anchor_generator {\n        num_layers: 6\n        min_scale: 0.2\n        max_scale: 0.95\n        aspect_ratios: 1.0\n        aspect_ratios: 2.0\n        aspect_ratios: 0.5\n        aspect_ratios: 3.0\n        aspect_ratios: 0.3333\n      }\n    }\n    image_resizer {\n      fixed_shape_resizer {\n        height: 300\n        width: 300\n      }\n    }\n    box_predictor {\n      convolutional_box_predictor {\n        min_depth: 0\n        max_depth: 0\n        num_layers_before_predictor: 0\n        use_dropout: false\n        dropout_keep_probability: 0.8\n        kernel_size: 1\n        box_code_size: 4\n        apply_sigmoid_to_scores: false\n        class_prediction_bias_init: -4.6\n        conv_hyperparams {\n          activation: RELU_6,\n          regularizer {\n            l2_regularizer {\n              weight: 0.00004\n            }\n          }\n          initializer {\n            random_normal_initializer {\n              stddev: 0.01\n              mean: 0.0\n            }\n          }\n          batch_norm {\n            train: true,\n            scale: true,\n            center: true,\n            decay: 0.97,\n            epsilon: 0.001,\n          }\n        }\n      }\n    }\n    feature_extractor {\n      type: 'ssd_mobilenet_v2_keras'\n      min_depth: 16\n      depth_multiplier: 1.0\n      conv_hyperparams {\n        activation: RELU_6,\n        regularizer {\n          l2_regularizer {\n            weight: 0.00004\n          }\n        }\n        initializer {\n          truncated_normal_initializer {\n            stddev: 0.03\n            mean: 0.0\n          }\n        }\n        batch_norm {\n          train: true,\n          scale: true,\n          center: true,\n          decay: 0.97,\n          epsilon: 0.001,\n        }\n      }\n      override_base_feature_extractor_hyperparams: true\n    }\n    loss {\n      classification_loss {\n        weighted_sigmoid_focal {\n          alpha: 0.75,\n          gamma: 2.0\n        }\n      }\n      localization_loss {\n        weighted_smooth_l1 {\n          delta: 1.0\n        }\n      }\n      classification_weight: 1.0\n      localization_weight: 1.0\n    }\n    normalize_loss_by_num_matches: true\n    normalize_loc_loss_by_codesize: true\n    post_processing {\n      batch_non_max_suppression {\n        score_threshold: 1e-8\n        iou_threshold: 0.6\n        max_detections_per_class: 100\n        max_total_detections: 100\n      }\n      score_converter: SIGMOID\n    }\n  }\n}\ntrain_config: {\n  fine_tune_checkpoint_version: V2\n  fine_tune_checkpoint: \"<<PUT YOUR PATH HERE /mobilenet_v2/mobilenet_v2.ckpt-1>>\"\n  fine_tune_checkpoint_type: \"classification\"\n  batch_size: 48\n  sync_replicas: true\n  startup_delay_steps: 0\n  replicas_to_aggregate: 8\n  num_steps: 5000\n  data_augmentation_options {\n    random_horizontal_flip {\n    }\n  }\n  data_augmentation_options {\n    ssd_random_crop {\n    }\n  }\n  optimizer {\n    momentum_optimizer: {\n      learning_rate: {\n        cosine_decay_learning_rate {\n          learning_rate_base: .8\n          total_steps: 50000\n          warmup_learning_rate: 0.13333\n          warmup_steps: 2000\n        }\n      }\n      momentum_optimizer_value: 0.9\n    }\n    use_moving_average: false\n  }\n  max_number_of_boxes: 100\n  unpad_groundtruth_tensors: false\n}\ntrain_input_reader: {\n  label_map_path: \"<<PUT YOUR PATH HERE /workspace/datasets/SKU110K/labelmap.pbtxt >>\"\n  tf_record_input_reader {\n    input_path: \"<<PUT YOUR PATH HERE workspace/datasets/SKU110K/train_1.record >>\"\n  }\n}\neval_config: {\n  metrics_set: \"coco_detection_metrics\"\n  use_moving_averages: false\n}\neval_input_reader: {\n  label_map_path: \"<<PUT YOUR PATH HERE /workspace/datasets/SKU110K/labelmap.pbtxt >>\"\n  shuffle: false\n  num_epochs: 1\n  tf_record_input_reader {\n    input_path: \"<<PUT YOUR PATH HERE /workspace/datasets/SKU110K/test.record >>\"\n  }\n}\nview raw\nmobilenet_v2.config hosted with \u2764 by GitHub\nWith the parameters set, start the training by executing the following command:\nViewer requires iframe.\nview raw\nstarting_training.ipynb hosted with \u2764 by GitHub\nTo identify how well the training is going, we use the loss value. Loss is a number indicating how bad the model\u2019s prediction was on the training samples. If the model\u2019s prediction is perfect, the loss is zero; otherwise, the loss is greater. The goal of training a model is to find a set of weights and biases that have low loss, on average, across all examples (Descending into ML: Training and Loss | Machine Learning Crash Course).\nThe training process was monitored through Tensorboard and took around 22h to finish on a 60GB machine using an NVIDIA Tesla P4. The final losses can be checked below\nTotal training loss\nValidate the model\nNow let\u2019s evaluate the trained model using the test data:\nViewer requires iframe.\nview raw\nevaluating.ipynb hosted with \u2764 by GitHub\nThe evaluation was done across 2740 images and provides three metrics based on the COCO detection evaluation metrics: precision, recall, and loss (Classification: Precision and Recall | Machine Learning Crash Course). The same metrics are available via Tensorboard and can be analyzed in an easier way\n%load_ext tensorboard\n%tensorboard --logdir '/content/training/'\nYou can then explore all training and evaluation metrics.\nMain evaluation metrics\nExporting the model\nNow that the training is validated, it\u2019s time to export the model. We\u2019re going to convert the training checkpoints to a protobuf (pb) file. This file is going to have the graph definition and the weights of the model.\nViewer requires iframe.\nview raw\nexporting-trained.ipynb hosted with \u2764 by GitHub\nAs we\u2019re going to deploy the model using TensorFlow.js and Google Colab has a maximum lifetime limit of 12 hours, let\u2019s download the trained weights and save them locally. When running the command files.download(\"/content/saved_model.zip\"), the Colab will prompt the file download automatically.\nDeploying the model\nThe model is going to be deployed in a way that anyone can open a PC or mobile camera and perform inference in real-time through a web browser. To do that, we\u2019re going to convert the saved model to the TensorFlow.js layers format, load the model in a JavaScript application and make everything available on CodeSandbox.\nConverting the model\nAt this point, you should have something similar to this structure saved locally:\n%MD\n\u251c\u2500\u2500 inference-graph\n\u2502 \u251c\u2500\u2500 saved_model\n\u2502 \u2502 \u251c\u2500\u2500 assets\n\u2502 \u2502 \u251c\u2500\u2500 saved_model.pb\n\u2502 \u2502 \u251c\u2500\u2500 variables\n\u2502 \u2502 \u251c\u2500\u2500 variables.data-00000-of-00001\n\u2502 \u2502 \u2514\u2500\u2500 variables.index\nBefore we start, let\u2019s create an isolated Python environment to work in an empty workspace and avoid any library conflict. Install virtualenv and then open a terminal in the inference-graph folder and create and activate a new virtual environment:\nvirtualenv -p python3 venv\nsource venv/bin/activate\nInstall the TensorFlow.js converter:\npip install tensorflowjs[wizard]\nStart the conversion wizard:\ntensorflowjs_wizard\nNow, the tool will guide you through the conversion, providing explanations for each choice you need to make. The image below shows all the choices that were made to convert the model. Most of them are the standard ones, but options like the shard sizes and compression can be changed according to your needs.\nTo enable the browser to cache the weights automatically, it\u2019s recommended to split them into shard files of around 4MB. To guarantee that the conversion is going to work, don\u2019t skip the op validation as well, not all TensorFlow operations are supported so some models can be incompatible with TensorFlow.js\u200a\u2014\u200aSee this list for which ops are currently supported on the various backends that TensorFlow.js executes on such as WebGL, WebAssembly, or plain JavaScript.\nModel conversion using TensorFlow.js Converter (Full resolution image here)\nIf everything works well, you\u2019re going to have the model converted to the TensorFlow.js layers format in the web_model directory. The folder contains a model.json file and a set of sharded weights files in a binary format. The model.json has both the model topology (aka \u201carchitecture\u201d or \u201cgraph\u201d: a description of the layers and how they are connected) and a manifest of the weight files (Lin, Tsung-Yi, et al). The contents of the web_model folder currently contains the files shown below:\n\u2514 web_model\n  \u251c\u2500\u2500 group1-shard1of5.bin\n  \u251c\u2500\u2500 group1-shard2of5.bin\n  \u251c\u2500\u2500 group1-shard3of5.bin\n  \u251c\u2500\u2500 group1-shard4of5.bin\n  \u251c\u2500\u2500 group1-shard5of5.bin\n  \u2514\u2500\u2500 model.json\nConfiguring the application\nThe model is ready to be loaded in JavaScript. I\u2019ve created an application to perform inference directly from the browser. Let\u2019s clone the repository to figure out how to use the converted model in real-time. This is the project structure:\n\u251c\u2500\u2500 models\n\u2502 \u251c\u2500\u2500 group1-shard1of5.bin\n\u2502 \u251c\u2500\u2500 group1-shard2of5.bin\n\u2502 \u251c\u2500\u2500 group1-shard3of5.bin\n\u2502 \u251c\u2500\u2500 group1-shard4of5.bin\n\u2502 \u251c\u2500\u2500 group1-shard5of5.bin\n\u2502 \u2514\u2500\u2500 model.json\n\u251c\u2500\u2500 package.json\n\u251c\u2500\u2500 package-lock.json\n\u251c\u2500\u2500 public\n\u2502   \u2514\u2500\u2500 index.html\n\u251c\u2500\u2500 README.MD\n\u2514\u2500\u2500 src\n \u251c\u2500\u2500 index.js\n \u2514\u2500\u2500 styles.css\nFor the sake of simplicity, I have already provided a converted SKU-detector model in the model's folder. However, let\u2019s put the web_model generated in the previous section in the models folder and test it.\nNext, install the http-server:\nnpm install http-server -g\nGo to the models folder and run the command below to make the model available at http://127.0.0.1:8080 . This is a good choice when you want to keep the model weights in a safe place and control who can request inferences to it. The -c1 parameter is added to disable caching, and the --cors flag enables cross-origin resource sharing allowing the hosted files to be used by the client-side JavaScript for a given domain.\nhttp-server -c1 --cors .\nAlternatively, you can upload the model files somewhere else - even on a different domain if needed. In my case, I chose my own Github repo and referenced the model.json folder URL in the load_model function as shown below:\nasync function load_model() {\n // It's possible to load the model locally or from a repo.\n // Load from localhost locally:\n      const model = await loadGraphModel(\"http://127.0.0.1:8080/model.json\");\n // Or Load from another domain using a folder that contains model.json.\n      // const model = await loadGraphModel(\"https://github.com/hugozanini/realtime-sku-detection/tree/web\");\n return model;\n}\nThis is a good option because it gives more flexibility to the application and makes it easier to run on public web servers.\nPick one of the methods to load the model files in the function load_model (lines 10\u201315 in the file src>index.js).\nWhen loading the model, TensorFlow.js will perform the following requests:\nGET /model.json\nGET /group1-shard1of5.bin\nGET /group1-shard2of5.bin\nGET /group1-shard3of5.bin\nGET /group1-shardo4f5.bin\nGET /group1-shardo5f5.bin\nPublishing in CodeSandbox\nCodeSandbox is a simple tool for creating web apps where we can upload the code and make the application available for everyone on the web. By uploading the model files in a GitHub repo and referencing them in the load_model function, we can simply log into CodeSandbox, click on New project > Import from Github, and select the app repository.\nWait some minutes to install the packages and your app will be available at a public URL that you can share with others. Click on Show > In a new window and a tab will open with a live preview. Copy this URL and paste it in any web browser (PC or Mobile) and your object detection will be ready to run. A ready to use project can be found here as well if you prefer.\nConclusion\nBesides the precision, an interesting part of these experiments is the inference time\u200a\u2014\u200aeverything runs in real-time in the browser via JavaScript. SKU detection models running in the browser, even offline, and using few computational resources is a must in many consumer packaged goods company applications, along with other industries too.\nEnabling a Machine Learning solution to run on the client-side is a key step to guarantee that the models are being used effectively at the point of interaction with minimal latency and solve the problems when they happen: right in the user's hand.\nDeep learning should not be costly and should be used beyond just research, for real world use cases, which JavaScript is great for production deployments. I hope this article will serve as a basis for new projects involving Computer Vision, TensorFlow, and create an easier flow between Python and Javascript.\nIf you have any questions or suggestions you can reach me on Twitter.\nThanks for reading!\nAcknowledgments\nI\u2019d like to thank the Google Developers Group, for providing all the computational resources for training the models, and the authors of the SKU 110K Dataset, for creating and open-sourcing the dataset used in this project.",
    "link": "https://blog.tensorflow.org/2022/05/real-time-sku-detection-in-browser.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiXJKi_ujdUUveIPpH2BeriYxl3K7TzTUfsocSRD1xKulteBDQZkZYmQBB1z_yW4C6orHg825mRAhFCJTEuOrtHjpMFpVUVcBUCQ33OYwkmPRDKdTs9oH4RBHrb2Ay9p_j3bDhsIfjwzukA1Sr-uT0uSXA_kpXnJSi62XSi0bHPDsaPMlQZZyGReTiB/s1600/image7.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-RgZkEm4Rb9QERMU7H4wVyk5Z6KdeiCKoyGOOlOnyEiefkPXnDbFlVAW-ORzatYYq2bgkEc9XXXux_WzoPzlT3u-13WyNL8p8-dNo73C1k-ywKzuAAe207cSvaMx84_MrcqrT-qaVE-yL39_BbFX6ba3G9PepBjusD3M_wS0gsSVcIBALq6XjHpiy/s1600/ezgif.com-gif-maker.gif",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiXJKi_ujdUUveIPpH2BeriYxl3K7TzTUfsocSRD1xKulteBDQZkZYmQBB1z_yW4C6orHg825mRAhFCJTEuOrtHjpMFpVUVcBUCQ33OYwkmPRDKdTs9oH4RBHrb2Ay9p_j3bDhsIfjwzukA1Sr-uT0uSXA_kpXnJSi62XSi0bHPDsaPMlQZZyGReTiB/s1600/image7.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhsTYleef2QUeRdcScNczV1XV50dtvAvS3M9jh47bAwfMEG0zJIOt0VJvlteFAaoPvA7D2U21L1zxmo_41WG6QVf46l4hnIsbCPn63EWCw4Dp_qSTEL2O9FIUOzPMElANPmlkMFd64ZFzD7eOrtkDqpu8_-CM4R5OM-v8VPtd9606_i1dzTjjD8NvSC/s1600/image3.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4k_zpucjpl8y6HA_I4lB-wH41VY_Eg9awRHYD9kZCE7sPDJpAareh0sOhzi0Mbaeae1cxSK2vuR_zn3ZAHrat0_r5Dk3ttFUXIPoBopVPJCefHsj_3DyhVqsBM4F9aCvAauEjwfQ9s_aG0asu8k9JGCZf4D6W6DlgKAIbLAjCgZgVLnLRZutj8etf/s1600/image9.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhIuzWSK8B52q9FIJJ0ARUu4Y--esT101VSCIkUEg9JMTm5H0zy1r9PmkOkLTHl9ve-3DWWKa09876MB0TGYkzJXgRGMpSgriZZmOivA7fK4FJ7z_hXkEsiMkiGJBSgpfrD-MYBUlowijEKqECWHUEKxGxon9JDwqIllWlFuOD0qbEwBrblrnWDuToP/s1600/image8.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMqCki32tUqEILujT0zrG-28wt9CxxQP1IQBwpWYi0hAQfix9TLFHcz9otuK_-kFn1KZbx068Z44cNtzfZ0zr-N6gev0SHhpQbDLt-PE5LeIInTphf9KN1FPPMUaTa14uRREpwhFJQuwy6mZd9oS7ZC5YT2yjrHGTTotKVt5vx6vJ0uGuFQtM9zdt9/s1600/image10.png"
    ],
    "time": "2023/12/08 00:58:43"
  },
  {
    "title": "Portrait Depth API: Turning a Single Image into a 3D Photo with TensorFlow.js",
    "content": "Posted by Ruofei Du, Yinda Zhang, Ahmed Sabie, Jason Mayes, Google.\nA depth map is essentially an image (or image channel) that contains information relating to the distance of the surfaces of objects in the scene from a given viewpoint (in this case, the camera itself) for every pixel in that image. Depth maps are a fundamental building block for a variety of computer graphics and computer vision applications, such as augmented reality, portrait mode, and 3D reconstruction. Despite the recent advances in depth sensing capabilities with ARCore Depth API, the majority of photographs on the web are still missing associated depth maps. This, combined with users from the web community expressing a growing interest in having depth capabilities within JavaScript to enhance existing web apps such as to bring images to live, apply real time AR effects to a human face and body, or even reconstruct items for use in VR environments, helped shape the path for what you see today.\nToday we are introducing the Depth API, the first depth estimation API from TensorFlow.js. With this new API, we are also introducing the first depth model for portrait, ARPortraitDepth, which estimates a depth map for a single portrait image. To demonstrate one of many possible usages of depth information, we also present a computational photography application, 3D photo, which utilizes the predicted depth and enables a 3D parallax effect on the given portrait image. Try the live demo below, everyone can easily make their social media profile photo 3D as shown below.\nTry out the 3D portrait demo for yourself!\nARPortraitDepth: Single Image Depth Estimation\nAt the core of the Portrait Depth API is a deep learning model, named ARPortraitDepth, that takes a single color portrait image as the input and produces a depth map. For the sake of computational efficiency, we adopt a light-weight U-Net architecture. As shown below, the encoder gradually downscales the image or feature map resolution by half, and the decoder increases the feature resolution to the same as the input. Deep learning features from the encoder are concatenated to the corresponding layers with the same spatial resolution in the decoders to bring high resolution signals for depth estimation. During training, we force the decoder to produce depth predictions with increasing resolutions at each layer, and add a loss for each of them with the ground truth. This empirically helps the decoder to predict accurate depth by gradually adding details.\nAbundant and diverse training data is critical for the machine learning model to achieve overall decent performance, e.g. accuracy and robustness. We synthetically render pairs of color and depth images with various camera configurations, e.g. focal length, camera pose, from 3D digital humans captured by a high quality performance capture system, and run relighting augmentation with High Dynamic Range environment illumination maps to increase the realism and diversity of the color images, e.g. shadows on the face. We also collect real data using mobile phones equipped with a front facing depth sensor, e.g. Google Pixel 4, where the depth quality, as the training ground truth, is not as accurate and complete as that in our synthetic data, but the color images are effective in improving the performance of our model when running on images in the wild.\nSingle image depth estimation pipeline.\nTo enhance the robustness against background variation, in practice, we run an off-the-shelf body segmentation model with MediaPipe and TensorFlow.js before sending the image into the neural network of depth estimation.\nThe portrait depth model could enable a whole host of creative applications orientated around the human body that could drive next generation web apps. We refer readers to ARCore Depth Lab for more inspirations.\nFor the 3D photo application, we created a high-performance rendering pipeline. It first generates a segmented mask using the TensorFlow.js existing body segmentation API. Next, we pass the masked portrait into the Portrait Depth API and obtain a depth map on the GPU. Eventually, we generate a depth mesh in three.js, with vertices arranged in a regular grid and displaced by re-projecting corresponding depth values (see the figure below for generating the depth mesh). Finally, we apply texture projection to the depth mesh and rotate the camera around the z axis in a circle. Users can download the animations in GIF or WebM format.\nGenerating the depth mesh from the depth map for the 3D photo application.\nPortrait Depth API Installation\nThe portrait depth API is currently offered as one variant of the new depth API.\nTo install the API and runtime library, you can either use the <script> tag in your html file or use NPM.\nThrough script tag:\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow-models/body-segmentation\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow-models/depth-estimation\"></script>\nThrough NPM:\nyarn add @tensorflow/tfjs-core @tensorflow/tfjs-backend-webgl\nyarn add @tensorflow/tfjs-converter\nyarn add @tensorflow-models/body-segmentation\nyarn add @tensorflow-models/depth-estimation\nTo reference the API in your JS code, it depends on how you installed the library.\nIf installed through script tag, you can reference the library through the global namespace depthEstimation.\nIf installed through NPM, you need to import the libraries first:\nimport '@tensorflow/tfjs-backend-core';\nimport '@tensorflow/tfjs-backend-webgl';\nimport '@tensorflow/tfjs-converter';\nimport '@tensorflow-models/body-segmentation;\nimport * as depthEstimation from '@tensorflow-models/depth-estimation;\nTry it yourself!\nFirst, you need to create an estimator:\nconst model = depthEstimation.SupportedModels.ARPortraitDepth;\n    estimator = await depthEstimation.createEstimator(model);\n\n\n    const video = document.getElementById('video');\n    const depthMap = await estimator.estimateDepth(video);\nOnce you have an estimator, you can pass in a video stream, static image, or TensorFlow.js tensors to estimate depth:\nconst video = document.getElementById('video');\n\n    const estimationConfig = {\n      minDepth: 0, // The minimum depth value outputted by the estimator.\n      maxDepth: 1, // The maximum depth value outputted by the estimator.\n    };\n\n   const depthMap = await estimator.estimateDepth(video, estimationConfig);\nHow to use the output?\nThe depthMap result above contains depth values for each pixel in the image.\nThe depthMap is an object which stores the underlying depth values. You can then utilize the provided asynchronous conversion functions such as toCanvasImageSource, toArray, and toTensor depending on the desired output type that you want for efficiency.\nIt should be noted that different models have different internal representations of data. Therefore converting from one form to another may be expensive. In the name of efficiency, you can call getUnderlyingType to determine what form the depth map is in already so you may choose to keep it in the same form for faster results.\nThe semantics of the depthMap are as follows: the depth map is the same size as the input image. For array and tensor representations, there is one depth value per pixel. For CanvasImageSource, the green and blue channels are always set to 0, whereas the red channel stores the depth value.\n\nSee below output snippet for example:\n  {\n    toCanvasImageSource(): ...\n    toArray(): ...\n    toTensor(): ...\n    getUnderlyingType(): ...\n  }\nBrowser Performance\nPortrait Depth model\n\nMacBook M1 Pro 2021. \n(FPS)\niPhone 13 Pro\n(FPS)\nDesktop PC \nIntel i9-10900K. Nvidia GTX 1070 GPU.\n(FPS)\nTFJS Runtime\nWith WebGL backend.\n51\n22\n47\n\nAcknowledgements\nWe would like to acknowledge our colleagues who participated in or sponsored creating Portrait Depth API in TensorFlow.js: Na Li, Xiuxiu Yuan, Rohit Pandey, Abhishek Kar, Sergio Orts Escolano, Christoph Rhemann, Idris Aleem, Sean Fanello, Adarsh Kowdle, Ping Yu, Alex Olwal, Sarah Heimlich, Cecilia Abadie. We would also like to acknowledge the body segmentation model provided by MediaPipe, and The Relightables for high quality synthetic data.",
    "link": "https://blog.tensorflow.org/2022/05/portrait-depth-api-turning-single-image.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEie8HBDNHcne_-2GHUqKFG0g2XmniU0vGySZRc3NPDEoqPqvK36cP-4vgvIawOJWNQzrLo90Xx7WAAOvY3V4HjFpXMGMcyWRvuI8e3JCzBEoyZv2P--8ldmF3_q3o2YgfjlCMZXOL_0YhWIOAKPCUTn4X3a5zkL9PMgctqPvGqJUohOFkjkmBMRbkiG/s1600/ezgif.com-gif-maker%20%2844%29.gif",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEie8HBDNHcne_-2GHUqKFG0g2XmniU0vGySZRc3NPDEoqPqvK36cP-4vgvIawOJWNQzrLo90Xx7WAAOvY3V4HjFpXMGMcyWRvuI8e3JCzBEoyZv2P--8ldmF3_q3o2YgfjlCMZXOL_0YhWIOAKPCUTn4X3a5zkL9PMgctqPvGqJUohOFkjkmBMRbkiG/s1600/ezgif.com-gif-maker%20%2844%29.gif",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglwEQ44QOMuts9Z8dfnSFs6Ebs0VcDNrG4FOQq7mg5-jRsVrlhvNgVd624Uv6g1GlCaBfyxPhSm9ej7bubbooVvBw_WojUi2FlqAaUFrwTAscojrNXSBH__TZ5kAFEu3fcDMbvYOcIzGhm83mLPdWMVr1Tl9pFqPxsKu9ZiYpnUhs9tRYjSKmxg1vj/s1600/image5.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEju3TSCS6xv5jRleKw6kGGwKZ9jgInpgr2T-M0CcvDor1x0JKuozsqD9mDew-dPADY17US8lzLXx0FEwdOTvkO6ADJM4va33KBZa5QGeTZPQ_nXPy3EEGn5xTdfwtBeX8O9sepuWuJ5f9iNAtI-crx3l7cmOV2zrRos-K28X-qbHOdNg69cdTQ2tM3L/s1600/image11.png"
    ],
    "time": "2023/12/08 00:58:43"
  },
  {
    "title": "How LinkedIn Personalized Performance for Millions of Members using TensorFlow.js",
    "content": "A guest post by LinkedIn\nMark Pascual, Sr. Staff Engineer\nNitin Pasumarthy, Staff Engineer\nIntroduction\nThe Performance team at LinkedIn optimizes latency to load web and mobile pages. Faster sites improve customer engagement and eventually revenue to LinkedIn. This concept is well documented by many other companies too who have had similar experiences but how do you define the optimal trade off between page load times and engagement?\nThe relationship between speed and engagement is non-linear. Fast loading sites, after a point, may not increase engagement by further reducing their load times. At LinkedIn we have used this relationship between engagement and speed to selectively customize the features on LinkedIn Lite - a lighter, faster version of LinkedIn, specifically built for mobile web browsers.\nTo do this, we trained a deep neural network to identify if a request to LinkedIn would result in a fast page load in real time. Based on the performance quality result predicted by this model we change the resolution of all images on a given user\u2019s news feed before the resulting webpage was sent to the client. This led to an increase in the magnitude of billions for extra Feed Viral Actions (+0.23%) taken, millions more Engaged Feed Users (+0.16%) and Sponsored Revenue increased significantly for us too (+0.76%).\nImage Quality Comparison: Image on the left uses 4x more memory than the one on the right which is less than ideal to send to users on slow network connections or when the device may be low on resources. Prior to using an ML model, we only showed the low resolution image which was not great for users that had capacity for higher quality images on newer devices.\nWe described in great detail why many of our performance optimization experiments failed back in 2017 and how we used those learnings to build a Performance Quality Model (PQM) in our Personalizing Performance: Adapting Application in real time to member environments blog.\n\nPQM\u2019s bold goal is to predict various performance metrics (e.g. page load time) of any web / mobile page using both device and network characteristics of end users to empower (web) developers to build impactful application features that are otherwise tricky to implement (like the one we described above).\nWe are happy to announce that we are open sourcing our first performance quality model that is trained on millions of RUM data samples from around the world free to use for your own website performance optimizations! Learn more and get started here.\nIn the rest of this blog, we will go over how our team of full stack developers deployed this PQM in production that works at Linkedin scale! We wish to prove that deploying TensorFlow.js ML models today is both easy and beneficial for those working on the Node.js stack.\nTensorFlow.js: Model Deployment in Node.js\nAt the time of our production deployment, LinkedIn\u2019s TensorFlow model deployment machinery was still being developed. Furthermore, using TensorFlow Serving was not yet a feasible option for us. So even though we had a model ready for use, we needed to figure out a way to deploy it.\nAs LinkedIn is primarily a Java/JVM stack for serving external traffic, it might seem like TensorFlow Java would be ideal, but it was still experimental and didn\u2019t have the API stability guarantees that we require.\nWe looked at our options and realized that we already use Node.js (behind the JVM) as part of our frontend web serving stack in order to perform server side optimizations when serving HTML pages. The architecture for this is unique in that we use the JVM to manage an external pool of Node.js processes to perform \u201cwork,\u201d e.g., the previously mentioned server side optimizations. The \u201cwork\u201d can really be anything that Node.js can perform. In our use case, this enables us to use TensorFlow.js in an architecture that was already proven.\nWe repurposed our frontend stack to use Node.js to deploy our custom model and ended up with great results. In terms of performance, our mixed stack of Java and Node.js easily met our SLAs. The 50th and 90th percentile production latencies as measured (a) from a client (within the datacenter), (b) from on host instrumentation, and (c) in terms of only Node.js performing inference using TensorFlow.js are shown in the table below.\n\n50th Percentile\n90th Percentile\nFrom client (within datacenter)\n10 ms\n12 ms\nOn host\n8 ms\n10 ms\nInference in Node.js\n2 ms\n3 ms\nThe resulting architecture is shown above in Figure 1 below.\nThe API request that requires a prediction is received by the JVM server and is routed to our Rest.li infrastructure which in turn routes the request to our performance prediction resource. To handle the request, the PaaS resource performs some feature generation based on the inputs and then makes an RPC call out to the Node.js process for the prediction.\nThe N Node.js processes are long-lived. They are started upon JVM startup and have already loaded the desired model using tf.node.loadSavedModel(). When a process receives a request for a prediction, it simply takes the input features, calls tf_model.predict(), and returns the result. Here is a simplified version of the Node.js code:\nconst tf = require(\u2018@tensorflow/tfjs-node\u2019);\n\nasync function main() {\n  // load the model when the process starts so it\u2019s always ready\n  const model = await tf.node.loadSavedModel(\u2018model_dir\u2019);\n\n  function predict(rawInput) {\n    return tf.tidy(() => {\n      // prepare the inputs as tensor arrays\n      const x = {}\n      for (const feature of Object.keys(predictionInput)) {\n        x[feature] = tf.tensor([input[feature]], [1, 1]);\n      }\n\n      const output = model.predict(x, {});\n      const probs = Array.from(output.probabilities.dataSync());\n      const classes = Array.from(output.all_class_ids.dataSync());\n      const result = Object.fromEntries(classes.map((classId, i) => [classId, probs[i]]));\n      return result; // {0: 0.8, 1: 0.15, 2: 0.05} probability of each performance quality\n    });\n  }\n\n  // Register our \u2018predict\u2019 RPC handler (pseudo-code)\n  // process is an abstraction of the Node.js side of the communication channel\n  // with the JVM\n  process.registerHandler(\u2018predict\u2019, input => {    \n    const result = predict(input);\n    return Promise.resolve(result);\n  });\n}\n\nmain();\nExpress could replace Rest.li\u2019s role and the feature generation pieces would need to be ported to Node.js, but everything else remains the same. As you can see, the architecture is cleaner and requires less mental hoops to manage both Java and Node.js in the same stack.\nAn Unexpected Win\nIn the architecture we described above, the external processes do not have to be Node.js. The library that we use to manage the external process is pretty straightforward to implement in most technologies. In fact, we could have chosen Python for the external processes as it\u2019s popular for this ML use case. So what are the reasons we stuck with Node.js? Well, there\u2019s two: (1) we already had a Node.js implementation for the external process infrastructure and would have had to develop a new one for Python, and (2) it turns out that Node.js is also slightly faster at making the predictions due to the pre/post processing benefitting from the JIT compiler of JavaScript.\nIn order to prove this to ourselves, we took samples (~100k) from our real world prediction inputs and ran them against both Node.js and Python. The test bed was not exactly our production stack (we didn\u2019t include the JVM side), but it was close enough for our purposes. The results are shown below:\nStack\n50th percentile\nDelta (from Python)\nPython\n1.872 ms\n0%\nNode.js\n1.713 ms\n-8.47%\n\nThe results show that Node.js is almost 10% faster at performing inference for our specific model. Of course, performance may vary based on model architectures and the amount of pre and post processing being performed in Node. These results were from our model running on a typical production machine. Results may also vary due to model complexity, machine differences, and so on.\nCheckout our README in the open source repo to find out how we tested the model in Python and Node.js.\nLooking to the future\nOur current unique architecture does have some areas for improvement. Probably the biggest opportunity is to address the uniqueness of this multi stack architecture itself. The mix of both Java and Node.js technologies adds additional cognitive overhead and complexity during design, development, debugging, operations, maintenance - however as previously stated you could move the whole stack to Node to simplify matters, so this is a solvable problem.\nAnother potential area for improvement comes from currently using a single threaded architecture on the Node.js side. Because of this, only a single prediction currently occurs at a time so latency sometimes includes some amount of queueing time. This can potentially be worked around by using Node worker threads for parallel execution that may be considered in future versions of this implementation. In our particular case, however, prediction is very fast even as it stands, so we do not feel the need to explore this right now.\nSummary\nThe availability of TensorFlow.js gave us an easy option to deploy our model to serve production use cases when other options were not quite suitable or available to us. While our unique requirements resulted in using non-standard architecture (the mixture of the JVM and Node.js), TensorFlow.js can be used to an even greater effect in a homogeneous Node.js serving stack resulting in a very clean and performant architecture. With our open source performance quality model, a full stack JS engineer can personalize performance and improve their user engagement and we look forward to seeing how others use our open sourced model to do just that on their own websites.\nAcknowledgements\nThis success would not be possible without the tremendous work by Prasanna Vijayanathan and Niranjan Sharma. A huge thank you to Ritesh Maheshwari and Rahul Kumar for their support and encouragement. Special thanks to Ping Yu (Google) and Jason Mayes (Google) for their continued support and feedback.",
    "link": "https://blog.tensorflow.org/2022/03/how-linkedin-personalized-performance.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMAEQ2KGrdiRdXqw46tw3E2as7NaxLPfJK_mKjqMOwpn47Hz0hjvMUefZXvzV4KdtJ-ndPH_wDw8Y0v4gQ0Cgu3gnPo8MZL7MbK9-4p2G6ore_NT9U3e3dhsmisHWBdBFoSqrAK1PcpExiSFfy10jeeA7emPKBsmoIezLMNBIDyWFC2On5JuAtSaEw/s1600/image1.gif",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg6ZVoFb8j7Iwy5AP_g_06WzvABGAxkh9pGj-KMlQdYljjwxKNnJVZsGsgCosn-kuAD2AcJgHdPrMWGQUdnvZ9zb6vjnTXtajv-3V5Ah5XD4bKVkXW5kK7-D1SOeb72gQkIHJEbUv5SHyolNGImBJRpGzLnCVWSguFO54VaM0cRtmmYDpM7FCvA9-bu/s1600/image2.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg3ARPZvycGXHKTn7Cz3RjtGR0cOE1inKHoe-91xNOq8eONMkoi8gLg2yJkVgV7h-eIUtRBnp4ohPlvlvo5td3uElKQHTAgTUcFIgLyFM95VmQhYhrd5poAQCAEkR1Uuoq2mizh5li_qJkZGHS91B4VSYSNTqelazUsWmS_I6SWC6Wq-bv5Se9G7yyg/s1600/image4.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh_LSM44GUKmnol9a7PawWI7jtkqcrll8eeU35Ms0_wU0FZ913ecCHkyfBI1AHY9ZIqGoxIkIzp6NwumZSsKBmW3hlUPPbfy1cxFVZDCLhxavQ4FBMX970sLpzO5qmPZ7zvcHDiDAIM3GmPUT3ojcZE1HzU7g5BAkBEmJ8Idw6nga8bb-9r7LxztMiK/s1600/image3.png"
    ],
    "time": "2023/12/08 00:58:43"
  },
  {
    "title": "Body Segmentation with MediaPipe and TensorFlow.js",
    "content": "Posted by Ivan Grishchenko, Valentin Bazarevsky, Ahmed Sabie, Jason Mayes, Google\nWith the rise in interest around health and fitness, we have seen a growing number of TensorFlow.js users take their first steps in 2021 with our existing body related ML models, such as face mesh, body pose, and hand pose estimation.\nToday we are launching two new highly optimized body segmentation models that are both accurate and fast as part of our updated body-segmentation and pose APIs in TensorFlow.js.\nFirst is the BlazePose GHUM pose estimation model that now has additional support for segmentation. This model is part of our unified pose-detection API offering that can perform full body segmentation and 3D pose estimation simultaneously as shown in the animation below. It\u2019s well suited for bodies in full view further away from the camera accurately capturing the feet and legs regions for example.\nTry out the live demo!\nThe second model we are releasing is Selfie Segmentation that is well suited for cases where someone is directly in front of a webcam on a video call (<2 meters). This model that is part of our unified body-segmentation API can have higher accuracy across the upper body as shown in the animation below, but may be less accurate for the lower body in some situations.\nTry out the live demo!\nBoth of these new models could enable a whole host of creative applications orientated around the human body that could drive next generation web apps. For example, the BlazePose GHUM Pose model may power services like digitally teleporting your presence anywhere in the world, estimating body measurements for a virtual tailor, or creating special effects for music videos and more, the possibilities are endless. In contrast the Selfie Segmentation model could enable user friendly features on web based video calls like the demo above where you can change or blur the background accurately.\nPrior to this launch, many of our users may have tried our BodyPix model, which was state of the art when it launched. With today\u2019s release, our two new models offer a much higher FPS and fidelity across devices for a variety of use cases.\nBody Segmentation API Installation\nThe body-segmentation API provides two runtimes for the Selfie Segmentation model, namely the MediaPipe runtime and TensorFlow.js runtime.\nTo install the API and runtime library, you can either use the <script> tag in your html file or use NPM.\nThrough script tag:\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl\">\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow-models/body-segmentation\">\n\n<!-- Optional: Include below scripts if you want to use TensorFlow.js runtime. -->\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter\">\n\n<!-- Optional: Include below scripts if you want to use MediaPipe runtime. -->\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/selfie_segmentation\">\nThrough NPM:\nyarn add @tensorflow/tfjs-core @tensorflow/tfjs-backend-webgl\nyarn add @tensorflow-models/body-segmentation\n\n# Run below commands if you want to use TensorFlow.js runtime.\nyarn add @tensorflow/tfjs-converter\n\n# Run below commands if you want to use MediaPipe runtime.\nyarn add @mediapipe/selfie_segmentation\nTo reference the API in your JS code, it depends on how you installed the library.\nIf installed through script tag, you can reference the library through the global namespace bodySegmentation.\nIf installed through NPM, you need to import the libraries first:\nimport '@tensorflow/tfjs-backend-core';\nimport '@tensorflow/tfjs-backend-webgl';\nimport * as bodySegmentation from '@tensorflow-models/body-segmentation';\n\n// Uncomment the line below if you want to use TensorFlow.js runtime.\n// import '@tensorflow/tfjs-converter';\n\n// Uncomment the line below if you want to use MediaPipe runtime.\n// import '@mediapipe/selfie_segmentation';\nTry it yourself!\nFirst, you need to create a segmenter:\nconst model = bodySegmentation.SupportedModels.MediaPipeSelfieSegmentation; // or 'BodyPix'\n\nconst segmenterConfig = {\n  runtime: 'mediapipe', // or 'tfjs'\n  modelType: 'general' // or 'landscape'\n};\n\nsegmenter = await bodySegmentation.createSegmenter(model, segmenterConfig);\nChoose a modelType that fits your application needs, there are two options for you to choose from: general, and landscape. From landscape to general, the accuracy increases while the inference speed decreases. Please try our live demo to compare different configurations.\nOnce you have a segmenter, you can pass in a video stream, static image, or TensorFlow.js tensors to segment people:\nconst video = document.getElementById('video');\nconst people = await segmenter.segmentPeople(video);\nHow to use the output?\nThe people result above represents an array of the found segmented people in the image frame. However, each model has its own semantics for a given segmentation.\nFor Selfie Segmentation, the array will be exactly of length 1, where the single segmentation corresponds to all people in the image frame. For each segmentation, it contains maskValueToLabel and mask properties detailed below.\nThe mask field stores an object which provides access to the underlying results of the segmentation. You can then utilize the provided asynchronous conversion functions such as toCanvasImageSource, toImageData, and toTensor depending on the desired output type that you want for efficiency.\nIt should be noted that different models have different internal representations of data. Therefore converting from one form to another may be expensive. In the name of efficiency, you can call getUnderlyingType to determine what form the segmentation is in already so you may choose to keep it in the same form for faster results.\nThe semantics of the RGBA values of the mask are as follows: the image mask is the same size as the input image, where green and blue channels are always set to 0. Different red values denote different body parts (see maskValueToLabel key below). Different alpha values denote the probability of a pixel being a body part pixel (0 being lowest probability and 255 being highest).\n\nmaskValueToLabel maps pixel\u2019s red channel value to the segmented part name for that pixel. This is not necessarily the same across different models (for example SelfieSegmentation will always return 'person' since it does not distinguish individual body parts, whereas a model like BodyPix would return the name of individual body parts that it can distinguish for each segmented pixel). See below output snippet for example:\n[\n  {\n    maskValueToLabel: (maskValue: number) => { return 'person' },\n    mask: {\n      toCanvasImageSource(): ...\n      toImageData(): ...\n      toTensor(): ...\n      getUnderlyingType(): ...\n    }\n  }\n]\nWe also provide an optional utility function that you can use to render the result of the segmentation. Use the toBinaryMask function to convert the segmentation to an ImageData object.\nThis function takes 5 parameters, the last 4 being optional:\nSegmentation results from segmentPeople call above.\nForeground color - an object representing the RGBA values to use for rendering foreground pixels.\nBackground color - object with RGBA values for background pixels\nDraw Contour - boolean value if to draw a contour line around the body of the found person.\nForeground threshold - at what point a pixel should be considered a foreground pixel vs background pixel. This is a floating point value from 0 to 1.\nOnce you have the imageData object from toBinaryMask you can use the drawMask function to render it to a canvas of your choice.\nExample code for using these two functions is shown below:\nconst foregroundColor = {r: 0, g: 0, b: 0, a: 0};\nconst backgroundColor = {r: 0, g: 0, b: 0, a: 255};\nconst drawContour = true;\nconst foregroundThreshold = 0.6;\n\nconst backgroundDarkeningMask = await bodySegmentation.toBinaryMask(people, foregroundColor, backgroundColor, drawContour, foregroundThreshold);\n\nconst opacity = 0.7;\nconst maskBlurAmount = 3; // Number of pixels to blur by.\nconst canvas = document.getElementById('canvas');\n\nconst people = await bodySegmentation.drawMask(canvas, video, backgroundDarkeningMask, opacity, maskBlurAmount);\nPose Detection API Usage\nTo load and use the BlazePose GHUM model please reference the unified Pose API documentation. This model has three outputs:\n2D keypoints\n3D keypoints\nSegmentation for each found pose.\nIf you need to grab the segmentation from the pose results, you can simply grab a reference to that pose\u2019s segmentation property a shown:\nconst poses = await detector.estimatePoses(video);\nconst firstSegmentation = poses.length > 0 ? poses[0].segmentation : null;\n\nModels deep dive\nBlazePose GHUM and MediaPipe Selfie Segmentation models segment the prominent humans in the frame. Both run in real-time across laptops and smartphones but vary in intended applications as discussed at the start of this blog. Selfie Segmentation focuses on selfie effects and conferencing for closeup cases (< 2m) where as BlazePose GHUM specializes in full-body cases like yoga, fitness, dance and works up to 4 meters from the camera.\nSelfie Segmentation\nSelfie Segmentation model predicts binary segmentation mask of foreground with humans. The pipeline is structured to run entirely on GPU, from image acquisition over neural network inference to rendering the segmented result on the screen. It avoids slow CPU-GPU syncs and achieves the maximum performance. Variations of the model are powering background replacement in Google Meet and a more general model is now available in TensorFlow.js and MediaPipe.\nBlazePose GHUM 2D landmarks and body segmentation\nBlazePose GHUM model now provides a body segmentation mask in addition to 2D and 3D landmarks introduced earlier. Having a single model that predicts both outputs gives us two gains. First, it allows outputs to supervise and improve each other as landmarks give semantic structure while segmentation focuses on edges. Second, it guarantees that predicted mask and points belong to the same person, which is hard to achieve with separate models. As BlazePose GHUM model runs only on the ROI crop of a person (vs. full image), segmentation mask quality depends only on the effective resolution within the ROI and doesn't change a lot when moving closer or further from the camera.\n\nConference\nASL\nYoga\nDance\nHIIT\nBlazePose GHUM (full)\n95.50%\n96.52%\n94.73%\n94.55%\n95.16%\nSelfie Segmentation (256x256)\n97.60%\n97.88%\n80.66%\n86.33%\n85.53%\nBlazePose GHUM and Selfie Segmentation IOUs across different domains\nMediaPipe and TensorFlow.js runtime\nThere are some pros and cons of using each runtime. As shown in the performance tables below, the MediaPipe runtime provides faster inference speed on desktop, laptop and android phones. The TensorFlow.js runtime provides faster inference speed on iPhones and iPads.\nFPS numbers here are the time taken to perform the inference through the model and wait for the GPU and CPU to sync. This is done to ensure the GPU has fully finished for benchmarking purposes, but for pure-GPU production pipelines no waiting is needed, so your numbers may be higher still. For pure GPU pipeline, if you are using the MediaPipe runtime, just use await mask.toCanvasImageSource(), and if you are using the TF.js runtime, reference this example on how to use texture directly to stay on GPU for rendering effects.\nBenchmarks\nSelfie segmentation model\n\nMacBook Pro 15\u201d 2019. \nIntel core i9. \nAMD Radeon Pro Vega 20 Graphics.\n(FPS)\niPhone 11\n(FPS - CPU Only for MediaPipe)\nPixel 6 Pro\n(FPS)\nDesktop PC \nIntel i9-10900K. Nvidia GTX 1070 GPU.\n(FPS)\nMediaPipe Runtime\nWith WASM & GPU Accel.\n125 | 130\n31 |  21\n35 | 33\n185 | 225\nTFJS Runtime\nWith WebGL backend.\n74 | 45\n42 | 30\n25 | 23\n80 | 62\nInference speed of Selfie Segmentation across different devices and runtimes. The first number in each cell is for the landscape model, and the second number is for the general model.\nBlazePose GHUM model\n\nMacBook Pro 15\u201d 2019. \nIntel core i9. \nAMD Radeon Pro Vega 20 Graphics.\n(FPS)\niPhone 11\n(FPS - CPU Only for MediaPipe)\nPixel 6 Pro\n(FPS)\nDesktop PC \nIntel i9-10900K. Nvidia GTX 1070 GPU.\n(FPS)\nMediaPipe Runtime\nWith WASM & GPU Accel\n70 | 59 | 31\n8 | 5 | 1\n22 | 19 | 10\n123 | 112 |  70\nTFJS Runtime\nWith WebGL backend.\n42 | 36 | 22\n14 | 12 | 8\n12 | 10 | 6\n35  | 33 | 26\nInference speed of BlazePose GHUM full body segmentation across different devices and runtimes. The first number in each cell is the lite model, second number is the full model, and third number is the heavy version of the model. Note that the segmentation output can be turned off by setting enableSegmentation to false in the model parameters, which would increase the model performance.\nLooking to the future\nWe are constantly working on new features and quality improvements of our tech (for instance this is the third BlazePose GHUM update in the last year after initial 2D release and consequent 3D update), so expect new exciting updates in the near future.\nAcknowledgements\nWe would like to acknowledge our colleagues who participated in or sponsored creating Selfie Segmentation, BlazePose GHUM and building the APIs: Siargey Pisarchyk, Tingbo Hou, Artsiom Ablavatski, Karthik Raveendran, Eduard Gabriel Bazavan, Andrei Zanfir, Cristian Sminchisescu, Chuo-Ling Chang, Matthias Grundmann, Michael Hays, Tyler Mullen, Na Li, Ping Yu.",
    "link": "https://blog.tensorflow.org/2022/01/body-segmentation.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/a/AVvXsEjw737OVmaE_hRKmKzusBgqeZDUfUUaDlQPKA39J5cg_YvYqmK2t242ov9iCFtOkx2lbUiRjS9Rt8WUMZx5ynKUlipIseD4uwNkQTiXaa_IT4BWRwTZRCRwjB69XfubVXdZc7Vz6ZsIyohdjydGfR0JDWuE29gcV8sSC7c3l_NvryDfn-8Kv4c3g1FV",
      "https://blogger.googleusercontent.com/img/a/AVvXsEjw737OVmaE_hRKmKzusBgqeZDUfUUaDlQPKA39J5cg_YvYqmK2t242ov9iCFtOkx2lbUiRjS9Rt8WUMZx5ynKUlipIseD4uwNkQTiXaa_IT4BWRwTZRCRwjB69XfubVXdZc7Vz6ZsIyohdjydGfR0JDWuE29gcV8sSC7c3l_NvryDfn-8Kv4c3g1FV",
      "https://blogger.googleusercontent.com/img/a/AVvXsEjeln5zg528M_35JebBTDn-JFK41IybTajhce7mNgAHX6mRJ_r7MG1H8K4jJcSyveG6bZLX8MzvZCZikPQKPR46o3S9yMUqd3M6f5pTD14YtZupdSLaMIDE_GtrD_JeHgEbfvStLCASMzV-1RbdJHtlpxg5ks_YqVyab4zTEvE-CH8rtg0DU4ZL9s_-",
      "https://blogger.googleusercontent.com/img/a/AVvXsEg-UNgpfH47TJCM1F-Hh1F75FSTQh2UcHn1sF5Cus_P7dOl26_CTkdNsAbVAub9W_Y5y3wci_BD3hwGosAbKDoL-3ia2Vnr1L2Orp4kG0HwyJugo3xJKdQcj0pt0tLdxN6kgnE6cNCIXIXkIKboyb1_vmSeU7ZKKArvSje-3vIp0-4pUOucs0DP9vxr",
      "https://blogger.googleusercontent.com/img/a/AVvXsEhFhNj8ilBM_IOXgdsQb46liZA-KYatnKY06ZuEGt5Odqq1V4meNL5kYsAL-t7OlR2rvdP2hWuyfFWX3q50puQSX_hPCL1WEyR8FNCBMfFA5Tw324ifDDAWPGpCnVBaPEWZ2NBGvqJV0VqVMLYd_YEOOH7Euo1X5Hu-C3vMQ8VTq9w7GZFJGQBvH4Qt",
      "https://blogger.googleusercontent.com/img/a/AVvXsEiLwuj_wZY3atkhwCOp5NddfLvQBKDD0XBlKzUwXf9oBubNsJrAM6m4DdwJmJcg-a3WLLeHtZCmWHv8MGuelGdsC9JLr5k0W1e2SzDUOETrn3QJ0CxP5zGBGqD-RbbnoZ90-YWwhSzTLAD-IGOTbdZMLAIYIq7Y6a60UN8G5146v16G4iVDEqlTZzUh"
    ],
    "time": "2023/12/08 00:58:43"
  },
  {
    "title": "3D Hand Pose with MediaPipe and TensorFlow.js",
    "content": "Posted by Valentin Bazarevsky, Ivan Grishchenko, Eduard Gabriel Bazavan, Andrei Zanfir, Mihai Zanfir, Jiuqiang Tang, Jason Mayes, Ahmed Sabie, Google\nToday, we're excited to share a new version of our model for hand pose detection, with improved accuracy for 2D, novel support for 3D, and the new ability to predict keypoints on both hands simultaneously. Support for multi-hand tracking was one of the most common requests from the developer community, and we're pleased to support it in this release.\nYou can try a live demo of the new model here. This work improves on our previous model which predicted 21 keypoints, but could only detect a single hand at a time. In this article, we'll describe the new model, and how you can get started.\nThe new hand pose detection model in action.\nTry out the live demo!\nHow to use it\n1. The first step is to import the library. You can either use the <script> tag in your html file or use NPM:\nThrough script tag:\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow-models/hand-pose-detection\">>/script>\n<!-- Optional: Include below scripts if you want to use MediaPipe runtime. -->\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/hands\"> </script > \nThrough NPM:\nyarn add @tensorflow-models/hand-pose-detection\n\n# Run below commands if you want to use TF.js runtime.\nyarn add @tensorflow/tfjs-core @tensorflow/tfjs-converter\nyarn add @tensorflow/tfjs-backend-webgl\n\n# Run below commands if you want to use MediaPipe runtime.\nyarn add @mediapipe/hands\nIf installed through NPM, you need to import the libraries first:\nimport * as handPoseDetection from '@tensorflow-models/hand-pose-detection';\nNext create an instance of the detector:\nconst model = handPoseDetection.SupportedModels.MediaPipeHands;\nconst detectorConfig = {\n  runtime: 'mediapipe', // or 'tfjs'\n  modelType: 'full'\n};\ndetector = await handPoseDetection.createDetector(model, detectorConfig);\nChoose a modelType that fits your application needs, there are two options for you to choose from: lite, and full. From lite to full, the accuracy increases while the inference speed decreases.\n2. Once you have a detector, you can pass in a video stream or static image to detect poses:\nconst video = document.getElementById('video');\nconst hands = await detector.estimateHands(video);\nThe output format is as follows: hands represent an array of detected hand predictions in the image frame. For each hand, the structure contains a prediction of the handedness (left or right) as well as a confidence score of this prediction. An array of 2D keypoints is also returned, where each keypoint contains x, y, and name. The x, y denotes the horizontal and vertical position of the hand keypoint in the image pixel space, and name denotes the joint label. In addition to 2D keypoints, we also return 3D keypoints (x, y, z values) in a metric scale, with the origin in auxiliary keypoint formed as an average between the first knuckles of index, middle, ring and pinky fingers.\n[\n  {\n    score: 0.8,\n    Handedness: 'Right',\n    keypoints: [\n      {x: 105, y: 107, name: \"wrist\"},\n      {x: 108, y: 160, name: \"pinky_finger_tip\"},\n      ...\n    ]\n    keypoints3D: [\n      {x: 0.00388, y: -0.0205, z: 0.0217, name: \"wrist\"},\n      {x: -0.025138, y: -0.0255, z: -0.0051, name: \"pinky_finger_tip\"},\n      ...\n    ]\n  }\n]\nYou can refer to our README for more details about the API.\nModel deep dive\nThe updated version of our hand pose detection API improves the quality for 2D keypoint prediction, handedness (classification output whether it is left or right hand), and minimizes the number of false positive detections. More details about the updated model can be found in our recent paper: On-device Real-time Hand Gesture Recognition.\nFollowing our recently released BlazePose GHUM 3D in TensorFlow.js, we also added metric-scale 3D keypoint prediction to hand pose detection in this release, with the origin being represented by an auxiliary keypoint, formed as a mean of first knuckles for index, middle, ring and pinky fingers. Our 3D ground truth is based on a statistical 3D human body model called GHUM, which is built using a large corpus of human shapes and motions.\nTo obtain hand pose ground truth, we fitted the GHUM hand model to our existing 2D hand dataset and recovered real world 3D keypoint coordinates. The shape and the hand pose variables of the GHUM hand model were optimized such that the reconstructed model aligns with the image evidence. This includes 2D keypoint alignment, shape, and pose regularization terms as well as anthropometric joint angle limits and model self contact penalties.\nSample GHUM hand fittings for hand images with 2D keypoint annotations overlaid. The data was used to train and test a variety of poses leading to better results for more extreme poses.\nModel quality\nIn this new release, we substantially improved the quality of models, and evaluated them on a dataset of American Sign Language (ASL) gestures. As evaluation metric for 2D screen coordinates, we used Mean Average Precision (mAP) suggested by the COCO keypoint challenge methodology.\nHand model evaluation on American Sign Language dataset\nFor 3D evaluation we used Mean Absolute Error in Euclidean 3D metric space, with the average error measured in centimeters.\nModel Name\n2D, mAP, %\n3D, mean 3D error, cm\nHandPose GHUM Lite\n79.2\n1.4\nHandPose GHUM Full\n83.8\n1.3\nPrevious TensorFlow.js HandPose\n66.5\nN/A\nQuality metrics for newly released HandPose GHUM models vs. previously released TensorFlow.js HandPose model in for 2D and 3D predictions\nBrowser performance\nWe benchmark the model across multiple devices. All the benchmarks are tested with two hands presented.\n\nMacBook Pro 15\u201d 2019. \nIntel core i9. \nAMD Radeon Pro Vega 20 Graphics.\n(FPS)\niPhone 11\n(FPS)\nPixel 5\n(FPS)\nDesktop \nIntel i9-10900K. Nvidia GTX 1070 GPU.\n(FPS)\nMediaPipe Runtime\nWith WASM & GPU Accel.\n62 | 48\n8 | 5\n19 | 15 \n  136 | 120\nTensorFlow.js Runtime\nWith WebGL backend\n36 | 31\n15 | 12\n11 | 8 \n 42 | 35 \nInference speed of HandPose across different devices and runtimes. The first number in each cell is for the lite model, and the second number is for the full model.\n\nTo see the model\u2019s FPS on your device, try our demo. You can switch the model type and runtime live in the demo UI to see what works best for your device.\nCross platform availability\nIn addition to the JavaScript hand pose detection API, these updated hand models are also available in MediaPipe Hands as a ready-to-use Android Solution API and Python Solution API, with prebuilt packages in Android Maven Repository and Python PyPI respectively.\nFor instance, for Android developers the Maven package can be easily integrated into an Android Studio project by adding the following into the project\u2019s Gradle dependencies:\ndependencies {\n    implementation 'com.google.mediapipe:solution-core:latest.release'\n    implementation 'com.google.mediapipe:hands:latest.release'\n}\nThe MediaPipe Android Solution is designed to handle different use scenarios such as processing live camera feeds, video files, as well as static images. It also comes with utilities to facilitate overlaying the output landmarks onto either CPU images (with Canvas) or GPU (using OpenGL). For instance, the following code snippet demonstrates how it can be used to process a live camera feed and render the output on screen in real-time:\n// Creates MediaPipe Hands.\nHandsOptions handsOptions =\n    HandsOptions.builder()\n        .setModelComplexity(1)\n        .setMaxNumHands(2)\n        .setRunOnGpu(true)\n        .build();\nHands hands = new Hands(activity, handsOptions);\n\n// Connects MediaPipe Hands to camera.\nCameraInput cameraInput = new CameraInput(activity);\ncameraInput.setNewFrameListener(textureFrame -> hands.send(textureFrame));\n\n// Registers a result listener.\nhands.setResultListener(\n     handsResult -> {\n        handsView.setRenderData(handsResult);\n        handsView.requestRender();\n      })\n\n// Starts the camera to feed data to MediaPipe Hands.\nhandsView.post(this::startCamera);\nTo learn more about MediaPipe Android Solutions, please refer to our documentation and try them out with the example Android Studio project. Also visit MediaPipe Solutions for more cross-platform solutions.\nAcknowledgements\nWe would like to acknowledge our colleagues who participated in or sponsored creating HandPose GHUM 3D and building the APIs: Cristian Sminchisescu, Michael Hays, Na Li, Ping Yu, George Sung, Jonathan Baccash, Esha Uboweja, David Tian, Kanstantsin Sokal, Gregory Karpiak, Tyler Mullen, Chuo-Ling Chang, Matthias Grundmann.",
    "link": "https://blog.tensorflow.org/2021/11/3D-handpose.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/a/AVvXsEj52ND544qrPnBNyU1WMoHkKM2PWHGOEdxzjRjFvG5evgwB6c2bbTvnSUMh-hzxTa3N6jnS2M-ehCO81Q-JEnwSYmSmDKdhdmfjHrEPZC_PchaDmw6kYIReWCIrtvAHnDuvpPcrp_pferZkYgyFe94pwmTunXR9t2NcnP6_Pv_ZyIC4ggNA27Ln35h6",
      "https://blogger.googleusercontent.com/img/a/AVvXsEj52ND544qrPnBNyU1WMoHkKM2PWHGOEdxzjRjFvG5evgwB6c2bbTvnSUMh-hzxTa3N6jnS2M-ehCO81Q-JEnwSYmSmDKdhdmfjHrEPZC_PchaDmw6kYIReWCIrtvAHnDuvpPcrp_pferZkYgyFe94pwmTunXR9t2NcnP6_Pv_ZyIC4ggNA27Ln35h6",
      "https://blogger.googleusercontent.com/img/a/AVvXsEiG6GSYue5TNXOIAeGPTnAmd94kmOE2_-GVo04IYkDjmt1RisMczxqOwcOfFSonlPvahMHFoa6FC4YRmZgEawyq2VjdVXhRrWpLa5MTRgQGOMR9jF7DJX9RCT4Vr3OVcrHxFrmpHDpENywGE3hs11uREJ050br18juUcqlBEBFvfIHh8Kt1XhYp_VZs",
      "https://blogger.googleusercontent.com/img/a/AVvXsEhFDPZnit324o50R6Qcui5OrfbIW8Q_B3Ygljta650JZwV7BG9y22OIspoLz-8hAiyThV0PHntNRApqJ41I1Jpkt7XWsJ_klSmLutHrcIWPcf3WDsEIfdJBljrYSA5eAO_5lbuuGgzL3HIyt582tNE2jKjNQjiMVhRLsSFxZ5XmCzzEBbX-cPvwzuQg"
    ],
    "time": "2023/12/08 00:58:43"
  },
  {
    "title": "How DermAssist uses TensorFlow.js for on-device image quality checks",
    "content": "Posted by Miles Hutson and Aaron Loh, Google Health\nAt Google I/O in May, we previewed our DermAssist web application designed to help people understand issues related to their skin. The tool is designed to be easy to use. Upon opening it, users are expected to take three images of their skin, hair, or nail concern from multiple angles, and provide some additional information about themselves and their condition.\nThis product has been CE marked as a Class I medical device in the EU. It is not available in the United States.\nWe recognize that when users take pictures on their phone, some images could be blurry or have poor lighting. To address this, we initially added a \u201cquality check\u201d after images had been uploaded, which would prompt people to retake an image when necessary. However, these prompts could be a frustrating experience for them, depending on their upload speed, how long they took to acquire the image, and the multiple retakes it might require to pass the quality check.\nLetting the user know they uploaded an image with insufficient quality and advising them to retake it before they proceed.\nTo improve their experience, we decided to give users image quality feedback both on-device as they line up a photo, and when they review the photo before uploading. The way this feature works can be seen below. As the user lines up their camera for a shot, they may get a notification that their environment has a lighting issue (right image). Or, they may be notified that they took a blurry photo as they moved their camera (left image); the model helpfully lets them know their image is blurred before they go to the trouble of uploading it. They can decide to go back and correct the issue without the need to upload the image.\nExamples of poor lighting or blurry images that obtain real time feedback so the user knows to take a new photo\nDeveloping the Model\nWhen developing the model, it was important to ensure that the model could comfortably run on-device. One such architecture designed for that purpose is MobileNetV2, which we selected as the backbone for the model.\nOur discussions with dermatologists highlighted recurrent issues with image quality, such as the image being too blurry, too badly lit, or inappropriate for interpreting skin diseases. We curated several datasets to tackle those issues, which also informed the outputs of the model. The datasets included a crowdsourced data collection, public datasets, data obtained from tele-dermatology services, and synthetically generated images, many of which were further labeled by trained human graders. Combined, we trained the model on more than 30k images.\nWe trained the model with multiple binary heads, one for each quality issue. In the diagram below, we see how the input image is fed into a MobileNet feature extractor. This feature embedding is then fed to multiple distinct fully connected layers, producing a binary output (yes/no), each corresponding to a certain quality issue.\nThe infrastructure we used to train the model was built using TensorFlow, and exported models in the standard SavedModel format.\nTranslating the model to TensorFlow.js\nOur team\u2019s infrastructure for training models makes use of TensorFlow examples, which meant that the exported SavedModel had nodes for loading and preprocessing TensorFlow Examples.\nTensorFlow.js at present does not support such preprocessing nodes. Therefore, we modified the signature of the SavedModel to use the image input node after the preprocessing nodes as input to the model. We re-implemented the processing in our Angular integration below.\nHaving rebuilt the SavedModel in the correct format for conversion, we employed the TensorFlow.js converter to convert it to the TensorFlow.js model format, which consists of a JSON file identifying the model topology, as well as the weights in sharded bin files.\ntensorflowjs_converter --input_format=keras /path/to/tfjs/signature/ /path/to/write/tfjs_model\nIntegrating TensorFlow.js with Observables and the Image Capture API\nWith the model trained, serialized, and made available for TensorFlow.js, it might feel like the job is pretty much done. However, we still had to integrate the TensorFlow.js model into our Angular 2 web application. While doing that, we had the goal that the model would ultimately be exposed as an API similar to other components. A good abstraction would allow frontend engineers to work with the TensorFlow.js model as they would work with any other part of the application, rather than as a unique component.\nTo begin, we created a wrapper class around the model ImageQualityPredictor. This Typescript class exposed only two methods:\nA static method createImageQualityPredictor that, given a URL for the model, returns a promise for an ImageQualityPredictor.\nA makePrediction method that takes ImageData and returns an array of quality predictions above a given threshold.\nWe found that the implementation of makePrediction was key for abstracting the inner workings of our model. The result of calling execute on our model was an array of Tensors representing yes/no probabilities for each binary head. But we didn\u2019t want the downstream application code to be responsible for the delicate task of thresholding these tensors and connecting them back to the heads\u2019 descriptions. Instead, we moved these details inside of our wrapper class. The final return value to the caller was instead an interface ImageQualityPrediction.\nexport interface ImageQualityPrediction {\n  score: number;\n  qualityIssue: QualityIssue;\n}\nIn order to make sure that a single ImageQualityPredictor was shared across the application, we in turn wrapped ImageQualityPredictor in a singleton ImageQualityModelService. This service handled the initialization of the predictor and tracked if the predictor already had a request in progress. It also contained helper methods for extracting frames from the ImageCapture API that our camera feature is built on and translating QualityIssue to plain English strings.\nFinally, we combined the CameraService and our ImageQualityModelService in an ImageQualityService. The final product exposed for use in any given front end component is a simple observable that provides text describing any quality issues.\n@Injectable()\nexport class ImageQualityService {\n  readonly realTimeImageQualityText$: Observable;\n\n  constructor(\n      private readonly cameraService: CameraService,\n      private readonly imageQualityModelService: ImageQualityModelService) {\n    const retrieveText = () =>\n        this.imageQualityModelService.runModel(this.cameraService.grabFrame());\n    this.realTimeImageQualityText$ =\n        interval(REFRESH_INTERVAL_MS)\n            .pipe(\n                filter(() => !imageQualityModelService.requestInProgress),\n                mergeMap(retrieveText),\n            );\n  }\n  // ... \n}\nThis lends itself well to Angular\u2019s normal templating system, accomplishing our goal of making a TensorFlow.js model in Angular as easy to work with as any other component for front end engineers. For example, a suggestion chip can be as easy to include in a component as\n <suggestive-chip *ngIf=\"(imageQualityText$ | async) as text\"\n>{{text}}</suggestive-chip>\n\nLooking Ahead\nWith helping users to capture better pictures in mind, we developed an on-device image quality check for the DermAssist app to provide real-time guidance on image intake. Part of making users\u2019 lives easier is making sure that this model works fast enough such that we can show a notification as quickly as possible while they\u2019re taking a picture. For us, this means finding ways to reduce the model size in order to reduce the time it takes to load on a user\u2019s device. Possible techniques to further advance this goal may be model quantization, or attempts at model distillation into smaller architectures.\nTo learn more about the DermAssist application, check out our blog post from Google I/O.\nTo learn more about TensorFlow.js, you can visit the main site here, also be sure to check out the tutorials and guide.",
    "link": "https://blog.tensorflow.org/2021/10/how-DermAssist-uses-TensorFlowJS.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-YGPQFIGap6c/YVW61p1_nxI/AAAAAAAAEio/HsZQSsaryb4ddRC-O6o2gSXdfdoDiLdSwCLcBGAsYHQ/s0/image5.gif",
      "https://blogger.googleusercontent.com/img/a/AVvXsEgYiFV-cedGnUpFleVODwya51VWfZ2O8Mzc9X7ilucw5LgabGwP_gkNh5hAeTjCeGmGCiesm26A0bkxwnOlV19rg4m77BXEGzsQhZChPEWyyhdO8PrDAGvBSSZe62HVodlKYnSCFSRIHBV16wEsb8Im6hAm6xudlTcvfdsZ3zv2TUTWvG1XjwafNYgI",
      "https://1.bp.blogspot.com/-tKeOjvZrgG0/YVW8kzsP9oI/AAAAAAAAEi8/3DzpLOzSOFU1OpnEv-6lpnO4OcDr0TutwCLcBGAsYHQ/s0/image3.png",
      "https://1.bp.blogspot.com/-Y6GccO83ZSg/YVW8ksbgD7I/AAAAAAAAEi4/aITVrdLcBDA0fUlEELLwE1OrFzpxXVOdQCLcBGAsYHQ/s0/image4.png",
      "https://1.bp.blogspot.com/-ops5gTGTQ_Y/YVW-7pfKpLI/AAAAAAAAEjI/w2dt4boCzr03dwpbufVMoQDxi92GFXDQwCLcBGAsYHQ/s0/image1.png"
    ],
    "time": "2023/12/08 00:58:43"
  },
  {
    "title": "3D Pose Detection with MediaPipe BlazePose GHUM and TensorFlow.js",
    "content": "Posted by Ivan Grishchenko, Valentin Bazarevsky, Eduard Gabriel Bazavan, Na Li, Jason Mayes, Google\nPose detection is an important step in understanding more about the human body in videos and images. Our existing models have supported 2D pose estimation for some time, which many of you may have already tried.\nToday, we are launching our first 3D model in TF.js pose-detection API. 3D pose estimation opens up new design opportunities for applications such as fitness, medical, motion capture and beyond - in many of these areas we\u2019ve seen a growing interest from the TensorFlow.js community. A great example of this is 3D motion capture to drive a character animation in the browser.\n3D motion capture with BlazePose GHUM by Richard Yee\n(used with permission, live demo available at 3d.kalidoface.com)\nThis community demo uses multiple models powered by MediaPipe and TensorFlow.js (namely FaceMesh, BlazePose and HandPose). Even better, no app install is needed as you just need to visit a webpage to enjoy the experience. So with that in mind, let\u2019s learn more and see this new model in action!\nTry out the live demo!\nInstallation\nThe pose-detection API provides two runtimes for BlazePose GHUM, namely MediaPipe runtime and TensorFlow.js runtime.\nTo install the API and runtime library, you can either use the <script> tag in your html file or use NPM.\nThrough script tag:\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow-models/pose-detection\"></script>\n<!-- Include below scripts if you want to use TF.js runtime. -->\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl\"></script>\n\n<!-- Optional: Include below scripts if you want to use MediaPipe runtime. -->\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/pose\"></script>\nThrough NPM:\nyarn add @tensorflow-models/pose-detection\n\n# Run below commands if you want to use TF.js runtime.\nyarn add @tensorflow/tfjs-core @tensorflow/tfjs-converter\nyarn add @tensorflow/tfjs-backend-webgl\n\n# Run below commands if you want to use MediaPipe runtime.\nyarn add @mediapipe/pose\nTo reference the API in your JS code, it depends on how you installed the library.\nIf installed through script tag, you can reference the library through the global namespace poseDetection.\nIf installed through NPM, you need to import the libraries first:\nimport * as poseDetection from '@tensorflow-models/pose-detection';\n// Uncomment the line below if you want to use TF.js runtime.\n// import '@tensorflow/tfjs-backend-webgl';\n// Uncomment the line below if you want to use MediaPipe runtime.\n// import '@mediapipe/pose';\nTry it yourself!\nFirst, you need to create a detector:\nconst model = poseDetection.SupportedModels.BlazePose;\nconst detectorConfig = {\n  runtime: 'mediapipe', // or 'tfjs'\n  modelType: 'full'\n};\ndetector = await poseDetection.createDetector(model, detectorConfig);\nChoose a modelType that fits your application needs, there are three options for you to choose from: lite, full, and heavy. From lite to heavy, the accuracy increases while the inference speed decreases. Please try our live demo to compare different configurations.\nOnce you have a detector, you can pass in a video stream to detect poses:\nconst video = document.getElementById('video');\nconst poses = await detector.estimatePoses(video);\nHow to use the output? poses represent an array of detected pose predictions in the image frame. For each pose, it contains keypoints and keypoints3D. The keypoints are the same as the 2D model we launched before, it is an array of 33 keypoint objects, each object has x, y in pixel units.\n\nkeypoints3D is an additional array with 33 keypoint objects, each object has x, y, z. The x, y, z are in meter units. The person is modeled as if they were in a 2m x 2m x 2m cubic space. The range for each axis goes from -1 to 1 (therefore 2m total delta). The origin of this 3D space is the hip center (0, 0, 0). From the origin, z is positive if moving closer to the camera, and negative if moving away from the camera. See below output snippet for example:\n[\n  {\n    score: 0.8,\n    keypoints: [\n      {x: 230, y: 220, score: 0.9, name: \"nose\"},\n      {x: 212, y: 190, score: 0.8, name: \"left_eye\"},\n      ...\n    ],\n    keypoints3D: [\n      {x: 0.5, y: 0.9, z: 0.06 score: 0.9, name: \"nose\"},\n      ...\n    ]\n  }\n]\nYou can refer to our ReadMe for more details about the API.\nAs you begin to play and develop with BlazePose GHUM, we would appreciate your feedback and contributions. If you make something using this model, tag it with #MadeWithTFJS on social media so we can find your work, as we would love to see what you create.\nModel deep dive\nThe key challenge to build the 3D part of our pose model was obtaining realistic, in-the-wild 3D data. In contrast to 2D, which can be obtained via human annotation, accurate manual 3D annotation becomes a uniquely challenging task. It requires either a lab setup or specialised hardware with depth sensors for 3D scans - which introduce additional challenges to preserve a good level of human and environment diversity in the dataset. Another alternative, which many researchers choose - to build a completely synthetic dataset, which introduces yet another challenge of domain adaptation to real-world pictures.\nOur approach is based on a statistical 3D human body model called GHUM, which is built using a large corpus of human shapes and motions. To obtain 3D human body pose ground truth, we fitted the GHUM model to our existing 2D pose dataset and extended it with a real world 3D keypoint coordinates in metric space. During the fitting process the shape and the pose variables of GHUM were optimized such that the reconstructed model aligns with the image evidence. This includes 2D keypoint and silhouette semantic segmentation alignment as well as shape and pose regularization terms. For more details see related work on 3D pose and shape inference (HUND, THUNDR).\nSample GHUM fitting for an input image. From left to right: original image, 3D GHUM reconstruction (different viewpoint) and blended result projected on top of the original image.\nDue to the nature of 3D to 2D projection, multiple points in 3D can have the same projection in 2D (i.e. with the same X and Y but different Z). So the fitting can result in several realistic 3D body poses for the given 2D annotation. To minimize this ambiguity, in addition to a 2D body pose, we asked annotators to provide depth order between pose skeleton edges where they are certain (check the figure below). This task proved to be an easy one (compared to a real depth annotation) showing high consistency between annotators (98% on cross-validation) and helped to reduce the depth ordering errors for the fitted GHUM reconstructions from 25% to 3%.\n\"Depth order\" annotation: the wider edge corner denotes the corner closer to the camera (e.g. the person\u2019s right shoulder is closer to camera than left shoulder on both examples)\nBlazePose GHUM utilizes a two-step detector-tracker approach where the tracker operates on a cropped human image. Thus the model is trained to predict 3D body pose in relative coordinates of a metric space with origin in the subject's hips center.\nMediaPipe vs. TF.js runtime\nThere are some pros and cons of using each runtime. As shown in the performance table below, the MediaPipe runtime provides faster inference speed on desktop, laptop and android phones. The TF.js runtime provides faster inference speed on iPhones and iPads. The TF.js runtime is also about 1 MB smaller than the MediaPipe runtime.\n\nMacBook Pro 15\u201d 2019. \nIntel core i9. \nAMD Radeon Pro Vega 20 Graphics.\n(FPS)\niPhone 11\n(FPS)\nPixel 5\n(FPS)\nDesktop \nIntel i9-10900K. Nvidia GTX 1070 GPU.\n(FPS)\nMediaPipe Runtime\nWith WASM & GPU Accel.\n75 | 67 | 34\n9 | 6 | N/A                   \n25 | 21 | 8\n150 | 130 | 97\nTFJS Runtime\nWith WebGL backend.\n52 | 40 | 24\n 43 | 32 | 22\n14 | 10 | 4\n42 | 35 | 29\nInference speed of BlazePose GHUM across different devices and runtimes. The first number in each cell is for the lite model, and the second number is for the full model, the third number is for the heavy model.\nAcknowledgements\nWe would like to acknowledge our colleagues, who participated in creating BlazePose GHUM 3D: Andrei Zanfir, Cristian Sminchisescu, Tyler Zhu, the other contributors to MediaPipe: Chuo-Ling Chang, Michael Hays, Ming Guang Yong, Matthias Grundmann, along with those involved with the TensorFlow.js pose-detection API: Ahmed Sabie and Ping Yu, and of course the community who are making amazing work with these models: Richard Yee.",
    "link": "https://blog.tensorflow.org/2021/08/3d-pose-detection-with-mediapipe-blazepose-ghum-tfjs.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-T-VwABuYXoo/YSlER0yqyjI/AAAAAAAAEdQ/PKk0E8DdViUSgEqII8IELWrCyHaNpLhZgCLcBGAsYHQ/s0/TF%2Bimage%2B2.gif",
      "https://1.bp.blogspot.com/-gh6BeIkRRgI/YSlLDf3k6kI/AAAAAAAAEdo/ldwdwMBOnYcEk2Xc5ks4xtBjK-q7OytugCLcBGAsYHQ/s0/ezgif-2-c72ec21b45eb.gif",
      "https://1.bp.blogspot.com/-T-VwABuYXoo/YSlER0yqyjI/AAAAAAAAEdQ/PKk0E8DdViUSgEqII8IELWrCyHaNpLhZgCLcBGAsYHQ/s0/TF%2Bimage%2B2.gif",
      "https://1.bp.blogspot.com/-fiG9mb6C8mY/YSlHDlyp6PI/AAAAAAAAEdY/8ay4e61-KtcCvjoTnFs9UR5hkGM9MGDNQCLcBGAsYHQ/s0/TF%2Bimage%2B3.jpg",
      "https://1.bp.blogspot.com/-lkClC46aqew/YSlILrdoPZI/AAAAAAAAEdg/hlEd6f_vbz0TFDHR8JyRGDY-_bDB3kMiwCLcBGAsYHQ/s0/TF_image_4.jpeg"
    ],
    "time": "2023/12/08 00:58:43"
  },
  {
    "title": "High Fidelity Pose Tracking with MediaPipe BlazePose and TensorFlow.js",
    "content": "Posted by Ivan Grishchenko, Valentin Bazarevsky and Na Li, Google Research\nToday we\u2019re excited to launch MediaPipe's BlazePose in our new pose-detection API. BlazePose is a high-fidelity body pose model designed specifically to support challenging domains like yoga, fitness and dance. It can detect 33 keypoints, extending the 17 keypoint topology of the original PoseNet model we launched a couple of years ago. These additional keypoints provide vital information about face, hands, and feet location with scale and rotation. Together with our face and hand models they can be used to unlock various domain-specific applications like gesture control or sign language without special hardware. With today\u2019s release we enable developers to use the same models on the web that are powering MLKit Pose and MediaPipe Python unlocking the same great performance across all devices.\nThe new TensorFlow.js pose-detection API supports two runtimes: TensorFlow.js and MediaPipe. TensorFlow.js provides the flexibility and wider adoption of JavaScript, optimized for several backends including WebGL (GPU), WASM (CPU), and Node. MediaPipe capitalizes on WASM with GPU accelerated processing and provides faster out-of-the-box inference speed. The MediaPipe runtime currently lacks Node and iOS Safari support, but we'll be adding the support soon.\nTry out the live demo!\nBlazePose can track 33 keypoints across a variety complex poses in real-time.\nInstallation\nTo use BlazePose with the new pose-detection API, you have to first decide whether to use the TensorFlow.js runtime or MediaPipe runtime. To understand the advantages of each runtime, check the performance and loading times section later in this document for further details.\nFor each runtime, you can use either script tag or NPM for installation.\nUsing TensorFlow.js runtime:\nThrough script tag:\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow-models/pose-detection\"></script>\nThrough NPM:\nyarn add @tensorflow/tfjs-core, @tensorflow/tfjs-converter\nyarn add @tensorflow/tfjs-backend-webgl\nyarn add @tensorflow-models/pose-detection\nUsing MediaPipe runtime:\nThrough script tag:\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/pose\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow-models/pose-detection\"></script>\nThrough NPM:\nyarn add @mediapipe/pose\nyarn add @tensorflow-models/pose-detection\nTry it yourself!\nOnce the package is installed, you only need to follow the few steps below to start using it. There are three variants of the model: lite, full, and heavy. The model accuracy increases from lite to heavy, while the inference speed decreases and memory footprint increases. The heavy variant is intended for applications that require high accuracy, while the lite variant is intended for latency-critical applications. The full variant is a balanced option, which is also the default option here.\nUsing TensorFlow.js runtime:\n// Import TFJS runtime with side effects.\nimport '@tensorflow/tfjs-backend-webgl';\nimport * as poseDetection from '@tensorflow-models/pose-detection';\n\n// Create a detector.\nconst detector = await poseDetection.createDetector(poseDetection.SupportedModels.BlazePose, {runtime: 'tfjs'});\nUsing MediaPipe runtime:\n// Import MediaPipe runtime with side effects.\nimport '@mediapipe/pose';\nimport * as poseDetection from '@tensorflow-models/pose-detection';\n\n// Create a detector.\nconst detector = await poseDetection.createDetector(poseDetection.SupportedModels.BlazePose, {runtime: 'mediapipe'});\nYou can also choose the lite or the heavy variant by setting the modelType field, as shown below:\n// Create a detector.\nconst detector = await poseDetection.createDetector(poseDetection.SupportedModels.BlazePose, {runtime, modelType:'lite'});\n// Pass in a video stream to the model to detect poses.\nconst video = document.getElementById('video');\nconst poses = await detector.estimatePoses(video);\nEach pose contains 33 keypoints, with absolute x, y coordinates, confidence score and name:\nconsole.log(poses[0].keypoints);\n// Outputs:\n// [\n//    {x: 230, y: 220, score: 0.9, name: \"nose\"},\n//    {x: 212, y: 190, score: 0.8, name: \"left_eye_inner\"},\n//    ...\n// ]\nRefer to our ReadMe (TFJS runtime, MediaPipe runtime) for more details about the API.\nAs you begin to play and develop with BlazePose, we would appreciate your feedback and contributions. If you make something using this model, tag it with #MadeWithTFJS on social media so we can find your work, as we would love to see what you create.\nModel deep dive\nBlazePose provides real-time human body pose perception in the browser, working up to 4 meters from the camera.\nWe trained BlazePose specifically for highly demanded single-person use cases like yoga, fitness, and dance which require precise tracking of challenging postures, enabling the overlay of digital content and information on top of the physical world in augmented reality, gesture control, and quantifying physical exercises.\nFor pose estimation, we utilize our proven two-step detector-tracker ML pipeline. Using a detector, this pipeline first locates the pose region-of-interest (ROI) within the frame. The tracker subsequently predicts all 33 pose keypoints from this ROI. Note that for video use cases, the detector is run only on the first frame. For subsequent frames we derive the ROI from the previous frame\u2019s pose keypoints as discussed below.\nBlazePose architecture.\nBlazePose\u2019s topology contains 33 points extending 17 points by COCO with additional points on palms and feet to provide lacking scale and orientation information for limbs, which is vital for practical applications like fitness, yoga and dance.\nSince the BlazePose CVPR'2020 release, MediaPipe has been constantly improving the models' quality to remain state-of-the-art on the web / edge for single person pose tracking. Besides running through the TensorFlow.js pose-detection API, BlazePose is also available on Android, iOS and Python via MediaPipe and ML Kit. For detailed information, read the Google AI Blog post and the model card.\nBlazePose Browser Performance\nTensorFlow.js continuously seeks opportunities to bring the latest and fastest runtime for browsers. To achieve the best performance for this BlazePose model, in addition to the TensorFlow.js runtime (w/ WebGL backend) we further integrated with the MediaPipe runtime via the MediaPipe JavaScript Solutions. The MediaPipe runtime leverages WASM to utilize state-of-the-art pipeline acceleration available across platforms, which also powers Google products such as Google Meet.\nInference speed:\nTo quantify the inference speed of BlazePose, we benchmark the model across multiple devices.\n\nMacBook Pro 15\u201d 2019. \nIntel core i9. \nAMD Radeon Pro Vega 20 Graphics.\n(FPS)\niPhone 11\n(FPS)\nPixel 5\n(FPS)\nDesktop \nIntel i9-10900K. Nvidia GTX 1070 GPU.\n(FPS)\nMediaPipe Runtime\nWith WASM & GPU Accel.\n92 | 81 | 38\nN/A\n32 | 22 | N/A\n160  | 140 | 98\nTensorFlow.js Runtime\nWith WebGL backend\n48 | 53 | 28\n34 | 30 | N/A\n 13 | 11 | 5\n44 | 40 | 30\nInference speed of BlazePose across different devices and runtimes. The first number in each cell is for the lite model, and the second number is for the full model, the third number is for the heavy model. Certain model types and runtime do not work at time of this release, and we will be adding the support soon.\nTo see the model\u2019s FPS on your device, try our demo. You can switch the model type and runtime live in the demo UI to see what works best for your device.\nLoading times:\nBundle size can affect initial page loading experience, such as Time-To-Interactive (TTI), UI rendering, etc. We evaluate the pose-detection API and the two runtime options. The bundle size affects file fetching time and UI smoothness, because processing the code and loading them into memory will compete with UI rendering on CPU. It also affects when the model is available to make inference.\nThere is a difference of how things are loaded between the two runtimes. For the MediaPipe runtime, only the @tensorflow-models/pose-detection and the @mediapipe/pose library are loaded at initial page download; the runtime and the model assets are loaded when the createDetector method is called. For the TF.js runtime with WebGL backend, the runtime is loaded at initial page download; only the model assets are loaded when the createDetector method is called. The TensorFlow.js package sizes can be further reduced with a custom bundle technique. Also, if your application is currently using TensorFlow.js, you don\u2019t need to load those packages again, models will share the same TensorFlow.js runtime. Choose the runtime that best suits your latency and bundle size requirements. A summary of loading times and bundle sizes is provided below:\n\nBundle Size\ngzipped + minified\nAverage Loading Time\nWiFi:\ndownload speed 100Mbps\nMediaPipe Runtime\n\n\n    Initial Page Load\n22.1KB\n0.04s\n    Initial Detector Creation:\n\n\n         Runtime\n1.57MB\n\n         Lite model\n10.6MB \n1.91s\n         Full model\n14MB\n1.91s\n         Heavy model\n34.9MB\n4.82s\nTensorFlow.js Runtime\n\n\n    Initial Page Load\n162.6KB\n0.07s\n    Initial Detector Creation:\n\n\n         Lite model\n10.4MB\n1.91s\n         Full model\n13.8MB\n1.91s\n         Heavy model\n34.7MB\n4.82s\nBundle size and loading time analysis for MediaPipe and TF.js runtime. The loading time is estimated based on a simulated WiFi network with 100Mbps download speed and includes time from request sent to content downloaded, see what is included in more detail here.\nLooking ahead\nIn the future, we plan to extend TensorFlow.js pose-detection API with new features like BlazePose GHUM 3D pose. We also plan to speed up the TensorFlow.js WebGL backend to make model execution even faster. This will be achieved through repeated benchmarking and backend optimization, such as operator fusion. We will also bring Node.js support in the near future.\nAcknowledgements\nWe would like to acknowledge our colleagues, who participated in creating BlazePose GHUM 3D: Eduard Gabriel Bazavan, Cristian Sminchisescu, Tyler Zhu, the other contributors to MediaPipe: Chuo-Ling Chang, Michael Hays and Ming Guang Yong, along with those involved with the TensorFlow.js pose-detection API: Ping Yu, Sandeep Gupta, Jason Mayes, and Masoud Charkhabi.",
    "link": "https://blog.tensorflow.org/2021/05/high-fidelity-pose-tracking-with-mediapipe-blazepose-and-tfjs.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-gWTiIA7rx5c/YKRYFwwcOfI/AAAAAAAAEN0/0EMLV2bODwUZdrgI0E41muxfNI6hVT7YwCLcBGAsYHQ/s0/tweet.3in1.1200x400.gif",
      "https://1.bp.blogspot.com/-WCioLRSZ_MU/YKRbp36pcOI/AAAAAAAAEN8/ujfa2xIzLm8yBYbbuXwLLIanJEZJJXoCgCLcBGAsYHQ/s0/Untitled%2B%25283%2529.png"
    ],
    "time": "2023/12/08 00:58:43"
  },
  {
    "title": "Speed-up your sites with web-page prefetching using Machine Learning",
    "content": "Posted by Minko Gechev, David Zats, Na Li, Ping Yu, Anusha Ramesh, and Sandeep Gupta\nPage load time is one of the most important determinants of user experience on a web site. Research shows that faster page load time directly leads to increased page views, conversion, and customer satisfaction. Retail superstore Newegg has seen a 50% increase in conversions after implementing web-page prefetching to optimize page load experience.\nUsing TensorFlow tooling, it is now possible to use machine learning to implement a powerful solution for your website to improve page load times. In this blog post, we show an end-to-end workflow for using your site\u2019s navigation data from Google Analytics and training a custom machine learning model that can predict the user's next actions. You can use these predictions in an Angular app to pre-fetch candidate pages and dramatically improve user experience on your web site. Fig. 1 illustrates this side-by-side with default page load experience with no optimization compared to the greatly improved page load times with machine learning based predictive prefetching implemented on the right. Both examples are running on an emulated slow 3G network.\nFig: Comparison of un-optimized and machine learning based page loading time in a sample web application\nA high-level schematic of our solution is as follows:\nFig: Solution overview\nWe use Google Cloud services (BigQuery and Dataflow) to store and preprocess the site\u2019s Google Analytics data, then train a custom model using TensorFlow Extended (TFX) to run our model training pipeline, produce a site-specific model, and then convert it into a web-deployable TensorFlow.js format. This client-side model will be loaded in a sample Angular web app for an e-store to demonstrate how to deploy the model in a web application. Let\u2019s take a look at these components in more detail.\nData Preparation & Ingestion\nGoogle Analytics stores each page visit as an event, providing key aspects such as the page name, visit time, and load time. This data contains everything we need to train our model. We need to:\nConvert this data to training examples containing features and labels\nMake it available to TFX for training.\nWe accomplish the first by leveraging existing support for exporting the Google Analytics data to a large-scale cloud data store called BigQuery. We accomplish the latter by creating an Apache Beam pipeline that:\nReads the data from BigQuery\nSorts and filters the events in a session\nWalks through each session, creating examples that take properties of the current event as features and the page visit in the next event as the label\nStores these generated examples in Google Cloud Storage so that they can be used by TFX for training.\nWe run our Beam pipeline in Dataflow.\nIn the following table, each row represents a training example:\ncur_page\nsession_index\nlabel\npage2\n0\npage3\npage3\n8\npage1\nWhile our training example only contains two training features (cur_page and session_index), additional features from Google Analytics can be easily added to create a richer dataset and used for training to create a more powerful model. To do so, extend the following code:\ndef ga_session_to_tensorflow_examples(session):\n  examples = []\n\n  for i in range(len(session)-1):\n    features = {\u2018cur_page\u2019: [session[i][\u2018page\u2019][\u2018pagePath\u2019]],\n                \u2018label\u2019: [session[i+1][\u2018page\u2019][\u2018pagePath\u2019]],\n                \u2018session_index\u2019: [i],\n                # Add additional features here.\n                \u2026\n               }\n    examples.append(create_tensorflow_example(features))\n  return examples\nModel Training\nTensorflow Extended (TFX) is an end to end production scale ML platform and is used to automate the process of data validation, training at scale (using accelerators), evaluation & validation of the generated model.\nTo create a model within TFX, you must provide the preprocessing function and the run function. The preprocessing function defines the operations that should be performed on the data before it is passed to the main model. These include operations that involve a full pass over the data, such as vocab creation. The run function defines the main model and how it is to be trained.\nOur example shows how to implement the preprocessing_fn and the run_fn to define and train a model for predicting the next page. And the TFX example pipelines demonstrate how to implement these functions for many other key use cases.\nCreating a Web Deployable Model\nAfter training our custom model, we want to deploy this model in our web application so it can be used to make live predictions when users visit our website. For this, we use TensorFlow.js, which is TensorFlow\u2019s framework for running machine learning models directly in the browser client-side. By running this code in the browser client-side, we can reduce latency associated with server-side roundtrip traffic, reduce server-side costs, and also keep user\u2019s data private by not having to send any session data to the server.\nTFX employs the Model Rewriting Library to automate conversion between trained TensorFlow models and the TensorFlow.js format. As part of this library, we have implemented a TensorFlow.js rewriter. We simply invoke this rewriter within the run_fn to perform the desired conversion. Please see the example for more details.\nAngular Application\nOnce we have the model we can use it within an Angular application. On each navigation, we will query the model and prefetch the resources associated with the pages that are likely to be visited in the future.\nAn alternative solution would be to prefetch the resources associated with all the possible future navigation paths, but this would have much higher bandwidth consumption. Using machine learning, we can predict only the pages, which are likely to be used next and reduce the number of false positives.\nDepending on the specifics of the application we may want to prefetch different types of assets, for example: JavaScript, images, or data. For the purposes of this demonstration we\u2019ll be prefetching images of products.\nA challenge is how to implement the mechanism in a performant way without impacting the application load time or runtime performance. Techniques to mitigate the risks of performance regressions we can use are:\nLoad the model and TensorFlow.js lazily without blocking the initial page load time\nQuery the model off the main thread so we don\u2019t drop frames in the main thread and achieve 60fps rendering experience\nA web platform API that satisfies both of these constraints is the service worker. A service worker is a script that your browser runs in the background in a new thread, separate from a web page. It also allows you to plug into a request cycle and provides you with cache control.\nWhen the user navigates across the application, we\u2019ll post messages to the service worker with the pages they have visited. Based on the navigation history, the service worker will make predictions for future navigation and prefetch relevant product assets.\nLet us look at a high-level overview of the individual moving parts.\nFrom within the main file of our Angular application, we can load the service worker:\n// main.ts\n\nif ('serviceWorker' in navigator) {\n  navigator.serviceWorker.register('/prefetch.worker.js', { scope: '/' });\n}\nThis snippet will download the prefetch.worker.js script and run it in the background. As the next step, we want to forward navigation events to it:\n// app.component.ts\n\nthis.route.params.subscribe((routeParams) => {\n  if (this._serviceWorker) {\n    this._serviceWorker.postMessage({ page: routeParams.category });\n  }\n});\nIn the snippet above, we watch for changes of the parameters of the URL. On change, we forward the category of the page to the service worker.\nIn the implementation of the service worker we need to handle messages from the main thread, make predictions based on them, and prefetch the relevant information. On a high-level this looks as follows:\n// prefetch.worker.js\n\naddEventListener('message', ({ data }) => prefetch(data.page));\n\nconst prefetch = async (path) => {\n  const predictions = await predict(path);\n  const cache = await caches.open(ImageCache);\n\n  predictions.forEach(async ([probability, category]) => {\n    const products = (await getProductList(category)).map(getUrl);\n    [...new Set(products)].forEach(url => {\n      const request = new Request(url, {\n        mode: 'no-cors',\n      });\n      fetch(request).then(response => cache.put(request, response));\n    });\n  });\n};\nWithin the service worker we listen for messages from the main thread. When we receive a message we trigger the logic responsible for making predictions and prefetching data.\nIn the prefetch function we first predict, which are the pages the user could visit next. After that, we iterate over all the predictions and fetch the corresponding resources to improve the user experience in subsequent navigation.\nFor details you can follow the sample app in the TensorFlow.js examples repository.\nTry it yourself\nCheck out the model training code sample which shows the TFX pipeline for training a page prefetching model as well as an Apache Beam pipeline that converts Google Analytics data to training examples, and the deployment sample showing how to deploy the TensorFlow.js model in a sample Angular app for client-side predictions.\nAcknowledgements\nThis project wouldn\u2019t have been possible without the incredible effort and support of Becky Chan, Deepak Aujla, Fei Dong, and Jason Mayes.",
    "link": "https://blog.tensorflow.org/2021/05/speed-up-your-sites-with-web-page-prefetching-using-ml.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-99pn_12U-RA/YKLfcsbsvoI/AAAAAAAAENs/lGT193iGf7YtHHVGWxUcS8KElV8YvztQQCLcBGAsYHQ/s0/final_60a2ddc6e9d034008598cb68_722423.gif",
      "https://1.bp.blogspot.com/-nKftoU_9QZY/YKLSC_hvO8I/AAAAAAAAENM/TErxSss9KGEgq53hbkikqG5VwAHLcSDWQCLcBGAsYHQ/s0/predictive-prefetching-infinite.2021-05-13%2B13_48_49.gif",
      "https://1.bp.blogspot.com/-kLuEvhqM5W4/YKLTSw148-I/AAAAAAAAENU/RtrgDtNH16IK_8ljSzLwZzVjoEjkX-SQwCLcBGAsYHQ/s0/Blank%2Bdiagram%2B%25281%2529.png",
      "https://1.bp.blogspot.com/-EnjtakiVSuQ/YKLUvFyygpI/AAAAAAAAENk/rwYFACzKu3kEMO9GscW1_75OYqipgDyOgCLcBGAsYHQ/s0/Screen%2BShot%2B2021-05-12%2Bat%2B11.25.02%2BAM.png"
    ],
    "time": "2023/12/08 00:58:43"
  },
  {
    "title": "Next-Generation Pose Detection with MoveNet and TensorFlow.js",
    "content": "Posted by Ronny Votel and Na Li, Google Research\nToday we\u2019re excited to launch our latest pose detection model, MoveNet, with our new pose-detection API in TensorFlow.js. MoveNet is an ultra fast and accurate model that detects 17 keypoints of a body. The model is offered on TF Hub with two variants, known as Lightning and Thunder. Lightning is intended for latency-critical applications, while Thunder is intended for applications that require high accuracy. Both models run faster than real time (30+ FPS) on most modern desktops, laptops, and phones, which proves crucial for live fitness, sports, and health applications. This is achieved by running the model completely client-side, in the browser using TensorFlow.js with no server calls needed after the initial page load and no dependencies to install.\nTry out the live demo!\nMoveNet can track keypoints through fast motions and atypical poses.\nHuman pose estimation has come a long way in the last five years, but surprisingly hasn\u2019t surfaced in many applications just yet. This is because more focus has been placed on making pose models larger and more accurate, rather than doing the engineering work to make them fast and deployable everywhere. With MoveNet, our mission was to design and optimize a model that leverages the best aspects of state-of-the-art architectures, while keeping inference times as low as possible. The result is a model that can deliver accurate keypoints across a wide variety of poses, environments, and hardware setups.\nUnlocking Live Health Applications with MoveNet\nWe teamed up with IncludeHealth, a digital health and performance company, to understand whether MoveNet can help unlock remote care for patients. IncludeHealth has developed an interactive web application that guides a patient through a variety of routines (using a phone, tablet, or laptop) from the comfort of their own home. The routines are digitally built and prescribed by physical therapists to test balance, strength, and range of motion.\nThe service requires web-based and locally run pose models for privacy that can deliver precise keypoints at high frame rates, which are then used to quantify and qualify human poses and movements. While a typical off-the-shelf detector is sufficient for easy movements such as shoulder abductions or full body squats, more complicated poses such as seated knee extensions or supine positions (laying down) cause grief for even state-of-the-art detectors trained on the wrong data.\nComparison of a traditional detector (top) vs MoveNet (bottom) on difficult poses.\nWe provided an early release of MoveNet to IncludeHealth, accessible through the new pose-detection API. This model is trained on fitness, dance, and yoga poses (see more details about the training dataset below). IncludeHealth integrated the model into their application and benchmarked MoveNet relative to other available pose detectors:\n\u201cThe MoveNet model has infused a powerful combination of speed and accuracy needed to deliver prescriptive care. While other models trade one for the other, this unique balance has unlocked the next generation of care delivery. The Google team has been a fantastic collaborator in this pursuit.\u201d - Ryan Eder, Founder & CEO at IncludeHealth.\nAs a next step, IncludeHealth is partnering with hospital systems, insurance plans, and the military to enable the extension of traditional care and training beyond brick and mortar.\nIncludeHealth demo application running in browser that quantifies balance and motion using keypoint estimation powered by MoveNet and TensorFlow.js\nInstallation\nThere are two ways to use MoveNet with the new pose-detection api:\nThrough NPM:\nimport * as poseDetection from '@tensorflow-models/pose-detection';\nThrough script tag:\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow-models/pose-detection\"></script>\nTry it yourself!\nOnce the package is installed, you only need to follow the few steps below to start using it:\n// Create a detector.\nconst detector = await poseDetection.createDetector(poseDetection.SupportedModels.MoveNet);\nThe detector defaults to use the Lightning version; to choose the Thunder version, create the detector as below:\n// Create a detector.\nconst detector = await poseDetection.createDetector(poseDetection.SupportedModels.MoveNet, {modelType: poseDetection.movenet.modelType.SINGLEPOSE_THUNDER});\n// Pass in a video stream to the model to detect poses.\nconst video = document.getElementById('video');\nconst poses = await detector.estimatePoses(video);\nEach pose contains 17 keypoints, with absolute x, y coordinates, confidence score and name:\nconsole.log(poses[0].keypoints);\n// Outputs:\n// [\n//    {x: 230, y: 220, score: 0.9, name: \"nose\"},\n//    {x: 212, y: 190, score: 0.8, name: \"left_eye\"},\n//    ...\n// ]\nRefer to our README for more details about the API.\nAs you begin to play and develop with MoveNet, we would appreciate your feedback and contributions. If you make something using this model, tag it with #MadeWithTFJS on social so we can find your work, as we would love to see what you create.\nMoveNet Deep Dive\nMoveNet Architecture\nMoveNet is a bottom-up estimation model, using heatmaps to accurately localize human keypoints. The architecture consists of two components: a feature extractor and a set of prediction heads. The prediction scheme loosely follows CenterNet, with notable changes that improve both speed and accuracy. All models are trained using the TensorFlow Object Detection API.\nThe feature extractor in MoveNet is MobileNetV2 with an attached feature pyramid network (FPN), which allows for a high resolution (output stride 4), semantically rich feature map output. There are four prediction heads attached to the feature extractor, responsible for densely predicting a:\nPerson center heatmap: predicts the geometric center of person instances\nKeypoint regression field: predicts full set of keypoints for a person, used for grouping keypoints into instances\nPerson keypoint heatmap: predicts the location of all keypoints, independent of person instances\n2D per-keypoint offset field: predicts local offsets from each output feature map pixel to the precise sub-pixel location of each keypoint\nMoveNet architecture\nAlthough these predictions are computed in parallel, one can gain insight into the model\u2019s operation by considering the following sequence of operations:\nStep 1: The person center heatmap is used to identify the centers of all individuals in the frame, defined as the arithmetic mean of all keypoints belonging to a person. The location with the highest score (weighted by the inverse-distance from the frame center) is selected.\nStep 2: An initial set of keypoints for the person is produced by slicing the keypoint regression output from the pixel corresponding to the object center. Since this is a center-out prediction \u2013 which must operate over different scales \u2013 the quality of regressed keypoints will not be very accurate.\nStep 3: Each pixel in the keypoint heatmap is multiplied by a weight which is inversely proportional to the distance from the corresponding regressed keypoint. This ensures that we do not accept keypoints from background people, since they typically will not be in the proximity of regressed keypoints, and hence will have low resulting scores.\nStep 4: The final set of keypoint predictions are selected by retrieving the coordinates of the maximum heatmap values in each keypoint channel. The local 2D offset predictions are then added to these coordinates to give refined estimates. See the figure below which illustrates these four steps.\nMoveNet post-processing steps.\nTraining Datasets\nMoveNet was trained on two datasets: COCO and an internal Google dataset called Active. While COCO is the standard benchmark dataset for detection \u2013 due to its scene and scale diversity \u2013 it is not suitable for fitness and dance applications, which exhibit challenging poses and significant motion blur. Active was produced by labeling keypoints (adopting COCO\u2019s standard 17 body keypoints) on yoga, fitness, and dance videos from YouTube. No more than three frames are selected from each video for training, to promote diversity of scenes and individuals.\nEvaluations on the Active validation dataset show a significant performance boost relative to identical architectures trained using only COCO. This isn\u2019t surprising since COCO infrequently exhibits individuals with extreme poses (e.g. yoga, pushups, headstands, and more).\nTo learn more about the dataset and how MoveNet performs across different categories, please see the model card.\nImages from Active keypoint dataset.\nOptimization\nWhile a lot of effort went into architecture design, post-processing logic, and data selection to make MoveNet a high-quality detector, an equal focus was given to inference speed. First, bottleneck layers from MobileNetV2 were selected for lateral connections in the FPN. Likewise, the number of convolution filters in each prediction head were slimmed down significantly to speed up execution on the output feature maps. Depthwise separable convolutions are used throughout the network, except in the first MobileNetV2 layer.\nMoveNet was repeatedly profiled, uncovering and removing particularly slow ops. For example, we replaced tf.math.top_k with tf.math.argmax, since it executes significantly faster and is adequate for the single-person setting.\nTo ensure fast execution with TensorFlow.js, all model outputs were packed into a single output tensor, so that there is only one download from GPU to CPU.\nPerhaps the most significant speedup is the use of 192x192 inputs to the model (256x256 for Thunder). To counteract the lower resolution, we apply intelligent cropping based on detections from the previous frame. This allows the model to devote its attention and resources to the main subject, and not the background.\nTemporal Filtering\nOperating on a high FPS camera stream provides the luxury of applying smoothing to keypoint estimates. Both Lightning and Thunder apply a robust, non-linear filter to the incoming stream of keypoint predictions. This filter is tuned to simultaneously suppress high-frequency noise (i.e. jitter) and outliers from the model, while also maintaining high-bandwidth throughput during quick motions. This leads to smooth keypoint visualizations with minimal lag in all circumstances.\nMoveNet Browser Performance\nTo quantify the inference speed of MoveNet, the model was benchmarked across multiple devices. The model latency (expressed in FPS) was measured on GPU with WebGL, as well as WebAssembly (WASM), which is the typical backend for devices with lower-end or no GPUs.\n\nMacBook Pro 15\u201d 2019. \nIntel core i9. \nAMD Radeon Pro Vega 20 Graphics.\n(FPS)\niPhone 12\n(FPS)\nPixel 5\n(FPS)\nDesktop \nIntel i9-10900K. Nvidia GTX 1070 GPU.\n(FPS)\nWebGL\n104  |  77\n51  |  43\n34  |  12\n87  |  82\nWASM \nwith SIMD + Multithread\n42  |  21\nN/A\nN/A\n71  |  30\nInference speed of MoveNet across different devices and TF.js backends. The first number in each cell is for Lightning, and the second number is for Thunder.\nTF.js continuously optimizes its backends to accelerate model execution across all supported devices. We applied several techniques here to help the models achieve this performance, such as implementing a packed WebGL kernel for the depthwise separable convolutions and improving GL scheduling for mobile Chrome.\nTo see the model\u2019s FPS on your device, try our demo. You can switch the model type and backends live in the demo UI to see what works best for your device.\nLooking Ahead\nThe next step is to extend Lightning and Thunder models to the multi-person domain, so that developers can support applications with multiple people in the camera field-of-view.\nWe also have plans to speed up the TensorFlow.js backends to make model execution even faster. This is achieved through repeated benchmarking and backend optimization.\nAcknowledgements\nWe would like to acknowledge the other contributors to MoveNet: Yu-Hui Chen, Ard Oerlemans, Francois Belletti, Andrew Bunner, and Vijay Sundaram, along with those involved with the TensorFlow.js pose-detection API: Ping Yu, Sandeep Gupta, Jason Mayes, and Masoud Charkhabi.",
    "link": "https://blog.tensorflow.org/2021/05/next-generation-pose-detection-with-movenet-and-tensorflowjs.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-25aGTL-RTnY/YJ29jgiiNHI/AAAAAAAAEMM/9qJC_xqlUKo4To9xyumqKmrqKr-vVFXzgCLcBGAsYHQ/s0/three_pane_aligned%2B%25281%2529.gif",
      "https://1.bp.blogspot.com/-25aGTL-RTnY/YJ29jgiiNHI/AAAAAAAAEMM/9qJC_xqlUKo4To9xyumqKmrqKr-vVFXzgCLcBGAsYHQ/s0/three_pane_aligned%2B%25281%2529.gif",
      "https://1.bp.blogspot.com/-ULQAErOwRtM/YJ2_N8IeT-I/AAAAAAAAEMU/OiFrIja4uf4HTV_PKVsPPdPozkzZNNc1gCLcBGAsYHQ/s0/final_609dbe3905f85d00a146ec24_883087.gif",
      "https://1.bp.blogspot.com/-MEmEDQEB-8A/YJ2_-3ljnvI/AAAAAAAAEMc/HFHFjLeBHkgF5YmGORkkXUJjC_obbbfhgCLcBGAsYHQ/s0/IncludeHealth_GooglePromoShort.gif",
      "https://1.bp.blogspot.com/-GvLNT9SFGJ8/YJ7qxvGTsDI/AAAAAAAAENE/J-nRn34k48UbDpMhPvjX1RG66WX1IsppwCLcBGAsYHQ/s0/MoveNetArchitecture%2B%25281%2529.png",
      "https://1.bp.blogspot.com/-Qq7pj5W-zeg/YJ6xVtNDQjI/AAAAAAAAEMs/pbxSPz0jZWE3Koq3AoZDs2CE7urcxfghwCLcBGAsYHQ/s0/MoveNetPostProcessing.png",
      "https://1.bp.blogspot.com/-z7eLvmyTc6Y/YJ6y4qWlW0I/AAAAAAAAEM0/GhsdUgw8dQk8zF1G4rXukd2PlCtGJ5PHACLcBGAsYHQ/s0/anastasia_labeled.jpeg"
    ],
    "time": "2023/12/07 00:58:43"
  },
  {
    "title": "Custom object detection in the browser using TensorFlow.js",
    "content": "A guest post by Hugo Zanini, Machine Learning Engineer\nObject detection is the task of detecting where in an image an object is located and classifying every object of interest in a given image. In computer vision, this technique is used in applications such as picture retrieval, security cameras, and autonomous vehicles.\nOne of the most famous families of Deep Convolutional Neural Networks (DNN) for object detection is the YOLO (You Only Look Once).\nIn this post, we are going to develop an end-to-end solution using TensorFlow to train a custom object-detection model in Python, then put it into production, and run real-time inferences in the browser through TensorFlow.js.\nThis post is going to be divided into four steps, as follows:\nObject detection pipeline\nPrepare the data\nThe first step to train a great model is to have good quality data. When developing this project, I did not find a suitable (and small enough) object detection dataset, so I decided to create my own.\nI looked around and saw a Kangaroo sign that I have in my bedroom\u200a\u2014\u200aa souvenir that I bought to remember my Aussie days. So I decided to build a Kangaroo detector.\nTo build my dataset, I downloaded 350 kangaroo images from an image search for kangaroos and labeled all of them by hand using the LabelImg application. As we can have more than one animal per image, the process resulted in 520 labeled kangaroos.\nLabelling example\nIn that case, I chose just one class, but the software can be used to annotate multiple classes as well. It\u2019s going to generate an XML file per image (Pascal VOC format) that contains all annotations and bounding boxes.\n<annotation>\n    <folder>images</folder>\n    <filename>kangaroo-0.jpg</filename>\n    <path>/home/hugo/Documents/projects/tfjs/dataset/images/kangaroo-0.jpg</path>\n  <source>\n    <database>Unknown</database>\n  </source>\n  <size>\n    <width>3872</width>\n    <height>2592</height>\n    <depth>3</depth>\n  </size>\n  <segmented>0</segmented>\n  <object>\n    <name>kangaroo</name>\n    <pose>Unspecified</pose>\n    <truncated>0</truncated>\n    <difficult>0</difficult>\n    <bndbox>\n      <xmin>60</xmin>\n      <ymin>367</ymin>\n      <xmax>2872</xmax>\n      <ymax>2399</ymax>\n    </bndbox>\n  </object>\n</annotation> \nXML Annotation example\nTo facilitate the conversion to TF.record format (below), I then converted the XML of the program above into two CSV files containing the data already split in train and test (80%-20%). These files have 9 columns:\nfilename: Image name\nwidth: Image width\nheight: Image height\nclass: Image class (kangaroo)\nxmin: Minimum bounding box x coordinate value\nymin: Minimum bounding box y coordinate value\nxmax: Maximum value of the x coordinate of the bounding box\nymax: Maximum value of the y coordinate of the bounding box\nsource: Image source\nUsing LabelImg makes it easy to create your own dataset, but feel free to use my kangaroo dataset, I\u2019ve uploaded it on Kaggle:\nKangaroo Dataset\nTraining the model\nWith a good dataset, it\u2019s time to think about the model.TensorFlow 2 provides an Object Detection API that makes it easy to construct, train, and deploy object detection models. In this project, we\u2019re going to use this API and train the model using a Google Colaboratory Notebook. The remainder of this section explains how to set up the environment, the model selection, and training. If you want to jump straight to the Colab Notebook, click here.\nSetting up the environment\nCreate a new Google Colab notebook and select a GPU as hardware accelerator:\n Runtime > Change runtime type > Hardware accelerator: GPU \nClone, install, and test the TensorFlow Object Detection API:\nViewer requires iframe.\nview raw\ninstall.ipynb hosted with \u2764 by GitHub\nGetting and processing the data\nAs mentioned before, the model is going to be trained using the Kangaroo dataset on Kaggle. If you want to use it as well, it\u2019s necessary to create a user, go into the account section of Kaggle, and get an API Token:\nGetting an API Token\nThen, you\u2019re ready to download the data:\nViewer requires iframe.\nview raw\ngetting-data.ipynb hosted with \u2764 by GitHub\nNow, it\u2019s necessary to create a labelmap file to define the classes that are going to be used. Kangaroo is the only one, so right-click in the File section on Google Colab and create a New file named labelmap.pbtxt as follows:\n item {\n    name: \"kangaroo\"\n    id: 1\n}\nThe last step is to convert the data into a sequence of binary records so that they can be fed into Tensorflow\u2019s object detection API. To do so, transform the data into the TFRecord format using the generate_tf_records.py script available in the Kangaroo Dataset:\nViewer requires iframe.\nview raw\ngenerate_tf_records.ipynb hosted with \u2764 by GitHub\nChoosing the model\nWe\u2019re ready to choose the model that\u2019s going to be the Kangaroo Detector. TensorFlow 2 provides 40 pre-trained detection models on the COCO 2017 Dataset. This collection is the TensorFlow 2 Detection Model Zoo and can be accessed here.\nEvery model has a Speed, Mean Average Precision(mAP) and Output. Generally, a higher mAP implies a lower speed, but as this project is based on a one-class object detection problem, the faster model (SSD MobileNet v2 320x320) should be enough.\nBesides the Model Zoo, TensorFlow provides a Models Configs Repository as well. There, it\u2019s possible to get the configuration file that has to be modified before the training. Let\u2019s download the files:\nViewer requires iframe.\nview raw\ngetting-weights-and-config.ipynb hosted with \u2764 by GitHub\nConfigure training\nAs mentioned before, the downloaded weights were pre-trained on the COCO 2017 Dataset, but the focus here is to train the model to recognize one class so these weights are going to be used only to initialize the network \u2014 this technique is known as transfer learning, and it\u2019s commonly used to speed up the learning process.\nFrom now, what has to be done is to set up the mobilenet_v2.config file, and start the training. I highly recommend reading the MobileNetV2 paper (Sandler, Mark, et al. - 2018) to get the gist of the architecture.\nChoosing the best hyperparameters is a task that requires some experimentation. As the resources are limited in the Google Colab, I am going to use the same batch size as the paper, set a number of steps to get a reasonably low loss, and leave all the other values as default. If you want to try something more sophisticated to find the hyperparameters, I recommend Keras Tuner - an easy-to-use framework that applies Bayesian Optimization, Hyperband, and Random Search algorithms.\nViewer requires iframe.\nview raw\nsetting-paramerts.ipynb hosted with \u2764 by GitHub\nWith the parameters set, start the training:\nViewer requires iframe.\nview raw\ntraining.ipynb hosted with \u2764 by GitHub\nTo identify how well the training is going, we use the loss value. Loss is a number indicating how bad the model\u2019s prediction was on the training samples. If the model\u2019s prediction is perfect, the loss is zero; otherwise, the loss is greater. The goal of training a model is to find a set of weights and biases that have low loss, on average, across all examples (Descending into ML: Training and Loss | Machine Learning Crash Course).\nFrom the logs, it\u2019s possible to see a downward trend in the values so we say that \u201cThe model is converging\u201d. In the next section, we\u2019re going to plot these values for all training steps and the trend will be even clearer.\nThe model took around 4h to train (with Colab GPU), but by setting different parameters, you can make the process faster or slower. Everything depends on the number of classes you are using and your Precision/Recall target. A highly accurate network that recognizes multiple classes will take more steps and require more detailed parameters tuning.\nValidate the model\nNow let\u2019s evaluate the trained model using the test data:\nViewer requires iframe.\nview raw\nvalidation.ipynb hosted with \u2764 by GitHub\nThe evaluation was done in 89 images and provides three metrics based on the COCO detection evaluation metrics: Precision, Recall and Loss.\nThe Recall measures how good the model is at hitting the positive class, That is, from the positive samples, how many did the algorithm get right?\nRecall\nPrecision defines how much you can rely on the positive class prediction: From the samples that the model said were positive, how many actually are?\nPrecision\nSetting a practical example: Imagine we have an image containing 10 kangaroos, our model returned 5 detections, being 3 real kangaroos (TP = 3, FN =7) and 2 wrong detections (FP = 2). In that case, we have a 30% recall (the model detected 3 out of 10 kangaroos in the image) and a 60% precision (from the 5 detections, 3 were correct).\nThe precision and recall were divided by Intersection over Union (IoU) thresholds. The IoU is defined as the area of the intersection divided by the area of the union of a predicted bounding box (B) to a ground-truth box (B)(Zeng, N. - 2018):\nIntersection over Union\nFor simplicity, it\u2019s possible to consider that the IoU thresholds are used to determine whether a detection is a true positive(TP), a false positive(FP) or a false negative (FN). See an example below:\nIoU threshold examples\nWith these concepts in mind, we can analyze some of the metrics we got from the evaluation. From the TensorFlow 2 Detection Model Zoo, the SSD MobileNet v2 320x320 has an mAP of 0.202. Our model presented the following average precisions (AP) for different IoUs:\n \nAP@[IoU=0.50:0.95 | area=all | maxDets=100] = 0.222\nAP@[IoU=0.50      | area=all | maxDets=100] = 0.405\nAP@[IoU=0.75      | area=all | maxDets=100] = 0.221\nThat\u2019s pretty good! And we can compare the obtained APs with the SSD MobileNet v2 320x320 mAP as from the COCO Dataset documentation:\nWe make no distinction between AP and mAP (and likewise AR and mAR) and assume the difference is clear from context.\nThe Average Recall(AR) was split by the max number of detection per image (1, 10, 100). When we have just one kangaroo per image, the recall is around 30% while when we have up to 100 kangaroos it is around 51%. These values are not that good but are reasonable for the kind of problem we\u2019re trying to solve.\n \n(AR)@[ IoU=0.50:0.95 | area=all | maxDets=  1] = 0.293\n(AR)@[ IoU=0.50:0.95 | area=all | maxDets= 10] = 0.414\n(AR)@[ IoU=0.50:0.95 | area=all | maxDets=100] = 0.514\nThe Loss analysis is very straightforward, we\u2019ve got 4 values:\n \nINFO:tensorflow: + Loss/localization_loss: 0.345804\nINFO:tensorflow: + Loss/classification_loss: 1.496982\nINFO:tensorflow: + Loss/regularization_loss: 0.130125\nINFO:tensorflow: + Loss/total_loss: 1.972911\nThe localization loss computes the difference between the predicted bounding boxes and the labeled ones. The classification loss indicates whether the bounding box class matches with the predicted class. The regularization loss is generated by the network\u2019s regularization function and helps to drive the optimization algorithm in the right direction. The last term is the total loss and is the sum of three previous ones.\nTensorflow provides a tool to visualize all these metrics in an easy way. It\u2019s called TensorBoard and can be initialized by the following command:\n \n%load_ext tensorboard\n%tensorboard --logdir '/content/training/'\nThis is going to be shown, and you can explore all training and evaluation metrics.\nTensorboard \u2014 Loss\nIn the tab IMAGES, it\u2019s possible to find some comparisons between the predictions and the ground truth side by side. A very interesting resource to explore during the validation process as well.\nTensorboard \u2014 Testing images\nExporting the model\nNow that the training is validated, it\u2019s time to export the model. We\u2019re going to convert the training checkpoints to a protobuf (pb) file. This file is going to have the graph definition and the weights of the model.\nViewer requires iframe.\nview raw\nexporting-pb.ipynb hosted with \u2764 by GitHub\nAs we\u2019re going to deploy the model using TensorFlow.js and Google Colab has a maximum lifetime limit of 12 hours, let\u2019s download the trained weights and save them locally. When running the command files.download('/content/saved_model.zip\"), the colab will prompt the file automatically.\nViewer requires iframe.\nview raw\ndownloading-pb.ipynb hosted with \u2764 by GitHub\nIf you want to check if the model was saved properly, load, and test it. I\u2019ve created some functions to make this process easier so feel free to clone the inferenceutils.py file from my GitHub to test some images.\nViewer requires iframe.\nview raw\ntesting.ipynb hosted with \u2764 by GitHub\nViewer requires iframe.\nview raw\ntesting.ipynb hosted with \u2764 by GitHub\nEverything is working well, so we\u2019re ready to put the model in production.\nDeploying the model\nThe model is going to be deployed in a way that anyone can open a PC or mobile camera and perform inferences in real-time through a web browser. To do that, we\u2019re going to convert the saved model to the Tensorflow.js layers format, load the model in a javascript application and make everything available on Glitch.\nConverting the model\nAt this point, you should have something similar to this structure saved locally:\n \n\u251c\u2500\u2500 inference-graph\n\u2502 \u251c\u2500\u2500 saved_model\n\u2502 \u2502 \u251c\u2500\u2500 assets\n\u2502 \u2502 \u251c\u2500\u2500 saved_model.pb\n\u2502 \u2502 \u251c\u2500\u2500 variables\n\u2502 \u2502 \u251c\u2500\u2500 variables.data-00000-of-00001\n\u2502 \u2502 \u2514\u2500\u2500 variables.index\nBefore we start, let's create an isolated Python environment to work in an empty workspace and avoid any library conflict. Install virtualenv and then open a terminal in the inference-graph folder and create and activate a new virtual environment:\n \nvirtualenv -p python3 venv\nsource venv/bin/activate\nInstall the TensorFlow.js converter:\n  pip install tensorflowjs[wizard]\nStart the conversion wizard:\n  tensorflowjs_wizard\nNow, the tool will guide you through the conversion, providing explanations for each choice you need to make. The image below shows all the choices that were made to convert the model. Most of them are the standard ones, but options like the shard sizes and compression can be changed according to your needs.\nTo enable the browser to cache the weights automatically, it\u2019s recommended to split them into shard files of around 4MB. To guarantee that the conversion is going to work, don\u2019t skip the op validation as well, not all TensorFlow operations are supported so some models can be incompatible with TensorFlow.js \u2014 See this list for which ops are currently supported.\nModel conversion using Tensorflow.js Converter (Full resolution image here\nIf everything worked well, you\u2019re going to have the model converted to the Tensorflow.js layers format in the web_modeldirectory. The folder contains a model.json file and a set of sharded weights files in a binary format. The model.json has both the model topology (aka \"architecture\" or \"graph\": a description of the layers and how they are connected) and a manifest of the weight files (Lin, Tsung-Yi, et al).\n  \n\u2514 web_model\n  \u251c\u2500\u2500 group1-shard1of5.bin\n  \u251c\u2500\u2500 group1-shard2of5.bin\n  \u251c\u2500\u2500 group1-shard3of5.bin\n  \u251c\u2500\u2500 group1-shard4of5.bin\n  \u251c\u2500\u2500 group1-shard5of5.bin\n  \u2514\u2500\u2500 model.json\nConfiguring the application\nThe model is ready to be loaded in javascript. I\u2019ve created an application to perform inferences directly from the browser. Let\u2019s clone the repository to figure out how to use the converted model in real-time. This is the project structure:\n \n\u251c\u2500\u2500 models\n\u2502   \u2514\u2500\u2500 kangaroo-detector\n\u2502       \u251c\u2500\u2500 group1-shard1of5.bin\n\u2502       \u251c\u2500\u2500 group1-shard2of5.bin\n\u2502       \u251c\u2500\u2500 group1-shard3of5.bin\n\u2502       \u251c\u2500\u2500 group1-shard4of5.bin\n\u2502       \u251c\u2500\u2500 group1-shard5of5.bin\n\u2502       \u2514\u2500\u2500 model.json\n\u251c\u2500\u2500 package.json\n\u251c\u2500\u2500 package-lock.json\n\u251c\u2500\u2500 public\n\u2502   \u2514\u2500\u2500 index.html\n\u251c\u2500\u2500 README.MD\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 index.js\n    \u2514\u2500\u2500 styles.css\nFor the sake of simplicity, I already provide a converted kangaroo-detector model in the models folder. However, let\u2019s put the web_model generated in the previous section in the models folder and test it.\nThe first thing to do is to define how the model is going to be loaded in the function load_model (lines 10\u201315 in the file src>index.js). There are two choices.\nThe first option is to create an HTTP server locally that will make the model available in a URL allowing requests and be treated as a REST API. When loading the model, TensorFlow.js will do the following requests:\n \nGET /model.json\nGET /group1-shard1of5.bin\nGET /group1-shard2of5.bin\nGET /group1-shard3of5.bin\nGET /group1-shardo4f5.bin\nGET /group1-shardo5f5.bin\nIf you choose this option, define the load_model function as follows:\n  async function load_model() {\n    // It's possible to load the model locally or from a repo\n    // You can choose whatever IP and PORT you want in the \"http://127.0.0.1:8080/model.json\"     just set it before in your https server\n    const model = await loadGraphModel(\"http://127.0.0.1:8080/model.json\");\n    //const model = await loadGraphModel(\"https://raw.githubusercontent.com/hugozanini/TFJS-object-detection/master/models/web_model/model.json\");\n    return model;\n}\nThen install the http-server:\n  npm install http-server -g\nGo to models > web_model and run the command below to make the model available at http://127.0.0.1:8080 . This a good choice when you want to keep the model weights in a safe place and control who can request inferences to it. The -c1 parameter is added to disable caching, and the --cors flag enables cross origin resource sharing allowing the hosted files to be used by the client side JavaScript for a given domain.\n  http-server -c1 --cors .\nAlternatively you can upload the model files somewhere, in my case, I chose my own Github repo and referenced to the model.json URL in the load_model function:\n \n\nasync function load_model() {\n    // It's possible to load the model locally or from a repo\n    //const model = await loadGraphModel(\"http://127.0.0.1:8080/model.json\");\n    const model = await loadGraphModel(\"https://raw.githubusercontent.com/hugozanini/TFJS-object-detection/master/models/web_model/model.json\");\n    return model;\n}\nThis is a good option because it gives more flexibility to the application and makes it easier to run on some platform as Glitch.\nRunning locally\nTo run the app locally, install the required packages:\n npm install\n And start:\n npm start\nThe application is going to run at http://localhost:3000 and you should see something similar to this:\nApplication running locally\nThe model takes from 1 to 2 seconds to load and, after that, you can show kangaroos images to the camera and the application is going to draw bounding boxes around them.\nPublishing in Glitch\nGlitch is a simple tool for creating web apps where we can upload the code and make the application available for everyone on the web. Uploading the model files in a GitHub repo and referencing to them in the load_model function, we can simply log into Glitch, click on New project > Import from Github and select the app repository.\nWait some minutes to install the packages and your app will be available in a public URL. Click on Show > In a new window and a tab will be open. Copy this URL and past it in any web browser (PC or Mobile) and your object detection will be ready to run. See some examples in the video below:\nRunning the model on different devices\nFirst, I did a test showing a kangaroo sign to verify the robustness of the application. It showed that the model is focusing specifically on the kangaroo features and did not specialize in irrelevant characteristics that were present in many images, such as pale colors or shrubs.\nThen, I opened the app on my mobile and showed some images from the test set. The model runs smoothly and identifies most of the kangaroos. If you want to test my live app, it is available here (glitch takes some minutes to wake up).\nBesides the accuracy, an interesting part of these experiments is the inference time \u2014 everything runs in real-time in the browser via JavaScript. Good object detection models running in the browser and using few computational resources is a must in many applications, mostly in industry. Putting the Machine Learning model on the client-side means cost reduction and safer applications as user privacy is preserved as there is no need to send the information to any server to perform the inference.\nNext steps\nObject detection in the browser can solve a lot of real-world problems and I hope this article will serve as a basis for new projects involving Computer Vision, Python, TensorFlow and Javascript.\nAs the next steps, I\u2019d like to make more detailed training experiments. Due to the lack of resources, I could not try many different parameters and I\u2019m sure that there is a lot of room for improvements in the model.\nI\u2019m more focused on the models' training, but I\u2019d like to see a better user interface for the app. If someone is interested in contributing to the project, feel free to create a pull request in the project repo. It will be nice to make a more user-friendly application.\nIf you have any questions or suggestions you can reach me out on Linkedin. Thanks for reading!",
    "link": "https://blog.tensorflow.org/2021/01/custom-object-detection-in-browser.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-qlxuThfHIhw/YAmrJXH1_-I/AAAAAAAAD6U/2vjeCED46c4_O2tHTKig2Xra82cH5i4bQCLcBGAsYHQ/s0/unnamed%2B%25289%2529.png",
      "https://1.bp.blogspot.com/-7y8ob8chvCg/YAmrmpgW1nI/AAAAAAAAD6c/6JGnY3y3sqAqWD6PkADslbzP17nVc4edwCLcBGAsYHQ/s0/pasted%2Bimage%2B0%2B%252824%2529.png",
      "https://1.bp.blogspot.com/-NiurDXWU2GU/YAmsXnM9T4I/AAAAAAAAD6o/VbGZ9UnZIEYY5qx1o-lMNRTFOLwHAl0_wCLcBGAsYHQ/s0/pasted%2Bimage%2B0%2B%252825%2529.png",
      "https://1.bp.blogspot.com/-zlbh9xkdllw/YAmsvZi0-6I/AAAAAAAAD6w/stOX5Dpnr0Ak84gPJer2WmM9n5QQoIAMACLcBGAsYHQ/s0/pasted%2Bimage%2B0%2B%252826%2529.png",
      "https://1.bp.blogspot.com/-3i4-JMX-R4g/YAmtMnuhGcI/AAAAAAAAD64/BxbhwXZJdjQlOrBEpBENUZ8YTVEeXK7iwCLcBGAsYHQ/s0/unnamed%2B%252810%2529.png",
      "https://1.bp.blogspot.com/-kwzwmTQ_EcA/YAmtkqdkH_I/AAAAAAAAD7A/1C02gLiowXUHJ2a_6LVr_iXTxbj303TaACLcBGAsYHQ/s0/pasted%2Bimage%2B0%2B%252827%2529.png",
      "https://1.bp.blogspot.com/-rLTXED3wZQw/YAmt4_alIJI/AAAAAAAAD7I/Jhr3OAqlgKYIgOkP_XD8PxCqBgt4pJOsACLcBGAsYHQ/s0/pasted%2Bimage%2B0%2B%252828%2529.png",
      "https://1.bp.blogspot.com/-i-gl5zGzsz4/YAm7ewnpBzI/AAAAAAAAD7U/XZIGxoTCPIU4qn-jH1X1FK6vVUwq1PNdACLcBGAsYHQ/s0/pasted%2Bimage%2B0%2B%252829%2529.png",
      "https://1.bp.blogspot.com/-Ixxh5qk5mdc/YAm7yUodOqI/AAAAAAAAD7c/etbACCS0L9ICPXPOcWO5XxbMxLPPEilZwCLcBGAsYHQ/s0/pasted%2Bimage%2B0%2B%252830%2529.png",
      "https://1.bp.blogspot.com/-Z0wREv9YFGs/YAm8eJchmaI/AAAAAAAAD7k/E_emL_b6B0cX8vQ9z6HsOVvSoo9dYx74ACLcBGAsYHQ/s0/pasted%2Bimage%2B0%2B%252832%2529.png",
      "https://1.bp.blogspot.com/-r0wT1rnuHts/YAm9FQnYGwI/AAAAAAAAD7s/VKQCrSV1ocIDP7wu44epNMT3UQJ5Vp3CwCLcBGAsYHQ/s0/unnamed%2B%252811%2529.png"
    ],
    "time": "2023/12/07 00:58:43"
  },
  {
    "title": "InSpace: A new video conferencing platform that uses TensorFlow.js for toxicity filters in chat",
    "content": "A guest post by Narine Hall, Assistant Professor at Champlain College, CEO of InSpace\nInSpace is a communication and virtual learning platform that gives people the ability to interact, collaborate, and educate in familiar physical ways, but in a virtual space. InSpace is built by educators for educators, putting education at the center of the platform.\nInSpace is designed to mirror the fluid, personal, and interactive nature of a real classroom. It allows participants to break free of \u201cBrady Bunch\u201d boxes in existing conference solutions to create a fun, natural, and engaging environment that fosters interaction and collaboration.\nEach person is represented in a video circle that can freely move around the space. When people are next to each other, they can hear and engage in conversation, and as they move away, the audio fades, allowing them to find new conversations.\nAs participants zoom out, they can see the entire space, which provides visual social cues. People can seamlessly switch from class discussions to private conversations or group/team-based work, similar to the format of a lab or classroom.\nTeachers can speak to everyone when needed, move between individual students and groups for more private discussion, and place groups of students in audio-isolated rooms for collaboration while still belonging to one virtual space.\nBeing a collaboration platform, a fundamental InSpace feature is chat. From day one, we wanted to provide a mechanism to help warn users from sending and receiving toxic messages. For example, a teacher in a classroom setting with young people may want a way to prevent students from typing inappropriate comments. Or, a moderator of a large discussion may want a way to reduce inappropriate spam. Or individual users may want to filter them out on their own.\nA simple way to identify toxic comments would be to check for the presence of a list of words, including profanity. Moving a step beyond this, we did not want to identify toxic messages just by words contained in the message, we also wanted to consider the context. So, we decided to use machine learning to accomplish that goal.\nAfter some research, we found a pre-trained model for toxicity detection in TensorFlow.js that could be easily integrated into our platform. Importantly, this model runs entirely in the browser, which means that we can warn users against sending toxic comments without their messages ever being stored or processed by a server.\nPerformance wise, we found that running the toxicity process in a browser's main thread would be detrimental to the user experience. We decided a good approach was to use the Web Workers API to separate message toxicity detection from the main application so the processes are independent and non-blocking.\nWeb Workers connect to the main application by sending and receiving messages, in which you can wrap your data. Whenever a user sends a message, it is automatically added to a so-called queue and is sent from the main application to the web worker. When the web worker receives the message from the main app, it starts classification of the message, and when the output is ready, it sends the results back to the main application. Based on the results from the web worker, the main app either sends the message to all participants or warns the user that it is toxic.\nBelow is the pseudocode for the main application, where we initialize the web worker by providing its path as an argument, then set the callback that will be called each time the worker sends a message, and also we declare the callback that will be called when the user submits a message.\n// main application\n// initializing the web worker\nconst toxicityFilter = new Worker('toxicity-filter.worker.js'));\n// now we need to set the callback which will process the data from the worker\nworker.onMessage = ({ data: { message, isToxic } }) => {\n if (isToxic) {\n   markAsToxic(message);\n } else {\n   sendToAll(message);\n }\n}\nWhen the user sends the message, we pass it to the web worker:\nonMessageSubmit = message => {\n worker.postMessage(message);\n addToQueue(message);\n}\nAfter the worker is initialized, it starts listening to the data messages from the main app, and handling them using the declared onmessage callback, which then sends a message back to the main app.\n// toxicity-filter worker\n// here we import dependencies\nimportScripts(\n // the main library to run Tenser Flow in the browser\n 'https://cdn.jsdelivr.net/npm/@tensorflow/tfjs',\n // trained models for toxicity detection\n 'https://cdn.jsdelivr.net/npm/@tensorflow-models/toxicity',\n);\n// threshold point for the decision\nconst threshold = 0.9;\n// the main model promise which would be used to classify the message\nconst modelPromise = toxicity.load(threshold);\n// registered callback to run when the main app sends the data message\nonmessage = ({ data: message }) => {\n modelPromise.then(model => {\n   model.classify([message.body]).then(predictions => {\n     // as we want to check the toxicity for all labels,\n     // `predictions` will contain the results for all 7 labels\n     // so we check, whether there is a match for any of them\n     const isToxic = predictions.some(prediction => prediction.results[0].match);\n     // here we send the data message back to the main app with the results\n     postMessage({ message, isToxic });\n   });\n });\n};\nAs you can see, the toxicity detector is straightforward to integrate the package with an app, and does not require significant changes to existing architecture. The main application only needs a small \"connector,\" and the logic of the filter is written in a separate file.\nTo learn more about InSpace visit https://inspace.chat.",
    "link": "https://blog.tensorflow.org/2020/12/inspace-new-video-conferencing-platform-uses-tensorflowjs-for-toxicity-filters-in-chat.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-X7RyNbxGZww/X9q_-cChVvI/AAAAAAAAD5M/YEY4CtUIaf4FwDmNYV2hfSzpcr4sD1F5ACLcBGAsYHQ/s0/toxic.gif",
      "https://1.bp.blogspot.com/-X7RyNbxGZww/X9q_-cChVvI/AAAAAAAAD5M/YEY4CtUIaf4FwDmNYV2hfSzpcr4sD1F5ACLcBGAsYHQ/s0/toxic.gif",
      "https://1.bp.blogspot.com/-tR_qrAMH3Nw/X9rAUfxRU5I/AAAAAAAAD5U/5R1idXopL0YiU7oGj9U1sYbTXPF7EkKPwCLcBGAsYHQ/s0/InSpace%2BTF%2BBlog%2Bpost.png"
    ],
    "time": "2023/12/07 00:58:43"
  },
  {
    "title": "Build sound classification models for mobile apps with Teachable Machine and TFLite",
    "content": "Posted by Khanh LeViet, TensorFlow Developer Advocate\n\nSound classification is a machine learning task where you input some sound to a machine learning model to categorize it into predefined categories such as dog barking, car horn and so on. There are already many applications of sound classification, including detecting illegal deforestation activities, or detecting sound of humpback whales for better understanding about their natural behaviors.\nWe are excited to announce that Teachable Machine now allows you to train your own sound classification model and export it in the TensorFlow Lite (TFLite) format. Then you can integrate the TFLite model to your mobile applications or your IoT devices. This is an easy way to quickly get up and running with sound classification, and you can then explore building production models in Python and exporting them to TFLite as a next step.\nModel architecture\nThe model that Teachable Machine uses to classify 1-second audio samples is a small convolutional neural network. As the diagram above illustrates, the model receives a spectrogram (2D time-frequency representation of sound obtained through Fourier transform). It first processes the spectrogram with successive layers of 2D convolution (Conv2D) and max pooling layers. The model ends in a number of dense (fully-connected) layers, which are interleaved with dropout layers for the purpose of reducing overfitting during training. The final output of the model is an array of probability scores, one for each class of sound the model is trained to recognize.\nYou can find a tutorial to train your own sound classifications models using this approach in Python here.\nTrain a model using your own dataset\nThere are two ways to train a sound classification model using your own dataset:\nSimple way: Use Teachable Machine to collect training data and train the model all within your browser without writing a single line of code. This approach is useful for those who want to build a prototype quickly and interactively.\nRobust way: Record sounds to use as your training dataset in advance then use Python to train and carefully evaluate your model. Of course, his approach is also more automated and repeatable than the simple way.\nTrain a model using Teachable Machine\nTeachable Machine is a GUI tool that allows you to create training dataset and train several types of machine learning models, including image classification, pose classification and sound classification. Teachable Machine uses TensorFlow.js under the hood to train your machine learning model. You can export the trained models in TensorFlow.js format to use in web browsers, or export in TensorFlow Lite format to use in mobile applications or IoT devices.\nHere are the steps to train your models:\nGo to Teachable Machine website\nCreate an audio project\nRecord some sound clips for each category that you want to recognize. You need only 8 seconds of sound for each category.\nStart training. Once it has finished, you can test your model on live audio feed.\nExport the model in TFLite format.\nTrain a model using Python\nIf you have a large training dataset with several hours of sound recording and or than a dozen of categories, then training a sound classification on a web browser will likely take a lot of time. In that case, you can collect the training dataset in advance, convert them to the WAV format and use this Colab notebook (which includes steps to convert the model to TFLite format) to train your sound classification. Google Colab offers a free GPU so that you can significantly speed up your model training.\nDeploy the model to Android with TensorFlow Lite\nOnce you have trained your TensorFlow Lite sound classification model, you can just put it in this Android sample app to try it out. Just follow these steps:\nClone the sample app from GitHub:\ngit clone https://github.com/tensorflow/examples.git\nImport the sound classification Android app into Android Studio. You can find it in the lite/examples/sound_classification/android folder.\nAdd your model (both the soundclassifier.tflite and labels.txt) into the src/main/assets folder replacing the example model that is already there.\nBuild the app and deploy it on an Android device. Now you can classify sound in real time!\nTo integrate the model into your own app, you can copy the SoundClassifier.kt class from the sample app and the TFLite model you have trained to your app. Then you can use the model as below:\n1. Initialize a `SoundClassifier` instance from your `Activity` or `Fragment` class.\nvar soundClassifier: SoundClassifier\nsoundClassifier = SoundClassifier(context).also {\n    it.lifecycleOwner = context\n}\n\n2. Start capturing live audio from the device's microphone and classify in real time:\nsoundClassifier.start()\n3. Receive classification results in real time as a map of human-readable class names and probabilities of the current sound belonging to each particular category.\nlet labelName = soundClassifier.labelList[0] // e.g. \"Clap\"\nsoundClassifier.probabilities.observe(this) { resultMap ->\n    let probability = result[labelName] // e.g. 0.7\n}\nWhat\u2019s next\nWe are working on an iOS version of the sample app that will be released in a few weeks. We will also extend TensorFlow Lite Model Maker to allow easy training of sound classification in Python. Stay tuned!\nAcknowledgements\nThis project is a joint effort between multiple teams inside Google. Special thanks to:\nGoogle Research: Shanqing Cai, Lisie Lillianfeld\nTensorFlow team: Tian Lin\nTeachable Machine team: Gautam Bose, Jonas Jongejan\nAndroid team: Saryong Kang, Daniel Galpin, Jean-Michel Trivi, Don Turner",
    "link": "https://blog.tensorflow.org/2020/12/build-sound-classification-models-for-mobile-apps-with-teachable-machine-and-tflite.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-Bd4VkTbmkK4/X8fIFvEWgdI/AAAAAAAAD00/IHaAL3m0js8PMHr-5KNc2g6rYxetfb9zwCLcBGAsYHQ/s0/Blog%2BHero%2BImage%2B%25281%2529.png",
      "https://1.bp.blogspot.com/-AFFWUD2SHfs/X77rV9stPbI/AAAAAAAADx0/4u5Xtg5OL2EjEHkcJb4uXX-jIazDD3OjwCLcBGAsYHQ/s0/image%2B1.png",
      "https://1.bp.blogspot.com/-hkeZHnV5Dic/X77sQp9XOBI/AAAAAAAADx8/SD4h7diGiK47VZcfCsd6u_q38su5OxeEgCLcBGAsYHQ/s0/image%2B2.png",
      "https://1.bp.blogspot.com/-8yrVDpr74z8/X77ssAlGyHI/AAAAAAAADyE/Hladt404RDwFLPTNPZBMi270N-k1TqjuACLcBGAsYHQ/s0/image%2B3.png"
    ],
    "time": "2023/12/07 00:58:43"
  },
  {
    "title": "A new YouTube show: TensorFlow.js Community Show & Tell",
    "content": "Posted by Jason Mayes, Developer Relations Engineer for TensorFlow.js\n\n\nThe TensorFlow YouTube channel has a new show called \"TensorFlow.js Community Show & Tell.\" In this program, we highlight amazing tech demos from the TensorFlow.js community every quarter. Our next show will be on 11th December 9AM PT over on the TensorFlow YouTube channel, but if you missed the previous ones, you can find past episodes on this playlist.\nAbout the show\nDo you love great tech demos that push the boundaries of what is possible for a given industry? If that sounds like you and you\u2019re looking for fresh inspiration, along with insights from the engineers themselves, then this may be the YouTube show for you.\nAfter hacking with many wonderful folk in the TensorFlow.js community it became clear to us that the creativity and the work you were producing was simply incredible. For that reason, we have put together a brand new format, known as the TensorFlow Show & Tell to showcase top projects that are being made and give developers a platform to share their work. With many subscribers who are as passionate about machine learning as we are, we figured this was a great way to do that and connect great minds.\nIf you missed our latest show you can check our most recent broadcast here:\nWe have seen a whole bunch of amazing demos on the first 3 shows using machine learning in truly novel ways. From making music from thin air using your hands which is then web connected to a Tesla coil (yes, that actually happened), to combining machine learning models with mind blowing mixed reality and 3D graphics, we have had a good variety of presenters from all around the world.\nWho has presented so far?\nWe\u2019ve had 25 presenters, and there are more in the making!\n1st Show: Rogerio Chaves (Netherlands), Max Bittker (USA), Junya Ishihara (Japan), Ben Farrell (USA), Lizzie Siegle (USA), Manish Raj (India), Yiwen Lin (UK), Jimmy (Thailand)\n2nd Show: Cyril Diagne (France), John Cohn (USA), Va Barbosa (USA), FollowTheDarkside (Japan), Jaume Sanchez (UK), Olesya Chernyavskaya (Russia), Alexandre Devaux (France), Amruta (India), Shan Huan (China).\n3rd Show: Gant Laborde (USA), Hugo Zanini (Brazil), Charlie Gerard (Amsterdam), Shivay Lamba (India), Anders Jessen (Denmark), Benson Ruan (Australia), Cristina Maillo (Spain), James Seo (USA).\n\nHow can I get on the show?\nIf you have made something using TensorFlow.js simply use the #MadeWithTFJS hashtag over on Twitter or LinkedIn with a post demonstrating your creation and link to try it out. We will select our top picks each quarter to be featured in the show and reach out to you directly if you are selected. You can view existing submissions for Twitter as an example.\nI missed an episode - where can I view them?\nCatch up on previous live episodes via this playlist, and if you wish to watch shorter bite sized videos over a coffee break you can do so with the Made With TensorFlow.js playlist to allow you to watch just the ones that are relevant to you on demand.\nWhen is the next show?\nJoin us in December for the next show and tell - we have 6 wonderful new demos lined up. We\u2019re aiming for 11th December, 9AM PT, over on the TensorFlow YouTube channel.\nBe sure to subscribe and click the bell icon to set notifications for when new videos are posted (we are aiming for every quarter), or add a Google calendar reminder and see you there!\nAlso, if you enjoyed the show and would like to see this format replicated for TensorFlow Core, TensorFlow Lite, and more, let us know! You can drop me a message on Twitter or LinkedIn - would love to see what you have made.\nAcknowledgements\nA huge thank you to all 24 of our show & tell presenters thus far, and to the amazing community who have submitted projects or helped tag amazing finds for us to reach out to. You truly make this show what it is and we are super excited to see even more in the future and look forward to seeing you all again soon.",
    "link": "https://blog.tensorflow.org/2020/11/a-new-youtube-show-tensorflowjs-show-and-tell.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-q-RA3V_lw90/X7bG4zbNXCI/AAAAAAAADvs/UCZ0P_wh0L44ZVGsu7mLvbPKXQpHHOhMwCLcBGAsYHQ/s0/pasted%2Bimage%2B0.png",
      "https://1.bp.blogspot.com/-q-RA3V_lw90/X7bG4zbNXCI/AAAAAAAADvs/UCZ0P_wh0L44ZVGsu7mLvbPKXQpHHOhMwCLcBGAsYHQ/s0/pasted%2Bimage%2B0.png"
    ],
    "time": "2023/12/07 00:58:43"
  },
  {
    "title": "Iris landmark tracking in the browser with MediaPipe and TensorFlow.js",
    "content": "Posted by Ann Yuan and Andrey Vakunov, Software Engineers at Google\n\nIris tracking enables a wide range of applications, such as hands-free interfaces for assistive technologies and understanding user behavior beyond clicks and gestures. Iris tracking is also a challenging computer vision problem. Eyes appear under variable light conditions, are often occluded by hair, and can be perceived as differently shaped depending on the head\u2019s angle of rotation and the person\u2019s expression. Existing solutions rely heavily on specialized hardware, often requiring a costly headset or a remote eye tracker system. These approaches are ill-suited for mobile devices with limited computing resources.\nAn example of eye re-coloring enabled.\nIn March we announced the release of a new package detecting facial landmarks in the browser. Today, we\u2019re excited to add iris tracking to this package through the TensorFlow.js face landmarks detection model. This work is made possible by the MediaPipe Iris model. We have deprecated the original facemesh model, and future updates will be made to the face landmarks detection model.\nNote that iris tracking does not infer the location at which people are looking, nor does it provide any form of identity recognition. In our model\u2019s documentation and the accompanying Model Card, we detail the model\u2019s intended uses, limitations and fairness attributes (aligned with Google\u2019s AI Principles).\nThe MediaPipe iris model is able to track landmarks for the iris and pupil using a single RGB camera, in real-time, without the need for specialized hardware. The model also returns landmarks for the eyelids and eyebrow regions, enabling detection of slight eye movements such as blinking. Try the model out yourself right now in your browser.\nIntroducing @tensorflow/face-landmarks-detection\nAbove left are predictions from @tensorflow-models/facemesh@0.0.4, above right are predictions from @tensorflow-models/face-landmarks-detection@0.0.1. Iris landmarks are in red.\nUsers familiar with our existing facemesh model will be able to upgrade to the new faceLandmarksDetection model with only a few code changes, detailed below. faceLandmarksDetection offers three major improvements over facemesh:\nIris keypoints detection\nImproved eyelid contour detection\nImproved detection for rotated faces\nThese improvements are highlighted in the GIF above, which demonstrates how the landmarks returned by faceLandmarksDetection and facemesh differ for the same image sequence.\nInstallation\nThere are two ways to install the faceLandmarksDetection package:\nThrough script tags:\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.6.0/dist/tf.js\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection\"></script>\nThrough NPM (via the yarn package manager):\n$ yarn add @tensorflow-models/face-landmarks-detection@0.0.1\n$ yarn add @tensorflow/tfjs@2.6.0\nUsage\nOnce the package is installed, you only need to load the model weights and then pass in an image to start detecting facial landmarks:\n// If you are using NPM, first require the model. If you are using script tags, you can skip this step because `faceLandmarksDetection` will already be available in the global scope.\nconst faceLandmarksDetection = require('@tensorflow-models/face-landmarks-detection');\n \n// Load the faceLandmarksDetection model assets.\nconst model = await faceLandmarksDetection.load(\n    faceLandmarksDetection.SupportedPackages.mediapipeFacemesh);\n \n// Pass in a video stream to the model to obtain an array of detected faces from the MediaPipe graph.\n// For Node users, the `estimateFaces` API also accepts a `tf.Tensor3D`, or an ImageData object.\nconst video = document.querySelector(\"video\");\nconst faces = await model.estimateFaces({ input: video });\nThe input to estimateFaces can be a video, a static image, a `tf.Tensor3D` or even an ImageData object for use in node.js pipelines. FaceLandmarksDetection then returns an array of prediction objects for the faces in the input, which include information about each face (e.g. a confidence score, and the locations of 478 landmarks within the face).\nHere is a sample prediction object:\n{\n    faceInViewConfidence: 1,\n    boundingBox: {\n        topLeft: [232.28, 145.26], // [x, y]\n        bottomRight: [449.75, 308.36],\n    },\n    mesh: [\n        [92.07, 119.49, -17.54], // [x, y, z]\n        [91.97, 102.52, -30.54],\n        ...\n    ],\n      // x,y,z positions of each facial landmark within the input space.\n    scaledMesh: [  \n        [322.32, 297.58, -17.54],\n        [322.18, 263.95, -30.54]\n    ],\n    // Semantic groupings of x,y,z positions.\n    annotations: {\n        silhouette: [\n            [326.19, 124.72, -3.82],\n            [351.06, 126.30, -3.00],\n            ...\n        ],\n        ...\n    }\n}\nRefer to our README for more details about the API.\nPerformance\nFaceLandmarksDetection is a lightweight package containing only ~3MB of weights, making it ideally suited for real-time inference on a variety of mobile devices. When testing, note that TensorFlow.js also provides several different backends to choose from, including WebGL and WebAssembly (WASM) with XNNPACK for devices with lower-end GPU's. The table below shows how the package performs across a few different devices and TensorFlow.js backends.:\nDesktop:\nMobile:\nAll benchmarks were collected in the Chrome browser. See our earlier blogpost for details on how to activate SIMD for the TF.js WebAssembly backend.\nLooking ahead\nBoth the TensorFlow.js and MediaPipe teams plan to add depth estimation capabilities to our face landmark detection solutions using the improved iris coordinates. We strongly believe in sharing code that enables reproducible research and rapid experimentation, and are looking forward to seeing how the wider community makes use of the MediaPipe iris model.\nTry the demo!\nUse this link to try our new package in your web browser. We look forward to seeing how you use it in your apps.\nMore information\nLearn more about the MediaPipe Iris model here: MediaPipe Iris\nRead about the model\u2019s intended uses, limitations and fairness attributes: Model Card\nRead the original Google AI blogpost announcing MediaPipe Iris: Real-time Iris Tracking & Depth Estimation\nRead our paper on arXiv: Real-time Pupil Tracking from Monocular Video for Digital Puppetry",
    "link": "https://blog.tensorflow.org/2020/11/iris-landmark-tracking-in-browser-with-MediaPipe-and-TensorFlowJS.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-1mEbyzby8c8/X6HqNdXnT-I/AAAAAAAADrc/OMC8mRSHdREfyAo9FF3buRsTOuZQY3zcQCLcBGAsYHQ/s0/gif_rsult.gif",
      "https://1.bp.blogspot.com/-aP5vRPFpXK0/X6CoRtYzSpI/AAAAAAAADqg/jqfU_kz4qzwmjPI1MnNWE7JeGeCVtnDkgCLcBGAsYHQ/s0/MediaPipe%2BGIF.gif",
      "https://1.bp.blogspot.com/-jBHiJ4YcBlQ/X6GZWyMiX5I/AAAAAAAADqs/8EVs_2HTxGEgmIp6fIZkeCpb2N-5QEtcwCLcBGAsYHQ/s0/remove_every_other_looped.gif",
      "https://1.bp.blogspot.com/-zwDuyYwcdfI/X6GeryO3oQI/AAAAAAAADq4/jrkFit5xdGg5qCnDDvfEHpSdCHHxVGR4gCLcBGAsYHQ/s0/Screen%2BShot%2B2020-10-27%2Bat%2B9.12.23%2BAM.png",
      "https://1.bp.blogspot.com/-hVL_nl5q5yM/X6GnffmSYaI/AAAAAAAADrQ/pDruEDfg2gAc_3yGRLDMWYM-j9_qNTgmACLcBGAsYHQ/s0/mobile.png"
    ],
    "time": "2023/12/07 00:58:43"
  },
  {
    "title": "Bringing the Mona Lisa Effect to Life with TensorFlow.js",
    "content": "A guest post by Emily Xie, Software Engineer\nBackground\nUrban legend says that Mona Lisa's eyes will follow you as you move around the room. This is known as the \u201cMona Lisa effect.\u201d For fun, I recently programmed an interactive digital portrait that brings this phenomenon to life through your browser and webcam.\nAt its core, the project leverages TensorFlow.js, deep learning, and some image processing techniques. The general idea is as follows: first, we must generate a sequence of images of Mona Lisa\u2019s head, with eyes gazing from the left to right. From this pool, we\u2019ll continuously select and display a single frame in real-time based on the viewer\u2019s location.\nIn this post, I\u2019ll walk through the specifics of the project\u2019s technical design and implementation.\nAnimating the Mona Lisa with Deep Learning\nImage animation is a technique that allows one to puppeteer a still image through a driving video. Using a deep-learning-based approach, I was able to generate a highly convincing animation of Mona Lisa\u2019s gaze.\nSpecifically, I used the First Order Motion Model (FOMM), released by Aliaksandr Siarohin et al. in 2019. At a very high level, this method is composed of two modules: one for motion extraction, and another for image generation. The motion module detects keypoints and local affine transformations from the driving video. Diffs of these values between consecutive frames are then used as input to a network that predicts a dense motion field, along with an occlusion mask which specifies the image regions that either need to be modified or contextually inferred. The image generation network, then, detects facial landmarks and produces the final output\u2013\u2013the source image, warped and in-painted according to the results of the motion module.\nI chose FOMM in particular because of its ease of use. Prior models in this domain had been \u201cobject-specific\u201d, meaning that they required detailed data of the object to be animated, whereas FOMM operated agnostically to this. More importantly, the authors released an open-source, out-of-the-box implementation with pre-trained weights for facial animation. Because of this, applying the model to the Mona Lisa became a surprisingly straightforward endeavor: I simply cloned the repo into a Colab notebook, produced a short driving video of me with my eyes moving around, and fed it through the model along with a screenshot of La Gioconda\u2019s head. The resulting movie was stellar. From this, I ultimately sampled just 33 images to constitute the final animation.\nExample of a driving video and the image animation predictions generated by FOMM.\nA subsample of the final animation frames, produced using the First Order Motion Model.\nImage Blending\nWhile I could\u2019ve re-trained the model for my project\u2019s purposes, I decided to work within the constraints of Siarohin\u2019s weights in order to avoid the time and computational resources that would\u2019ve been otherwise required. This, however, meant that the resulting frames were fixed at a lower resolution than desired, and consisted of just the subject\u2019s head. But since I wanted the final visual to include the entirety of Mona Lisa\u2013\u2013hands, torso, and background included\u2013\u2013my plan was to simply superimpose the output head frames onto an image of the painting.\nAn example of a head frame overlaid on top of the underlying image. To best illustrate the problem, the version shown here is from an earlier iteration of the project where there was further resolution loss in the head frame.\nThis, however, produced its own set of challenges. If you look at the example above, you\u2019ll notice that the lower-resolution output of the model\u2013\u2013coupled with some subtle collateral background changes due to FOMM\u2019s warping procedure\u2013\u2013causes the head frame to visually jut out. In other words, it was a bit obvious that this was just a picture on top of another picture. To address this, I did some image processing in Python to \u201cblend\u201d the head image into the underlying one.\nFirst, I resized the head frame to its original resolution. From there, I created a new frame using a weighted average of these blurred out pixels and the corresponding pixels in the underlying image, where the weight\u2013\u2013or alpha\u2013\u2013of a pixel in the head frame decreases as it moves away from the midpoint.\nThe function to determine alpha was adapted from a 2D sigmoid, and is expressed as:\nWhere j determines the logistic function\u2019s slope, k is the inflection point, and m is the midpoint of the input values. Graphed out, the function looks like:\nAfter I applied the above procedure to all 33 frames in the animation set, the resulting superimpositions each appeared to be a single image to the unsuspecting eye:\nTracking the Viewer\u2019s Head via BlazeFace\nAll that was left at this point was to determine how to track the user via the webcam and display the corresponding frame.\nNaturally, I turned to TensorFlow.js for the job. The library offered a fairly robust set of models to detect the presence of a human given visual input, but after some research and thinking, I landed on BlazeFace as my method of choice.\nBlazeFace is a deep-learning based object recognition model that detects human faces and facial landmarks. It is specifically trained for using mobile camera input. This worked well for my use case, as I expected most viewers to be using their webcam in a similar manner\u2013\u2013with their heads in frame, front-facing, and fairly close to the camera\u2013\u2013whether through their mobile devices or on their laptops.\nMy foremost consideration in selecting this model, however, was its extraordinary speed of detection. To make this project convincing, I needed to be able to run the entire animation in real time, including the facial recognition step. BlazeFace adapts the single-shot detection (SSD) model, a deep-learning-based object detection algorithm that simultaneously proposes bounding boxes and detects objects in just one forward pass of the network. BlazeFace\u2019s lightweight detector is capable of recognizing facial landmarks at speeds as fast as 200 frames per second.\nA demo of what BlazeFace can capture given an input image: bounding boxes for a human head, along with facial landmarks.\nHaving settled on the model, I then wrote code to continually pipe the user\u2019s webcam data into BlazeFace. On each run, the model outputted an array of facial landmarks and their corresponding 2D coordinate positions. Using this, I approximated the X coordinate of the face\u2019s center by calculating the midpoint between the eyes.\nFinally, I mapped this result to an integer between 0 and 32. These values, as you may recall, each represented a frame in the animated sequence\u2013\u2013with 0 depicting Mona Lisa with her eyes to the left, and 32 with her eyes to the right. From there, it was just a matter of displaying the frame on the screen.\nTry it out!\nYou can play with the project at monalisaeffect.com. To follow more of my work, feel free to check out my personal website, Github, or Twitter.\nAcknowledgements\nThanks to Andrew Fu for reading this post and providing me feedback, to Nick Platt for lending his ear and thoughts on a frontend bug, and to Jason Mayes along with the rest of the team at Google for their work in reaching out and amplifying this project.",
    "link": "https://blog.tensorflow.org/2020/09/bringing-mona-lisa-effect-to-life-tensorflow-js.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-4V7RRkONT2o/X2zQ2xmTgsI/AAAAAAAADmI/hk-VmwsUi1o3R3wTpwdgtS_g829vrXpgACLcBGAsYHQ/s0/monalisa.gif",
      "https://1.bp.blogspot.com/-x152CllefWk/X2zR1GQhyYI/AAAAAAAADmQ/FRC1KbeM6_APsH6PwF41-j0F3E4rrkiOACLcBGAsYHQ/s0/FOMMgenerated.gif",
      "https://1.bp.blogspot.com/-WbkeK9garDY/X2zSB7vEgQI/AAAAAAAADmU/dMj-mIybGTIwKQBDjmI-qGeuHDJTpkZdQCLcBGAsYHQ/s0/firstordermotionmodel.png",
      "https://1.bp.blogspot.com/-dHhW1ZGZjIg/X2zSajToqII/AAAAAAAADmg/a32dyKmB1cwteJXwiWxFefheBFiSv1djwCLcBGAsYHQ/s0/headframe.png",
      "https://1.bp.blogspot.com/-JJBs5N2W-g4/X2zSmCMcvLI/AAAAAAAADmk/efJHAmT4BvgAioQElAHYM8pUE78cVtX7ACLcBGAsYHQ/s0/function.png",
      "https://1.bp.blogspot.com/-0arcbACUu2s/X2zSv4E3FKI/AAAAAAAADms/0w364yY7uEgjJvpm1BBC8r7_HhaMmh3gACLcBGAsYHQ/s0/functiongraph.png",
      "https://1.bp.blogspot.com/-VoHmKuHw95w/X2zS8YPmQtI/AAAAAAAADm0/6SDW9XSUP2A2K0PHDK0yZBKjbJR1pUOggCLcBGAsYHQ/s0/superimpositions.png",
      "https://1.bp.blogspot.com/-QnblmyFUwQM/X2zTNkafpyI/AAAAAAAADnA/8it3hFXxN2oVJWPKrJjauQjwWJ70A_FegCLcBGAsYHQ/s0/BlazeFace%2Bdemo.png"
    ],
    "time": "2023/12/07 00:58:43"
  },
  {
    "title": "Supercharging the TensorFlow.js WebAssembly backend with SIMD and multi-threading",
    "content": "Posted by Ann Yuan and Marat Dukhan, Software Engineers at Google\n\nIn March we introduced a new WebAssembly (Wasm) accelerated backend for TensorFlow.js (scroll further down to learn more about Wasm and why this is important). Today we are excited to announce a major performance update: as of TensorFlow.js version 2.3.0, our Wasm backend has become up to 10X faster by leveraging SIMD (vector) instructions and multithreading via XNNPACK, a highly optimized library of neural network operators.\nBenchmarks\nSIMD and multithreading bring major performance improvements to our Wasm backend. Below are benchmarks in Google Chrome that demonstrate the improvements on BlazeFace - a light model with 0.1 million parameters and about 20 million multiply-add operations:\n\n(times listed are milliseconds per inference)\n\nLarger models, such as MobileNet V2, a medium-sized model with 3.5 million parameters and roughly 300 million multiply-add operations, attain even greater speedups:\n\n*Note: Benchmarks for the TF.js multi-threaded Wasm backend are not available for Pixel 4 because multi-threading support in mobile browsers is still a work-in-progress. SIMD support in iOS is also still under development.\n\n**Note: Node support for the TF.js multi-threaded Wasm backend is coming soon.\n\nThe performance gains from SIMD and multithreading are independent of each other. These benchmarks show that SIMD brings a 1.7-4.5X performance improvement to plain Wasm, and multithreading brings another 1.8-2.9X speedup on top of that.\n\n\nUsage\nSIMD is supported as of TensorFlow.js 2.1.0, and multithreading is supported as of TensorFlow.js 2.3.0.\n\nAt runtime we test for SIMD and multithreading support and serve the appropriate Wasm binary. Today we serve a different binary for each of the following cases:\nDefault: The runtime does not support SIMD or multithreading\nSIMD: The runtime supports SIMD but not multithreading\nSIMD + multithreading: The runtime supports SIMD and multithreading\nSince most runtimes that support multi-threading also support SIMD, we decided to omit the multi-threading-only runtime to keep our bundle size down. This means that if your runtime supports multithreading but not SIMD, you will be served the default binary. There are two ways to use the Wasm backend:\nWith NPM\n// Import @tensorflow/tfjs or @tensorflow/tfjs-core\nconst tf = require('@tensorflow/tfjs');\n// Add the WAsm backend to the global backend registry.\nrequire('@tensorflow/tfjs-backend-wasm');\n \n// Set the backend to WAsm and wait for the module to be ready.\ntf.setBackend('wasm').then(() => main());\nThe library expects the Wasm binaries to be located relative to the main JS file. If you\u2019re using a bundler such as parcel or webpack, you may need to manually indicate the location of the Wasm binaries with our setWasmPaths helper:\nimport {setWasmPaths} from '@tensorflow/tfjs-backend-wasm';\nsetWasmPaths(yourCustomFolder);\ntf.setBackend('wasm').then(() => {...});\nSee the \u201cUsing bundlers\u201d section in our README for more information.\nWith script tags\n<!-- Import @tensorflow/tfjs or @tensorflow/tfjs-core -->\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs\"></script>\n \n<!-- Adds the WAsm backend to the global backend registry -->\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm/dist/tf-backend-wasm.js\"></script>\n \n<script>\n  tf.setBackend('wasm').then(() => main());\n</script>\nNOTE: TensorFlow.js defines a priority for each backend and will automatically choose the best supported backend for a given environment. Today, WebGL has the highest priority, followed by Wasm, then the vanilla JS backend. To always use the Wasm backend, we need to explicitly call tf.setBackend(\u2018wasm\u2019).\nDemo\nTo see the performance improvements for yourself, check out this demo of our BlazeFace model, which has been updated to use the new Wasm backend: https://tfjs-wasm-simd-demo.netlify.app/ To compare against the unoptimized binary, try this version of the demo, which manually turns off SIMD and multithreading support.\nWhat is Wasm?\nWebAssembly (Wasm) is a cross-browser binary format that brings near-native code execution speed to the web. Wasm serves as a compilation target for programs written in statically typed high level languages, such as C, C++, Go, and Rust. In TensorFlow.js, we implement our Wasm backend in C++ and compile with Emscripten. The XNNPACK library provides a heavily optimized implementation of neural network operators underneath.\n\nWasm has been supported by Chrome, Safari, Firefox, and Edge since 2017, and is supported by 90% of devices worldwide.\n\nThe WebAssembly specification is evolving quickly and browsers are working hard to support a growing number of experimental features. You can visit this site to see which features are supported by your runtime, including:\nSIMD\nSIMD stands for Single Instruction, Multiple Data, which means that SIMD instructions operate on small fixed-size vectors of elements rather than individual scalars. The Wasm SIMD proposal makes the SIMD instructions supported by modern processors usable inside Web browsers, unlocking significant performance gains.\n\nWasm SIMD is a phase 3 proposal, and is available via an origin trial in Chrome 84-86. This means developers can opt in their websites to Wasm SIMD and all their visitors will enjoy its benefits without needing to explicitly enable the feature in their browser settings. Besides Google Chrome, Firefox Nightly supports Wasm SIMD by default.\n\nMulti-threading\nNearly all modern processors have multiple cores, each of which is able to carry out instructions independently and concurrently. WebAssembly programs can spread their work across cores via the threads proposal for performance. This proposal allows multiple Wasm instances in separate web workers to share a single WebAssembly.Memory object for fast communication between workers.\n\nWasm threads is a phase 2 proposal, and has been available in Chrome desktop by default since version 74. There is an ongoing cross-browser effort to enable this functionality for mobile devices as well.\nTo see which browsers support SIMD, threads, and other experimental features, check out the WebAssembly roadmap.\nOther improvements\nSince the original launch of our Wasm backend in March, we have extended operator coverage and now support over 70 operators. Many of the new operators are accelerated through the XNNPACK library, and unlock support for additional models, like the HandPose model.\nLooking ahead\nWe expect the performance of our Wasm backend to keep improving. We\u2019re closely following the progress of several evolving specifications in WebAssembly, including flexible vectors for wider SIMD, quasi fused multiply-add, and pseudo-minimum and maximum instructions. We\u2019re also looking forward to ES6 module support for WebAssembly modules. As with SIMD and multithreading, we intend to take advantage of these features as they become available with no implications for TF.js user code.\nMore information\nCheck out the WebAssembly roadmap: https://webassembly.org/roadmap/\nFollow progress on the Wasm spec: https://github.com/WebAssembly/spec\nRead more information about the Wasm SIMD proposal: https://github.com/WebAssembly/simd\nRead more information about the Wasm threads proposal: https://github.com/WebAssembly/threads\nSubmit feedback and contributions via issues and PRs on GitHub.\nShare what you build with these optimizations on social media with the #MadeWithTFJS hashtag, and join the TensorFlow.js community discussion forum for future updates.\nAcknowledgements\nWe would like to thank Daniel Smilkov and Nikhil Thorat for laying the groundwork of the WebAssembly backend and the integration with XNNPACK, Matsvei Zhdanovich for collecting Pixel 4 benchmark numbers, and Frank Barchard for implementing low-level Wasm SIMD optimizations in XNNPACK.",
    "link": "https://blog.tensorflow.org/2020/09/supercharging-tensorflowjs-webassembly.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-_oBmlCkqxIw/X07eAr--xQI/AAAAAAAADiQ/GB-O6TRgkJYvxmtpeyffo4iTlGTmTHuLACLcBGAsYHQ/s1600/table1.png",
      "https://4.bp.blogspot.com/-KlrmWKybSVo/X07eJKWi2XI/AAAAAAAADiU/8sOfUK2T0HYUukRg3Km7blaiwUeDv5bQQCLcBGAsYHQ/s1600/table2.png",
      "https://3.bp.blogspot.com/-Da6MEQxxd3Y/X07eUZJREqI/AAAAAAAADic/bOTnFXDNxqYZXuCGld0H4S3aYkxzcPxmQCLcBGAsYHQ/s1600/graph1.png",
      "https://1.bp.blogspot.com/-zSKWmncfXi4/X07eWqHAm-I/AAAAAAAADig/FxXawBuAwPQXvDjS6fX2cyucJAjkaobOgCLcBGAsYHQ/s1600/graph2.png"
    ],
    "time": "2023/12/07 00:58:43"
  },
  {
    "title": "Introducing Danfo.js, a Pandas-like Library in JavaScript",
    "content": "A guest post by Rising Odegua, Independent Researcher; Stephen Oni, Data Science Nigeria\n\nDanfo.js is an open-source JavaScript library that provides high-performance, intuitive, and easy-to-use data structures for manipulating and processing structured data. Danfo.js is heavily inspired by the Python Pandas library and provides a similar interface/API. This means that users familiar with the Pandas API and know JavaScript can easily pick it up.\nOne of the main goals of Danfo.js is to bring data processing, machine learning and AI tools to JavaScript developers. This is in line with our vision and essentially the vision of the TensorFlow.js team, which is to bring ML to the web. Open-source libraries like Numpy and Pandas revolutionise the ease of manipulating data in Python and lots of tools were built around them, thus driving the bubbling ecosystem of ML in Python.\n\nDanfo.js is built on TensorFlow.js. That is, as Numpy powers Pandas arithmetic operations, we leverage TensorFlow.js to power our low-level arithmetic operations.\nSome of the main features of Danfo.js\nDanfo.js is fast. It is built on TensorFlow.js, and supports tensors out of the box. This means you can load Tensors in Danfo and also convert Danfo data structure to Tensors. Leveraging these two libraries, you have a data processing library on one hand (Danfo.js), and a powerful ML library on the other hand (TensorFlow.js).\n\nIn the example below, we show you how to create a Danfo DataFrame from a tensor object:\nconst dfd = require(\"danfojs-node\")\nconst tf = require(\"@tensorflow/tfjs-node\")\n\nlet data = tf.tensor2d([[20,30,40], [23,90, 28]])\nlet df = new dfd.DataFrame(data)\nlet tf_tensor = df.tensor\nconsole.log(tf_tensor);\ntf_tensor.print()\nOutput:\nTensor {\n  kept: false,\n  isDisposedInternal: false,\n  shape: [ 2, 3 ],\n  dtype: 'float32',\n  size: 6,\n  strides: [ 3 ],\n  dataId: {},\n  id: 3,\n  rankType: '2'\n}\nTensor\n    [[20, 30, 40],\n     [23, 90, 28]]\nYou can easily convert Arrays, JSONs, or Objects to DataFrame objects for manipulation.\n\nJSON object to DataFrame:\nconst dfd = require(\"danfojs-node\")\njson_data = [{ A: 0.4612, B: 4.28283, C: -1.509, D: -1.1352 },\n            { A: 0.5112, B: -0.22863, C: -3.39059, D: 1.1632 },\n            { A: 0.6911, B: -0.82863, C: -1.5059, D: 2.1352 },\n            { A: 0.4692, B: -1.28863, C: 4.5059, D: 4.1632 }]\ndf = new dfd.DataFrame(json_data)\ndf.print()\nOutput:\n\nObject array with column labels to DataFrame:\nconst dfd = require(\"danfojs-node\")\nobj_data = {'A': [\u201cA1\u201d, \u201cA2\u201d, \u201cA3\u201d, \u201cA4\u201d],\n            'B': [\"bval1\", \"bval2\", \"bval3\", \"bval4\"],\n            'C': [10, 20, 30, 40],\n            'D': [1.2, 3.45, 60.1, 45],\n            'E': [\"test\", \"train\", \"test\", \"train\"]\n            }\ndf = new dfd.DataFrame(obj_data)\ndf.print()\nOutput:\n\nYou can easily handle missing data (represented as NaN) in floating point as well as non-floating point data:\nconst dfd = require(\"danfojs-node\")\nlet data = {\"Name\":[\"Apples\", \"Mango\", \"Banana\", undefined],\n            \"Count\": [NaN, 5, NaN, 10], \n            \"Price\": [200, 300, 40, 250]}        \nlet df = new dfd.DataFrame(data)\nlet df_filled = df.fillna({columns: [\"Name\", \"Count\"], values: [\"Apples\", \ndf[\"Count\"].mean()]})\ndf_filled.print()\nOutput:\n\nIntelligent label-based slicing, fancy indexing, and querying of large data sets:\nconst dfd = require(\"danfojs-node\")\nlet data = { \"Name\": [\"Apples\", \"Mango\", \"Banana\", \"Pear\"] ,\n            \"Count\": [21, 5, 30, 10],\n             \"Price\": [200, 300, 40, 250] }\n\nlet df = new dfd.DataFrame(data)\nlet sub_df = df.loc({ rows: [\"0:2\"], columns: [\"Name\", \"Price\"] })\nsub_df.print()\nOutput:\n\nRobust IO tools for loading data from flat-files (CSV and delimited). Both in full and chunks:\nconst dfd = require(\"danfojs-node\")\n//read the first 10000 rows\ndfd.read_csv(\"file:///home/Desktop/bigdata.csv\", chunk=10000)\n  .then(df => {\n    df.tail().print()\n  }).catch(err=>{\n       console.log(err);\n  })\nRobust data preprocessing functions like OneHotEncoders, LabelEncoders, and scalers like StandardScaler and MinMaxScaler are supported on DataFrame and Series:\nconst dfd = require(\"danfojs-node\")\nlet data = [\"dog\",\"cat\",\"man\",\"dog\",\"cat\",\"man\",\"man\",\"cat\"]\nlet series = new dfd.Series(data)\nlet encode = new dfd.LabelEncoder()\nencode.fit(series)\nlet sf_enc = encode.transform(series)\nlet new_sf = encode.transform([\"dog\",\"man\"])\nOutput:\n\nInteractive, flexible and intuitive API for plotting DataFrames and Series in the browser:\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <script src=\"https://cdn.jsdelivr.net/npm/danfojs@0.1.1/dist/index.min.js\"></script>\n    <title>Document</title>\n</head>\n<body>\n    <div id=\"plot_div\"></div>\n    <script>\n         dfd.read_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/finance-charts-apple.csv\")\n\n            .then(df => {\n                var layout = {\n                    title: 'A financial charts',\n                    xaxis: {title: 'Date'},\n                    yaxis: {title: 'Count'}\n                }\n    new_df = df.set_index({ key: \"Date\" })\n   new_df.plot(\"plot_div\").line({ columns: [\"AAPL.Open\", \"AAPL.High\"], layout: layout \n})\n            }).catch(err => {\n                console.log(err);\n            })\n    </script>\n</body>\n</html>\nOutput:\n\nTitanic Survival Prediction using Danfo.js and Tensorflow.js\nBelow we show a simple end-to-end classification task using Danfo.js and TensorFlow.js. We use Danfo for data loading, manipulating and preprocessing of the dataset, and then export the tensor object.\nconst dfd = require(\"danfojs-node\")\nconst tf = require(\"@tensorflow/tfjs-node\")\n\nasync function load_process_data() {\n    let df = await dfd.read_csv(\"https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\")\n\n    //A feature engineering: Extract all titles from names columns\n    let title = df['Name'].apply((x) => { return x.split(\".\")[0] }).values\n    //replace in df\n    df.addColumn({ column: \"Name\", value: title })\n\n    //label Encode Name feature\n    let encoder = new dfd.LabelEncoder()\n    let cols = [\"Sex\", \"Name\"]\n    cols.forEach(col => {\n        encoder.fit(df[col])\n        enc_val = encoder.transform(df[col])\n        df.addColumn({ column: col, value: enc_val })\n    })\n\n    let Xtrain,ytrain;\n    Xtrain = df.iloc({ columns: [`1:`] })\n    ytrain = df['Survived']\n\n    // Standardize the data with MinMaxScaler\n    let scaler = new dfd.MinMaxScaler()\n    scaler.fit(Xtrain)\n    Xtrain = scaler.transform(Xtrain)\n\n    return [Xtrain.tensor, ytrain.tensor] //return the data as tensors\n}\nNext, we create a simple neural network using TensorFlow.js.\nfunction get_model() {\n    const model = tf.sequential();\n    model.add(tf.layers.dense({ inputShape: [7], units: 124, activation: 'relu', kernelInitializer: 'leCunNormal' }));\n    model.add(tf.layers.dense({ units: 64, activation: 'relu' }));\n    model.add(tf.layers.dense({ units: 32, activation: 'relu' }));\n    model.add(tf.layers.dense({ units: 1, activation: \"sigmoid\" }))\n    model.summary();\n    return model\n}\nFinally, we perform training, by first loading the model and the processed data as tensors. This can be fed directly to the neural network.\nasync function train() {\n    const model = await get_model()\n    const data = await load_process_data()\n    const Xtrain = data[0]\n    const ytrain = data[1]\n\n    model.compile({\n        optimizer: \"rmsprop\",\n        loss: 'binaryCrossentropy',\n        metrics: ['accuracy'],\n    });\n\n    console.log(\"Training started....\")\n    await model.fit(Xtrain, ytrain,{\n        batchSize: 32,\n        epochs: 15,\n        validationSplit: 0.2,\n        callbacks:{\n            onEpochEnd: async(epoch, logs)=>{\n                console.log(`EPOCH (${epoch + 1}): Train Accuracy: ${(logs.acc * 100).toFixed(2)},\n                                                     Val Accuracy:  ${(logs.val_acc * 100).toFixed(2)}\\n`);\n            }\n        }\n    });\n};\n\ntrain()\nThe reader will notice that the API of Danfo is very similar to Pandas, and a non-Javascript programmer can easily read and understand the code. You can find the full source code of the demo above here (https://gist.github.com/risenW/f54e4e5b6d92e7b1b9b1f30e884ca83c).\nClosing Remarks\nAs web-based machine learning has matured, it is imperative to have efficient data science tools built specifically for it. Tools like Danfo.js will enable web-based applications to easily support ML features, thus opening the space to an ecosystem of exciting applications. TensorFlow.js started the revolution by providing ML capabilities available in Python, and we hope to see Danfo.js as an efficient partner in this journey. We can\u2019t wait to see what Danfo.js grows into! Hopefully, it becomes indispensable to the web community as well.\nPlay with Danfo.js on CodePen\nLink to the official getting started guide\nLink to Github repository",
    "link": "https://blog.tensorflow.org/2020/08/introducing-danfo-js-pandas-like-library-in-javascript.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-kuDrk0ZKUg4/X0Q3ro57n0I/AAAAAAAADfs/0YQmHV9pY08iGImUl7Bn-cy4FIz6OhGygCLcBGAsYHQ/s1600/Danfo%2BJS.gif",
      "https://2.bp.blogspot.com/-Xm0oqXq9Wh8/X0Q4bTGW2GI/AAAAAAAADf0/4H71fuzWhDgPgwyoJdEmcWGHWuQ2FxcwgCLcBGAsYHQ/s1600/table1.png",
      "https://3.bp.blogspot.com/-LK1guR4j71g/X0Q4o9KgU6I/AAAAAAAADf4/f1-xLPFoDswh69fhjgUBe-cIgTadaaXTQCLcBGAsYHQ/s1600/table2.png",
      "https://2.bp.blogspot.com/-0lKMBdRSLN0/X0Q43K6JdxI/AAAAAAAADgA/suEvjeRyRKsE4KVb6qwCzTsTvO-N7H0mQCLcBGAsYHQ/s1600/table3.png",
      "https://4.bp.blogspot.com/-VQT-uqjq4xI/X0Q5GOPdDBI/AAAAAAAADgI/Dk0P1Xs1yS4gF56lKziOwSITAdXYukPvgCLcBGAsYHQ/s1600/table4.png",
      "https://3.bp.blogspot.com/--RD5JltcRfY/X0Q5fryLMOI/AAAAAAAADgU/IT2jYgvOGsAHrutltSNngain7J85q8dzQCLcBGAsYHQ/s1600/table5.png",
      "https://3.bp.blogspot.com/-m6ZJcPXiz1A/X0Q6PQAB3CI/AAAAAAAADgc/Z1YDGfm8c9IdoRYJiJOdPbDYD22izUahwCLcBGAsYHQ/s1600/financialcharts.png"
    ],
    "time": "2023/12/07 00:58:43"
  },
  {
    "title": "Introducing Semantic Reactor: Explore NLP in Google Sheets",
    "content": "Posted by Dale Markowitz, Applied AI Engineer\n\nEditor\u2019s note: An earlier version of this article was published on Dale\u2019s blog.\n\nMachine learning can be tricky, so being able to prototype ML apps quickly is a boon. If you\u2019re building a language-powered app -- like a video game with characters players can talk to or a customer service bot -- the Semantic Reactor is a tool that will help you do just that.\n\nThe Semantic Reactor is a new plugin for Google Sheets that lets you run natural language understanding (NLU) models (variations the Universal Sentence Encoder) on your own data, right from a spreadsheet.\nIn this post, I\u2019ll show you how to work with the tool and the NLU models it uses, but first, how does NLP actually work? What\u2019s going on under the hood? (Want to skip straight to the tool? Scrolling to the next section.)\nUnderstanding Embeddings\nWhat are Word Embeddings?\nOne simple (but powerful) technique for building natural-language-powered software is to use \u201cembeddings.\u201d\n\nIn machine learning, embeddings are a learned way of representing data in space (i.e. points plotted on an n-dimensional grid) such that the distances between points are meaningful. Word vectors are one popular example:\nThe picture above is a rough visual example of how words can be closer or further away from each other. Note that the words \u201cAustin,\u201d \u201cTexas,\u201d and \u201cbarbecue\u201d have a close relationship with each other, as do \u201cpet\u201d and \u201cdog,\u201d and \u201cwalk\u201d and \u201crun.\u201d Each word is represented by a set of coordinates (or a vector) and are placed on a graph where we can see relationships. For instance, we can see that the word \u201crat\u201d is close to both \u201cpet\u201d and also \u201ccat\u201d.\n\nWhere do these numbers come from? They\u2019re learned by a machine learning model through many bits of conversational and language data. By showing all those examples, the model learns which words tend to occur in the same spots in sentences.\n\nConsider these two sentences:\n\u201cMy mother gave birth to a son.\u201d\n\u201cMy mother gave birth to a daughter.\u201d\nBecause the words \u201cdaughter\u201d and \u201cson\u201d are often used in similar contexts, the model will learn that they should be represented close to each other in space. Word embeddings are useful in natural language processing. They can be used to find synonyms (\u201csemantic similarity\u201d), to solve analogies, or as a preprocessing step for a more complicated model. You can quickly train your own basic word embeddings with TensorFlow here.\nWhat are Sentence Embeddings?\nIt turns out that entire sentences (and even short paragraphs) can be effectively embedded in space too, using a type of model called a universal sentence encoder. Using sentence embeddings, we can figure out if two sentences are similar. This is useful, for example, if you\u2019re building a chatbot and want to know if a question a user asked (i.e. \u201cWhen will you wake me up?\u201d) is semantically similar to a question you \u2013 the chatbot programmer \u2013 have anticipated and written a response to (\u201cWhat time is my alarm?\u201d).\nSemantic Reactor: Prototype using NLP in a Google Sheet\nAlright, now onto the fun part: Building things! There are three NLP models available in the Semantic Reactor:\nLocal - A small TensorFlow.js version of the Universal Sentence Encoder that can run entirely within a webpage.\nBasic Online - A full sized, general-use version of the Universal Sentence Encoder.\nMultilingual Online - A full-sized Universal Sentence Encoder model trained on question/answer pairs in 16 languages.\nEach model offers two ranking methods:\nSemantic Similarity: How similar are two blocks of text?\n\nGreat for applications where you can anticipate what users might ask, like an FAQ bot. (Many customer service bots use semantic similarity to help deliver good answers to users.)\n\nInput / Response: How good of a response is one block of text to another?\n\nUseful for when you have a large, and constantly changing, set of texts and you don\u2019t know what users might ask. For instance, Talk to Books, a semantic search tool for a regularly updated collection of 100,000 books, uses input / response.\nYou can use the Semantic Reactor to test a response list against each model and ranking method. Sometimes it takes a good bit of experimenting before you get your response list and model selection to one you think will work for your application. The good news is that doing that work in a Google Sheet makes it fast and easy.\n\nOnce you have your response list, model selection and ranking method decided on, you can then begin writing code, and if you want to keep all operations within a website or on device (without requiring online API calls), you can use the newly updated TensorFlow.js model.\n\nAs mentioned, there are lots of great uses for NLU tech, and more interesting applications come out almost everyday. Every digital assistant, customer service bot, and search engine is likely using some flavor of machine learning. Smart Reply and Smart Compose in Gmail are two well-used features that make good use of semantic tech.\n\nHowever, it\u2019s fun and helpful to play with the tech within applications where the quality demands aren\u2019t so high, where failure is okay and even entertaining. To that end, we\u2019ve used the same tech that\u2019s within the Semantic Reactor to create a couple of example games. Semantris is a word association game that uses the input-response ranking method, and The Mystery of the Three Bots uses semantic similarity.\n\nPlaying those two games, and finding out where they work and where they don\u2019t, might give you ideas on what experiences you might create.\nSemantris, a word-association game powered by word embeddings.\nThe Mystery of the Three Bots is a simple game powered by NLU and available as open source code. (It\u2019s also playable here.)\nOne of the coolest applications of this tech comes from Anna Kipnis, a former game designer at Double Fine who now works with Stadia. She used Semantic Reactor to prototype a video game world that infers how the environment should react to player inputs using ML. Check out our conversation here.\n\nIn Anna\u2019s game, players interact with a virtual fox by asking any question they think of:\n\u201cFox, can I have some coffee?\u201d\nThen, using Semantic ML, the game engine (or the utility system) considers all of the possible ways the game might respond:\n\u201cFox turns on lights.\u201c\n\u201cFox turns on radio.\u201c\n\u201cFox move to you.\u201c\n\u201cFox brings you mug.\u201c\nUsing a sentence encoder model, the game decides what the best response is and executes it (in this case, the best response is \u201cFox brings you a mug,\u201d so the game animates the Fox bringing you a mug). If that sounds a little abstract, definitely watch the video linked above.\n\nLet\u2019s see how you might build something like Anna\u2019s game with Semantic Reactor (for all the nitty gritties of the fox demo, check out her original post).\n\nFirst, create a new Google sheet and write some sentences in the first column. I put these sentences in the first column of my Google sheet:\nI grab a ball\nI go to you\nI play with a ball\nI go to school.\nI go to the mug.\nI bring you the mug.\nI turn on music.\nI take a nap.\nI go for a hike.\nI tell you a secret.\nI snuggle with you.\nI ask for a belly rub.\nI send a text.\nI answer the phone.\nI make a sandwich.\nI drink some water.\nI play a board game.\nI do some coding.\nYou\u2019ll have to use your imagination here and think of these \u201cactions\u201d that a potential character (e.g. a chatbot or an actor in a video game) might take.\n\nOnce you\u2019ve applied for and been given access to Semantic Reactor, you\u2019ll be able to enable it by clicking on \u201cAdd-ons -> Semantic Reactor -> Start\u201d.\nClicking \u201cStart\u201d will open a panel that allows you to type in an input and hit \u201cReact\u201d:\nWhen you hit \u201cReact\u201d, Semantic Reactor uses a model to embed all of the responses you\u2019ve written in that first column, calculate a score (how good a response is this sentence to the query?), and sort the results. For example, when my input was \u201cI want some coffee,\u201d the top ranked responses from my spreadsheet were, \u201cI go to the mug\u201d and \u201cI bring you the mug.\u201d You\u2019ll also notice that there are two different ways to rank sentences using this tool: \u201cInput/Response\u201d and \u201cSemantic Similarity.\u201d As the name implies, the former ranks sentences by how good they are as responses to the given query, whereas \u201cSemantic Similarity\u201d simply rates how similar the sentences are to the query.\nFrom Spreadsheet to Code with TensorFlow.js\nUnderneath the hood, Semantic Reactor is powered by the open-source TensorFlow.js models found here.\n\nLet\u2019s take a look at how to use those models in JavaScript, so that you can convert your spreadsheet prototype into a working app.\n\n1 - Create a new Node project and install the module:\nnpm init\nnpm install @tensorflow/tfjs @tensorflow-models/universal-sentence-encoder\n2 - Create a new file (use_demo.js) and require the library:\nrequire('@tensorflow/tfjs');\nconst encoder = require('@tensorflow-models/universal-sentence-encoder');\n3 - Load the model:\nconst model = await encoder.loadQnA();\n4 - Encode your sentences and query:\nconst input = {\n  queries: \\[\"I want some coffee\"\\],\n  responses: \\[\n  \"I grab a ball\",\n  \"I go to you\",\n  \"I play with a ball\",\n  \"I go to school.\",\n  \"I go to the mug.\",\n  \"I bring you the mug.\"\n  \\]\n };\n\n  const embeddings = await model.embed(input);\n5 - Voila! You\u2019ve transformed your responses and query into vectors. Unfortunately, vectors are just points in space. To rank the responses, you\u2019ll want to compute the distance between those points (you can do this by computing the dot product, which gives you the squared Euclidean distance between points):\n  //zipWith :: (a -> b -> c) -> \\[a\\] -> \\[b\\] -> \\[c\\]\n   const zipWith =\n   (f, xs, ys) => {\n   const ny = ys.length;\n   return (xs.length <= ny ? xs : xs.slice(0, ny))\n   .map((x, i) => f(x, ys\\[i\\]));\n   }\n\n   // Calculate the dot product of two vector arrays.\n   const dotProduct = (xs, ys) => {\n   const sum = xs => xs ? xs.reduce((a, b) => a + b, 0) : undefined;\n\n   return xs.length === ys.length ?\n   sum(zipWith((a, b) => a * b, xs, ys))\n   : undefined;\n   }\nIf you run this code, you should see output like:\n [\n      { response: 'I grab a ball', score: 10.788130270345432 },\n      { response: 'I go to you', score: 11.597091717283469 },\n      { response: 'I play with a ball', score: 9.346379028479209 },\n      { response: 'I go to school.', score: 10.130473646521292 },\n      { response: 'I go to the mug.', score: 12.475453722603106 },\n      { response: 'I bring you the mug.', score: 13.229019199245684 }\n    ]\nCheck out the full code sample here.\n\nAnd that\u2019s it\u2013that\u2019s how you go from a Semantic ML spreadsheet to code fast!\n\nAn earlier version of this post was published at https://daleonai.com/semantic-ml.",
    "link": "https://blog.tensorflow.org/2020/08/introducing-semantic-reactor-explore-nlp-sheets.html",
    "imgSource": [
      "https://3.bp.blogspot.com/-0wn4xpVjLGI/Xz6oDz-XmBI/AAAAAAAADew/zL5DK3osv2Yb8e9u7pe5JndV-uBvyNmqgCLcBGAsYHQ/s1600/semantic_reactor.gif",
      "https://2.bp.blogspot.com/-JNUD6equdVc/Xz6oP46DQeI/AAAAAAAADe0/TYz5MY80iC4OCE-XXeRTasd4woT9_t-tQCLcBGAsYHQ/s1600/Word%2BVector%2BExample%2BDiagram.png",
      "https://4.bp.blogspot.com/-Pz2mwWoJhrI/Xz6qlrqgmgI/AAAAAAAADfE/NrAblz8eJUAueUuQhiqj3ddQsqj4LDxewCLcBGAsYHQ/s1600/semantris.gif",
      "https://2.bp.blogspot.com/-SbpcZBxQgwc/Xz6sEwTpFkI/AAAAAAAADfQ/wxCjTe7fV507zqhn4whqdJo_VW_Vw1q1ACLcBGAsYHQ/s1600/mysteryofthreebots.png",
      "https://4.bp.blogspot.com/-18DFgAUdLRw/Xz6tCsXeXmI/AAAAAAAADfY/UL8ljJ0SGjQuZF0gvXnN_7c3Uc1E4B6ZACLcBGAsYHQ/s1600/semanticreactorexperiments.png",
      "https://3.bp.blogspot.com/-Za4-SnwhAYA/Xz6tPA5IX3I/AAAAAAAADfc/5oRdoJg6ko0L3cD1L3egU3jcNyfhvIp4QCLcBGAsYHQ/s1600/sheet1.png"
    ],
    "time": "2023/12/07 00:58:43"
  },
  {
    "title": "Creating Sounds Of India: An on device, AI powered, musical experience built with TensorFlow",
    "content": "Posted by Anusha Ramesh, Product Manager TFX, David Zats, Software Engineer TFX, Ping Yu, Software Engineer TensorFlow.js, Lamtharn (Hanoi) Hantrakul, AI Resident Magenta\nIntroduction\nSounds of India is a unique and fun interactive musical experience launching for India\u2019s 74th Independence Day, inspired by Indian tradition and powered by machine learning. When users throughout India (and around the world) sing the Indian National Anthem into the microphone of their mobile devices, machine learning models transform their voices into a range of classical Indian musical instruments live in the browser. The entire process of creating this experience took only 12 weeks, showing how rapidly developers can take models from research to production at scale using the TensorFlow Ecosystem.\nThe Research: Magenta\u2019s Differentiable Digital Signal Processing (DDSP)\nMagenta is an open source research project within Google AI exploring the role of machine learning in the creative process. Differentiable Digital Signal Processing or DDSP is a new open source library fusing modern machine learning with interpretable signal processing. Instead of training a pure deep learning model like WaveNet to render waveforms sample-by-sample, we can train lightweight models that output time varying control signals into these differentiable DSP modules (hence the extra \u201cD\u201d in DDSP) which synthesize the final sound. Both recurrent and convolutional models incorporating DDSP in TensorFlow Keras layers can efficiently generate audio 1000 times faster than their larger autoregressive counterparts, with 100x reduction in model parameters and training data requirements. One particularly fun application of DDSP is Tone Transfer, which transforms sounds into musical instruments. Try it by first training a DDSP model on 15 minutes of a target saxophone. You can then sing a melody and the trained DDSP model will re-render it as a saxophone. For Sounds of India, we applied this technology to three classical Indian instruments: the Bansuri, the Shehnai, and the Sarangi.\n\nTrain with TFX, deploy to the browser with TensorFlow.js\nTFX\nTensorFlow Extended (TFX) is an end-to-end platform for production ML, which includes preparing data, training, validating, and deploying models in production environments. TFX was used to train the models responsible for transforming the user\u2019s voice to one of the instruments, and these models were then converted to TensorFlow.js for deployment on a standard Web browser.\n\nDeploying to the browser provides a seamless experience for users to interact with the machine learning model: simply click a hyperlink and load the page just like any other website. No complicated installs necessary. By executing client side in the browser, we are able to perform inference right at the source of the sensor data, minimising latency and reducing server costs associated with large graphics cards, CPU, and memory. Moreover, given the application uses your voice as input, user privacy is quite important. Since the entire end-to-end experience happens client-side and in the browser, absolutely no sensor or microphone data is sent to the server side.\n\nBrowser-based machine learning models are often optimized to be as small as possible to minimize bandwidth used. In this case, the ideal hyperparameters for each musical instrument can also vary drastically. We leveraged TFX to perform large-scale training and tuning over hundreds of models to determine the smallest size for each instrument. As a result, we were able to dramatically reduce their memory footprints. For example, the Bansuri instrument model had a reduction in its on-disk size of ~20x without a noticeable impact on sound quality.\n\nTFX also empowered us to perform rapid iteration over different model architectures (GRU, CNN), different types of inputs (loudness, RMS energy), and varying musical instrument data sources. Each time, we were able to quickly and effectively run the TFX pipeline to produce a new model with the desired characteristics.\n\nTensorFlow.js\nCreating a TensorFlow.js DDSP model was uniquely challenging because of the need to hit tight performance and model quality targets. We needed the model to be highly efficient at performing tone transfer so that it could effectively run on mobile devices. At the same time, any degradation in model quality would quickly lead to audio distortions and a poor user experience.\n\nWe started by exploring a wide range of TensorFlow.js backends and model architectures. The WebGL backend is the most optimized, while the WebAssembly backend works well on low end phones. Given the computational requirements of DDSP, we settled on a Convnet-based DDSP model and leveraged the WebGL backend.\n\nTo minimize the model download time, we studied the topology of the model, and compressed a large set of constant tensors with Fill/ZeroLike ops, which reduced the size from 10MB to 300KB.\n\nWe also focused on three key areas to make the TensorFlow.js model ready for production scale deployment on devices: inference performance, memory footprint, and numerical stability.\n\nInference Performance Optimization\nDDSP models contain both a neural network and a signal synthesizer. The synthesizer part has many signal processing ops that require large amounts of computation. To improve performance on mobile devices, we re-wrote several kernels with special WebGL shaders to fully utilize the GPU. For example, a parallel version of the cumulative summation op reduced inference time by 90%.\n\nReduce memory footprint\nOur goal is to be able to run the model on as many types of mobile devices as possible. Since many phones have very limited GPU memory, we need to make sure that model has a minimal memory footprint. We achieve this by disposing of intermediate tensors and adding a new flag to allow early disposal of GPU textures. Through these approaches we were able to reduce memory size by 60%.\n\nNumerical stability\nThe DDSP model requires very high numerical precision in order to generate beautiful music. This is quite different from typical classification models, where a certain level of precision loss does not affect the final classifications. DDSP models used in this experience are generative models. Any loss in precision and discontinuities in the audio output are easily picked up by our sensitive ears. We encountered numerical stability problems with float16 WebGL texture. We therefore rewrote some of the key ops to reduce the overflow and underflow of the outputs. For example, in the Cumulative Summation op, we make sure cumulation is done within the shader with full float precision, and apply modulo calculation to avoid overflow before we write the output to a float16 texture.\nTry it yourself!\nYou can try out the experience on your mobile phone at g.co/SoundsofIndia - and please share your results with us if you wish. We would love to see what you create with your voice.\n\nIf you are excited about how machine learning can augment creativity and innovation, you can learn more about Magenta through the team\u2019s blog and contribute to their open source github, or check out #MadeWithTFJS for even more examples of browser-based machine learning from the TensorFlow.js community. If you are interested in training and deploying models at production scale using ML best practices, check out the Tensorflow Extended blog.\nAcknowledgements\nThis project wouldn\u2019t have been possible without the incredible effort of Miguel de Andr\u00e9s-Clavera, Yiling Liu, Aditya Mirchandani, KC Chung, Alap Bharadwaj, Kiattiyot (Boon) Panichprecha, Pittayathorn (Kim) Nomrak, Phatchara (Lek) Pongsakorntorn, Nattadet Chinthanathatset, Hieu Dang, Ann Yuan, Sandeep Gupta, Chong Li, Edwin Toh, Jesse Engel and additional help from Michelle Carney, Nida Zada, Doug Eck, Hannes Widsomer and Greg Mikels. Huge thanks to Tris Warkentin and Mitch Trott for their tremendous support.",
    "link": "https://blog.tensorflow.org/2020/08/creating-sounds-of-india-with-tensorflow.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-N4BqE_TWAug/XzV3j8NxrgI/AAAAAAAADdQ/0OGjiyzyanoVuoV0iMI6kHGEpjephklXgCLcBGAsYHQ/s1600/Sounds%2Bof%2BIndia-2.gif",
      "https://4.bp.blogspot.com/-CMEZ10MuiIw/XzV4b7hhl-I/AAAAAAAADdY/ME6k2ptgjTINSwC9gxbqSCpcj9yYkxt9ACLcBGAsYHQ/s1600/flowchart%2B.png",
      "https://3.bp.blogspot.com/-BRgXpzdQHP0/XzV79xXEM-I/AAAAAAAADdk/8Dn8B6Q7_mY91MJDAXbF3qZ0ZkzsIKxTQCLcBGAsYHQ/s1600/trainingdata.png"
    ],
    "time": "2023/12/07 00:58:43"
  },
  {
    "title": "Using machine learning in the browser to lip sync to your favorite songs",
    "content": "Posted by Pohung Chen, Creative Technologist, Google Partner Innovation\n\nToday we are releasing LipSync, a web experience that lets you lip sync to music live in the web browser. LipSync was created as a playful way to demonstrate the facemesh model used with TensorFlow.js. We partnered with Australian singer Tones and I to let you lip sync to Dance Monkey in this demonstration.\nUsing TensorFlow.js and FaceMesh\nThe FaceMesh model (built by MediaPipe) provides a real-time high density estimate of key points of your facial expression using only a webcam and on device machine learning - meaning no data ever leaves your machine for inference. We essentially use the key points around the mouth and lips to estimate how well you synchronize to the lyrics of the Dance Monkey song.\nDetermining Correctness\nWhen first testing the demo, many people assumed we used a complex lip reading algorithm to match the mouth shapes with lyrics. Lip reading is quite difficult to achieve, so we came up with a simpler solution. We capture a frame by frame recording of the \u201ccorrect\u201d mouth shapes lined up with the music, and then when the user is playing the game, we compare the mouth shapes to the pre-recorded baseline.\n\nMeasuring the shape of your mouth\nWhat is a mouth shape? There are many different ways to measure the shape of your mouth. We needed a technique that allows the user to move their head around while singing and is relatively forgiving in different mouth shapes, sizes, and distance to the camera.\nMouth Ratio\nOne way of comparing mouth shapes is to use the width to height ratio of your mouth. For example, if your mouth is closed and forming the \u201cmmm\u201d sound, you have a high width to height ratio. If your mouth is open in an \u201cooo\u201d sound, your mouth will be closer to a 1:1 width to height ratio.\nWhile this method mostly works, there were still edge cases that made the detection algorithm not as robust, so we explored another method called Hu Moments explained below.\nOpenCV matchShapes Hu Moments\nIn the OpenCV library, there is a matchShapes function which compares contours and returns a similarity score. Underneath the hood, the matchShapes function uses a technique called Hu Moments which provides a set of numbers calculated using central moments that are invariant to image transformations. This allowed us to compare shapes regardless of translation, scale, and rotation. So the user can freely rotate their head without impacting the detection of the mouth shape itself.\n\nWe use this in addition to the mouth shape above to determine how closely the shape of the mouth contours match.\nVisual and Audio Feedback\nIn our original prototype, we wanted to create immediate audible feedback on how well the user is doing. We separated out the vocal track from the rest of the song and changed its volume based on real-time user performance score of their mouth shapes.\nVocal Track\nInstrumental Track\nThis allowed us to create the effect such that if you stop lip syncing to the song, the lyrical portion of the song stops playing (but the background music continues to play).\n\nWhile this was a fun way to demonstrate the mouth shape matching algorithm, however it still missed that satisfactory rush of joy you get when you hit the right notes during karaoke or nail a long sequence of moves just right in arcade rhythm games.\n\nWe started by adding a real-time score that is then accumulated over time shown to the player as they played the game. In our initial testing, this didn\u2019t work as well as we had hoped. It was confusing what the score was and the exact numbers weren\u2019t particularly meaningful. We also wanted the user to focus their attention on the lyrics and the center of the screen as opposed to a score off to the side.\n\nSo we went with a different approach, preferring to lean on visual effects overlaid on top of the player\u2019s face as they lip synced to the music and colors to indicate how well the player was doing.\nTry Lip Sync yourself!\nTensorFlow.js and FaceMesh enable web-based, playful, interactive experiences that go beyond basic face filters, and with a little bit of creative thinking, we could get a lip sync experience without needing the full complexity of a full lip reading ML model.\n\nSo go ahead and try our live demo yourself right now. You can also check out an example of how the mouth shape matching works in this open source repo.\n\nWe would also like to give a special shout out to Kiattiyot Panichprecha, Bryan Tanaka, KC Chung, Dave Bowman, Matty Burton, Roger Chang, Ann Yuan, Sandeep Gupta, Miguel de Andr\u00e9s-Clavera, Alessandra Donati, and Ethan Converse for their help in bringing this experience to life, and to thank the MediaPipe team who designed Facemesh.",
    "link": "https://blog.tensorflow.org/2020/07/using-machine-learning-in-browser-to-lip-sync.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-0EOXng3h7cA/XwyqREPudbI/AAAAAAAADXk/VfDFuYl3stMt02Xs4NhbkgiG0fMgu3R1wCLcBGAsYHQ/s1600/lipsync.gif",
      "https://4.bp.blogspot.com/-XlAwJ1ONX1g/XwysV5yrxBI/AAAAAAAADXw/SEeanUJCKPkk0l_SgDJIALGPl4_eYegVgCLcBGAsYHQ/s1600/determiningcorrectness.gif",
      "https://1.bp.blogspot.com/-X-dkugYBHwA/XwytTG6vxNI/AAAAAAAADX4/YguzICQKiz4nZhx_rHfQtP_sguBNJnIMwCLcBGAsYHQ/s1600/face.png",
      "https://4.bp.blogspot.com/-OYNd7DOgxbc/Xwyt2qL2qpI/AAAAAAAADYA/7ySoLbAO3bQrq80v13GltXxhazk87vWbgCLcBGAsYHQ/s1600/vocaltrack.png",
      "https://3.bp.blogspot.com/-_fKJm17Sm50/Xwyt7D4vEzI/AAAAAAAADYE/QzWQonFbpUAFqQRVFlRIUEf2cSsNdb8VwCLcBGAsYHQ/s1600/instrumentaltrack.png",
      "https://1.bp.blogspot.com/-_vqnOkMPBic/Xwy2USJB1wI/AAAAAAAADYw/6fwaYjKC6TAzhdcL51HhAi_OjMO62Ek5gCLcBGAsYHQ/s1600/singdemo1.gif",
      "https://1.bp.blogspot.com/-RThrdpQTtio/Xwy2Xb1nCbI/AAAAAAAADY0/ZcCLHEl7-Rs7cmOajrVONN6crNuZ23bqQCLcBGAsYHQ/s1600/singingdemo2.gif"
    ],
    "time": "2023/12/07 00:58:43"
  },
  {
    "title": "Pose Animator - An open source tool to bring SVG characters to life in the browser via motion capture",
    "content": "By Shan Huang, Creative Technologist, Google Partner Innovation\nBackground\nThe PoseNet and Facemesh (from Mediapipe) TensorFlow.js models made real time human perception in the browser possible through a simple webcam. As an animation enthusiast who struggles to master the complex art of character animation, I saw hope and was really excited to experiment using these models for interactive, body-controlled animation.\n\nThe result is Pose Animator, an open-source web animation tool that brings SVG characters to life with body detection results from webcam. This blog post covers the technical design of Pose Animator, as well as the steps for designers to create and animate their own characters.\nUsing FaceMesh and PoseNet with TensorFlow.js to animate full body character\nThe overall idea of Pose Animator is to take a 2D vector illustration and update its containing curves in real-time based on the recognition result from PoseNet and FaceMesh. To achieve this, Pose Animator borrows the idea of skeleton-based animation from computer graphics and applies it to vector characters.\nIn skeletal animation a character is represented in two parts:\na surface used to draw the character, and\na hierarchical set of interconnected bones used to animate the surface.\nIn Pose Animator, the surface is defined by the 2D vector paths in the input SVG files. For the bone structure, Pose Animator provides a predefined rig (bone hierarchy) representation, based on the key points from PoseNet and FaceMesh. This bone structure\u2019s initial pose is specified in the input SVG file, along with the character illustration, while the real time bone positions are updated by the recognition result from ML models.\nDetection keypoints from PoseNet (blue) and FaceMesh (red)\nCheck out these steps to create your own SVG character for Pose Animator.\nAnimated bezier curves controlled by PoseNet and FaceMesh output/td>\nRigging Flow Overview\nThe full rigging (skeleton binding) flow requires the following steps:\nParse the input SVG file for the vector illustration and the predefined skeleton, both of which are in T-pose (initial pose).\nIterate through every segment in vector paths to compute the weight influence and transformation from each bone using Linear Blend Skinning (explained later in this post).\nIn real time, run FaceMesh and PoseNet on each input frame and use result keypoints to update the bone positions.\nCompute new positions of vector segments from the updated bone positions, bone weights and transformations.\nThere are other tools that provide similar puppeteering functionality, however, most of them only update asset bounding boxes and do not deform the actual geometry of characters with recognition key points. Also, few tools provide full body recognition and animation. By deforming individual curves, Pose Animator is good at capturing the nuances of facial and full body movement and hopefully provides more expressive animation.\nRig Definition\nThe rig structure is designed according to the output key points from PoseNet and FaceMesh. PoseNet returns 17 key points for the full body, which is simple enough to directly include in the rig. FaceMesh however provides 486 keypoints, so I needed to be more selective about which ones to include. In the end I selected 73 key points from the FaceMesh output and together we have a full body rig of 90 keypoints and 78 bones as shown below:\nThe 90 keypoints, 78 bones full body rig\nEvery input SVG file is expected to contain this skeleton in default position. More specifically, Pose Animator will look for a group called \u2018skeleton\u2019 containing anchor elements named with the respective joint they represent. A sample rig SVG can be found here. Designers have the freedom to move the joints around in their design files to best embed the rig into the character. Pose Animator will compute skinning according to the default position in the SVG file, although extreme cases (e.g. very short leg / arm bones) may not be well supported by the rigging algorithm and may produce unnatural results.\nThe illustration with embedded rig in design software (Adobe Illustrator)\nLinear Blend Skinning for vector paths\nPose Animator uses one of the most common rigging algorithms for deforming surfaces using skeletal structures - Linear Blend Skinning (LBS), which transforms a vertex on a surface by blending together its transformation controlled by each bone alone, weighted by each bone\u2019s influence. In our case, a vertex refers to an anchor point on a vector path, and bones are defined by two connected keypoints in the above rig (e.g. the \u2018leftWrist\u2019 and \u2018leftElbow\u2019 keypoints define the bone \u2018leftWrist-leftElbow\u2019).\n\nTo put into math formula, the world space position of the vertex vi\u2019 is computed as\nwhere\n- wi is the influence of bone i on vertex i,\n- vi describes vertex i\u2019s initial position,\n- Tj describes the spatial transformation that aligns the initial pose of bone j with its current pose.\n\nThe influence of bones can be automatically generated or manually assigned through weight painting. Pose Animator currently only supports auto weight assignment. The raw influence of bone j on vertex i is calculated as:\nWhere d is the distance from vi to the nearest point on bone j. Finally we normalize the weight of all bones for a vertex to sum up to 1.\nNow, to apply LBS on 2D vector paths, which are composed of straight lines and bezier curves, we need some special treatment for bezier curve segments with in and out handles. We need to compute weights separately for curve points, in control point, and out control point. This produces better looking results because the bone influence for control points are more accurately captured.\n\nThere is one exception case. When the in control point, curve point, and out control point are collinear, we use the curve point weight for all three points to guarantee that they stay collinear when animated. This helps to preserve the smoothness of curves.\nCollinear curve handles share the same weight to stay collinear\nMotion stabilization\nWhile LBS already gives us animated frames, there\u2019s a noticeable amount of jittering introduced by FaceMesh and PoseNet raw output. To reduce the jitter and get smoother animation, we can use the confidence scores from prediction results to weigh each input frame unevenly, granting less influence to low-confidence frames.\n\nFollowing this idea, Pose Animator computes the smoothed position of joint i at frame t as\nwhere\nThe smoothed confidence score of frame i is computed as\nConsider extreme cases. When two consecutive frames both have confidence score 1, position approaches the latest position at 50% speed, which looks responsive and reasonably smooth. (To further play with responsiveness, you can tweak the approach speed by changing the weight on the latest frame.) When the latest frame has confidence score 0, its influence is completely ignored, preventing low confidence results from introducing sudden jerkiness.\nConfidence score based clipping\nIn addition to interpolating joint positions with confidence scores, we also introduce a minimum threshold to decide if a path should be rendered at all.\n\nThe confidence score of a path is the averaged confidence score of its segment points, which in turn is the weighted average of the influence bones\u2019 scores. The whole path is hidden for a particular frame when its score is below a certain threshold.\n\nThis is useful for hiding paths in low confidence areas, which are often body parts out of the camera view. Imagine an upper body shot: PoseNet will always return keypoint predictions for legs and hips though they will have low confidence scores. With this clamping mechanism we can make sure lower body parts are properly hidden instead of showing up as strangely distorted paths.\nLooking ahead\nTo mesh or not to mesh\nThe current rigging algorithm is heavily centered around 2D curves. This is because the 2D rig constructed from PoseNet and FaceMesh has a large range of motion and varying bone lengths - unlike animation in games where bones have relatively fixed length. I currently get smoother results from deforming bezier curves than deforming the triangulated mesh from input paths, because bezier curves preserve the curvature / straightness of input lines better.\n\nI am keen to improve the rigging algorithm for meshes. Besides, I want to explore a more advanced rigging algorithm than Linear Blend Skinning, which has limitations such as volume thinning around the bent areas.\nNew editing features\nPose Animator delegates illustration editing to design softwares like Illustrator, which are powerful for editing vector graphics, but not tailored for animation / skinning requirements. I want to support more animation features through in-browser UI, including:\nSkinning weight painting tool, to enable tweaking individual weights on keypoints manually. This will provide more precision than auto weight assignment.\nSupport raster images in the input SVG files, so artists may use photos / drawings in their design. Image bounding boxes can be easily represented as vector paths so it\u2019s straightforward to compute its deformation using the current rigging algorithm.\nTry it yourself!\nTry out the live demos, where you can either play with existing characters, or add in your own SVG character and see them come to life.\n\nI\u2019m the most excited to see what kind of interactive animation the creative community will create. While the demos are human characters, Pose Animator will work for any 2D vector design, so you can go as abstract / avant-garde as you want to push its limits.\n\nTo create your own animatable illustration, please check out this guide! Don\u2019t forget to share your creations with us using #PoseAnimator on social media. Feel free to reach out to me on twitter @yemount for any questions.\n\nAlternatively if you want to view the source code directly, it is available to fork on github here. Happy hacking!",
    "link": "https://blog.tensorflow.org/2020/05/pose-animator-open-source-tool-to-bring-svg-characters-to-life.html",
    "imgSource": [
      "https://3.bp.blogspot.com/-oQH2DgTJeEg/Xs12y_vKkgI/AAAAAAAADHw/-6jUtjgK6DAYUGvHQOGULIOKOJpDhiDLwCLcBGAsYHQ/s1600/movementgif.gif",
      "https://1.bp.blogspot.com/-aaj3tZFj8xM/Xs132zq-ndI/AAAAAAAADH8/9o9b7FbLthUgZ9EkKL-dnZMOrpGIXx9dwCLcBGAsYHQ/s1600/detectmovement.png",
      "https://3.bp.blogspot.com/-JVYEFsu_eYs/Xs14PMxaEyI/AAAAAAAADIE/NHg-o1Ta_Ig-IlHCI5K2RgWz9Jwvf1z7ACLcBGAsYHQ/s1600/avatar-new-bezier-1.gif",
      "https://3.bp.blogspot.com/-rqvef3pwnPA/Xs15GXyCBUI/AAAAAAAADIQ/7gOWzeoFgH8E6Q8mEQfkcW1ZT6_tPQq7QCLcBGAsYHQ/s1600/90keypoints.png",
      "https://3.bp.blogspot.com/-9xvnX5ko6A4/Xs15XZY57JI/AAAAAAAADIY/k7soMWnskM0PTkLFMHq49d9R18GBgidLACLcBGAsYHQ/s1600/illustrationinadobe.png",
      "https://1.bp.blogspot.com/-UUHpoYKI1Fw/Xs16SsxdR8I/AAAAAAAADIk/55t1PnwaAKgffdErAQ_X9m3seFQqOo_FwCLcBGAsYHQ/s1600/formula1.png",
      "https://4.bp.blogspot.com/-6u7Ks9PqLok/Xs17PVqBETI/AAAAAAAADIs/Fn84d0fu8l4b4uRV5cO_tL9TUlvHFfFEwCLcBGAsYHQ/s1600/formula2.png",
      "https://4.bp.blogspot.com/-PXbEr0ateLM/Xs17d20rA6I/AAAAAAAADIw/56P1Hrc-auEbLFF7l5b38HzzHEgT4wzvQCLcBGAsYHQ/s1600/formula3.png",
      "https://3.bp.blogspot.com/-_wOxNljujbc/Xs17yKkZFMI/AAAAAAAADI8/uHLghYD13cMcVi2i8r4kup743l9SDyNTgCLcBGAsYHQ/s1600/collinear.png",
      "https://3.bp.blogspot.com/-0qZhKi59XX8/Xs18N7Zg24I/AAAAAAAADJE/iHwy-94pP20llOvQAQRvvQO8E6JyWaugACLcBGAsYHQ/s1600/formula4.png",
      "https://4.bp.blogspot.com/-hogcUzrMops/Xs2CchW__4I/AAAAAAAADJs/XR_Ztd4dxJkSEUSiFD902fcrm_P4KWLYQCLcBGAsYHQ/s1600/formula.jpg",
      "https://1.bp.blogspot.com/-thxJsFaCrv4/Xs1-JSWuvVI/AAAAAAAADJg/nhEaG1jWEMc-zKEZFTs1hxEm2jO2rbBbwCLcBGAsYHQ/s1600/formula5.png"
    ],
    "time": "2023/12/06 00:58:43"
  },
  {
    "title": "How Hugging Face achieved a 2x performance boost for Question Answering with DistilBERT in Node.js",
    "content": "A guest post by Hugging Face: Pierric Cistac, Software Engineer; Victor Sanh, Scientist; Anthony Moi, Technical Lead.\n\nHugging Face \ud83e\udd17 is an AI startup with the goal of contributing to Natural Language Processing (NLP) by developing tools to improve collaboration in the community, and by being an active part of research efforts.\n\nBecause NLP is a difficult field, we believe that solving it is only possible if all actors share their research and results. That\u2019s why we created \ud83e\udd17 Transformers, a leading NLP library with more than 2M downloads and used by researchers and engineers across many companies. It allows the amazing international NLP community to quickly experiment, iterate, create and publish new models for a variety of tasks (text/token generation, text classification, question answering\u2026) in a variety of languages (English of course, but also French, Italian, Spanish, German, Turkish, Swedish, Dutch, Arabic and many others!) More than 300 different models are available today through Transformers.\n\nWhile Transformers is very handy for research, we are also working hard on the production aspects of NLP, looking at and implementing solutions that can ease its adoption everywhere. In this blog post, we\u2019re going to showcase one of the paths we believe can help fulfill this goal: the use of \u201csmall\u201d, yet performant models (such as DistilBERT), and frameworks targeting ecosystems different from Python such as Node via TensorFlow.js.\nThe need for small models: DistilBERT\nOne of the areas we\u2019re interested in is \u201clow-resource\u201d model with close to state-of-the-art results, while being a lot smaller and also a lot faster to run. That\u2019s why we created DistilBERT, a distilled version of BERT: it has 40% fewer parameters, runs 60% faster while preserving 97% of BERT's performance as measured on the GLUE language understanding benchmark.\nNLP models through time, with their number of parameters\nTo create DistilBERT, we\u2019ve been applying knowledge distillation to BERT (hence its name), a compression technique in which a small model is trained to reproduce the behavior of a larger model (or an ensemble of models), demonstrated by Hinton et al.\n\nIn the teacher-student training, we train a student network to mimic the full output distribution of the teacher network (its knowledge). Rather than training with a cross-entropy over the hard targets (one-hot encoding of the gold class), we transfer the knowledge from the teacher to the student with a cross-entropy over the soft targets (probabilities of the teacher). Our training loss thus becomes:\nWith t the logits from the teacher and s the logits of the student\nOur student is a small version of BERT in which we removed the token-type embeddings and the pooler (used for the next sentence classification task). We kept the rest of the architecture identical while reducing the numbers of layers by taking one layer out of two, leveraging the common hidden size between student and teacher. We trained DistilBERT on very large batches leveraging gradient accumulation (up to 4000 examples per batch), with dynamic masking, and removed the next sentence prediction objective.\n\n\nWith this, we were then able to fine-tune our model on the specific task of Question Answering. To do so, we used the BERT-cased model fine-tuned on SQuAD 1.1 as a teacher with a knowledge distillation loss. In other words, we distilled a question answering model into a language model previously pre-trained with knowledge distillation! That\u2019s a lot of teachers and students: DistilBERT-cased was first taught by BERT-cased, and then \u201ctaught again\u201d by the SQuAD-finetuned BERT-cased version in order to get the DistilBERT-cased-finetuned-squad model.\n\nThis results in very interesting performances given the size of the network: our DistilBERT-cased fine-tuned model reaches an F1 score of 87.1 on the dev set, less than 2 points behind the full BERT-cased fine-tuned model! (88.7 F1 score).\n\nIf you\u2019re interested in learning more about the distillation process, you can read our dedicated blog post.\nThe need for a language-neutral format: SavedModel\nUsing the previous process, we end up with a 240MB Keras file (.h5) containing the weights of our DistilBERT-cased-squad model. In this format, the architecture of the model resides in an associated Python class. But our final goal is to be able to use this model in as many environments as possible (Node.js + TensorFlow.js for this blog post), and the TensorFlow SavedModel format is perfect for this: it\u2019s a \u201cserialized\u201d format, meaning that all the information necessary to run the model is contained into the model files. It is also a language-neutral format, so we can use it in Python, but also in JS, C++, and Go.\n\nTo convert to SavedModel, we first need to construct a graph from the model code. In Python, we can use tf.function to do so:\nimport tensorflow as tf\nfrom transformers import TFDistilBertForQuestionAnswering\n\ndistilbert = TFDistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')\ncallable = tf.function(distilbert.call)\nHere we passed to tf.function the function called in our Keras model, call. What we get in return is a callable that we can in turn use to trace our call function with a specific signature and shapes thanks to get_concrete_function:\nconcrete_function = callable.get_concrete_function([tf.TensorSpec([None, 384], tf.int32, name=\"input_ids\"), tf.TensorSpec([None, 384], tf.int32, name=\"attention_mask\")])\nBy calling get_concrete_function, we trace-compile the TensorFlow operations of the model for an input signature composed of two Tensors of shape [None, 384], the first one being the input ids and the second one the attention mask.\n\nThen we can finally save our model to the SavedModel format:\ntf.saved_model.save(distilbert, 'distilbert_cased_savedmodel', signatures=concrete_function)\nA conversion in 4 lines of code, thanks to TensorFlow! We can check that our resulting SavedModel contains the correct signature by using the\nsaved_model_cli:\n\n$ saved_model_cli show --dir distilbert_cased_savedmodel --tag_set serve --signature_def serving_default\nOutput:\nThe given SavedModel SignatureDef contains the following input(s):\n  inputs['attention_mask'] tensor_info:\n   dtype: DT_INT32\n   shape: (-1, 384)\n   name: serving_default_attention_mask:0\n  inputs['input_ids'] tensor_info:\n   dtype: DT_INT32\n   shape: (-1, 384)\n   name: serving_default_input_ids:0\nThe given SavedModel SignatureDef contains the following output(s):\n  outputs['output_0'] tensor_info:\n   dtype: DT_FLOAT\n   shape: (-1, 384)\n   name: StatefulPartitionedCall:0\n  outputs['output_1'] tensor_info:\n   dtype: DT_FLOAT\n   shape: (-1, 384)\n   name: StatefulPartitionedCall:1\nMethod name is: tensorflow/serving/predict\nPerfect! You can play with the conversion code yourself by opening this colab notebook. We are now ready to use our SavedModel with TensorFlow.js!\nThe need for ML in Node.js: TensorFlow.js\nHere at Hugging Face we strongly believe that in order to reach its full adoption potential, NLP has to be accessible in other languages that are more widely used in production than Python, with APIs simple enough to be manipulated with software engineers without a Ph.D. in Machine Learning; one of those languages is obviously Javascript.\n\nThanks to the API provided by TensorFlow.js, interacting with the SavedModel we created previously in Node.js is very straightforward. Here is a slightly simplified version of the Typescript code in our NPM Question Answering package:\nconst model = await tf.node.loadSavedModel(path); // Load the model located in path\n\nconst result = tf.tidy(() => {\n   // ids and attentionMask are of type number[][]\n   const inputTensor = tf.tensor(ids, undefined, \"int32\");\n   const maskTensor = tf.tensor(attentionMask, undefined, \"int32\");\n\n   // Run model inference\n   return model.predict({\n     // \u201cinput_ids\u201d and \u201cattention_mask\u201d correspond to the names specified in the signature passed to get_concrete_function during the model conversion\n     \u201cinput_ids\u201d: inputTensor, \u201cattention_mask\u201d: maskTensor\n   }) as tf.NamedTensorMap;\n});\n\n// Extract the start and end logits from the tensors returned by model.predict\nconst [startLogits, endLogits] = await Promise.all([\n   result[\u201coutput_0\"].squeeze().array() as Promise,\n   result[\u201coutput_1\u201d].squeeze().array() as Promise\n]);\n\ntf.dispose(result); // Clean up memory used by the result tensor since we don\u2019t need it anymore\nNote the use of the very helpful TensorFlow.js function tf.tidy, which takes care of automatically cleaning up intermediate tensors like inputTensor and maskTensor while returning the result of the model inference.\n\n\nHow do we know we need to use \"ouput_0\" and \"output_1\" to extract the start and end logits (beginning and end of the possible spans answering the question) from the result returned by the model? We just have to look at the output names indicated by the saved_model_cli command we ran previously after exporting to SavedModel.\nThe need for fast and easy to use tokenizer: \ud83e\udd17 Tokenizers\nOur goal while building our Node.js library was to make the API as simple as possible. As we just saw, running model inference once we have our SavedModel is quite simple, thanks to TensorFlow.js. Now, the most difficult part is passing the data in the right format to the input ids and attention mask tensors. What we collect from a user is usually a string, but the tensors require arrays of numbers: we need to tokenize the user input.\n\nEnter \ud83e\udd17 Tokenizers: a performant library written in Rust that we\u2019ve been working on at Hugging Face. It allows you to play with different tokenizers such as BertWordpiece very easily, and it works in Node.js too thanks to the provided bindings:\nconst tokenizer = await BertWordPieceTokenizer.fromOptions({\n     vocabFile: vocabPath, lowercase: false\n});\n\ntokenizer.setPadding({ maxLength: 384 }); // 384 matches the shape of the signature input provided while exporting to SavedModel\n\n// Here question and context are in their original string format\nconst encoding = await tokenizer.encode(question, context);\nconst { ids, attentionMask } = encoding;\nThat\u2019s it! In just 4 lines of code, we are able to convert the user input to a format we can then use to feed our model with TensorFlow.js.\nThe Final Result: Powerful Question Answering in Node.js\nThanks to the powers of the SavedModel format, TensorFlow.js for inference, and Tokenizers for tokenization, we\u2019ve reached our goal to offer a very simple, yet very powerful, public API in our NPM package:\nimport { QAClient } from \"question-answering\"; // If using Typescript or Babel\n// const { QAClient } = require(\"question-answering\"); // If using vanilla JS\n\nconst text = `\n  Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season.\n  The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24\u201310 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\n  As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n`;\n\nconst question = \"Who won the Super Bowl?\";\n\nconst qaClient = await QAClient.fromOptions();\nconst answer = await qaClient.predict(question, text);\n\nconsole.log(answer); // { text: 'Denver Broncos', score: 0.3 }\nPowerful? Yes! Thanks to the native support of SavedModel format in TensorFlow.js, we get very good performances: here is a benchmark comparing our Node.js package and our popular transformers Python library, running the same DistilBERT-cased-squad model. As you can see, we achieve a 2X speed gain! Who said Javascript was slow?\nShort texts are texts between 500 and 1000 characters, long texts are between 4000 and 5000 characters. You can check the Node.js benchmark script here (the Python one is equivalent). Benchmark run on a standard 2019 MacBook Pro running on macOS 10.15.2.\nIt\u2019s a very interesting time for NLP: big models such as GPT2 or T5 keep getting better and better, and research on how to \u201cminify\u201d those good but heavy and costly models is also getting more and more traction, with distillation being one technique among others. Adding to the equation tools that allow big developer communities to be part of the revolution (such as TensorFlow.js with the Javascript ecosystem), only makes the future of NLP more exciting and more production-ready than ever!\n\nFor further reading, feel free to check our Github repositories:\nhttps://github.com/huggingface",
    "link": "https://blog.tensorflow.org/2020/05/how-hugging-face-achieved-2x-performance-boost-question-answering.html",
    "imgSource": [
      "https://4.bp.blogspot.com/-v0xrp7eJRfM/Xr77DD85ObI/AAAAAAAADDY/KjIlWlFZExQA84VRDrMEMrB534euKAzlgCLcBGAsYHQ/s1600/NLP%2Bmodels.png",
      "https://3.bp.blogspot.com/-bIVULlZg_I4/Xr77c6hHZxI/AAAAAAAADDg/iGNrokslFdwiDPr3QnHqqof2nmr34yK2QCLcBGAsYHQ/s1600/log.png",
      "https://3.bp.blogspot.com/-B-I2ftpL1J0/Xr8JBo7B0wI/AAAAAAAADDs/QQ3k9XN-tMApPv1Qou5gnlwZQPgMbuPCQCLcBGAsYHQ/s1600/questionansweringgraph.png"
    ],
    "time": "2023/12/06 00:58:43"
  },
  {
    "title": "Upcoming changes to TensorFlow.js",
    "content": "Posted by Yannick Assogba, Software Engineer, Google Research\n\nAs TensorFlow.js is used more and more in production environments, our team recognizes the need for the community to be able to produce small, production optimized bundles for browsers that use TensorFlow.js. We have been laying out the groundwork for this and want to share our upcoming plans with you.\n\nOne primary goal we have for upcoming releases of TensorFlow.js is to make it more modular and more tree shakeable, while preserving ease of use for beginners. To that end, we are planning two major version releases to move us in that direction. We are releasing this work over two major versions in order to maintain semver as we make breaking changes.\nTensorFlow.js 2.0\nIn TensorFlow.js 2.x, the only breaking change will be moving the CPU and WebGL backends from tfjs-core into their own NPM packages (tfjs-backend-cpu and tfjs-backend-webgl respectively). While today these are included by default, we want to make tfjs-core as modular and lean as possible.\nWhat does this mean for me as a user?\nIf you are using the union package (i.e. @tensorflow/tfjs), you should see no impact to your code. If you are using @tensorflow/tfjs-core directly, you will need to import a package for each backend you want to use.\nWhat benefit do I get?\nIf you are using @tensorflow/tfjs-core directly, you will now have the option of omitting any backend you do not want to use in your application. For example, if you only want the WebGL backend, you will be able to get modest savings by dropping the CPU backend. You will also be able to lazily load the CPU backend as a fallback if your build tooling/app supports that.\nTensorFlow.js 3.0\nIn this release, we will have fully modularized all our ops and kernels (backend specific implementations of the math behind an operation). This will allow tree shakers in bundlers like WebPack, Rollup, and Closure Compiler to do better dead-code elimination and produce smaller bundles.\n\nWe will move to a dynamic gradient and kernel registration scheme as well as provide tooling to aid in creating custom bundles that only contain kernels for a given model or TensorFlow.js program.\n\nWe will also start shipping ES2017 bundles by default. Users who need to deploy to browsers that only support earlier versions can transpile down to their desired target.\nWhat does this mean for me as a user?\nIf you are using the union package (i.e. @tensorflow/tfjs), we anticipate the changes will be minimal. In order to support ease of use in getting started with tfjs, we want the default use of the union package to remain close to what it is today.\n\nFor users who want smaller production oriented bundles, you will need to change your code to take advantage of ES2015 modules to import only the ops (and other functionality) you would like to end up in your bundle.\n\nIn addition, we will provide command-line tooling to enable builds that only load and register the kernels used by the models/programs you are deploying.\nWhat benefit do I get?\nProduction oriented users will be able to opt into writing code that results in smaller more optimized builds. Other users will still be able to use the union package pretty much as is, but will not get the advantage of the smallest builds possible.\n\nDynamic gradient and kernel registration will make it easier to implement custom kernels and gradients for researchers and other advanced users.\nFAQ\nWhen will this be ready?\nWe plan to release TensorFlow.js 2.0 this month. We do not have a release date for Tensorflow 3.0 yet because of the magnitude of the change. Since we need to touch almost every file in tfjs-core, we are also taking the opportunity to clean up technical debt where we can.\nShould I upgrade to TensorFlow.js 2.x or just wait for 3.x?\nWe recommend that you upgrade to TensorFlow 2.x if you are actively developing a TensorFlow.js project. It should be a relatively painless upgrade, and any future bug fixes will be on this release train. We do not yet have a release date for TensorFlow.js 3.x.\nHow do I migrate my app to 2.x or 3.x? Will there be a tutorial to follow?\nAs we release these versions, we will publish full release notes with instructions on how to upgrade. Separately, with the launch of 3.x, we will publish a guide on making production builds.\nHow much will I have to change my code to get smaller builds?\nWe\u2019ll have more details as we get closer to the release of 3.x, but at a high level, we want to take advantage of the ES2015 module system to let you control what code gets into your bundle.\n\nIn general, you will need to do things like import {max, div, mul, depthToSpace} from @tensorflow/tjfs (rather than import * as tf from @tensorflow/tfjs) in order for our tooling to determine which kernels to register from the backends you have selected for deployment. We are even working on making the chaining API on the Tensor class opt-in when targeting production builds.\nWill this make TensorFlow.js harder to use?\nWe do not want to make the barrier to entry higher for using TensorFlow.js so we are designing this in a way that only production oriented users need to do extra work to get optimized builds. For end-users developing applications using the union package (@tensorflow/tfjs) from either a hosted script or from NPM in concert with our collection of pre-trained models we expect there will be no changes as a result of these updates",
    "link": "https://blog.tensorflow.org/2020/04/upcoming-changes-to-tensorflowjs.html",
    "imgSource": [
      "https://2.bp.blogspot.com/-Qx-2Wj_4bIM/Xotwd_KTsRI/AAAAAAAAC6E/mfxRF_QrpQMojd03jiS9Nxd-3s9ND2kJQCLcBGAsYHQ/s1600/TF_Updates_v2.png"
    ],
    "time": "2023/12/06 00:58:43"
  },
  {
    "title": "Using TensorFlow.js to deploy a radiology-based image search application for radiologists",
    "content": "A guest post by Erwin John T. Carpio, MD\n\nAs a doctor and radiologist from the Philippines, I\u2019ve always wanted to learn how to develop and apply machine learning (ML from here on) models to my field of practice. However, machine learning was like a foreign language to me when I began. I had limited programming experience, and without a formal computer science background, I felt this field would be beyond my reach, especially when I tried to look at existing research. But that was soon about to change as I started my learning journey and I discovered this field was much more accessible than I initially thought.\nRadLens: A web app for reverse image search\nI\u2019m currently focusing on creating a tool named RadLens to inspire practitioners like myself. I hope it helps others in my field to consider how they may use machine learning tools to assist in their day to day practice. It\u2019s a work in progress (it is currently not FDA approved as a medical device and should not be used for diagnosis).\n\nOf course, in any medical machine learning application, the most important things are collecting a large, diverse training dataset and performing a rigorous evaluation. For a clinical grade application, this could include many thousands or more of expertly graded images. In my application, I focused on building a small tool as a proof of concept. I hoped that the tool would assist with my work although I would still ultimately be the physician responsible for making an accurate diagnosis.\n\nMy day to day work includes identifying and diagnosing fractures which is why I first wanted to see if I could, as a minimum viable prototype, build a web app that could classify 2 fracture types for the forearm (Monteggia and Galeazzi). Aside from detecting actual fractures, however, radiologists are also trained to be aware of various anatomic (normal) variants of the body so that we can differentiate these from actual pathology. Examples of anatomic variants are the foot ossicles. Radiologists train to know the difference between these anatomic variants and actual fracture fragments.\n\nWe sometimes look up anatomic variants in reference textbooks to ensure that what we\u2019re looking at is just an ossicle and not an actual fracture fragment. There are numerous foot ossicles, and it can be cumbersome to manually find the corresponding page in a textbook from memory. I decided to see if I could train a new ML model for the 2nd version of RadLens to detect several different foot ossicles I commonly see in my own practice.\n\nI wanted to build web apps that could:\nRemember images of the types of fractures for the 1st web app and 6 types of foot ossicles/anatomic variants for the 2nd version.\nUse the device\u2019s camera to scan in real time what I suspected to be a fracture (for the first web app) or a foot ossicle/anatomic variant (for the second web app)\nAutomatically direct me to a Google Image Search so I could cross reference the suspected \"fracture\" or \"foot ossicle/anatomic variant\u201d for myself.\nThe two versions or iterations of RadLens are detailed below.\n\nRadLens (version 1) is a web app that uses your device\u2019s camera to scan an x-ray image and tries to predict what type of fracture is shown in that image (if there is a fracture) without ever having to upload any images to the cloud to preserve patient privacy. If one of the fractures the system is trained on is found, it returns a score indicating how confident it is on the classification. As all inference is performed on the local machine, it makes this process even faster, as no server is required for classification. Importantly, RadLens also returns a link to a Google Image Search, so I can then browse for other images of this fracture, and use them to aid my diagnosis.\n\nThe initial version of RadLens focused on classifying two types of fractures (e.g. Monteggia and Galeazzi fractures of the forearm). The user experience flow looks like this:\nUsing RadLens version 1.\nLeft: An X-ray image of the forearm is scanned using the phone's camera in real time.\nCenter: RadLens classifies the fracture type as Monteggia or Galeazzi\nRight: Click the hyperlink to browse a Google Image Search for images of the detected fracture type for cross reference.\nThe hyperlink to Google Image Search is important, especially as RadLens is trained on only a small dataset. It makes using this tool interactive, and I can assess the accuracy as I go and use any mistakes it may make to help guide my future work. I can explore similar cases and verify if the model is correct, or decide if additional training data needs to be gathered, or if other medical professionals need to be consulted.\nBuilding RadLens\nInstead of developing an AI model that is as accurate as a radiologist, I instead decided to focus on small models that help me search for references faster. To build the first version of RadLens (for fractures), my initial prototypes were coded in Python using Tensorflow, taking advantage of a technique called Transfer Learning that enables you to train a new model which reuses some of the features learned by another model trained on a large dataset. After experimenting with this approach for a while, I decided to go with something even simpler and had greater reach.\n\nI discovered Teachable Machine, a website by Google that allows your computer to recognize your own images, sounds, & poses. You can even upload training data using the UI if you wish so training happens live in the web browser. I used the models produced by Teachable Machine to create my 2nd prototype for RadLens (for foot ossicles/anatomic variants). Teachable Machine is excellent for building simple and interactive prototypes that can help to communicate to radiologists how potential uses cases of ML could facilitate their work. The current process I chose for training ML models may not be suitable for building clinical grade applications (for that you will need a team of computer scientists and physicians working together with more data), but for my goal of helping other radiologists understand how ML can help with their day to day work, I find it to be extremely useful.\n\nAs an added bonus, using Teachable Machine meant that instead of using 2 programming languages for my project (Python and JavaScript), I could concentrate on using just one (JavaScript). Even better, inference is performed in the web browser with TensorFlow.js running on your laptop or phone; this means patient data remains private, and is never uploaded to a central server for classification, and inference time is faster too since there is no round trip time to the server.\nA 2nd prototype of RadLens. If the app detects a foot X-ray, it will then ask you to zoom in on the image. Once zoomed in, the app will try to infer what that ossicle is. You can then start an image search for the predicted class. Base code for the RadLens prototypes are available on GitHub.\nLooking forward\nMost of today\u2019s ML solutions for healthcare come prepackaged, and while robust, have many limitations. These systems have huge file sizes and limited deployment as the model must stay within a central IT system. In addition, they can be very expensive so only large hospitals and clinics can afford to use them. Since they are already pre-trained and pre-packaged, it can be hard for local radiologists to retrain them for use cases that may be more attuned to the local practice\u2019s needs. I essentially want to put cost effective ML into the palm of the local radiologist. Building a proof of concept system was more within reach than I initially thought.\n\nIn the future, I am hoping to further improve upon the web app by adding object detection to highlight the found fracture or ossicle with a visible bounding box on the image itself. Currently, the app performs image classification only, which detects the presence but does not show the location.\n\nI have learned that the spread of ML as a technology can both be horizontal and vertical. Horizontal applications are broad and widespread. These are usually made possible through the efforts of larger teams of AI experts, as they traverse the wide canvas of computer vision in medicine. I hope to spark interest in the vertical spread of AI development as it becomes more customized to the individual use cases of specific radiologists around the world. I can think of no better way of doing that right now than with the web and TensorFlow.js to easily enable people to try and experiment with the possibilities of using machine learning in their niche areas.",
    "link": "https://blog.tensorflow.org/2020/03/using-tensorflowjs-to-deploy-radiology-based-image-search-application.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-633a-5C-PfU/XnKWjhYVTvI/AAAAAAAAC4M/DWmo_IKvzqMwpSae-EyGZCwIsjNKEjsfACLcBGAsYHQ/s1600/fig1.png",
      "https://4.bp.blogspot.com/-92JR6jDrVXs/XnKXPt1HdiI/AAAAAAAAC4U/akyf9rpWp0oX8H8qyScr--BVrxmnRPfJwCLcBGAsYHQ/s1600/fig2.png"
    ],
    "time": "2023/12/06 00:58:43"
  },
  {
    "title": "Half-precision Inference Doubles On-Device Inference Performance",
    "content": "Posted by Marat Dukhan and Frank Barchard, Software Engineers\nCPUs deliver the widest reach for ML inference and remain the default target for TensorFlow Lite. Consequently, improving CPU inference performance is a top priority, and we are excited to announce that we doubled floating-point inference performance in TensorFlow Lite\u2019s XNNPack backend by enabling half-precision inference on ARM CPUs. This means that more AI powered features may be deployed to older and lower tier devices.\nTraditionally, TensorFlow Lite supported two kinds of numerical computations in machine learning models: a) floating-point using IEEE 754 single-precision (32-bit) format and b) quantized using low-precision integers. While single-precision floating-point numbers provide maximum flexibility and ease of use, they come at the cost of 4X overhead in storage and memory and exhibit a performance overhead compared to 8-bit integer computations. In contrast, half-precision (FP16) floating-point numbers pose an interesting alternative balancing ease-of-use and performance: the processor needs to transfer twice fewer bytes and each vector operation produces twice more elements. By virtue of this property, FP16 inference paves the way for 2X speedup for floating-point models compared to the traditional FP32 way.\nFor a long time FP16 inference on CPUs primarily remained a research topic, as the lack of hardware support for FP16 computations limited production use-cases. However, around 2017 new mobile chipsets started to include support for native FP16 computations, and by now most mobile phones, both on the high-end and the low-end. Building upon this broad availability, we are pleased to announce the general availability for half-precision inference in TensorFlow Lite and XNNPack.\nPerformance Improvements\nHalf-precision inference has already been battle-tested in production across Google Assistant, Google Meet, YouTube, and ML Kit, and demonstrated close to 2X speedups across a wide range of neural network architectures and mobile devices. Below, we present benchmarks on nine public models covering common computer vision tasks:\nMobileNet v2 image classification [download]\nMobileNet v3-Small image classification [download]\nDeepLab v3 segmentation [download]\nBlazeFace face detection [download]\nSSDLite 2D object detection [download]\nObjectron 3D object detection [download]\nFace Mesh landmarks [download]\nMediaPipe Hands landmarks [download]\nKNIFT local feature descriptor [download]\nThese models were benchmarked on 5 popular mobile devices, including recent and older devices (Pixel 3a, Pixel 5a, Pixel 7, Galaxy M12 and Galaxy S22). The average speedup is shown below.\nSingle-threaded inference speedup with half-precision (FP16) inference compared to single-precision (FP32) across 5 mobile devices. Higher numbers are better.\nThe same models were also benchmarked on three laptop computers (MacBook Air M1, Surface Pro X and Surface Pro 9)\nSingle-threaded inference speedup with half-precision (FP16) inference compared to single-precision (FP32) across 3 laptop computers. Higher numbers are better.\nCurrently, the FP16-capable hardware supported in XNNPack is limited to ARM & ARM64 devices with ARMv8.2 FP16 arithmetics extension, which includes Android phones starting with Pixel 3, Galaxy S9 (Snapdragon SoC), Galaxy S10 (Exynos SoC), iOS devices with A11 or newer SoCs, all Apple Silicon Macs, and Windows ARM64 laptops based with Snapdragon 850 SoC or newer.\nHow Can I Use It?\nTo benefit from the half-precision inference in XNNPack, the user must provide a floating-point (FP32) model with FP16 weights and special \"reduced_precision_support\" metadata to indicate model compatibility with FP16 inference. The metadata can be added during model conversion using the _experimental_supported_accumulation_type attribute of the tf.lite.TargetSpec object:\n...\nconverter.target_spec.supported_types = [tf.float16]\nconverter.target_spec._experimental_supported_accumulation_type = tf.dtypes.float16\nWhen the compatible model is delegated to XNNPack on a hardware with native support for FP16 computations, XNNPack will transparently replace FP32 operators with their FP16 equivalents, and insert additional operators to convert model inputs from FP32 to FP16 and convert model outputs back from FP16 to FP32. If the hardware is not capable of FP16 arithmetics, XNNPack will perform model inference with FP32 calculations. Therefore, a single model can be transparently deployed on both recent and legacy devices.\nAdditionally, the XNNPack delegate provides an option to force FP16 inference regardless of the model metadata. This option is intended for development workflows, and in particular for testing end-to-end accuracy of the model when FP16 inference is used. In addition to devices with native FP16 arithmetics support, forced FP16 inference is supported on x86/x86-64 devices with AVX2 extension in emulation mode: all elementary floating-point operations are computed in FP32, then converted to FP16 and back to FP32. Note that such simulation is slow and not a bit-exact equivalent to native FP16 inference, but simulates the effects of restricted mantissa precision and exponent range in the native FP16 arithmetics. To force FP16 inference, either build TensorFlow Lite with --define xnnpack_force_float_precision=fp16 Bazel option, or apply XNNPack delegate explicitly and add TFLITE_XNNPACK_DELEGATE_FLAG_FORCE_FP16 flag to the TfLiteXNNPackDelegateOptions.flags bitmask passed into the TfLiteXNNPackDelegateCreate call:\nTfLiteXNNPackDelegateOptions xnnpack_options =\n    TfLiteXNNPackDelegateOptionsDefault();\n...\nxnnpack_options.flags |= TFLITE_XNNPACK_DELEGATE_FLAG_FORCE_FP16;\nTfLiteDelegate* xnnpack_delegate =\n    TfLiteXNNPackDelegateCreate(&xnnpack_options);\nXNNPack provides full feature parity between FP32 and FP16 operators: all operators that are supported for FP32 inference are also supported for FP16 inference, and vice versa. In particular, sparse inference operators are supported for FP16 inference on ARM processors. Therefore, users can combine the performance benefits of sparse and FP16 inference in the same model.\nFuture Work\nIn addition to most ARM and ARM64 processors, the most recent Intel processors, code-named Sapphire Rapids, support native FP16 arithmetics via the AVX512-FP16 instruction set, and the recently announced AVX10 instruction set promises to make this capability widely available on x86 platform. We plan to optimize XNNPack for these instruction sets in a future release.\n\nAcknowledgements\nWe would like to thank Alan Kelly, Zhi An Ng, Artsiom Ablavatski, Sachin Joglekar, T.J. Alumbaugh, Andrei Kulik, Jared Duke, Matthias Grundmann for contributions towards half-precision inference in TensorFlow Lite and XNNPack.",
    "link": "https://blog.tensorflow.org/2023/11/half-precision-inference-doubles-on-device-inference-performance.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLe00EnYz8eKv0IaUn3KfPZnEa4ubDV7Ay2qqIFnOYEMLOh6ybHdH9RhUUrwhYgaccnNkTLe8pID8hvyyKd88JqJL2jK6-ePMxmsddBPcGPktJ_i_EUKmJI1x_YMv6gK3DHZzMqtWIw7Zc5Rx5eHDJH0zNSc-Cnp92ue4WYVWX9P5ATGCnOVeFx-jsI2c/s1600/TensorFlow_HalfPrecisionInference_1024x512.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhoSV-IQalCzvy3lhcb61AXM4IyP_t2Gwj8nBK1IhvH4fbi0nRB7f3ljwOBCxbRnu_xZwAtZUREerXmSclm8Q09hpTdha_i1-Bvoyw3grtqhW0RrdtDGNMebc7BVZPb6wsBBF_pIYKqnse03TFW07K7OBSPuB_TuuDyn06cmDRKkB2c0lN0-fv3wJO0xKk/s1600/TensorFlow_HalfPrecisionInference_4209x1253.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi9Jr2e_1Pg03j5uR3HDCiwIL7XPkVnVm4rfvGcZWzfZbE3yubuCmbmf2mxnOaj2lvbCUXRQLjkhkdAo7gPYQlmsud_hzIMoQz9-I-PLwnRngauGZf3dNZRAofs-EXyXNnK-nFa-rPAIOBz_AeB-H2k8PdLhqe2vv6yoO-FqfF3iMWhvbYskNSfWFK8fOg/s1600/image2.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiOymbNjV4dNtIyoBJOEc0akMN8crCQ-Yb5LdEu1SRfzE0038Prb8zRtwI7J9tUF8fY_d5aSllTXa_ff0IeTc7UNDW-ezY4PI4-AC7SrpNa7x_VK7pKleZUb7IUz8Q6_wiV_NiLhOKq3sUnkuEsJXszdrI70wqpEgXYjwMr4sr31sJO-L48ZBg9hTpwZTI/s1600/image1.png"
    ],
    "time": "2023/12/06 00:58:43"
  },
  {
    "title": "Building a board game with the TFLite plugin for Flutter",
    "content": "Posted by Wei Wei, Developer Advocate\nIn our previous blog posts Building a board game app with TensorFlow: a new TensorFlow Lite reference app and Building a reinforcement learning agent with JAX, and deploying it on Android with TensorFlow Lite, we demonstrated how to train a reinforcement learning (RL) agent with TensorFlow, TensorFlow Agents and JAX respectively, and then deploy the converted TFLite model in an Android app using TensorFlow Lite, to play a simple board game \u2018Plane Strike\u2019.\nWhile these end-to-end tutorials are helpful for Android developers, we have heard from the Flutter developer community that it would be interesting to make the app cross-platform. Inspired by the officially released TensorFlow Lite Plugin for Flutter recently, we are going to write one last tutorial and port the app to Flutter.\nSince we already have the model trained with TensorFlow and converted to TFLite, we can just load the model with TFLite interpreter:\nvoid _loadModel() async {\n  // Create the interpreter\n  _interpreter = await Interpreter.fromAsset(_modelFile);\n}\nThen we pass in the user board state and help the game agent identify the most promising position to strike next (please refer to our previous blog posts if you need a refresher on the game rules) by running TFLite inference:\nint predict(List<List<double>> boardState) {\n  var input = [boardState];\n  var output = List.filled(_boardSize * _boardSize, 0)\n      .reshape([1, _boardSize * _boardSize]);\n\n  // Run inference\n  _interpreter.run(input, output);\n\n  // Argmax\n  double max = output[0][0 ];\n  int  maxIdx = 0;\n  for (int i = 1; i < _boardSize * _boardSize; i++) {\n    if (max < output[0][i]) {\n      maxIdx = i;\n      max = output[0][i];\n    }\n  }\n\n  return maxIdx;\n}\nThat's it! With some additional Flutter frontend code to render the game boards and track game progress, we can immediately run the game on both Android and iOS (currently the plugin only supports these two mobile platforms). You can find the complete code on GitHub.\nIf you want to dig digger, there are a couple of things you can try:\nConvert the TFAgents-trained model to TFLite and run it with the plugin\nLeverage the RL technique we have used and build a new agent for the tic tac toe game in the Flutter Casual Games Toolkit. You will need to create a new RL environment and train the model from scratch before deployment, but the core concept and technique are pretty much the same.\nThis concludes this mini-series of blogs on leveraging TensorFlow/JAX to build games for Android and Flutter. And we very much look forward to all the exciting things you build with our tooling, so be sure to share them with @googledevs, @TensorFlow, and your developer communities!",
    "link": "https://blog.tensorflow.org/2023/10/building-board-game-with-tflite-plugin-for-flutter.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhOOPfPbEUuINDsxtTqIQnxPMLg_ThHYU0PSK5n12pt0hHAESQem0lIsnv-LO6LFCv8sq68n0dua5wPoXqiKvP8KF-WFF1s1-vpnTD3DMBrbL0IA019h1UWREjSCDEE2CEkImJ71OFBmchJWt8X5jtZkJiVIV06Ogxqhp12hzvvinVDNh1rMTecH9KIjZ8/s1600/SOCIAL-Building-a-board-game-with-the-TFlite-plugin-for-Flutter.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi9dGb3glYqGGek7G5cko5_R9dDez1txACGP5qt-dmn7EGgxdjGrt7ZIjNXmeu3KudSTHFs-Euh3xybYplG3arzRIcA8J9jx1gneCUEh_I9QRTMUWkvShjNW9ilbQLxd49Ocvts_w0T-FKCmjFn3lewlxbnkY0QGgzLKGofMwuHRR1bzamk7USpW0LnJmQ/s1600/HEADER-Building-a-board-game-with-the-TFlite-plugin-for-Flutter.png",
      null,
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQDl4Xa2yqwJdBabBma9TpiDP6oOt9DApNSnBd0u3dammHHQV5QuMS8WYj0h2xEMPWyG4bZKSDK0pFzjavXn7XscaIItUytQ-6GAtV2PBGyUZg0T7z_DXSHAhDDu8YgDeZxomS0ingTQI2llhqShlFPMVqgAK3UtgbaxKa-AE05RPabKfVFK_ezxyh5pc/s1600/CHART-Building-a-board-game-with-the-TFlite-plugin-for-Flutter.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj6OW57o43rPY9TavqdgnkbUoNqI1x0Fm57Avpuazd7DD4syTVW3NUWnvc6noNebb56Vm62EojXZBRhyphenhyphenj87JZVXSACv6BfIUAiiKnioK2Iy2wgKInFi0u_41Cnpkv9ZJUkPyfG2EhzxE1ckQuqJmZ1rRbogk0Tkdb-8gEzDDUGUgeQtoz0dy0h_YX0fxyw/s1600/Board-game-TF-Lite-Flutter%20%282%29.gif"
    ],
    "time": "2023/12/06 00:58:43"
  },
  {
    "title": "Simpleperf case study: Fast initialization of TFLite\u2019s Memory Arena",
    "content": "Posted by Alan Kelly, Software Engineer\nOne of our previous articles, Optimizing TensorFlow Lite Runtime Memory, discusses how TFLite\u2019s memory arena minimizes memory usage by sharing buffers between tensors. This means we can run models on even smaller edge devices. In today\u2019s article, I will describe the performance optimization of the memory arena initialization so that our users get the benefit of low memory usage with little additional overhead.\nML is normally deployed on-device as part of a larger pipeline. TFLite is used because it\u2019s fast and lightweight, but the rest of the pipeline must also be fast. Profiling on the target device with representative data lets us identify the slowest parts of the pipeline so that we can optimize the most important part of the code.\nIn this article, I will describe the profiling and optimization of TFLite\u2019s memory arena with instructions on how to use Simpleperf and visualize the results. Sample commands are given. It is assumed that the Android NDK is installed and that you have a development device that you can connect to using adb.\nSimpleperf\nSimpleperf comes with some scripts to make it easier to use. run_simpleperf_on_device.py pushes simpleperf to the device and runs your binary with the given arguments.\n/usr/lib/android-ndk/simpleperf/run_simpleperf_on_device.py record \u2013call-graph fp /data/local/tmp/my_binary arg0 arg1 \u2026\nThis will generate the output file perf.data which you must then copy back to your computer.\nadb pull /data/local/tmp/perf.data\nYou then generate the binary cache which contains all the information needed later to generate a useful profile.\n/usr/lib/android-ndk/simpleperf/binary_cache_builder.py -lib /your/binarys/folder -i perf.data\nAnd generate the proto buffer used for visualization:\n/usr/lib/android-ndk/simpleperf/pprof_proto_generator.py --ndk_path=/path/to/android-ndk -i perf.data -o profile.proto\nYou can then display the results this using pprof:\npprof -http :8888 profile.proto\nAnd open localhost:8888 in your browser to view the profile. I find flame graphs to be the most useful:\n\nOptimizing TFLite\u2019s Memory Arena\nArenaPlanner::ExecuteAllocations accounts for 54.3% of the runtime of this model. I was expecting to find that ML operators such as fully connected layers or convolutions to be the bottleneck of this model, and not runtime overhead. This is a particularly bad case, the memory arena overhead isn\u2019t this bad for every model, but improvements here will impact all models. This model has variable input sizes and many dynamic tensors, whose output size isn\u2019t known until operator evaluation, which trigger frequent tensor re-allocations. This really is as bad as it gets. Let\u2019s zoom in on the profile.\nInterpreterInfo::num_tensors() accounts for 10.4% of the runtime. The reason this function is so expensive is because it is a virtual function which calls another function and it is called within a loop. I would never have suspected this.\nfor (int i = 0; i < static_cast<int>(graph_info_->num_tensors()); ++i) {\n  \u2026\n}\nArena planner does not create or destroy tensors so the number of tensors is constant. Let\u2019s cache it.\nconst int num_tensors = static_cast<int>(graph_info_->num_tensors());\nfor (int i = 0; i < num_tensors); ++i) {\n  \u2026\n}\nOur next piece of low hanging fruit is InterpreterInfo::tensor(unsigned long) which is another virtual function which does bounds checking and then returns a pointer to a tensor. Tensors are stored in an array so let\u2019s add a function to get a pointer to this array. The commits are here and here.\nAfter these simple changes the runtime of this model has reduced by 25% and then overhead of the memory allocator by half. Simpleperf made identifying these inefficiencies easy! Time to profile again to measure the impact of these changes.\nArenaPlanner::CalculateAllocations is now the most expensive function at 12.7%. This calls two functions: SimpleMemoryArena::Allocate and ArenaPlanner::CreateTensorAllocationVector.\nAlthough ArenaPlanner::CreateTensorAllocationVector is the cheaper of the two, the code is far simpler so it might be easier to optimize. This function identifies which tensors are allocated between two nodes in the graph and then sorts them by size as a Greedy by Size allocation algorithm is used where the largest tensors are allocated first. The structure of the graph is constant so we can store a map of tensors allocated at each node. Instead of checking each tensor in the model to see if it is allocated between the two nodes, we can identify the tensors to be allocated by iterating through the map. The cost of ArenaPlanner::CreateTensorAllocationVector has gone from 4.8% to 0.8% of the runtime. Code can be seen here. Sort does not appear in the profile so we ignore it.\nThe next function to look at is ArenaPlanner::ResolveTensorAllocation which is 10.9% of the runtime after the previous optimizations. This function resets each tensor\u2019s data pointer after allocation. However, these pointers don\u2019t always change. How about keeping track of and only updating the ones which change? After this change, ArenaPlanner::ResolveTensorAllocation doesn\u2019t appear in the profile anymore.\nLet\u2019s now take a look at allocation and deallocation. SimpleMemoryArena::Allocate accounts for 7% and SimpleMemoryArena::Deallocate accounts for 6.8% of the runtime. A record of all allocations in the arena are stored in a vector ordered by their offsets within the memory arena. Entries in this sorted data structure are inserted, removed and searched. These operations are all O(N) in a vector. Could an std::multimap with the offset as the key be better? A multimap is needed because the records are ordered by their offsets and there may be multiple tensors with the same offset. Removal and insertion are O(logN) but search would still be O(N) as we are searching for the tensor id and not the offset. The best way to find out is to test and profile.\nReplacing the vector with a multimap actually slows down the arena code: it is almost three times slower than using a vector! While this goes against intuition, this is commonly found when optimizing code. Operations on a set or a map have linear or logarithmic complexities, however, there is also a constant value in the complexity. This value is higher than the constant value for the complexity of a vector. We also iterate through the records, which is much cheaper for a vector than for a list or multimap. A list was also tested, coming in at twice as slow as a vector.\nDeallocation can still be improved though. SimpleMemoryArena::Deallocate iterates through the records and when it finds the record to deallocate, it removes it from the vector. This has O(N2) complexity. The memcpy seen in the profile above comes from the frequent calls to std::vector::erase. It is much more efficient to mark records to be erased and then to erase them in one pass using std::remove_if. The second optimization here is to look at how tensors are typically deallocated: ArenaPlanner::ResetAllocationsAfter deallocates all tensors from a node until the end of the graph. To address this, SimpleMemoryArena::DeallocateAfter(int32_t node) was added which iterates once through all the records, marking those which are allocated after the node. SimpleMemoryArena::ResolveDeallocations erases these in one pass making deallocation O(N). After these changes, ResetAllocationsAfter no longer appears in the profile! Commits are here and here.\nThe profile now looks very different with the overhead of tensor allocation gone from 49.9% of the runtime to 11%. This profile already looks much more reasonable. SimpleMemoryArena::Allocate is the last function left to optimize. For each tensor, this function iterates through the vector of records trying to find space for the current allocation. The complexity of this is O(N2). This is a fundamental limitation of the Greedy By Size algorithm. Efficient use of memory comes at the cost of increased overhead. Although the complexity can\u2019t be reduced, N can. We process nodes in the order in which they are executed. Allocation information for tensors which have been deallocated on already executed nodes is not needed anymore, it is only slowing things down. Records are purged periodically so that only records which are active are considered. On a large model, this significantly reduces N. ArenaPlanner::ExecuteAllocations is no longer the most expensive function! It has gone from 11% to 6% and a fully connected operator is now the most expensive function in the profile, which is what we expect when profiling neural network inference.\nThis is what a neural network profile should look like. Time should be spent running your model\u2019s operators, not in the inference runtime.\nThe optimized memory arena is now publicly available as part of TensorFlow 2.13.\nNext Steps\nToday\u2019s post walked you through an example of Simpleperf helping to find easy to fix inefficiencies in TFLite\u2019s memory arena that would never have been found by just looking at the code. Pprof can display annotated source code, disassembly and graphs making it easy to find the bottlenecks in your on-device pipelines.",
    "link": "https://blog.tensorflow.org/2023/08/simpleperf-case-study-fast.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj-46_SfAtZJniOxOGJLavcD545oJnNBs3Q-AhPbzy57QVleiDFyY6-GgS8ABhWdCEXdweomDoLJjIn_GjtQx73iZ2zmccrou5jCRy6_fDrM9LpZlmEbd3x9DV8V2pBxFZNks6ytR9VlmK9Cw_Em1u5_tXp-pvhBjHj2SGjDd39mSAsHA0KrTCAHoTY2uI/s1600/tfsocial%20%281%29.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiNa6_GH3xPgUogQjPA_RUBB5vga9TXVtt6i3TjX9Dqpl4iCzRlpfmtehk3P7f15H0pNVTM3HzZKnGOWB5qICy9BHnDxoRjfRvD5TZXqyH9kEfiSzUADxTalQaJRtmI7CdKMfU73Y-rPb7F9uAYslkzPuwDYuaJyvsjdGSawh2pk2CzrAAdqiKw9W3y1r8/s1600/Simpleperf-TF-Header.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiVItxfwNM9arJL4E__71igirLNeD4oEu2Ytfx49k-ty-TXxvS3vd-J53sO2FIPbay0qabdu4CNVSz4olF7xzl6sgAufXhX2Ag5w_8Ak_jt76Dk_GQq7rgmzJ5L-GwS-ZdOVb58EIH9NjC8K74jC1O4BXRWbVhVsUY1yWm1zoAfxJNdOk8z-FYyyKZmhCQ/s1600/image2.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIOKkwSLXtGYzdqHThekCF4VjNgo2LePTK13zrWGU4jPTsi8PfReFGD9HUh0p44FDW-T1ZWW_2lxLNP6_YRFJdTGIxv6W08uDb8ehR98uOWmTyfBBHpfUSRN0vf7DCak-BvturTj-LPITLC3OdcevYJM6sNdRlYZXK1NWzr1XBcFplTwQ_iNbeoi4Qcs0/s1600/image5.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh5M5lS4MnwYZ7kHG_6c9t3nsipbgiR4gzxBTYaLQeyhuKU0lXx9U15K1x5a4oRpVnH622OPNBAQgbA2YN08BTP7SHxZPGdmZe6SsAsEG-6MVZ95_Vgz3ija9aXGRVmQrWwgVsqiolynHpXm9ljLC3siSXrb7R9_50MBcJT9yF6N_wv9MZUaKxiLwdMLr8/s1600/image1.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmAbsVuiceykKRiNA-XlEA9R3BLLfvoYZrtd8c5Hd4k97tlHm9tBuMEag4-UDU5439fWZysh2kDJmkQcmeIU4LVYjlQpN_ukTwQC3mFjofKqc09LKKwXpG5VmSPOjsVlr0WV-C0N2J61hkNi3Xpkd9iopSmtgU8sjdE1aECcXtn8wxchnQkfNbfvO-COM/s1600/image3.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjcVMdXbVIA-debMqWR-kltkIDR0381nhAGz9No2gHR_tONq4fbSPRrtEr3gO_J0A-hRK2OCiZ7Vb48l7tQjOlixc-Rx1JisMu5cAmxnDoUAAb3UQ_uZswvrwyJrpt2_CmWB7HcrKrzjPCBF9EmEpFMKbwYLNmlnziMxt5ixPu4iFDlpNtRofU2HLHXRA8/s1600/image4.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpwRYUSEqYWG2iBb-jdKpEHB9n7LqmIycVUdbZImZZUX_SsBtcpjnZ4neVsAdzusonvoruTnVMqomuQ7rEo_mnSQTyI8Bp9xN4hMEZibsrVv8LMMu6YMvpWf8AoXeLMNR-txstn66E-hN-QKaGJNjm-bC4wi2gHVx6GTfhcZ2MSoM4VN5ZmkL_ZP2XQ10/s1600/image6.png"
    ],
    "time": "2023/12/06 00:58:43"
  },
  {
    "title": "On-device fetal ultrasound assessment with TensorFlow Lite",
    "content": "Posted by Angelica Willis and Akib Uddin, Health AI Team, Google Research\nHow researchers at Google are working to expand global access to maternal healthcare with the help of AI\nTensorFlow Lite* is an open-source framework to run machine learning models on mobile and edge devices. It\u2019s popular for use cases ranging from image classification, object detection, speech recognition, natural language tasks, and more. From helping parents of deaf children learn sign language, to predicting air quality, projects using TensorFlow Lite are demonstrating how on-device ML could directly and positively impact lives by making these socially beneficial applications of AI more accessible, globally. In this post, we describe how TensorFlow Lite is being used to help develop ultrasound tools in under-resourced settings.\nMotivation\nAccording to the WHO, complications from pregnancy and childbirth contribute to roughly 287,000 maternal deaths and 2.4 million neonatal deaths worldwide each year. As many as 95% of these deaths occur in under-resourced settings and many are preventable if detected early. Obstetric diagnostics, such as determining gestational age and fetal presentation, are important indicators in planning prenatal care, monitoring the health of the birthing parent and fetus, and determining when intervention is required. Many of these factors are traditionally determined by ultrasound.\nAdvancements in sensor technology have made ultrasound devices more affordable and portable, integrating directly with smartphones. However, ultrasound requires years of training and experience, and, in many rural or underserved regions, there is a shortage of trained ultrasonography experts, making it difficult for people to access care. Due to this global lack of availability, it has been estimated that as many as two-thirds of pregnant people in these settings do not receive ultrasound screening during pregnancy.\nExpanding access by enabling non-experts\nGoogle Research is building AI models to help expand access to ultrasound, including models to predict gestational age and fetal presentation, to allow health workers with no background in ultrasonography to collect clinically useful ultrasound scans. These models make predictions from ultrasound video obtained using an easy-to-teach operating procedure, a blind sweep protocol, in which a user blindly sweeps the ultrasound probe over the patient's abdomen. In our recent paper, \u201cA mobile-optimized artificial intelligence system for gestational age and fetal malpresentation assessment\u201d, published in Nature Communications Medicine, we demonstrated that, when utilizing blind sweeps, these models enable these non-experts to match standard of care performance in predicting these diagnostics.\nBlind Sweep Operating Procedure\nThis blind-sweep ultrasound acquisition procedure can be performed by non-experts with only a few hours of ultrasound training.\nFigure A compares our blind sweep-based gestational age regression model performance with that of the clinical standard of care method for fetal age estimation from fetal biometry measured by expert sonographers. Boxes indicate 25th, 50th, and 75th percentile absolute error in days, and whiskers indicate 5th and 95th percentile absolute error (n\u2009=\u2009407 study participants). Figure B shows the Receiver Operating Characteristic (ROC) curves for our blind sweep-based fetal malpresentation classification model, as well as specific performance curves for cases in which blind sweeps were collected by expert sonographers or novices (n\u2009=\u2009623 study participants). See our recent paper for further details and additional analysis.\nModel development\nUnderstanding that our target deployment environment is one in which users might not have reliable access to power and internet, we designed these models to be mobile-optimized. Our grouped convolutional LSTM architecture utilizes MobileNetV2 for feature extraction on each video frame as it is received. The final feature layer produces a sequence of image embeddings which are processed by the convolutional LSTM cell state. Since the recurrent connections only operate on the less memory-intensive embeddings, this model can run efficiently in a mobile environment.\nFor each subsequence of video frames that make up a sweep, we generate a clip-level diagnostic result, and in the case of gestational age, also produce a model confidence estimate represented as the predicted variance in the detected age. Clip-level gestational age predictions are aggregated via inverse variance weighting to produce a final case-level prediction.\nOptimization through TensorFlow Lite\nOn-device ML has many advantages, including providing enhanced privacy and security by ensuring that sensitive input data never needs to leave the device. Another important advantage of on-device ML, particularly for our use case, is the ability to leverage ML offline in regions with low internet connectivity, including where smartphones serve as a stand-in for more expensive traditional devices. Our prioritization of on-device ML made TensorFlow Lite a natural choice for optimizing and evaluating the memory use and execution speed of our existing models, without significant changes to model structure or prediction performance.\nAfter converting our models to TensorFlow Lite using the converter API, we explored various optimization strategies, including post-training quantization and alternative delegate configurations. Leveraging a TensorFlow Lite GPU delegate, optimized for sustained inference speed, provided the most significant boost to execution speed. There was a roughly 2x speed improvement with no loss in model accuracy, which equated to real-time inference of more than 30 frames/second with both the gestational age and fetal presentation models running in parallel on Pixel devices. We benchmarked model initialization time, inference time and memory usage for various delegate configurations using TensorFlow Lite performance measurement tools, finding the optimal configuration across multiple mobile device manufacturers.\nThese critical speed improvements allow us to leverage the model confidence estimate to provide sweep-quality feedback to the user immediately after the sweep was captured. When low-quality sweeps are detected, users can be provided with tips on how their sweep can be improved (for example, applying more pressure or ultrasound gel), then prompted to re-do the sweep.\nWe developed a mobile application that demonstrates what a potential user experience could look like and allows us to evaluate our TensorFlow Lite models in realistic environments. This app enables ultrasound video frames to be received directly from portable ultrasound devices that support this use case.\nLooking ahead\nOur vision is to enable safer pregnancy journeys using AI-driven ultrasound that could broaden access globally. We want to be thoughtful and responsible in how we develop our AI to maximize positive benefits and address challenges, guided by our AI Principles. TensorFlow Lite has helped enable our research team to explore, prototype, and de-risk impactful care-delivery strategies designed with the needs of lower-resource communities in mind.\nThis research is in its early stages and we look forward to opportunities to expand our work. To achieve our goals and scale this technology for wider reach globally, partnerships are critical. We are excited about our partnerships with Northwestern Medicine in the US and Jacaranda Health in Kenya to further develop and evaluate these models. With more automated and accurate evaluations of maternal and fetal health risks, we hope to lower barriers and help people get timely care.\nAcknowledgements\nThis work was developed by an interdisciplinary team within Google Research: Ryan G. Gomes, Chace Lee, Angelica Willis, Marcin Sieniek, Christina Chen, James A. Taylor, Scott Mayer McKinney, George E. Dahl, Justin Gilmer, Charles Lau, Terry Spitz, T. Saensuksopa, Kris Liu, Tiya Tiyasirichokchai, Jonny Wong, Rory Pilgrim, Akib Uddin, Greg Corrado, Lily Peng, Katherine Chou, Daniel Tse, & Shravya Shetty.\nThis work was developed in collaboration with:\nDepartment of Obstetrics and Gynaecology, University of Zambia School of Medicine, Lusaka, Zambia\nDepartment of Obstetrics and Gynecology, University of North Carolina School of Medicine, Chapel Hill, NC, USA\nUNC Global Projects\u2014Zambia, LLC, Lusaka, Zambia\nSpecial thanks to: Yun Liu, Cameron Chen, Sami Lachgar, Lauren Winer, Annisah Um\u2019rani, and Sachin Kotwani\n*TensorFlow Lite has not been certified or validated for clinical, medical, or diagnostic purposes. TensorFlow Lite users are solely responsible for their use of the framework and independently validating any outputs generated by their project.",
    "link": "https://blog.tensorflow.org/2023/06/on-device-fetal-ultrasound-assessment-with-tensorflow-lite.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi8sF1ZR3G1VqS2QV_ezdQQUzHHhrj2dArTKxiA1AVgsqfIxwj03tGHKyHOp5KXArD73iI0ddC46hewEPwN9NH4ZEB7g8SMvBnMqnESruQXqxj-U2-iMLUdDlAuyYvsRu-j1QImxa0d95UTq5mQJd9cTn94Oz8ZceRwHTnm79BATrj4ZJg6qqJsA7Et/s1600/Social%20-%20TensorFlow%20-%20Fetal%20ultrasound%20assessment%20with%20TensorFlow%20Lite.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgLP4cZ-jPTE6AJWMnVjy9Md7BLgc-mIuXbLMPzJ2n9mksPyJJpIHw2mrHoANJHAaTsp4yaXcjegQPHXFm1UgLFSB3V7BrnTZRZn_6lPl-d_Htc1Uvg-2lp5RdTiqVk2jaG4pz6_2VKxaEfnKolND_0yi1bLyb9p2iplrBkklM55G9ZnBnlau7aJ9Ae/s1600/Header%20-%20TensorFlow%20-%20Fetal%20ultrasound%20assessment%20with%20TensorFlow%20Lite.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQRaFpMQ5oRPg5V-yuJFqnRAyPWS_YTAGQLXSESFJOh8gDuBGrXJwVJQ7WtgBfmcGH_nOI1881OruWG3zeAc3rWpEWXLd-Nu8AO7OZifapLEnJUPp1EFjIFI9eaFN6EKwbLqzSi6nxsaqXqAf2cVCelqqLY727PgD6pdQNMBeQ9ImO0zeYCp7K7z1iKHI/s1600/On-device-fetal-ultrasound-side-by-side-1.gif",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFKHCUo8RKKY9QL0OeDrMAzDVDFAwUb1LF5MKuQHHoB-Om1k-rMqAHbcvuslm7Bv-cZ_W_aQHjFez-6Eut0VvOW5lDueZP4JU_ugD-Ty2WWLeBUEiB8kj_HUekqipXw10f5mwldasgHZlh6hkGDUUoMAqB9uOvAgn0Z-NauHnd_R8crkjN0yil2qAEZFw/s1600/TF-Model-Performance-charts%20%281%29.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgH2p3X83t7oyMYkTrEepL-1cLFz2UzOJuhm1J6iKopYw5Z78LTw1-35u1468TQsrVgPJfb5Kn7q-Tkid4-mceGjTXAMB3Em-BtXekUxgl4KlAsgSaWKZBjjJRtAjefBBLQ177C2E0xYrvX3F2ZayN8cJjFd8lhYDoZFMprBhRJS7KPctBl81wjSwft/s1600/image5.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgp6xg54KydyzChZ8vhJ8XEyiC2kgHnGeZeuZAkuSkd3CiIqo0odUqcU496DUxtnQN4wE7V213GsSsDgGaWNIQ3V88AEXLIdm6Q-KpXhUufT1wmDL7O2qethVYTDwMvux18tthOIMO-0dSE8yeFQZ6moXR_KIAuG5eJaKBWJUA14muPcB-s0dQ0g1UJDCo/s1600/image4.gif"
    ],
    "time": "2023/12/06 00:58:43"
  },
  {
    "title": "TensorFlow Lite Micro with ML acceleration",
    "content": "Posted by Scott Main, Technical Writer, and the Coral team\nIn just a few years, ML models for mobile and embedded systems have come a very long way. With TensorFlow Lite (TFLite), you can now run sophisticated models that perform pose estimation and object segmentation, but these models still require a relatively powerful processor and a high-level OS in a mobile device or small computer like a Raspberry Pi. Alternatively, you can use TensorFlow Lite Micro (TFLM) on low-power microcontrollers (MCUs) to run simple models such as image and audio classification. However, the models for MCUs are much smaller, so they have limited capabilities and accuracy.\nSo there's an opportunity cost when you must select between TFLM (low power but limited model performance) and regular TFLite (great model performance but higher power cost). Wouldn't it be nice if you could get both on one board? Well, we're happy to announce that the Coral Dev Board Micro is now available to provide exactly that.\nA tiny board with big muscle\nThe Dev Board Micro is a microcontroller board (with a dual-core Cortex-M7 and Cortex-M4), so it's small and power efficient, but it also includes the Coral Edge TPU\u2122 on board, so it offers outstanding inferencing speeds for larger TFLite models. Plus, it has an on-board camera (324x324) and microphone. Naturally, there are plenty of GPIO pins and high-density connectors for add-on boards (such as our own Wireless Add-on and PoE Add-on).\nThe Dev Board Micro executes your models using TFLM, which supports only a subset of operations in TFLite. Even if TFLM did support all the same ops, the MCU would still be much too slow for practical applications that use complex models such as for object detection and pose estimation. However, when you compile a TFLite model for the Edge TPU, all the MCU needs to do is set the model's input, delegate the model ops to the Edge TPU, and then read the output.\nAs such, even though you're still using the smaller TFLM interpreter, you can run sophisticated TFLite models that otherwise are not compatible with the TFLM interpreter, because they actually execute on the Edge TPU. For example, with the Dev Board Micro, you can run PoseNet for pose estimation, BodyPix for body segmentation, SSD MobileNet for object detection, and much more, at realtime speeds. For example:\nOf course, running the Edge TPU demands more power, but the beauty of this board's dual-core MCU is that you can run low-power apps on the M4 (which supports tiny TFLM models) and then activate the M7 and Edge TPU only as needed to run more sophisticated TFLite models.\n\nTo better understand how this board compares to our other Coral board, here's a brief comparison of our different developer boards:\nGet started\nWe built a new platform for the Dev Board Micro based on FreeRTOS and included compatibility with the Arduino programming language. So you can build a C++ app with CMake and flash it to the board with our command line tools, or you can write and upload an Arduino sketch with the Arduino IDE. We call this new platform coralmicro and it's fully open sourced on GitHub.\nIf you choose to code with FreeRTOS, coralmicro includes all the core FreeRTOS APIs you need to build multi-tasking apps on the MCU, plus custom coralmicro APIs for interacting with GPIOs, capturing photos, listening to audio, performing multi-core processing, and much more.\nBecause coralmicro uses TensorFlow Lite for Microcontrollers for inferencing, running a TensorFlow Lite model on the Dev Board Micro works almost exactly the way you expect, if you've used TensorFlow Lite on other platforms. One difference with TFLM, compared to TFLite, is that you need to specify the ops used by your model by adding them to the MicroMutableOpResolver. For example, if your model uses 2D convolution, then you need to call AddConv2D(). This way, you conserve memory by compiling only the op kernels you actually need to run your model on the MCU. However, if your model is compiled to run on the Edge TPU, then you also need to add the Edge TPU custom op, which accounts for all the ops that run on the Edge TPU. For example, when using SSD MobileNet for object detection on the Edge TPU, only the dequantize and post-processing ops run on the MCU, and the rest are delegated to the Edge TPU custom op, so the code to set up the MicroInterpreter looks like this:\nauto tpu_context = coralmicro::EdgeTpuManager::GetSingleton()->OpenDevice();\nif (!tpu_context) {\n  printf(\"ERROR: Failed to get EdgeTpu context\\r\\n\");\n  vTaskSuspend(nullptr);\n}\n\ntflite::MicroErrorReporter error_reporter;\ntflite::MicroMutableOpResolver<3> resolver;\nresolver.AddDequantize();\nresolver.AddDetectionPostprocess();\nresolver.AddCustom(coralmicro::kCustomOp, coralmicro::RegisterCustomOp());\n\ntflite::MicroInterpreter interpreter(tflite::GetModel(model.data()), resolver,\n                                     tensor_arena, kTensorArenaSize,\n                                     &error_reporter);\nNotice that you also need to turn on the Edge TPU with OpenDevice(). Other than that and AddCustom(), the code to run an inference on the Dev Board Micro is pretty standard TensorFlow code. For more details, see our API reference for TFLM, and check out our code examples for FreeRTOS.\nIf you prefer to code with the Arduino IDE, we offer Arduino-style APIs for most of the same features available in FreeRTOS (multi-core processing is not available in Arduino). All you need to do is install the \"Coral\" boards package in the Arduino IDE's Board Manager, select the Dev Board Micro board, and then you can browse all our examples for the Dev Board Micro in File > Examples.\nYou can learn more about the board and find a seller here, and start running the code examples by following our get started guide.",
    "link": "https://blog.tensorflow.org/2023/02/tensorflow-lite-micro-with-ml-acceleration.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi56Z-LGz6atWKJyZYrOliUcQ9ZmqDnpWRCGr7VfmfWkFhT7ZANiWRymE3P110_t25ClB5hgO1Fw00dZwe0q4Bzzk7yci4J1RHvZ2C35U87SYvYKbT5tKkbdIoDQGLU6JbD8JXDbrdTYEv3jJkmKofxSU25-khUhrL5WfwqoaFLummdh7ab3k0gZgih/s1600/micro-inhand_fafafa.jpg",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi56Z-LGz6atWKJyZYrOliUcQ9ZmqDnpWRCGr7VfmfWkFhT7ZANiWRymE3P110_t25ClB5hgO1Fw00dZwe0q4Bzzk7yci4J1RHvZ2C35U87SYvYKbT5tKkbdIoDQGLU6JbD8JXDbrdTYEv3jJkmKofxSU25-khUhrL5WfwqoaFLummdh7ab3k0gZgih/s1600/micro-inhand_fafafa.jpg",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGWHiSXXNHpqOdFWwZuNq-Z6xtiFmRPoGhPHzgXrkSdU293lvclIa_kpGhn-pxfy7bAfyF0AMRIDzVtLuq4tQSBZDZiE0DKgeJtGILDfwsbdDbXGWoG1jGY6tBtIG5UM1S_xvln447MttV7uMULGmkTRuUbzzGZImj3-2-3BwxHPNi53FPoYj91rwP/s1600/Screen%20Shot%202022-12-14%20at%205.31.38%20PM.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh96QZGP9LiFT5xlsXgqIuqudpXy_0SBX01TtdeW-D8j9Zac8N5TCoC5PQIppOPcnVhRhx5VYxEgziDFvXFsPodXA47DjPbak6I__3hzcTljSREYPDH48h3MiT3sPaF5X-RULbRRzr2JCSGKKYnjnwcRLUuL8mUSjsN3tv3YkSNKRCUFhzaGGqCCvTY/s2052/Screen%20Shot%202022-12-14%20at%205.34.12%20PM.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh96QZGP9LiFT5xlsXgqIuqudpXy_0SBX01TtdeW-D8j9Zac8N5TCoC5PQIppOPcnVhRhx5VYxEgziDFvXFsPodXA47DjPbak6I__3hzcTljSREYPDH48h3MiT3sPaF5X-RULbRRzr2JCSGKKYnjnwcRLUuL8mUSjsN3tv3YkSNKRCUFhzaGGqCCvTY/s2052/Screen%20Shot%202022-12-14%20at%205.34.12%20PM.png"
    ],
    "time": "2023/12/06 00:58:43"
  },
  {
    "title": "Building a TensorFlow Lite based computer vision emoji input device with OpenMV",
    "content": "A guest post by Sandeep Mistry, Arm\nIntroduction\nEmojis allow us to express emotions in the digital world, they are relatively easy to input on smartphone and tablet devices equipped with touch screen based virtual keyboards, but they are not as easy to input on traditional computing devices that have physical keyboards. To input emojis on these devices, users typically use a keyboard shortcut or mouse to bring up an on-screen emoji selector, and then use a mouse to select the desired emoji from a series of categories.\nThis blog will highlight an in-depth open-source guide that uses tinyML on an Arm Cortex-M based device to create a dedicated input device. This device will take real-time input from a camera and applies a machine learning (ML) image classification model to detect if the image from the camera contains a set of known hand gestures (\u270b, \ud83d\udc4e, \ud83d\udc4d, \ud83d\udc4a). When the hand gesture is detected with high certainty, the device will then use the USB Human Interface Device (HID) protocol to \u201ctype\u201d the emoji on the PC.\nThe TensorFlow Lite for Microcontrollers run-time with Arm CMSIS-NN is used as the on-device ML inferencing framework on the dedicated input device. On-device inferencing will allow us to reduce the latency of the system, as the image data will be processed at the source (instead of being transmitted to a cloud service). The user\u2019s privacy will also be preserved, as no image data will leave the device at inference time.\nNOTE: The complete in-depth and interactive tutorial is available on Google Colab and all technical assets for the guide can be found on GitHub.\nMicrocontrollers and Keyboards\nMicrocontroller Units (MCUs) are self-contained computing systems embedded in the devices you use every day, including your keyboard! Like all computing systems, they have inputs and outputs.\nThe MCU inside a USB keyboard reacts to the digital events that occur when one or more of the key switches on the keyboard are pressed or released. The MCU determines which key(s) triggered the event and then translates the event into a USB HID message to send to the PC using the USB standard.\nBlock diagram of USB keyboard\nThe emoji \u2018keyboard\u2019 will use an image sensor for input (instead of key switches) and then process the image data locally on a more powerful Arm Cortex-M7 based microcontroller. All operations, including ML inferencing, are performed on a STM32H7 MCU, which contains an Arm Cortex-M7 CPU along with a digital interface for the image sensor and USB communications.\nBlock diagram of computer vision based emoji \u201ckeyboard\u201d\nEven though the STM32 H7 is a constrained computing platform that runs at 480 MHz with 1 MB of on-board RAM - we can still process a grayscale 96x96 pixel image input from the camera at just under 20 frames per second (fps)!\nThe OpenMV development platform\nOpenMV is an open source (Micro) Python powered Machine Vision platform. The OpenMV product line-up consists of several Arm Cortex-M based development boards. Each board is equipped with an on-board camera and MCU. For this project, the OpenMV Cam H7 or OpenMV Cam H7 R2 board will suit our needs.\nWhat we will need\nOpenMV Cam H7 Camera (left) and microSD card (right)\nHardware\nOpenMV Cam H7 or OpenMV Cam H7 R2 board\nMicroSD card with at least 2 MB of storage space\nUSB micro cable\nSoftware\nOpenMV IDE\nServices\nGoogle Colab\nKaggle Account\nDataset\nKaggle user Sparsh Gupta (@imsparsh) has previously curated and shared an excellent Gesture Recognition dataset and made it publicly available on Kaggle under a permissive CC0 1.0 Universal (CC0 1.0) Public Domain license.\nThe dataset contains ~23k image files of people performing various hand gestures over a 30 second period.\nImages from the dataset will need to be relabeled as follows:\nOriginal Labels\nNew Labels\nLeft hand swipe\nRight hand swipe\nThumbs down\nThumbs up\n\ud83d\udeab - No gesture\n\u270b - Hand up\n\ud83d\udc4e - Thumbs down\n\ud83d\udc4d - Thumbs up\n\ud83d\udc4a - Fist\n\nSince the swipe right and swipe left gestures in the Kaggle dataset do not correspond to any of these classes, any images in these classes will need to be discarded for our model.\nImages in the Kaggle dataset are taken over a 30 second period, they might contain other gestures at the start or end of the series. For example, some of the people in the dataset started with their hands in a fist position before eventually going to the labeled gesture hand up, thumbs up and thumbs down. Other times the person in the dataset starts off with no hand gesture in frame.\nWe have gone ahead and manually re-labeled the images into the classes, it can be found in CSV format in the data folder on GitHub, and contains labels for ~14k images.\nTensorFlow model\nYou can find more details on the training pipeline used here in this Colab Notebook.\nLoading and Augmenting Images\nImages from the dataset can be loaded as a TensorFlow Dataset using the tf.keras.utils.image_dataset_from_directory(...) API. This API supports adjusting the image\u2019s color mode (to grayscale) and size (96x96 pixels) to meet the model\u2019s desired input format. Built-in Keras layers for data augmentation (random: flipping, rotation, zooming, and contrast adjustments) will also be used during training.\nModel Architecture\nMobileNetV1 is a well-known model architecture used for image classification tasks, including the TensorLite for Microcontrollers Person detection example. This model architecture is trained on our dataset, with the same alpha (0.25) and image sizes (96x96x1) used in the Visual Wake Words Dataset paper. A MobileNetV1 model is composed of 28 layers, but a single call to the Keras tf.keras.applications.mobilenet.MobileNet(...) API can be used to easily create a MobileNetV1 model for 5 output classes and the desired alpha and input shape values:\npython\nmobilenet_025_96 = tf.keras.applications.mobilenet.MobileNet(\n    input_shape=(96, 96, 1),\n    alpha=0.25,\n    dropout=0.10,\n    weights=None,\n    pooling='avg',\n    classes=5,\n)\n\nThe MicroPython based firmware used on the OpenMV Cam H7 does not include support for all of the layer types in the MobileNetV1 model created using the Keras API, however it can be adapted to use supported layers using only ~30 lines of Python code. Once the model is adapted and trained it can then be converted to TensorFlow Lite format using the tf.lite.TFLiteConverter.from_keras_model(..) API. The resulting .tflite file can then be used for on-device inference on the OpenMV development board.\nOpenMV Application and inferencing\n\nThe .tflite model can then be integrated into the OpenMV application. You can find more details on the inference application in the Colab Notebook and full source code in the openmv folder on GitHub.\n\nThe application will loop continuously performing the following steps:\nBlock Diagram of Application processing pipeline\nGrab an image frame from the camera.\nGet the ML model\u2019s output for the captured image frame.\nFilter the ML model\u2019s output for high certainty predictions using \u201clow activation\u201d and \u201cmargin of confidence\u201d techniques.\nUse an exponential smoothing function to smooth the model\u2019s noisy (Softmax) outputs.\nUse the exponentially smoothed model outputs to determine if a new hand gesture is present.\nThen \u201ctype\u201d the associated emoji on a PC using the USB HID protocol.\nConclusion\nThroughout this project we\u2019ve covered an end-to-end flow of training a custom image classification model and how to deploy it locally to a Arm Cortex-M7 based OpenMV development board using TensorFlow Lite! TensorFlow was used in a Google Colab notebook to train the model on a re-labeled public dataset from Kaggle. After training, the model was converted into TensorFlow Lite format to run on the OpenMV board using the TensorFlow Lite for Microcontrollers run-time along with accelerated Arm CMSIS-NN kernels.\nAt inference time the model\u2019s outputs were processed using model certainty techniques, and then fed output from the (Softmax) activation output into an exponential smoothing function to determine when to send keystrokes over USB HID to type emojis on a PC. The dedicated input device we created was able to capture and process grayscale 96x96 image data at just under 20 fps on an Arm Cortex-M7 processor running at 480 MHz. On-device inferencing provided a low latency response and preserved the privacy of the user by keeping all image data at the source and processing it locally.\nBuild one yourself by purchasing an OpenMV Cam H7 R2 board on openmv.io or a distributor. The project can be extended by fine tuning the model on your own data or applying transfer learning techniques and using the model we developed as base to train other hand gestures. Maybe you can find another public dataset for facial gestures and use it to type \ud83d\ude00 emojis when you smile!\n\nA big thanks to Sparsh Gupta for sharing the Gesture Recognition dataset on Kaggle under a public domain license and my Arm colleagues Rod Crawford, Prathyusha Venkata, Elham Harirpoush, and Liliya Wu for their help in reviewing the material for this blog post and associated tutorial!",
    "link": "https://blog.tensorflow.org/2022/11/building-tensorflow-lite-based-computer-vision-emoji-input-device-openmv.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjc40D2_Rf0thFm_3_jvI0tCAqEclYeWViMpGH-3GU-uBOOHVEecs0Qxz2vEW9vn2gJOPqtsdM30yXHYXLdrFv-caVtMZOVq2hhfyvOQeSlsjJ9rpUMKxdCohIFtqACoHokDBIeFrZ2Er5qkDvbwgQt-xFukK8REUJANDDrIskgBKx8LUrDWQ7M9UkU/w665-h374/image5.gif",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjjTkCRxTT94Y1EhXhwvin6CNezdQxG94qgUFgBByB3ylblpr1ODLoHcdgE8k3p2G_sIG4xr6mViA6Hau6tXNbssA-ccdLS5ImVt-FnZB4ZRi84plDCqbQ5Up_bU157S3GQRzXIScGkh2IxZyZ-oGkg94gPaw4912C1VphwM2AmoW4GnxucTySX_kpP/s1600/image2.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_x6kuwMtUyT88UPG-5CAIJCfE_68Yxza0cdV1Tsb6mKlOcdB-1JIkDziV_CAznEQfSnwA1BZ46XB0f3--f3hCDUCi4wsSsTHSwp1smBtRPOIdsPLgT4IO8w8FrmCYu9M2Fwd74UlROQoQIQ0W5bPXXOavuPvMVPores8esWPARsy0ZmgS5ch_XclW/s1600/image3.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJapZTSyc8Tol-VLMH-FU8JJdVf2nySGCJchSSj8TwfLtROPoPJ3F_h0sjpyokLeDs0r_XRocfeXBI-YoYma2Uz5n82XaxbdVZcZXsXat6iRYEWZiGZMI_3bj_6q-4RMwdwFpdxpYbCZ4hw1ly5gL1OCeBZvsu17HfmEfzvnZunlMhmzokF849bgO0/w400-h300/image4.jpg",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiU1pfsY_PVH34u5PXKAIUfMn0O_0vhczRrZAyNerRxVGSA560v11yx2VKvqo0deZ4DScifUCThMb-sEGiszGoyzFSHUJX7XPLg42vZ7Fj6HxSdoRSAjisjQCEJ-IRDXW2oWLfehHX1PTiEaMLnaeDsD-H6l_-BC19xsr7-6-2s2Y2_-WtuOU5Cykz6/s1600/image1.png"
    ],
    "time": "2023/12/06 00:58:43"
  },
  {
    "title": "Integrating Arm Virtual Hardware with the TensorFlow Lite Micro Continuous Integration Infrastructure",
    "content": "A guest post by Matthias Hertel and Annie Tallund of Arm\nMicrocontrollers power the world around us. They come with low memory resources and high requirements for energy efficiency. At the same time, they are expected to perform advanced machine learning interference in real time. In the embedded space, countless engineers are working to solve this challenge. The powerful Arm Cortex-M-based microcontrollers are a dedicated platform, optimized to run energy-efficient ML. Arm and the TensorFlow Lite Micro (TFLM) team have a long-running collaboration to enable optimized inference of ML models on a variety of Arm microcontrollers.\nAdditionally, with well-established technologies like CMSIS-Pack, the TFLM library is ready to run on to 10000+ different Cortex-M microcontroller devices with almost no integration effort. Combining these two offers a great variety of platforms and configurations. In this article, we will describe how we have collaborated with the TFLM team to use Arm Virtual Hardware (AVH) as part of the TFLM projects open-source continuous integration (CI) framework to verify many Arm-based processors with TFLM. This enables developers to test their projects on Arm intellectual property IP without the additional complexity of maintaining hardware.\nArm Virtual Hardware - Models for all Cortex-M microcontrollers\nArm Virtual Hardware (AVH) is a new way to host Arm IP models that can be accessed remotely. In an ML context, it offers a platform to test models without requiring the actual hardware. The following Arm M-profile processors are currently available through AVH:\nCortex-M0, Cortex-M0+\nCortex-M3, Cortex-M4, Cortex-M7\nCortex-M23, Cortex-M33\nArm Corstone is another virtualization technology, in the form of a silicon IP subsystem, helping developers verify and integrate their devices. The Corstone framework builds the foundation for many modern Cortex-M microcontrollers. AVH supports multiple platforms including Corstone-300, Corstone-310 and Corstone-1000. The full list of supported platforms can be found here.\nThrough Arm Virtual Hardware, these building blocks are available as Amazon Machine Image (AMI) on Amazon Web Services (AWS) Marketplace and locally through Keil MDK-Professional.\nThe Arm Virtual Hardware end-to-end workflow, from developer to the cloud.\nGitHub Actions and Arm Virtual Hardware\nGitHub Actions provides a popular CI solution for open-source projects, including TensorFlow Lite Micro. The AVH technology can be integrated with the GitHub Actions runner and that can be used to run tests on the different Arm platforms as natively compiled code without the need to have the hardware available. \nLet\u2019s get into how it\u2019s done!\nDefining a AVH use-case through a GitHub Actions workflow\nOverview\nOver the past year, we have made it possible to set up Arm IP verification in GitHub Actions. We will walk you through the steps needed to perform this integration with TFLM. The same process can be repeated for other open-source projects that use GitHub Actions as well.\nA GitHub workflow file (such as Corstone-300 workflow in the TFLM repository) can be used to run code on an AWS EC2 instance, which has Arm IP installed. This workflow builds the TFLM project with Corstone-300 as a target, and runs the unit tests using both GCC and armclang, displaying the results directly in the GitHub UI via a hierarchical process as visualized below.\nThe workflow contains one or more jobs, which points to a file containing steps. The steps are defined in a separate file (cortex_m_corstone_300_avh.yml). In our example, the steps will then point to a test script (test_cortex_m_corstone_300.sh), which is sent using an Arm-provided API (AVH Client) to the AWS instance where it is then executed accordingly. The script will send back output, which is obtained by the AVH client and can be displayed in the GitHub Actions UI.\nDepending on the nature of the use case, this can happen one or several times, which all depends on the number of jobs and steps defined. In the Corstone-300 case, we use a single job with steps that will only run one test script only. This is not a limitation however, as visualized in the flowchart above.\nConnecting the GitHub Actions runner to the AWS EC2 instance running AVH\nLet\u2019s have a look at how the AVH client connects to our AWS EC2 instance. The AVH client is a python-based tool that makes accessing AVH services easier. It sets up a VM which has the virtual hardware target (VHT) installed. The client can be installed from pypi.org using pip into any environment running Python. From there it can offload any compilation and test job onto Arm Virtual Hardware. For our Corstone-300 example, it is installed on the GitHub Actions runner by adding a pip install to the workflow file.\n\n- name: Install AVH Client for Python \n     run: | \n       pip install git+https://github.com/ARM-software/avhclient.git@v0.1 \nThe AWS credentials are configured to allow the AVH client to connect to the AWS EC2 instance, though there are various other ways to authenticate with AWS services. These include adding the AWS keypair onto GitHub secrets, or using an allow-listed GitHub repository to ordinate a predefined role, as shown here.\n\n- name: Configure AWS Credentials \n     uses: aws-actions/configure-aws-credentials@v1 \n     with: \n       role-to-assume: arn:aws:iam::720528183931:role/Proj-vht-assume-role \n            aws-region: eu-west-1 \nDefining and executing a workload\nFinally, let\u2019s look at how the workload itself is executed using the AVH client. In this example, the AVH workload is described in a YAML file which we point to in the Github workflow file.\n\n- name: Execute test suite on Arm Virtual Hardware at AWS \n     run: | \n       avhclient -b aws execute --specfile   ./tensorflow/lite/micro/tools/github/arm_virtual_hardware/cortex_m_generic_avh.yml \n\nThis is where we define a list of steps to be executed. The steps will point to an inventory of files to be transferred, like the TFLM repository itself. Additionally, we define the code that we want to execute using these files, which can be done through the script that we provided earlier.\n\nsteps: \n  - run: | \n   git clone https://github.com/tensorflow/tflite-micro.git  \n   mv ./tflite-micro/TensorFlow/ . \n   TensorFlow/lite/micro/tools/ci_build/test_cortex_m_corstone_300.sh armclang &> ./corstone300.log \n\nNext, we set up a list of files to copy back to the GitHub Actions runner. For the TFLM unit test, a complete command line log will be written to a file, corstone300.log \u2013 that is returned to the GitHub Actions runner to analyze the test run outcome:\n\n- name: Fetch results from Arm Virtual Hardware \n     run: | \n       cat ./tensorflow/lite/micro/tools/github/arm_virtual_hardware/cortex_m_generic.log \n\nYou can find a detailed explanation of avhclient and its usage on the Arm Virtual Hardware Client GitHub repository and the getting started guide.\nExpanding the toolbox by adding more hardware targets\nUsing AVH it is easy to extend tests to all available Arm platforms. You can also avoid a negative impact on the overall CI workflow execution time by hosting through cloud services like AWS and spawning an arbitrary number of AVH instances in parallel.\n\nVirtual Hardware targets like the Corstone-310 demonstrate how software validation is feasible even before silicon is available. This will make well-tested software stacks available for new Cortex-M devices from day one and we plan to expand the support. The introduction of Corstone-1000 will extend the range of tested architectures into the world of Cortex-A application processors, including Cortex-A32, Cortex-A35, Cortex-A53.\nWrapping up\nTo summarize: by providing a workflow file, a use-case file, and a workload (in our case, a test script), we have enabled running all the TFLM unit tests on the Corstone-300 and will work to extend it to all AVH targets available.\n\nThanks to the AVH integration, CI flows with virtual hardware targets open up new possibilities. Choosing the right architecture, integrating, and verifying has never been easier. We believe it is an important step in making embedded ML more accessible and that it will pave the way for future applications.\n\nThank you for reading!\nAcknowledgements\nWe would like to acknowledge a number of our colleagues at Arm who have contributed to this project, including Samuel Peligrinello Caipers, Fredrik Knutsson, and M\u00e5ns Nilsson.\n\nWe would also like to thank Advait Jain from Google and John Withers of Berkeley Design Technology, Inc. for architecting a continuous integration system using GitHub Actions that has enabled the Arm Virtual Hardware integration described in this article.",
    "link": "https://blog.tensorflow.org/2022/10/integrating-arm-virtual-hardware-with-tensorflow-lite-micro-continuous-integration-infrastructure.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglEEkfOwhq-pl53rVdcsChMuUF5laWHPqrBqSkZX5XEJF8zUwyn6lH_TDrr_L7mZRwC7T9f6BUuD3kTD4fTXSxI5Gbyi4TLG2HJH9PsaSEtx57yes15-iZzCAycjOM1sERg7kNb-95YLsRRRpe0ZYBAqKI9D6Ruh1ZGw51a8S0QxyJd4Ns1P8EIELh/s1600/Tensorflow-ARM-virtual-hardware-02.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgU9Jdu_GkNKM8h59abag8qT0jyNJ1_4y9TeGOt_IkBiEfOenf0d9kxBsEDzSLnsvYFnXQ2zo3J6JR-8eb8YzHhZsTZ3APGWg1PZf9ymtQ6IwFPZT2fu8Ymk82tkBUIbW1yPIFoI9y5Ss8w9zyuiN1lB6DChkZbIMncPZJJI2H1WvWbnF3TsdqbAcG/s1600/Tensorflow-ARM-virtual-hardware-01.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgUeD-hOQnb3g3Ern1Djh2LskE-KszFhp2pgTE-Gx4b1z30vYG54AK9D8QBYyGCqzHRCcyZETCU37eNkbwy5xRZtLV9CUacy-pfW95AsCV69mdKwW-9kqUgzda3vG5VJjJI9CkZ2rdldAbOwl6Riu-hjkwtsYK9_j9m_5WhhLKZ8vDF3hXQW1ByAull/s1600/image1.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifTYYjqciGUOYzEa5slOwAPMAfIrfbka6xm5QdzCipt2fuuqI6Xaj2nitnZAxxE2xxV5jtMh9gsvAxH1I28hdXHmgeeYGSC0Id0ZLUbPMDgoHnc2Cpxss5bamKbCJk7P9Z8uQYuoDUburvrJvqjLp57-fKxYEr_G4MLcJYRrUREgXiik01MoHqFLRv/s1600/image2.png"
    ],
    "time": "2023/12/06 00:58:43"
  },
  {
    "title": "Building a reinforcement learning agent with JAX, and deploying it on Android with TensorFlow Lite",
    "content": "Posted by Wei Wei, Developer Advocate\nIn our previous blog post Building a board game app with TensorFlow: a new TensorFlow Lite reference app, we showed you how to use TensorFlow and TensorFlow Agents to train a reinforcement learning (RL) agent to play a simple board game \u2018Plane Strike\u2019. We also converted the trained model to TensorFlow Lite and then deployed it into a fully-functional Android app. In this blog, we will demonstrate a new path: train the same RL agent with Flax/JAX and deploy it into the same Android app we have built before. The complete code has been open sourced in the tensorflow/examples repository for your reference.\n\nTo refresh your memory, our RL-based agent needs to predict a strike position based on the human player\u2019s board position so that it can finish the game before the human player does. For more detailed game rules, please refer to our previous blog.\nDemo game play in \u2018Plane Strike\u2019\nBackground: JAX and TensorFlow\nJAX is a NumPy-like library developed by Google Research for high performance computing. It uses XLA to compile programs optimized for GPUs and TPUs. Flax is a popular neural network library built on top of JAX. Researchers have been using JAX/Flax to train very large models with billions of parameters (such as PaLM for language understanding and generation, or Imagen for image generation), making full use of modern hardware. If you're new to JAX and Flax, start with this JAX 101 tutorial and this Flax Getting Started example.\n\nTensorFlow started as a library for ML towards the end of 2015 and has since become a rich ecosystem that includes tools for productionizing ML pipelines (TFX), data visualization (TensorBoard), deploying ML models to edge devices (TensorFlow Lite), and devices running on a web browser or any device capable of executing JavaScript (TensorFlow.js). Models developed in JAX or Flax can tap into this rich ecosystem by first converting such a model to the TensorFlow SavedModel format, and then using the same tooling as if they had been developed in TensorFlow natively.\n\nIf you already have a JAX-trained model and want to deploy it today, we have put together a list of resources for you:\nServing JAX models with TensorFlow Serving video shows you how to deploy JAX models with TensorFlow Serving\nJAX on the Web with TensorFlow.js blog walks through how to convert JAX models to TFJS and run them within web apps\nThis blog post demos how to convert a Flax/JAX model to TFLite and run it in a native Android app\nOverall, no matter what your deployment target is (server, web or mobile), we got you covered.\nImplementing the game agent with Flax/JAX\n\nComing back to our board game, to implement our RL agent, we will leverage the same gym environment as before. We will train the same policy gradient model using Flax/JAX this time. Recall that mathematically the policy gradient is defined as:\n \nwhere:\nT: the number of timesteps per episode, which can vary per episode\nst: the state at timestep t\nat: chosen action at timestep t given state s\n\u03c0\u03b8: the policy parameterized by \u03b8\nR(*): the reward gathered, given the policy\nWe define a 3-layer MLP as our policy network, which predicts the agent\u2019s next strike position.\nclass PolicyGradient(nn.Module):\n  \"\"\"Neural network to predict the next strike position.\"\"\"\n   @nn.compact\n  def __call__(self, x):\n    dtype = jnp.float32\n    x = x.reshape((x.shape[0], -1))\n    x = nn.Dense(\n        features=2 * common.BOARD_SIZE**2, name='hidden1', dtype=dtype)(\n           x)\n    x = nn.relu(x)\n    x = nn.Dense(features=common.BOARD_SIZE**2, name='hidden2', dtype=dtype)(x)\n    x = nn.relu(x)\n    x = nn.Dense(features=common.BOARD_SIZE**2, name='logits', dtype=dtype)(x)\n    policy_probabilities = nn.softmax(x)\n    return policy_probabilities\n\nIn our main training loop, in each iteration we use the neural network to play a round of the game, gather the trajectory information (game board positions, actions taken and rewards), discount the rewards, and then train the model with the trajectories.\n\nfor i in tqdm(range(iterations)):\n   predict_fn = functools.partial(run_inference, params)\n   board_log, action_log, result_log = common.play_game(predict_fn)\n   rewards = common.compute_rewards(result_log)\n   optimizer, params, opt_state = train_step(optimizer, params, opt_state,\n                                             board_log, action_log, rewards)\n\nIn the train_step() method, we first compute the loss using the trajectories. Then we use jax.grad() to compute the gradients. Lastly we use Optax, a gradient processing and optimization library for JAX, to update the model parameters.\n\ndef compute_loss(logits, labels, rewards):\n  one_hot_labels = jax.nn.one_hot(labels, num_classes=common.BOARD_SIZE**2)\n  loss = -jnp.mean(\n      jnp.sum(one_hot_labels * jnp.log(logits), axis=-1) * jnp.asarray(rewards))\n  return loss\n  def train_step(model_optimizer, params, opt_state, game_board_log,\n              predicted_action_log, action_result_log):\n  \"\"\"Run one training step.\"\"\"\n   def loss_fn(model_params):\n    logits = run_inference(model_params, game_board_log)\n    loss = compute_loss(logits, predicted_action_log, action_result_log)\n    return loss\n   def compute_grads(params):\n    return jax.grad(loss_fn)(params)\n   grads = compute_grads(params)\n  updates, opt_state = model_optimizer.update(grads, opt_state)\n  params = optax.apply_updates(params, updates)\n  return model_optimizer, params, opt_state\n  @jax.jit\ndef run_inference(model_params, board):\n  logits = PolicyGradient().apply({'params': model_params}, board)\n  return logits\n\nThat\u2019s it for the training loop. We can visualize the training progress in TensorBoard as below; here we use the proxy metric \u2018game_length\u2019 (the number of steps to finish the game) to track the progress. The intuition is that when the agent becomes smarter, it can finish the game in fewer steps.\n\nConverting the Flax/JAX model to TensorFlow Lite and integrating with the Android app\nAfter the model is trained, we use the jax2tf, a TensorFlow-JAX interoperation tool, to convert the JAX model into a TensorFlow concrete function. And the final step is to call TensorFlow Lite converter to convert the concrete function into a TFLite model.\n\n # Convert to tflite model\n model = PolicyGradient()\n jax_predict_fn = lambda input: model.apply({'params': params}, input)\n  tf_predict = tf.function(\n     jax2tf.convert(jax_predict_fn, enable_xla=False),\n     input_signature=[\n         tf.TensorSpec(\n             shape=[1, common.BOARD_SIZE, common.BOARD_SIZE],\n             dtype=tf.float32,\n             name='input')\n     ],\n     autograph=False,\n )\n  converter = tf.lite.TFLiteConverter.from_concrete_functions(\n     [tf_predict.get_concrete_function()], tf_predict)\n  tflite_model = converter.convert()\n  # Save the model\n with open(os.path.join(modeldir, 'planestrike.tflite'), 'wb') as f:\n   f.write(tflite_model)\n\nThe JAX-converted TFLite model behaves exactly like any TensorFlow-trained TFLite model. You can visualize it with Netron:\nVisualizing TFLite model converted from Flax/JAX using Netron\nWe can use exactly the same Java code as before to invoke the model and get the prediction.\nconvertBoardStateToByteBuffer(board);\ntflite.run(boardData, outputProbArrays);\nfloat[] probArray = outputProbArrays[0];\nint agentStrikePosition = -1;\nfloat maxProb = 0;\nfor (int i = 0; i < probArray.length; i++) {\n  int x = i / Constants.BOARD_SIZE;\n  int y = i % Constants.BOARD_SIZE;\n  if (board[x][y] == BoardCellStatus.UNTRIED && probArray[i] > maxProb) {\n    agentStrikePosition = i;\n    maxProb = probArray[i];\n  }\n}\nConclusion\nIn summary, this article walks you through how to train a simple reinforcement learning model with Flax/JAX, leverage jax2tf to convert it to TensorFlow Lite, and integrate the converted model into an Android app.\n\nNow you have learned how to build neural network models with Flax/JAX, and tap into the powerful TensorFlow ecosystem to deploy your models pretty much anywhere you want. We can\u2019t wait to see the fantastic apps you build with both JAX and TensorFlow!",
    "link": "https://blog.tensorflow.org/2022/09/building-reinforcement-learning-agent-with-JAX-and-deploying-it-on-android-with-tensorflow-lite.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiOti36pT-2IwVvWjrezVC87xl1pw2W9SQgYhfI0sRUOSG5EDDtdp6jQR9iJx6k3me_zklHq_2RxaMqYD8628T3bteI0gZ5ZDdogQxSleJppglgBo-MKPDdQUWEEw9vX0UjFvDZ2lU0UT4eOVg64CqmqQPP6AH9BntCVpA5kx3tiQZvrI66_GYxgwwG/s1600/Tensorflow-building-reinforcement-learning-agent-with-JAX-social.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5XPGcGH5styeLZUJPU4pF0VhSor6M2lT_nGVjsid6Nr-tOuObab9Pqwsn83FiOO8XUS2b-g8ugd2BnwAqTFjKxxUiDZaIabDMxVeO4Bvl4Y7EduMOCQ4IqU3JgKaa3fJUWUVfmkWuGZ8dsH2EtDqe2gj8NMw4GfhkZe1bYQRdTTY3sauYMLn1xaqn/w666-h198/Tensorflow-building-reinforcement-learning-agent-with-JAX-header.png",
      "https://blogger.googleusercontent.com/img/a/AVvXsEj7K9sQBJ8GVQcgPNGxsMGJmgCkoPaOxvbTJttSqn0kRxpRDhVSW7gPzr93vcDWOhZDY73YKOSG0_ERiJHwcw9T08EuwUvUXUAt7bFC8giFOu-Shl6FWnWmILajHGZ2K41XSzCtKNoiEpMCc2WnxGavNPac2Ua8T2iGemtud1NQ_pVfRutYVlzv6HZt",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjaOdJkvZr0cr1-e-5ZrmcejmyUcrtp6BH82QaEQyehH02rUXCdIpl6yj5FEh8brzWV2FlVqrdDBVYwejA25B5cmk3spWoK0cSLjBCdq8QPL9G45dVfl_ZlPvyVGoXpvmc3x4X44E4Q2F-DrwinPw1AMWFckqrYjvT3s-a4k3F6Alv-qdMjCDfKyHMS/s16000/unnamed%20(12).png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi1O2B--pbmUaBlRBbCR_5ug9zeIj9vD6W7iwokGbvTKHo1l3hIgbYcZOcugSnskgXtU4tIvfuEsgncv-L5I050wHOaytu6o5xVLVQy4b6ilO9IoqzfesjZWY1HD5bCs_785xbjjFUOkyTuwpBewnKo1tHGxo7-PwmBEBFkjR40q3QrfV23d9sM5Pad/s16000/image2.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg0gjG2lKGAENN2Y9d0WH3WyGY6Grwx09hel-yRK0bkfD3KjGnsmLEYIFpfexazqhSvmuZ7HIPOmFKqF1cw6Qq4VWzWK4C6v4pYg0aMBcHjbwVnlfDW5TnYPmJRm48jAwtlLcCFGjy5lT6rJ0Pi4cIaAfUxFkNiQJ-NQSm347xO9nDpKxV4L_Y22-oB/s1600/image1.png"
    ],
    "time": "2023/12/05 00:58:43"
  },
  {
    "title": "Fast Reduce and Mean in TensorFlow Lite",
    "content": "Posted by Alan Kelly, Software Engineer\nWe are happy to share that TensorFlow Lite version 2.10 has optimized Reduce (All, Any, Max, Min, Prod, Sum) and Mean operators. These common operators replace one or more dimensions of a multi-dimensional tensor with a scalar. Sum, Product, Min, Max, Bitwise And, Bitwise Or and Mean variants of reduce are available. Reduce is now fast for all possible inputs.\nBenchmark for Reduce Mean on Google Pixel 6 Pro Cortex A55 (small core). Input tensor is 4D of shape [32, 256, 5, 128] reduced over axis [1, 3], Output is a 2D tensor of shape [32, 5].\n\nBenchmark for Reduce Prod on Google Pixel 6 Pro Cortex A55 (small core). Input tensor is 4D of shape [32, 256, 5, 128] reduced over axis [1, 3], Output is a 2D tensor of shape [32, 5].\n\nBenchmark for Reduce Sum on Google Pixel 6 Pro Cortex A55 (small core). Input tensor is 4D of shape [32, 256, 5, 128] reduced over axis [0, 2], Output is a 2D tensor of shape [256, 128].\n\nThese speed-ups are available by default using the latest version of TFLite on all architectures.\nHow does this work?\nTo understand how these improvements were made, we need to look at the problem from a different perspective. Let\u2019s take a 3D tensor of shape [3, 2, 5].\nLet\u2019s reduce this tensor over axes [0] using Reduce Max. This will give us an output tensor of shape [2, 5] as dimension 0 will be removed. Each element in the output tensor will contain the max of the three elements in the same position along dimension 0. So the first element will be max{0, 10, 20} = 20. This gives us the following output:\nTo simplify things, let\u2019s reshape the original 3D tensor as a 2D tensor of shape [3, 10]. This is the exact same tensor, just visualized differently.\nReducing this over dimension 0 by taking the max of each column gives us:\nWhich we then reshape back to its original shape of [2, 5]\nThis demonstrates how simply changing how we visualize the tensor dramatically simplifies the implementation. In this case, dimensions 1 and 2 are adjacent and not being reduced over. This means that we can fold them into one larger dimension of size 2 x 5 = 10, transforming the 3D tensor into a 2D one. We can do the same to adjacent dimensions which are being reduced over.\n\nLet\u2019s take a look at all possible Reduce permutations for the same 3D tensor of shape [3, 2, 5].\nOf all 8 permutations, only two 3D permutations remain after we re-visualize the input tensor. For any number of dimensions, there are only two possible reduction permutations: the rows or the columns. All other ones simplify to a lower dimension.\n\nThis is the trick to an efficient and simple reduction operator as we no longer need to calculate input and output tensor indices and our memory access patterns are much more cache friendly.\n\nThis also allows the compiler to auto-vectorize the integer reductions. The compiler won\u2019t auto-vectorize floats as float addition is not commutative. You can see the code which removes redundant axes here and the reduction code here.\n\nChanging how we visualize tensors is a powerful code simplification and optimization technique which is used by many TensorFlow Lite operators.\nNext steps\nWe are always working on adding new operators and speeding up existing ones. We\u2019d love to hear about models of yours which have benefited from this work. Get in touch via the TensorFlow Forum. Thanks for reading!",
    "link": "https://blog.tensorflow.org/2022/09/fast-reduce-and-mean-in-tensorflow-lite.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjSWMVCl2We_NSDS2a3pt9rcN01yn5qsfD6AxqPHZa6gRCaF8k_0rwvgLpFBl9qQTjeLX0Q08Q7tejJjRC6Lc6cHHwPm4F54T7X2oFe0IE6ry1GoNS96cjP0qI2IBLcxqWk6iZsmbXehucFCjMTV7AIvFtUS_L21RCxnti0iBse0ZZ-vUlwCed0pfsn/s1600/tensorflow-Fast-Reduce-and-Mean-in-TensorFlow-Lite-02.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgEKG6Fb1LawOqOb1HUIPsemOcYSKAFAjmRuX5vBAuTZrDGvsXD1Ce9ImFqUs7Y9owHfvmFQ17seZz6ibDcXd5DIxKpXXm4bsuXsF8jChEUc-cuOtLO7rSl0VyLyCkKiPbjTSAjUdISjWx2qWntseT57vhWPb4km7QZ9_5YEniaN2bOMF4NE8hGDumO/s1600/tensorflow-Fast-Reduce-and-Mean-in-TensorFlow-Lite-01.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidN0lR-9rZA7loO_OTjknccpshiV_tCHJ_qwUo6IevmNlT6nH9v2S1hbQ4wn9B0tqYjZ88BWflKlh1EDp6QEz0R_RUUdoRMwz8JaU3E_Ra5gmzrFmZ_gJ8wwM7hVQ6Alkc-z8NhWkF6YZK_w-4UA6LbGNglBGdzVHZtESGJPUqRyngwwtKA2G0MB2D/s1600/image6.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiZetG64CeVO5gCpth31-sxHHbzI4evoGEYC8V3rsL5QZ-RVH46vEYINnuMtPZLXWl2Z5Jld-yRuH3e_4qXPoz3FjoDFaNnWe-tSP2NuoP3k84jZutUPF9Rov8jQROpOrXC7lZ4-iGS-hasoeL8F4dRF0ZBkoCF2fHnrCjlolkpGV4NCzrNz6dCtWZM/s1600/image5.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhVXX0ITYypBDOe2-oqwqcJ5SrsJtn_Hv-C9X4a4QBCvK399ZbjQTUOS9Dvb-3PYej1aC7G5wAQZsRV-s_ZLltpS_I8f28NhTfFers8bBKsiAKReHM6bGQYExJBhc53uHpiCxt7cCPRW2rItuxQTBJymN3D3YXq5M0HyJVUg8PfBDRih_qlWOQ6A0lJ/s1600/image1.png",
      "data:image/png;base64,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",
      "data:image/png;base64,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",
      "data:image/png;base64,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",
      "data:image/png;base64,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",
      "data:image/png;base64,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",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8TLZFvjr76j-ZKPwL5EWhbAT0bOKqWt6aL8qLyBWGwaAPfsz6-UXxIHgGc9c1XBZcnErhZjweo4iX5wzfoDWW54KfU_-LS3LMK3jV7f1M0Qa0jySagOdL0HVzIbaemTcwNoiKy1xcgSAGhLuY5Yy5VfFcnyr-apRmu63f2GLRclwZQyY1g-LGE5CG/s16000/Screen%20Shot%202022-09-20%20at%206.02.28%20PM.png"
    ],
    "time": "2023/12/05 00:58:43"
  },
  {
    "title": "Announcing TensorFlow Lite in Google Play Services General Availability",
    "content": "Posted by Bernhard Bauer and Terry Heo, Software Engineers, Google\nToday we\u2019re excited to announce that the Google Play services API for TensorFlow Lite is generally available on Android devices. We recommend this distribution as the path to adding custom machine learning to your apps. Last year, we launched a public beta of TensorFlow Lite in Google Play services at Google I/O. Since then, we\u2019ve received lots of feedback and made improvements to the API. Most recently, we added the GPU delegate and Task Library support. Today we\u2019re moving from beta to general availability on billions of Android devices globally.\nTensorFlow Lite in Google Play services is already used by Google teams, including ML Kit, serving over a billion monthly active users and running more than 100 billion daily inferences.\n\nTensorFlow Lite is an inference runtime optimized for mobile devices, and now that it's part of Google Play services, it helps you deliver better ML experiences because it:\nReduces your app size by up to 5 MB compared to statically bundling TensorFlow Lite with your app\nUses the same API as available when bundling TF Lite into your app\nReceives regular performance updates in the background so it\u2019s always getting better automatically\nGet started by learning how to add TensorFlow Lite in Google Play Services to your Android app.",
    "link": "https://blog.tensorflow.org/2022/09/announcing-tensorflow-lite-in-google-play-services-general-availability.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYDycjJd2AXe8THRHfSca3ij7DMb7cf7ZYWprCtAeh0IHiqvjUOSr9fe1oHtFqzTvqD2EgCGnK7GFq1ZZrvAzuUt4nh--3GY6TWIW5YbKuYExZJOC1s-b30I8G9hGAtnCugc3vaG8bM3qyskxIInOiQQ9hltnYhqBd-39j1WJyGwHm5F5uXb8Ne1rr/s1600/image2%20copy%208.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYDycjJd2AXe8THRHfSca3ij7DMb7cf7ZYWprCtAeh0IHiqvjUOSr9fe1oHtFqzTvqD2EgCGnK7GFq1ZZrvAzuUt4nh--3GY6TWIW5YbKuYExZJOC1s-b30I8G9hGAtnCugc3vaG8bM3qyskxIInOiQQ9hltnYhqBd-39j1WJyGwHm5F5uXb8Ne1rr/s1600/image2%20copy%208.png"
    ],
    "time": "2023/12/05 00:58:43"
  },
  {
    "title": "Profiling XNNPACK with TFLite",
    "content": "Posted by Alan Kelly, Software Engineer\nWe are happy to share that detailed profiling information for XNNPACK is now available in TensorFlow 2.9.1 and later. XNNPACK is a highly optimized library of floating-point neural network inference operators for ARM, WebAssembly, and x86 platforms, and it is the default TensorFlow Lite CPU inference engine for floating-point models.\nThe most common and expensive neural network operators, such as fully connected layers and convolutions, are executed by XNNPACK so that you get the best performance possible from your model. Historically the profiler would measure the runtime for the entire section of delegated graph, meaning that the runtime of all delegated operators was accumulated in one result, making it difficult to identify the individual operations that were slow.\nPrevious TFLite profiling results when XNNPACK was used. The runtime of all delegated operators was accumulated in one row.\nIf you are using TensorFlow Lite 2.9.1 or later, it gives the per operator profile even for the section that is delegated to XNNPACK so that you no longer need to decide between fast inference and detailed performance information. The operator name, data layout (NHWC for example), datatype (FP32) and microkernel type (if applicable) are shown.\nNew detailed per-operator profiling information is now shown. The operator name, data layout, data type and microkernel type are visible.\nNow, you get lots of helpful information, such as the runtime per operator and the percentage of the total runtime that it accounts for. The runtime of each node is given in the order in which they were executed. The most expensive operators are also listed.\nThe most expensive operators are listed. In this example, you can see that a deconvolution accounted for 33.91% of the total runtime.\nXNNPACK can also perform inference in half-precision (16 bit) floating point format if the hardware supports these operations natively, and IEEE16 inference is supported for every floating-point operator in the model, and the model\u2019s `reduced_precision_support` metadata indicates that it is compatible with FP16 inference. FP16 inference can also be forced. More information is available here. If half precision has been used, then F16 will be present in the Name column:\nFP16 inference has been used.\nHere, unsigned quantized inference has been used (QU8).\nQU8 indicates that unsigned quantized inference has been used\nAnd finally, sparse inference has been used. Sparse operators require that the data layout change from NHWC to NCHW as this is more efficient. This can be seen in the operator name.\nSPMM microkernel indicates that the operator is evaluated via SParse matrix-dense Matrix Multiplication. Note that sparse inference use NCHW layout (vs the typical NHWC) for the operators.\nNote that when some operators are delegated to XNNPACK, and others aren\u2019t, two sets of profile information are shown. This happens when not all operators in the model are supported by XNNPACK. The next step in this project is to merge profile information from XNNPACK operators and TensorFlow Lite into one profile.\nNext Steps\nYou can learn more about performance measurement and profiling in TensorFlow Lite by visiting this guide. Thanks for reading!",
    "link": "https://blog.tensorflow.org/2022/06/Profiling-XNNPACK-with-TFLite.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGaMOZjmmcdeCHArC2JZGpTO3nsFS6Pdv4_mR7Krfqjyw5hbMWoy1TBJkDg5h9P62LPDIXoaPvj8NdwcszzXK_IhsS3Z39jx-q25Ud-Os7ShQkm2YjIhNX0Bn8R3Cfa-hcz_jZXvF_a8W9tpE2PDiX9A5d32qkAgfNpled0X_1DJuxHfoFOOtMdC4b/s1600/image6.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjC4hVKqTuz3ZI3BDW-N9XFOthfw0GAmJrCuh0QCjgXglrO69vef8Nzj8-o9NZjF5sW9_mvqg3fQa8PuQO1b14ITkAcMx2cjqfcXKAnU3CpF7L_JE7qyjt8F-SmVXS-Foug7IKB7bYpknoYu1GWhyJqey-ZLL44YtitJyBrGYbYLCLU8p1VyqKjecm_/s16000/image2.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjszotPDs2GEy4drnKKcO1gPTCmmEYpf60Yk3iTA2MCogHpXF-pBWUYsH__DIoFkWhJisBBTP6uebX3MAyC0XFthmV5vcGFBndJF0L1EodeESG4tMJ9uY9z0IjotVNySAjcghi40WGRLZOFyneNB2J96pXlMEMXijMxRikoT68yzL1j1jgBMygupjWV/s1600/image3.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxQNnx-7gGV730igjxQLFBYhj8EH9B7FXaiRmJ61E2bD5lDfpwJ_6UFC0ViXc_EdjX4bpuxJkSDfhrRuHvu9UB0-GRYsyF9co3aqIpYBDyh2wQVq0_7yKDaFGwSN2om7c18piUo_6SYD5uU6N4J1yzzjiBbeM4u1krWhzTOTkiuHlNCi1NUUSxp3a6/s1600/image5.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5mSDtQ3FzM_t0tuazFhgIEOdjRhbZG_ReGkegnKKmqfIeEXAzVwS3Xcl-SPFz6EoBFsgx2SlZWWOJ5wMOg-Jra8_hvZjAE_9eAB-3XVgDcv2qOHYDPqMzk7XIynvAA2qMWAEwPWgjwh5uaWpVePcHG4UyxXU7kjPI7VqaLBf0zh5WOlugtT7boDdN/s1600/TF%20Blog.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJW-1poK1ASuATN0IjayI53Hj6OYeyWvsuQdPOitXVjNbtdJiMpOK6jQVAK5QBTyeoCvsiZqMG93SrA9NZNyk2hVHEWWgF_7PILtAIdgBIAni-UWivl07GhDxYqYzMt8hDsO1PbjVMsaBgGIet5oTcbe0kijN9t-QIqtdIYVStmIExc_RaSZoOCng1/s1600/image1.png"
    ],
    "time": "2023/12/05 00:58:43"
  },
  {
    "title": "Memory-efficient inference with XNNPack weights cache",
    "content": "Posted by Zhi An Ng and Marat Dukhan, Google\nXNNPack is the default TensorFlow Lite CPU inference engine for floating-point models, and delivers meaningful speedups across mobile, desktop, and Web platforms. One of the optimizations employed in XNNPack is repacking the static weights of the Convolution, Depthwise Convolution, Transposed Convolution, and Fully Connected operators into an internal layout optimized for inference computations. During inference, the repacked weights are accessed in a sequential pattern that is friendly to the processors\u2019 pipelines.\nThe inference latency reduction comes at a cost: repacking essentially creates an extra copy of the weights inside XNNPack. When the TensorFlow Lite model is memory-mapped, the operating system eventually releases the original copy of the weights and makes the overhead disappear. However, some use-cases require creating multiple copies of a TensorFlow Lite interpreter, each with its own XNNPack delegate, for the same model. As the XNNPack delegates belonging to different TensorFlow Lite interpreters are unaware of each other, every one of them creates its own copy of repacked weights, and the memory overhead grows linearly with the number of delegate instances. Furthermore, since the original weights in the model are static, the repacked weights in XNNPack are also the same across all instances, making these copies wasteful and unnecessary.\nWeights cache is a mechanism that allows multiple instances of the XNNPack delegate accelerating the same model to optimize their memory usage for repacked weights. With a weights cache, all instances use the same underlying repacked weights, resulting in a constant memory usage, no matter how many interpreter instances are created. Moreover, elimination of duplicates due to weights cache may improve performance through increased efficiency of a processor\u2019s cache hierarchy. Note: the weights cache is an opt-in feature available only via the C++ API.\nThe chart below shows the high water mark memory usage (vertical axis) of creating multiple instances (horizontal axis). It compares the baseline, which does not use weights cache, with using weights cache with soft finalization. The peak memory usage when using weights cache grows much slower with respect to the number of instances created. For this example, using weights cache allows you to double the number of instances created with the same peak memory budget.\nThe weights cache object is created by the TfLiteXNNPackDelegateWeightsCacheCreate function, and passed to the XNNPack delegate via the delegate options. XNNPack delegate will then use the weights cache to store repacked weights. Importantly, the weights cache must be finalized before any inference invocation.\n// Example demonstrating how to create and finalize a weights cache.\nstd::unique_ptr<tflite::Interpreter> interpreter;\nTfLiteXNNPackDelegateWeightsCache* weights_cache =\n    TfLiteXNNPackDelegateWeightsCacheCreate();\nTfLiteXNNPackDelegateOptions xnnpack_options =\n    TfLiteXNNPackDelegateOptionsDefault();\nxnnpack_options.weights_cache = weights_cache;\nTfLiteDelegate* delegate =\n    TfLiteXNNPackDelegateCreate(&xnnpack_options);\nif (interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk) {\n    // Static weights will be packed and written into weights_cache.\n}\nTfLiteXNNPackDelegateWeightsCacheFinalizeHard(weights_cache);\n\n// Calls to interpreter->Invoke and interpreter->AllocateTensors must\n// be made here, between finalization and deletion of the cache.\n// After the hard finalization any attempts to create a new XNNPack\n// delegate instance using the same weights cache object will fail.\n\nTfLiteXNNPackWeightsCacheDelete(weights_cache);\nThere are two ways to finalize a weights cache, and in the example above we use TfLiteXNNPackDelegateWeightsCacheFinalizeHard which performs hard finalization. The hard finalization has the least memory overhead, as it will trim the memory used by the weights cache to the absolute minimum. However, no new delegates can be created with this weights cache object after the hard finalization - the number of XNNPack delegate instances using this cache is fixed in advance. The other kind of finalization is a soft finalization. Soft finalization has higher memory overhead, as it leaves sufficient space in the weights cache for some internal bookkeeping. The advantage of the soft finalization is that the same weights cache can be used to create new XNNPack delegate instances, provided that the delegate instances use exactly the same model. This is useful if the number of delegate instances is not fixed or known beforehand.\n// Example demonstrating soft finalization and creating multiple\n// XNNPack delegate instances using the same weights cache.\nstd::unique_ptr<tflite::Interpreter> interpreter;\nTfLiteXNNPackDelegateWeightsCache* weights_cache =\n    TfLiteXNNPackDelegateWeightsCacheCreate();\nTfLiteXNNPackDelegateOptions xnnpack_options =\n    TfLiteXNNPackDelegateOptionsDefault();\nxnnpack_options.weights_cache = weights_cache;\nTfLiteDelegate* delegate =\n    TfLiteXNNPackDelegateCreate(&xnnpack_options);\nif (interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk) {\n    // Static weights will be packed and written into weights_cache.\n}\nTfLiteXNNPackDelegateWeightsCacheFinalizeSoft(weights_cache);\n\n// Calls to interpreter->Invoke and interpreter->AllocateTensors can\n// be made here, between finalization and deletion of the cache.\n// Notably, new XNNPack delegate instances using the same cache can\n// still be created, so long as they are used for the same model.\n\nstd::unique_ptr<tflite::Interpreter> new_interpreter;\nTfLiteDelegate* new_delegate =\n    TfLiteXNNPackDelegateCreate(&xnnpack_options);\nif (new_interpreter->ModifyGraphWithDelegate(new_delegate) !=\n    kTfLiteOk)\n{\n    // Repacked weights inside of the weights cache will be reused,\n    // no growth in memory usage\n}\n\n// Calls to new_interpreter->Invoke and\n// new_interpreter->AllocateTensors can be made here.\n// More interpreters with XNNPack delegates can be created as needed.\n\nTfLiteXNNPackWeightsCacheDelete(weights_cache);\nNext steps\nWith the weights cache, using XNNPack for batch inference will reduce memory usage, leading to better performance. Read more about how to use weights cache with XNNPack at the README and report any issues at XNNPack's GitHub page.\nTo stay up to date, you can read the TensorFlow blog, follow twitter.com/tensorflow, or subscribe to youtube.com/tensorflow. If you\u2019ve built something you\u2019d like to share, please submit it for our Community Spotlight at goo.gle/TFCS. For feedback, please file an issue on GitHub or post to the TensorFlow Forum. Thank you!",
    "link": "https://blog.tensorflow.org/2022/06/memory-efficient-inference-with-xnnpack.html",
    "imgSource": [
      "https://blog.tensorflow.org/2022/06/memory-efficient-inference-with-xnnpack.html",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0_1yuRglS2gJE3vbjfqhRa7iFDORvqKjNBDG82MoqnFqMIw6CIF7j39LNNVYInCKRWrWJUmbHajUwncM2ZSfN6X4OJSbRFfdySrGSyte2Z9K-cUzMfvRbqXN20J_ZmZQwMRnHdcFL474AJB_-WGjTYumA2K87uIPQeRN30kgKADAT9gqSAWLxNMEm/s1600/image2.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiNOdfLIzlJZY9Oo8ivGTWLpVNTPiKgDrRkPR3pBo_0pqmpeuyd1Pm-K2F0wRvgzELx-vKyyI-sgYTddVL14L1YyhcJ59hm9dB4DgrlTm0qrOcUsW1fiH5zCXdbFB_NEnX7tVD59LxfibNH702Z59EHY6jTEn-blXwYqlEOqqb6xQwkc3O_lXaCHcJ6/s1600/image3.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiXNXKTze0EiLb2O_RS1jkeDy8k5u5ksYVdJYOndYstPacET-lfeNU_9wyxS9UcMqpHUoiFqxePad3MPXXDRHcXDOYbmA5vAya-aGK5OUS_kS5LTQmqvoPF-eeQ4MInpHHLg0Y4EXJbxPTDAwtnJuN_ioOohKdyp8Ngu8dtQVYgjyM0N8sFrn_fTrfE/s1600/image1.png"
    ],
    "time": "2023/12/05 00:58:43"
  },
  {
    "title": "TensorFlow Lite for education and makers",
    "content": "Posted by Scott Main, AIY Projects and Coral\nBack in 2017, we began AIY Projects to make do-it-yourself artificial intelligence projects accessible to anybody. Our first project was the AIY Voice Kit, which allows you to build your own intelligent device that responds to voice commands. Then we released the AIY Vision Kit, which can recognize objects seen by its camera using on-device TensorFlow models. We were amazed by the projects people built with these kits and thrilled to see educational programs use them to introduce young engineers to the possibilities of computer science and machine learning (ML). So I'm excited to continue our mission to bring machine learning to everyone with the more powerful and more customizable AIY Maker Kit.\nMaking ML accessible to all\nThe Voice Kit and Vision Kit are a lot of fun to put together and they include great programs that demonstrate the possibilities of ML on a small device. However, they don't provide the tools or procedures to help beginners achieve their own ML project ideas. When we released those kits in 2017, it was actually quite difficult to train an ML model, and getting a model to run on a device like a Raspberry Pi was even more challenging. Nowadays, if you have some experience with ML and know where to look for help, it's not so surprising that you can train an object detection model in your web browser in less than an hour, or that you can run a pose detection model on a battery-powered device. But if you don't have any experience, it can be difficult to discover the latest ML tools, let alone get started with them.\nWe intend to solve that with the Maker Kit. With this kit, we're not offering any new hardware or ML tools; we're offering a simplified workflow and a series of tutorials that use the latest tools to train TensorFlow Lite models and execute them on small devices. So it's all existing technology, but better packaged so beginners can stop searching and start building incredible things right away.\nSimplified tools for success\nThe material we've collected and created for the Maker Kit offers an end-to-end experience that's ideal for educational programs and users who just want to make something with ML as fast as possible.\nThe hardware setup requires a Raspberry Pi, a Pi Camera, a USB microphone, and a Coral USB Accelerator so you can execute advanced vision models at high speed on the Coral Edge TPU. If you want your hardware in a case, we offer two DIY options: a 3D-printed case design or a cardboard case you can build using materials at home.\nOnce it's booted up with our Maker Kit system image, just run some of our code examples and follow our coding tutorials. You'll quickly discover how easy it is to accomplish amazing things with ML that were recently considered accessible only to experts, including object detection, pose classification, and speech recognition.\nOur code examples use some pre-trained models and you can get more models that are accelerated on the Edge TPU from the Coral models library. However, training your own models allows you to explore all new project ideas. So the Maker Kit also offers step-by-step tutorials that show you how to collect your own datasets and train your own vision and audio models.\nLast but not least, we want you to spend nearly all your time writing the code that's unique to your project. So we created a Python library that reduces the amount of code needed to perform an inference down to a tiny part of your project. For example, this is how you can run an object detection model and draw labeled bounding boxes on a live camera feed:\nfrom aiymakerkit import vision\nfrom aiymakerkit import utils\nimport models\n\ndetector = vision.Detector(models.OBJECT_DETECTION_MODEL)\nlabels = utils.read_labels_from_metadata(models.OBJECT_DETECTION_MODEL)\n\nfor frame in vision.get_frames():\n    objects = detector.get_objects(frame, threshold=0.4)\n    vision.draw_objects(frame, objects, labels)\nOur intent is to hide the code you don't absolutely need. You still have access to structured inference results and program flow, but without any boilerplate code to handle the model.\nThis aiymakerkit library is built upon TensorFlow Lite and it's available on GitHub, so we invite you to explore the innards and extend the Maker Kit API for your projects.\nGetting started\nWe created the Maker Kit to be fully customizable for your projects. So rather than provide all the materials in a box with a predetermined design, we designed it with hardware that's already available in stores (listed on our website) and with optional instructions to build your own case.\nTo get started, visit our website at g.co/aiy/maker, gather the required materials, flash our system image, and follow our programming tutorials to start exploring the possibilities. With this head start toward building smart applications that run entirely on an embedded system, we can't wait to see what you will create.",
    "link": "https://blog.tensorflow.org/2022/05/tensorflow-lite-for-education-and-makers.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiqEDXa1_eLG5p2UEpcB9lncW024b695qrPeQyjNx15n_rnSQ68lV4VOPTWZKDm6S6l1OcXIcxkAp_-MQc906pOCYd8kvAaHLQF_0TrRIOGKG-MHRMKalvZWqmWFiWASghlbFQNOgjYQ1DbBwkw8echZ7u4Pre844E-ASgSIbr1UV5oQg4kM2DPsFvb/s1600/image1.jpg",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiqEDXa1_eLG5p2UEpcB9lncW024b695qrPeQyjNx15n_rnSQ68lV4VOPTWZKDm6S6l1OcXIcxkAp_-MQc906pOCYd8kvAaHLQF_0TrRIOGKG-MHRMKalvZWqmWFiWASghlbFQNOgjYQ1DbBwkw8echZ7u4Pre844E-ASgSIbr1UV5oQg4kM2DPsFvb/s1600/image1.jpg",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh5nc5Cgt8SUJFEeLKzRprsr-7lKqY1zXrce3T2NxIRlw0PDSxYey65ucFGWeufPKc4K5qAMsM3KsBnRIRfyrdmnX2xgxEvyRkT0sHNJmnT-UFY5NBlae6def8ej_Tg8oR_gJfZEu2Xg46RykNQlb23Q8hfllvtf62Jiw4nZE9dcm3cAu1yt4f-jFmi/s1600/image2.png"
    ],
    "time": "2023/12/05 00:58:43"
  },
  {
    "title": "On-device Text-to-Image Search with TensorFlow Lite Searcher Library",
    "content": "Posted by Zonglin Li, Lu Wang, Maxime Br\u00e9non, and Yuqi Li, Software Engineers\nToday, we're excited to announce a new on-device embedding-based search library that allows you to quickly find similar images, text or audio from millions of data samples in a few milliseconds.\nIt works by using a model to embed the search query into a high-dimensional vector representing the semantic meaning of the query. Then it uses ScaNN (Scalable Nearest Neighbors) to search for similar items from a predefined database. In order to apply it to your dataset, you need to use Model Maker Searcher API (tutorial) to build a custom TFLite Searcher model, and then deploy it onto devices using Task Library Searcher API (vision/text).\nFor example, with the Searcher model trained on COCO, searching the query, A passenger plane on the runway, will return the following images:\nFigure 1: All images are from COCO 2014 train and validation datasets. Image 1 by Mark Jones Jr. under Attribution License. Image 2 by 305 Seahill under Attribution-NoDerivs License. Image 3 by tataquax under Attribution-ShareAlike License.\nIn this post, we will walk you through an end-to-end example of building a text-to-image search feature (retrieve the images given textual queries) using the new TensorFlow Lite Searcher Library. Here are the major steps:\nTrain a dual encoder model for image and text query encoding using the COCO dataset.\nCreate a text-to-image Searcher model using the Model Maker Searcher API.\nRetrieve images with text queries using the Task Library Searcher API.\nTrain a Dual Encoder Model\nFigure 2: Train the dual encoder model with dot product similarity distance. The loss encourages related images and text to have larger dot products (the shaded green squares).\nThe dual encoder model consists of an image encoder and a text encoder. The two encoders map the images and text, respectively, to embeddings in a high-dimensional space. The model computes the dot product between the image and text embeddings, and the loss encourages relevant image and text to have larger dot product (closer), and unrelated ones to have smaller dot product (farther apart).\nThe training procedure is inspired by the CLIP paper and this Keras example. The image encoder is based on a pre-trained EfficientNet model and the text encoder is based on a pre-trained Universal Sentence Encoder model. The outputs from both encoders are then projected to a 128 dimensional space and are L2 normalized. For the dataset, we chose to use COCO, as its train and validation splits have human generated captions for each image. Please take a look at the companion Colab notebook for the details of the training process.\nThe dual encoder model makes it possible to retrieve images from a database without captions because once trained, the image embedder can directly extract the semantic meaning from the image without any need for human-generated captions.\nCreate the text-to-image Searcher model using Model Maker\nFigure 3: Generate image embeddings using the image encoder and use Model Maker to create the TFLite Searcher model.\nOnce the dual encoder model is trained, we can use it to create the TFLite Searcher model that searches for the most relevant images from an image dataset based on the text queries. This can be done by the following three steps:\nGenerate the embeddings of the image dataset using the TensorFlow image encoder. ScaNN is capable of searching through a very large dataset, hence we combined the train and validation splits of COCO 2014 totaling 123K+ images to demonstrate its capabilities. See the code here.\nConvert the TensorFlow text encoder model into TFLite format. See the code here.\nUse Model Maker to create the TFLite Searcher model from the TFLite text encoder and the image embeddings using the code below:\n#Configure ScaNN options. See the API doc for how to configure ScaNN. \nscann_options = searcher.ScaNNOptions(\n      distance_measure='dot_product',\n      tree=searcher.Tree(num_leaves=351, num_leaves_to_search=4),\n      score_ah=searcher.ScoreAH(1, anisotropic_quantization_threshold=0.2))\n\n# Load the image embeddings and corresponding metadata if any.\ndata = searcher.DataLoader(tflite_embedder_path, image_embeddings, metadata)\n\n# Create the TFLite Searcher model.\nmodel = searcher.Searcher.create_from_data(data, scann_options)\n\n# Export the TFLite Searcher model.\nmodel.export(\n      export_filename='searcher.tflite',\n      userinfo='',\n      export_format=searcher.ExportFormat.TFLITE)\n     \nAPI doc can be found here.\nWhen creating the Searcher model, Model Maker leverages ScaNN to index the embedding vectors. The embedding dataset is first partitioned into multiple subsets. In each of the subsets, ScaNN stores the quantized representation of the embedding vectors. At retrieval time, ScaNN selects a few most relevant partitions and scores the quantized representations with fast, approximate distances. This procedure saves both the model size (through quantization) and achieves speed up (through partition selection). See the in-depth examination to learn more about the ScaNN algorithm.\nIn the above example, we divide the dataset into 351 partitions (roughly the square root of the number of embeddings we have), and search 4 of them during retrieval, which is roughly 1% of the dataset. We also quantize the 128 dimensional float embeddings to 128 int8 values to save space.\nRun inference using Task Library\nFigure 4: Run inference using Task Library with the TFLite Searcher model. It takes the query text and returns the top neighbor\u2019s metadata. From there we can find the corresponding images.\nTo query images using the Searcher model, you only need a couple of lines of code like the following using Task Library:\nfrom tflite_support.task import text\n\n# Initialize a TextSearcher object\nsearcher = text.TextSearcher.create_from_file('searcher.tflite')\n\n# Search the input query\nresults = searcher.search(query_text)\n\n# Show the results\nfor rank in range(len(results.nearest_neighbors)):\n  print('Rank #', rank, ':')\n  image_id = results.nearest_neighbors[rank].metadata\n  print('image_id: ', image_id)\n  print('distance: ', results.nearest_neighbors[rank].distance)\n  show_image_by_id(image_id)\nTry the code from the Colab. Also, see more information on how to integrate the model using the Task Library Java and C++ API, especially on Android. Each query in general takes only 6 milliseconds on Pixel 6.\nHere are some example results:\nQuery: A man riding a bike\nResults are ranked according to the approximate similarity distance. Here is a sample of retrieved images. Note that we are only showing images if their licenses allow.\nFigure 5: All images are from COCO 2014 train and validation datasets. Image 1 by Reuel Mark Delez under Attribution License. Image 2 by Richard Masoner / Cyclelicious under Attribution-ShareAlike License. Image 3 by Julia under Attribution-ShareAlike License. Image 4 by Aaron Fulkerson under Attribution-ShareAlike License. Image 5 by Richard Masoner / Cyclelicious under Attribution-ShareAlike License. Image 6 by Richard Masoner / Cyclelicious under Attribution-ShareAlike License.\nFuture work\nWe\u2019ll be working on enabling more search types beyond image and text, such as audio clips.\nContact odml-pipelines-team@google.com if you want to leave any feedback. Our goal is to make on-device ML even easier for you and we value your input!\nAcknowledgements\nWe would like to thank Khanh LeViet, Chuo-Ling Chang, Ruiqi Guo, Lawrence Chan, Laurence Moroney, Yu-Cheng Ling, Matthias Grundmann, as well as Robby Neale, Chung-Ching Chang, Tom Small and Khalid Salama for their active support of this work. We would also like to thank the entire ScaNN team: David Simcha, Erik Lindgren, Felix Chern, Phil Sun and Sanjiv Kumar.",
    "link": "https://blog.tensorflow.org/2022/05/on-device-text-to-image-search-with.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhRKSVzRJ89LQ4GppKujCFnXR1F-8kmxpQwb7shUiaIPBPOvf3axOqEsZO54J4uzlYP0RVQ9VNlyeLMSIk8M_npJc70n7o0LqnfBQBzvxphU1vZmjwDCs47cpYMbd27Sh9VYKI9expP8lrpaxOJAmJrCgeRR5eQzTVxEcT5lfW1CMIKcEesUDJUGIuJ/s1600/image3.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjw4uZh_Pvl7hamimWYoFo2moCBXww7L_J7wdunVVxNDj3dFpGU4u2klFvFfRPnvfe-jYIRUQup8bBSt0ZjWp5wRYc6YHMd3eLebmBWqscNeW4KsINcLH1XJhLLvOikkz_26_qwHjGqZhps3DnvaXyHfvwRT20XTqXIHiIetXoCpweZjcnVX6DE0yY7/s1600/Copy%20of%20I_O22_BlogBanners_RB_v09.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiaTXZDmasMaRs-z9tIu6R9a7zddIdDLv1IxyOI2OiAzgH0o6wHQVmRAJZfTdirnWYUbRE-5_G-rGk7SDXuFE4NnLuWgJco-tXvcZS39GI0Nk0hhSp-x3fH4V1B61alL8F9n8RV937GcLTGphNbwni8L_4tuWjeg5zriN1rPhox3O9XhvwVYpUuTiqe/s1600/image4.gif",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhooOxW8DRyg0f6TLCFjJJs1L1gWy7ic9vhDJSAomVaZQdxE_3-xlKsMcWi2b2bVwilK2ivmJPK7w3Q2jMwJvuRahVoO5WYlA9X4X0H111dReGI4Z7ckHel-Oux2bu--dfHw1-yKXcUzjl1h_TZxR2nbKH_csjQHfZtqUzxm4Ogc4aPP65NcaaNWCDa/s1600/image2.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhRKSVzRJ89LQ4GppKujCFnXR1F-8kmxpQwb7shUiaIPBPOvf3axOqEsZO54J4uzlYP0RVQ9VNlyeLMSIk8M_npJc70n7o0LqnfBQBzvxphU1vZmjwDCs47cpYMbd27Sh9VYKI9expP8lrpaxOJAmJrCgeRR5eQzTVxEcT5lfW1CMIKcEesUDJUGIuJ/s1600/image3.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhImt-tBGiK8Z1W_JOAsuu3qlnDHrfko5bQ38e-njB3TAjxXk2lBtw_t2xxNMMrNt0BCu7uBarZtuMg0eCwNFkn7CoWEaKsUfl9f94b1EAICHmz2vXfMCDFhKpzK8tzsTv-KhS0l7AR3hC1RLQB4Fg5w2LAv0qYTrO7ea18j8Bnb0RhNItKqOK3oR8r/s1600/image1.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhRfPCu6QCkMxXTJVV0Y7A2kWaHpImHFEuPIEHZTwgsxJ27JYvcI7ThzNCqLiVBB9u2P67F00FvGzdZzHvBBv0UO9PSyj4BmsUCjXjYAO4PSOUogKR4WNgBfLudNppCbNrnQgSWh6a3v-lWrFHiFUYBSmYd2A2LE9e9jNqsvk67XMvsicgOeJwYIcz/s1600/image1.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdVewBVwxQOAORYm4Jvd0W6g2lLEtvFbLs-qwCctuYT1KKOKYeZwbDhL08E7Kz2YPdIl4Ts6zmo0hqr8L6zatbdnMThSLesESxv6AnfFP_-gmHU6covHOSdOFHKhmP6KNRmadEt1yGS-LCWJH6MWIyu3v8MPwWqCdymNkmzFIgs_BbIJSG7y6tdrOH/s1600/image6.png"
    ],
    "time": "2023/12/05 00:58:43"
  },
  {
    "title": "Video Classification on Edge Devices with TensorFlow Lite and MoViNet",
    "content": "Posted by Dan Kondratyuk, Liangzhe Yuan, Google Research and Khanh LeViet, TensorFlow Developer Relations\nWe are excited to announce MoViNets (pronounced \u201cmovie nets\u201d), a family of new mobile-optimized model architectures for video classification. The models are trained on the Kinetics-600 dataset to be able to recognize 600 different human actions (such as playing trumpet, robot dancing, bowling, and more) and can classify video streams captured on a modern smartphone in real time. You can download the pre-trained TensorFlow Lite models from TensorFlow Hub or try it out using our Android and Raspberry Pi demo apps, as well as fine-tune your own MoViNets with the Colab demo and the code in the TensorFlow Model Garden.\nDemo from the TensorFlow Lite video classification reference app\nVideo classification is a machine learning task that takes video frames as input and predicts a single class from a larger set of classes. Video action recognition is a type of video classification where the set of predicted classes consists of human actions that happened in the frames. Video action recognition is similar to image recognition in that both take input images and output the probabilities of the images belonging to each of the predefined classes. However, a video action recognition model has to look at both the content of each frame and the spatial relationships between adjacent frames to understand the actions in the video. For example, if you look at these still images, it\u2019s difficult to tell what the person is doing.\nBut if you look at the full video, it becomes clear that the person is performing jumping jacks.\nMoViNet Model Architecture\nMoViNets are a family of convolutional neural networks which efficiently process video streams, outputting accurate predictions with a fraction of the latency of convolutional video classifiers like 3D ResNets or transformer-based classifiers like ViT.\nFrame-based classifiers output predictions on each 2D frame independently, resulting in sub-optimal performance due to their lack of temporal reasoning. On the other hand, 3D video classifiers offer high accuracy predictions by processing all frames in a video clip simultaneously, at a cost of significant memory and latency penalties as the number of input frames increases. MoViNets offer key advantages from both 2D frame-based classifiers and 3D video classifiers while mitigating their disadvantages.\nThe following figure shows a typical approach to using 3D networks with multi-clip evaluation, where the predictions of multiple overlapping subclips are averaged together. Shorter subclips result in lower latency, but reduce the overall accuracy.\nDiagram illustrating Multi-Clip Evaluation for 3D Video Networks\nMoViNets take a hybrid approach, which proposes the use of causal convolutions in place of 3D convolutions, allowing intermediate activations to be cached across frames with a Stream Buffer. The Stream Buffer copies the input activations of all 3D operations, which is output by the model and then input back into the model on the next clip input.\nDiagram illustrating Streaming Evaluation for MoViNets\nThe result is that MoViNets can receive one frame input at a time, reducing peak memory usage while resulting in no loss of accuracy, with predictions equivalent to inputting all frames at once like a 3D video classifier. MoViNets additionally leverage Neural Architecture Search (NAS) by searching for efficient configurations of models on video datasets (specifically Kinetics 600) across network width, depth, and resolution.\nThe result is a set of action classifiers that can output temporally-stable predictions that smoothly transition based on frame content. Below is an example plot of MoViNet-A2 making predictions on each frame on a video clip of skateboarding. Notice how the initial scene with a small amount of motion has relatively constant predictions, while the next scene with much larger motion causes a dramatic shift in predicted classes.\nA video plotting the top-5 predictions of MoViNet-A2 over time on an example 8-second (25 fps) skateboarding video clip. Create your own plots with this Colab notebook.\nMoViNets need a few modifications to be able to run effectively on edge devices. We start with MoViNet-A0-Stream, MoViNet-A1-Stream, and MoViNet-A2-Stream, which represent the smaller models that can feasibly run in real time (20 fps or higher). To effectively quantize MoViNet, we adapt a few modifications to the model architecture - the hard swish activation is replaced by ReLU6, and Squeeze-and-Excitation layers are removed in the original architectures, which results in 3-4 p.p accuracy drop on Kinetics-600. We then convert the models to TensorFlow Lite and use integer-based post-training quantization (as well as float16 quantization) to reduce the model sizes and make them run faster on mobile CPUs. The integer-based post-training quantization process further introduces 2-3 p.p. accuracy loss. Compared to the original MoViNets, quantized MoViNets lag behind in accuracy on full 10-second Kinetics 600 clips (5-7 p.p. accuracy reduction in total), but in practice they are able to provide very accurate predictions on daily human actions, e.g., push ups, dancing, and playing piano. In the future, we plan to train with quantization-aware training to bridge this accuracy gap.\nWe benchmark quantized A0, A1, and A2 on real hardware and the model inference time achieves 200, 120, and 60 fps respectively on Pixel 4 CPU. In practice, due to the input pipeline overhead, we see increased latency closer to 20-60 fps when running on Android with a camera as input.\nModel\nQuantization\nTop-1 Accuracy (%)\nLatency \n(ms, Pixel 4 CPU)\nModel Size (MB)\nRecommended Input\nA0-Stream\nint8\n65.0\n4.80\n3.1\n172 x 172, 5 fps\nA1-Stream\nint8\n70.1\n8.35\n4.5\n172 x 172, 5 fps\nA2-Stream\nint8\n72.2\n15.76\n5.1\n224 x 224, 5 fps\nA0-Stream\nfloat16\n71.5\n17.47\n7.6\n172 x 172, 5 fps\nA1-Stream\nfloat16\n76.0\n34.82\n13\n172 x 172, 5 fps\nA2-Stream\nfloat16\n77.5\n76.31\n15\n224 x 224, 5 fps\nTrain a Custom Model\nYou can train your own video classifier model using the MoViNet codebase in the TensorFlow Model Garden. The provided Colab notebook provides specific steps on how to fine-tune a pretrained video classifier on another dataset.\nFuture Steps\nWe are excited to see on-device online video action recognition powered by MoViNets, which demonstrate highly efficient performance. In the future, we plan to support quantize-aware training for MoViNets to mitigate the quantization accuracy loss. We also are interested in extending MoViNets as the backbone for more on-device video tasks, e.g. video object detection, video object segmentation, visual tracking, pose estimation, and more.\nAcknowledgement\nWe would like to extend a big thanks to Yeqing Li for supporting MoViNets in TensorFlow Model Garden, Boqing Gong, Huisheng Wang, and Ting Liu for project guidance, Lu Wang for code reviews, and the TensorFlow Hub team for hosting our models.",
    "link": "https://blog.tensorflow.org/2022/04/video-classification-on-edge-devices.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiZEUGTtz5pQ1Oz86fmupeAjEoXcDZ9Ar4I4F5YhL6aNJbyAbz-AIJl9LRb-mzQ4Yh80VgEUM1HFhQaLWHeArM39HlOAwuVxHQw4lczTLqP0sSVloprJTx-blhannqi5tnsJksAQv7PpRQqj5IwgUdoxcrMpDjBNiwUJ32ljMMSPl9djYefKqpmnhWF/s1600/image3.gif",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiZEUGTtz5pQ1Oz86fmupeAjEoXcDZ9Ar4I4F5YhL6aNJbyAbz-AIJl9LRb-mzQ4Yh80VgEUM1HFhQaLWHeArM39HlOAwuVxHQw4lczTLqP0sSVloprJTx-blhannqi5tnsJksAQv7PpRQqj5IwgUdoxcrMpDjBNiwUJ32ljMMSPl9djYefKqpmnhWF/s1600/image3.gif",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFmymdyfpjdWPYjs8kD-D_MV2WUZ3JCDWv9OHYFEuDW_jQNuGVjpJRAVP3EUoy_vYIcreeSuds0VAnxRi6XiU0fEDn0evaM2JyUtcKLDaNkit8YYMpTBgx71NEBwO0wM6vrsWFpb08Ea-31gUKSbazy4FTIe2pWs-Iw7jPxHa_HCsyxfoME4sfGem-/s1600/image1.jpg",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3pwXcRtZKm2p2rj-f4xQxZ3ZPP8_1Prp7lsvzvsrFLLRdTHQytxvxmS5UWF6RsN8qbKQik4zHu12ubEw8iE5wVPbfDf7P1ODkFd-hhUaf1kg0krVhzP1SlqJq1sFAcIh_pqy046KsdAMSZs2srGLPKQgo6tsrW-LR-86YOyXzNnUdK34I9SoltDTt/s1600/image2.jpg",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjp2vkqwPGn8Ge8eSM8ZYFOWde-e__ckXSdsnk9oQB98a1JyOLJiWAU_1Vv0NwXc7Z0baJXNSoz8WU-UqF_IKiWdgoXSLek-IoYg8HL057_nNhucvhC90Z4Qrqg_w-TjDLwoAkMnzXFI4NQP5Keh6kCgEdd0THMhiX4gpGclCZcQhLF7gEVBgjYpTzo/s1600/image8.jpg",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhn0aKRhnGGZ0f7RafLO82vGefPVFDBbKT0oHXTL1esoaMFMUMnaiDW8Jt2aEXFK2rweNAeYVeKeu1failQaa99xOE0V1Uoj7XgmR_BFxXvsGaFR-lKXi06ZTxNdFhYBtSDUZbHVw7DaRXYC4dRL1lvu1rjXzZRjDs1WxevTXx_ay2SbmVULn5nykg0/s1600/image6.gif",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiZ73NcVJRnzVrcQs5DZpMho4F9Xg0SyoLCd6IgFWQ9iGxW3G_-fRGqY_O-wHPDrqjLW8oSAnOkM9ZynHIiWKYAwDNjOFekHzpvfuJ_0jZEm8adKNk0Z8eiC1KzvK0XXxO-xY8PK492OQ49FNUDoepuxUYe3CYGrGBSIovz3gGiIOiyS3SvDvAxnloc/s1600/image5.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQNM1sRHmOVQfExxkKVjiF81UcbtUpbw8phofsuuRa9x0Y5cvUks5qwODh6rbpHBq-0uQ-zsrOds7gBSfl0l65wulIjX9znqjzJbzAu2Zq5ROmgV2NKQEvkEF3_V3F_D2keZJ8yqJdxZws1Lr7NdlYRzRxtlyvAAMF3al0E7fac3cY6rvb6yzAl5rJ/s1600/image7.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiX_uoXPIJrekwRHmlnPgVZSckNBwz5B1cBB-oeWPXIc-19jpLjyL9lRcL1ZETHtjdD15AkWR4PsOAyDUCsvJyQ_jBfMfQ3Y6aEX5rMn0mdmonRHKBpWG0QdZRIL3ZHOJrm2zDOK_c3hg5iZLElZUw2tn2yROi_ASL8oRwC3-B7pL_3MZW5nVALvJxJ/s1600/image4.gif"
    ],
    "time": "2023/12/05 00:58:43"
  },
  {
    "title": "Accelerating TensorFlow Lite Micro on Cadence Audio Digital Signal Processors",
    "content": "Posted by Raj Pawate (Cadence) and Advait Jain (Google)\nDigital Signal Processors (DSPs) are a key part of any battery-powered device offering a way to process audio data with a very low power consumption. These chips run signal processing algorithms such as audio codecs, noise canceling and beam forming.\nIncreasingly these DSPs are also being used to run neural networks such as wake-word detection, speech recognition, and noise suppression. A key part of enabling such applications is the ability to execute these neural networks as efficiently as possible.\nHowever, productization paths for machine learning on DSPs can often be ad-hoc. In contrast, speech, audio, and video codecs have worldwide standards bodies such as ITU and 3GPP creating algorithms for compression and decompression addressing several aspects of quality measurement, fixed point arithmetic considerations and interoperability.\nTensorFlow Lite Micro (TFLM) is a generic open-sourced inference framework that runs machine learning models on embedded targets, including DSPs. Similarly, Cadence has invested heavily in PPA-optimized hardware-software platforms such as Cadence Tensilica HiFi DSP family for audio and Cadence Tensilica Vision DSP family for vision.\nGoogle and Cadence \u2013 A Multi-Year Partnership for Enabling AI at the Edge\nThis was the genesis of the collaboration between the TFLM team and the Audio DSP teams at Cadence, starting in 2019. The TFLM team is focusing on leveraging the broad TensorFlow framework and developing a smooth path from training to embedded and DSP deployment via an interpreter and reference kernels. Cadence is developing a highly optimized software library, called NeuralNet library (NNLIB), that leverages the SIMD and VLIW capabilities of their low-power HiFi DSPs. This collaboration started with three optimized kernels for one Xtensa DSP, and now encompasses over 50 kernels across a variety of platforms such as HiFi 5, HiFi 4, HiFi 3z, Fusion F1 as well as Vision DSPs such as P6, and includes the ability to offload to an accelerator, if available.\nAdditionally, we have collaborated to add continuous integration for all the optimized code targeted for the Cadence DSPs. This includes infrastructure that tests that every pull request to the TFLM repository passes all the unit tests for the Tensilica toolchain with various HiFix and Vision P6 cores. As such, we ensure that the combined TFLM and NNLIB open source software is both tightly integrated and has good automated test coverage.\nPerformance Improvements\nMost recently, we have collaborated on adding optimizations for models that are quantized with int16 activations. Specifically in the domain of audio, int16 activations can be critical for the quality of quantized generative models. We expect that these optimized kernels will enable a new class of ML-powered audio signal processing. The table below shows a few operators that are required for implementing a noise suppression neural net. We show a 267x improvement in cycle count for a variant of SEANet, an example noise suppression neural net.\nThe following table shows the improvement with the optimized kernels relative to the reference implementations as measured with the Xtensa instruction set simulation tool.\nOperator\nImprovement\nTranspose Conv\n458x\nConv2D\n287x\nSub\n39x\nAdd\n24x\nLeaky ReLU\n18x\nSrided_Slice\n10x\nPad\n6x\nOverall Network\n267x\nHow to use these optimizations\nAll of the code can be used from the TFLite Micro GitHub repository.\nTo use HiFi 3z targeted TFLM optimizations, the following conditions need to be met:\nthe TensorFlow Lite (TFLite) flatbuffer model is quantized with int16 activations and int8 weights\nit uses one or more of the operators listed in the table above\nTFLM is compiled with OPTIMIZED_KERNEL_DIR=xtensa\nFor example, you can run Conv2D kernel integration tests with reference C++ code with:\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa TARGET_ARCH=hifi4 XTENSA_CORE= test_integration_tests_seanet_conv\nAnd compare that to the optimized kernels by adding OPTIMIZED_KERNEL_DIR=xtensa:\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa TARGET_ARCH=hifi4 OPTIMIZED_KERNEL_DIR=xtensa XTENSA_CORE= test_integration_tests_seanet_conv\nLooking Ahead\nWhile the work thus far has been primarily focused on convolutional neural networks, Google and Cadence are also working together to develop an optimized LSTM operator and have released a first example of an LSTM-based key-word recognizer. We expect to expand on this and continue to bring optimized and production-ready implementations of the latest developments in AI/ML to Tensilica Xtensa DSPs.\nAcknowledgements\nWe would like to acknowledge a number of our colleagues who have contributed to making this collaboration successful.\nCadence: Int16 optimizations: Manjunath CP, Bhanu Prakash Venkata, Anirban Mandal LSTM implementation: Niranjan Yadla, Lukman Rahumathulla, Manjunath CP, Pramodkumar Surana, Arjun Medinakere NNLIB optimizations: Vijay Pawar, Prasad Nikam, Harshavardhan, Mayur Jagtap, Raj Pawate\nGoogle: Advait Jain, Deqiang Chen, Lawrence Chan, Marco Tagliasacchi, Nat Jeffries, Nick Kreeger, Pete Warden, Rocky Rhodes, Ting Yan, Yunpeng Li, Victor Ungureanu",
    "link": "https://blog.tensorflow.org/2022/03/Accelerating-TFLite-Micro-On-Cadence.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtmewLNKzKRZm7tPlPsm3AzC8g47bwqeo7EsJwCruRRi9s1gY2gRhfdNUPU6-SZhGq-AGTEW3FrcQpVWgF48w3OaAxZ7If3kYHuROVuqdWN1x5J3AJx43WVorfIsYsb2ulFJIJGjOfP4zR0P1KQ8cp1ajaHsGB1w5vfrvZrsfubTIh9Ju13vo4p1ry/s1600/image1.jpg",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtmewLNKzKRZm7tPlPsm3AzC8g47bwqeo7EsJwCruRRi9s1gY2gRhfdNUPU6-SZhGq-AGTEW3FrcQpVWgF48w3OaAxZ7If3kYHuROVuqdWN1x5J3AJx43WVorfIsYsb2ulFJIJGjOfP4zR0P1KQ8cp1ajaHsGB1w5vfrvZrsfubTIh9Ju13vo4p1ry/s1600/image1.jpg"
    ],
    "time": "2023/12/05 00:58:43"
  },
  {
    "title": "On-device one-shot learning for image classifiers with Classification-by-Retrieval",
    "content": "Posted by Zu Kim and Louis Romero, Software Engineers, Google Research\nClassification-by-retrieval provides an easy way to create a neural network-based classifier without computationally expensive training via backpropagation. Using this technology, you can create a lightweight mobile model with as little as one image per class, or you can create an on-device model that can classify as many as tens of thousands of classes. For example, we created mobile models that can recognize tens of thousands of landmarks with the classification-by-retrieval technology.\nThere are many use-cases for classification-by-retrieval, including:\nMachine learning education (e.g., an educational hackathon event).\nEasily prototyping, or demonstrating image classification.\nCustom product recognition (e.g., developing a product recognition app for a small/medium business without the need to gather extensive training data or write lots of code).\nTechnical background\nClassification and retrieval are two distinct methods of image recognition. A typical object recognition approach is to build a neural network classifier and train it with a large amount of training data (often thousands of images, or more). On the contrary, the retrieval approach uses a pre-trained feature extractor (e.g., an image embedding model) with feature matching based on a nearest neighbor search algorithm. The retrieval approach is scalable and flexible. For example, it can handle a large number of classes (say, > 1 million), and adding or removing classes does not require extra training. One would need as little as a single training data per class, which makes it effectively few-shot learning. A downside of the retrieval approach is that it requires extra infrastructure, and is less intuitive to use than a classification model. You can learn about modern retrieval systems in this article on TensorFlow Similarity.\nClassification-by-retrieval (CbR) is a neural network model with image retrieval layers baked into it. With the CbR technology, you can easily create a TensorFlow classification model without any training.\nAn image describing conventional image retrieval and conventional classification. Conventional image retrieval requires special retrieval infrastructure, and conventional classification requires expensive training with a large amount of data.\nAn image describing how classification-by-retrieval composes with a pre-trained embedding network and a final retrieval layer. It can be built without expensive training, and does not require special infrastructure for inference.\nHow do the retrieval layers work?\nA classification-by-retrieval model is an extension of an embedding model with extra retrieval layers. The retrieval layers are computed (not trained) from the training data, i.e., the index data. The retrieval layers consists of two components:\nNearest neighbor matching component\nResult aggregation component\nThe nearest neighbor matching component is essentially a fully connected layer where its weights are the normalized embeddings of the index data. Note that a dot-product of two normalized vectors (cosine similarity) is linear (with a negative coefficient) to the squared L2 distance. Therefore, the output of the fully connected layer is effectively identical to the nearest neighbor matching result.\nThe retrieval result is given for each training instance, not for each class. Therefore, we add another result aggregation component on top of the nearest neighbor matching layer. The aggregation component consists of a selection layer for each class followed by an aggregation (e.g., max) layer for each of them. Finally, the results are concatenated to form a single output vector.\nBase embedding model\nYou may choose a base embedding model that best fits the domain. There are many embedding models available, for example, in TensorFlow Hub. The provided iOS demo uses a MobileNet V3 trained with ImageNet, which is a generic and efficient on-device model.\nModel accuracy: Comparison with typical few-shot learning approaches\nIn some sense, CbR (indexing) can be considered as a few-shot learning approach without training. Although it is not apples to apples to compare CbR with an arbitrary pre-trained base embedding model with a typical few-shot learning approach where the whole model trained with given training data, there is a research that compares nearest neighbor retrieval (which is equivalent to CbR) with few-shot learning approaches. It shows that nearest neighbor retrieval can be comparable or even better than many few-shot learning approaches.\nHow to use this tool\nCross-platform C++ library\nThe code is available at https://github.com/tensorflow/examples/tree/master/lite/examples/classification_by_retrieval/lib.\niOS mobile app\nTo demo the ease of use of the Classification-by-Retrieval library, we built a mobile app that lets users select albums in their photo library as input data to create a new, tailor-made, image classification TFLite model. No coding required.\nThe iOS lets users create a new model by selecting albums in their library. Then the app lets them try the classification model on the live camera feed.\nWe encourage you to use these tools to build a model that is fair and responsible. To learn more about building a responsible model:\nhttps://www.tensorflow.org/responsible_ai\nhttps://design.google/library/fair-not-default/\nhttps://developers.google.com/machine-learning/crash-course/fairness/video-lecture\nFuture Work\nWe will explore possible ways to extend TensorFlow Lite Model Maker for on-device training capability based on this work.\nAcknowledgments\nMany people contributed to this work. We would like to thank Maxime Br\u00e9non, C\u00e9dric Deltheil, Denis Brul\u00e9, Chenyang Zhang, Christine Kaeser-Chen, Jack Sim, Tian Lin, Lu Wang, Shuangfeng Li, and everyone else involved in the project.",
    "link": "https://blog.tensorflow.org/2022/01/on-device-one-shot-learning-for-image.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/a/AVvXsEh5fl2r-O0N0nLMdMy_AxsZWMwEnnVSbpuYXm1lkNBbn9UzxkOLMZ4kLN7LpgznSb6sAU9Jxqh7mNrSmZqVZoYulheHPeFsk2gQExuAQVV31EPkAAd8YUpX5R6lA7c3V6WpfaveahRxt2nFKu9Qs-63meHe0BRVJUB9Ct_ra6l6aqi02UdZJh8hOS6u",
      "https://blogger.googleusercontent.com/img/a/AVvXsEh5fl2r-O0N0nLMdMy_AxsZWMwEnnVSbpuYXm1lkNBbn9UzxkOLMZ4kLN7LpgznSb6sAU9Jxqh7mNrSmZqVZoYulheHPeFsk2gQExuAQVV31EPkAAd8YUpX5R6lA7c3V6WpfaveahRxt2nFKu9Qs-63meHe0BRVJUB9Ct_ra6l6aqi02UdZJh8hOS6u",
      "https://blogger.googleusercontent.com/img/a/AVvXsEiIzq3EDjl5Mmy6H5VAZzdLJbL18J_mxNN9fkQYHDvLn1AByjb_kgg6lLveI0hMizgAufuU24NZCuyLv5lPWKbpg7Eqb1PP_U7J9WY3acs_3Rh2u93TnWTSlmhSOuqZXXP750ZWu4zNmXZLTCbS8oRHOJ9OKy2nyRR2bdjftI_OeQWI95kaygkf6owu",
      "https://blogger.googleusercontent.com/img/a/AVvXsEhcs5l0aG7ifUL8XU6Jpvdu3jgyG8eSldzPHyxOjV6rUb333Cs9WES_7WFS3d_MCDUOSwopZVQR3K03_7xLexDqKcKveOfVpoMcZNQ58UdPrPVrtdJ3WYnBN7rivBONGN_ca1-gVnhB70Ume6if0RsehwK0-foAlah2ZMpBqBY7B6v10kdvDFE7B156"
    ],
    "time": "2023/12/05 00:58:43"
  },
  {
    "title": "On-device training in TensorFlow Lite",
    "content": "Posted by the TensorFlow Lite team\nTensorFlow Lite is Google\u2019s machine learning framework to deploy machine learning models on multiple devices and surfaces such as mobile (iOS and Android), desktops and other edge devices. Recently, we added support to run TensorFlow Lite models in a browser as well. In order to build apps using TensorFlow Lite, you can either use an off-the shelf model from TensorFlow Hub, or convert an existing TensorFlow Model to a TensorFlow Lite model using the converter. Once the model is deployed in an app, you can run inference on the model based on input data.\nTensorFlow Lite now supports training your models on-device, in addition to running inference. On-device training enables interesting personalization use cases where models can be fine-tuned based on user needs. For instance, you could deploy an image classification model and allow a user to fine-tune the model to recognize bird species using transfer learning, while allowing another user to retrain the same model to recognize fruits. This new feature is available in TensorFlow 2.7 and later and is currently available for Android apps. (iOS support will be added in the future.)\nOn-device training is also a necessary foundation for Federated Learning use cases to train global models on decentralized data. This blog post does not cover Federated Learning and instead focuses on helping you integrate on-device training in your Android apps.\nLater in this article we will reference a Colab and Android sample app as we walk you through the end-to-end implementation path for on-device learning to fine-tune an image classification model.\nImprovements over the earlier approach\nIn our 2019 blog post, we introduced on-device training concepts and an example of on-device training in TensorFlow Lite. However, there were several limitations. For example, it was not easy to customize the model structure and optimizers. You also had to deal with multiple physical TensorFlow Lite (.tflite) models instead of a single TensorFlow Lite model. Similarly, there was no easy way to store and update the training weights. Our latest TensorFlow Lite version streamlines this process by providing more convenient options for on-device training, as explained below.\nHow does it work?\nIn order to deploy a TensorFlow Lite model with on-device training built-in, here are the high level steps:\nBuild a TensorFlow model for training and inference\nConvert the TensorFlow model to TensorFlow Lite format\nIntegrate the model in your Android app\nInvoke model training in the app, similar to how you would invoke model inference\nThese steps are explained below.\nBuild a TensorFlow model for training and inference\nThe TensorFlow Lite model should not only support model inference, but also model training, which typically involves saving the model\u2019s weights to the file system and restoring the weights from the file system. This is done to save the training weights after each training epoch, so that the next training epoch can use the weights from the previous one, instead of starting training from scratch.\nOur suggested approach is to implement these tf.functions to represent training, inference, saving weights, and loading weights:\nA train function that trains the model using training data. The train function below makes a prediction, calculates the loss (or error), and uses tf.GradientTape() to record operations for automatic differentiation and update the model\u2019s parameters.\n# The `train` function takes a batch of input images and labels.\n@tf.function(input_signature=[\n     tf.TensorSpec([None, IMG_SIZE, IMG_SIZE], tf.float32),\n     tf.TensorSpec([None, 10], tf.float32),\n ])\ndef train(self, x, y):\n   with tf.GradientTape() as tape:\n     prediction = self.model(x)\n     loss = self._LOSS_FN(prediction, y)\n   gradients = tape.gradient(loss, self.model.trainable_variables)\n   self._OPTIM.apply_gradients(\n       zip(gradients, self.model.trainable_variables))\n   result = {\"loss\": loss}\n   for grad in gradients:\n     result[grad.name] = grad\n   return result\nAn infer or a predict function that invokes model inference. This is similar to how you currently use TensorFlow Lite for inference.\n@tf.function(input_signature=[tf.TensorSpec([None, IMG_SIZE, IMG_SIZE], tf.float32)])\n def predict(self, x):\n   return {\n       \"output\": self.model(x)\n   }\nA save/restore function that saves training weights (i.e., parameters used by the model) in Checkpoints format to the file system. The save function\u2019s code is shown below.\n@tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n def save(self, checkpoint_path):\n   tensor_names = [weight.name for weight in self.model.weights]\n   tensors_to_save = [weight.read_value() for weight in self.model.weights]\n   tf.raw_ops.Save(\n       filename=checkpoint_path, tensor_names=tensor_names,\n       data=tensors_to_save, name='save')\n   return {\n       \"checkpoint_path\": checkpoint_path\n   }\nConvert to TensorFlow Lite format\nYou may already be familiar with the workflow to convert your TensorFlow model to the TensorFlow Lite format. Some of the low level features for on-device training (e.g., variables to store the model parameters) are still experimental, and others (e.g., weight serialization) currently rely on TF Select operators, so you will need to set these flags during conversion. You can find an example of all the flags you need to set in the Colab.\n# Convert the model\nconverter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_DIR)\nconverter.target_spec.supported_ops = [\n   tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\n   tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.\n]\nconverter.experimental_enable_resource_variables = True\ntflite_model = converter.convert()\nIntegrate the model in your Android app\nOnce you have converted your model to the TensorFlow Lite format, you\u2019re ready to integrate the model into your app! Refer to the Android app samples for more details.\nInvoke model training and inference in app\nOn Android, TensorFlow Lite on-device training can be performed using either Java or C++ APIs. You can create an instance of the TensorFlow Lite Interpreter to load a model and drive model training tasks. We had previously defined multiple tf.functions: these functions can be invoked using TensorFlow Lite\u2019s support for Signatures, which allow a single TensorFlow Lite model to support multiple \u2018entry\u2019 points. For example, we had defined a train function for on-device training, which is one of the model\u2019s signatures. The train function can be invoked using TensorFlow Lite\u2019s runSignature method by specifying the name of the signature (\u2018train\u2019):\n // Run training for a few steps.\nfloat[] losses = new float[NUM_EPOCHS];\nfor (int epoch = 0; epoch < NUM_EPOCHS; ++epoch) {\n    for (int batchIdx = 0; batchIdx < NUM_BATCHES; ++batchIdx) {\n        Map<String, Object> inputs = new HashMap<>>();\n        inputs.put(\"x\", trainImageBatches.get(batchIdx));\n        inputs.put(\"y\", trainLabelBatches.get(batchIdx));\n\n        Map<String, Object> outputs = new HashMap<>();\n        FloatBuffer loss = FloatBuffer.allocate(1);\n        outputs.put(\"loss\", loss);\n\n        interpreter.runSignature(inputs, outputs, \"train\");\n\n        // Record the last loss.\n        if (batchIdx == NUM_BATCHES - 1) losses[epoch] = loss.get(0);\n    }\n}\nSimilarly, the following example shows how to invoke inference using the model\u2019s \u2018infer\u2019 signature:\ntry (Interpreter anotherInterpreter = new Interpreter(modelBuffer)) {\n    // Restore the weights from the checkpoint file.\n\n    int NUM_TESTS = 10;\n    FloatBuffer testImages = FloatBuffer.allocateDirect(NUM_TESTS * 28 * 28).order(ByteOrder.nativeOrder());\n    FloatBuffer output = FloatBuffer.allocateDirect(NUM_TESTS * 10).order(ByteOrder.nativeOrder());\n\n    // Fill the test data.\n\n    // Run the inference.\n    Map<String, Object> inputs = new HashMap<>>();\n    inputs.put(\"x\", testImages.rewind());\n    Map<String, Object> outputs = new HashMap<>();\n    outputs.put(\"output\", output);\n    anotherInterpreter.runSignature(inputs, outputs, \"infer\");\n    output.rewind();\n\n    // Process the result to get the final category values.\n    int[] testLabels = new int[NUM_TESTS];\n    for (int i = 0; i < NUM_TESTS; ++i) {\n        int index = 0;\n        for (int j = 1; j < 10; ++j) {\n            if (output.get(i * 10 + index) < output.get(i * 10 + j))\n                index = testLabels[j];\n        }\n        testLabels[i] = index;\n    }\n}\nAnd, that\u2019s it! You now have a TensorFlow Lite model that is able to use on-device training. We hope that this code walkthrough gives you a good idea on how to run on-device training in TensorFlow Lite, and we\u2019re excited to see where you take it.\nPractical considerations\nIn theory, you should be able to apply on-device training in TensorFlow Lite to any use case that TensorFlow supports. However, in reality there are a few practical considerations that you need to keep in mind before you deploy on-device training in your apps:\nUse cases: The Colab example shows an example of on-device training for a vision use case. If you run into issues for specific models or use cases, please let us know on GitHub.\nPerformance: Depending on the use case, on-device training could take anywhere from a few seconds to much longer. If you run on-device training as part of a user-facing feature (e.g., your end user is interacting with the feature), you should measure the time taken for a wide range of possible training inputs in your app to limit the training time. If your use-case requires very long on-device training times, consider training a model using a desktop or the cloud first, then fine-tuning it on-device.\nBattery usage: Just like model inference, invoking model training on device may result in a battery drain. If model training is part of a feature that is not user facing, we recommend following Android\u2019s guidelines to implement background tasks.\nTraining from scratch vs. retraining: In theory, it should be possible to train a model from scratch on device using the above features. However, in reality, training from scratch involves an enormous amount of training data and may take several days even on servers with powerful processors. Consequently, for on-device applications, we recommend retraining on an already trained model (i.e., transfer learning) as shown in the Colab example.\nRoadmap\nFuture work includes (but is not limited to) on-device training support on iOS, performance improvements to leverage on-device accelerators (e.g. GPUs) for on-device training, reducing the binary size by implementing more training ops natively in TensorFlow Lite, higher level API support (e.g. via the TensorFlow Lite Task Library) to abstract away the implementation details and examples covering other on-device training use cases (e.g. NLP). Our long term roadmap involves potentially providing on-device end-to-end Federated Learning solutions.\nNext steps\nThank you for reading! We\u2019re excited to see what you build using on-device learning. Once again, here are links to the sample app and Colab. If you have any feedback, please let us know on the TensorFlow Forum, or on GitHub.\nAcknowledgements\nThis post reflects the significant contributions of many people in Google\u2019s TensorFlow Lite team including Michelle Carney, Lawrence Chan, Jaesung Chung, Jared Duke, Terry Heo, Jared Lim, Yu-Cheng Ling, Thai Nguyen, Karim Nosseir, Arun Venkatesan, Haoliang Zhang, other TensorFlow Lite team members, and our collaborators in Google Research.",
    "link": "https://blog.tensorflow.org/2021/11/on-device-training-in-tensorflow-lite.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/a/AVvXsEgPwg4FKbY2OsQHEOqKCbWqBNTmGe2pgh01H2MNOZlPA_SN8tbMur9Mv1bAkzm9dyTXRWyUMqF13b4dXiepeknHngy8yup1g7ueZJ3WZjOyY8n0oOKalvL9Gef2DKyZIzMa0AdEnMhss5HHkr88-W-sGk_pLnb_Zj1Xrch2mFj3xRgaCIdPJF5aMBCu",
      "https://blogger.googleusercontent.com/img/a/AVvXsEgPwg4FKbY2OsQHEOqKCbWqBNTmGe2pgh01H2MNOZlPA_SN8tbMur9Mv1bAkzm9dyTXRWyUMqF13b4dXiepeknHngy8yup1g7ueZJ3WZjOyY8n0oOKalvL9Gef2DKyZIzMa0AdEnMhss5HHkr88-W-sGk_pLnb_Zj1Xrch2mFj3xRgaCIdPJF5aMBCu"
    ],
    "time": "2023/12/05 00:58:43"
  },
  {
    "title": "Building a board game app with TensorFlow: a new TensorFlow Lite reference app",
    "content": "Posted by Wei Wei, Developer Advocate\nGames are often used as test grounds for various reinforcement learning (RL) algorithms. While it is very exciting that machine learning researchers invent new RL algorithms to master challenging games, we are also curious to see that game developers are using RL to build gaming bots in TensorFlow for various purposes, such as quality testing, game balance tuning and game difficulty assessment.\nWe already have a detailed tutorial that demonstrates how to implement the actor-critic RL method for the classical CartPole gym environment with TensorFlow. In this end-to-end tutorial, we are going to show you how to use TensorFlow core, TensorFlow Agents and TensorFlow Lite to build a game agent to play against a human user in a small board game app. The end result is an Android reference app that looks like below, and we have open sourced all the code in tensorflow/examples repository for your reference.\nDemo game play in \u2018Plane Strike\u2019\nThe game is called \u2018Plane Strike\u2019, a small board game that resembles the board game \u2018Battleship\u2019. The rules are very simple:\nAt the beginning of the game, the user and the agent each have a \u2018plane\u2019 object (8 blue cells that form a \u2018plane\u2019 as you can see in the animation above) on their own boards; these planes are only visible to the owners of the board and hidden to their opponents.\nThe user and the agent take turns to strike at one cell of each other\u2019s board. The user can tap any cell in the agent\u2019s board, while the agent will automatically make the choice based on the prediction of a machine learning model. The attempted cell turns red if it is a \u2018plane\u2019 cell (\u2018hit\u2019); otherwise it turns yellow (\u2018miss\u2019).\nWhoever achieves 8 red cells first wins the game; then the game is restarted with fresh boards.\nEven though it may be possible to create handcrafted rules for such a small game, we turn to reinforcement learning to create a smart agent that a human player can\u2019t easily beat. For a general introduction of reinforcement learning, please refer to this RL course from DeepMind and UCL.\nWe provide 2 paths of training and deployment for this game app\nTensorFlow Lite with a Python model written from scratch\nIn this path, to train the agent, we first create a custom OpenAI gym environment \u2018PlaneStrike-v0\u2019, which allows us to easily roll out game plays and gather game logs. Then we use the reward-to-go policy gradient algorithm to train the agent. REINFORCE is a policy gradient algorithm in RL. Its basic idea is to adjust the policy network parameters based on the reward signals collected during the gameplay, so that the policy network can maximize the return in future plays.\nMathematically, the policy gradient is defined as:\nwhere:\nT: the number of timesteps per episode, which can vary per episode\nst: the state at timestep t\nat: chosen action at timestep t given state s\n\u03c0\u03b8: is the policy parameterized by \u03b8\nR(*): is the reward gathered, given the policy\nPlease refer to this DeepMind lecture on policy gradient for a more detailed discussion. To implement it with TensorFlow, we define a simple 3-layer MLP as our policy network, which predicts the agent\u2019s next strike position, given the human player\u2019s board state. Note that the log expression of the above policy gradient without the reward part is the equivalent of negative cross entropy loss. In this case, since we want to maximize the rewards, we can just minimize the categorical cross entropy loss to achieve that.\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=sgd)\nWe create a play_game() function to roll out the game and help us gather game logs. After each episode, we train the agent via Keras fit() function:\nmodel.fit(x=board_log, y=action_log, sample_weight=rewards)\nNote that we pass the discounted rewards-to-go as \u2018sample_weight\u2019 into the Keras fit() function as a shortcut, to implement the policy gradient algorithm without writing a custom training loop. An intuitive way to think about this is we need a tuple of (x, y, reward) instead of just (x, y) as in supervised learning. Rewards, which can be negative, help the predictor output move toward/away from y, based on x. This is different from supervised learning (in which case your 'sample_weight' can never be negative).\nSince what we are doing isn\u2019t supervised learning, we can\u2019t really use training loss to monitor the training progress. Instead, we are going to use a proxy metric \u2018game_length\u2019, which indicates how many steps the agent takes to finish each episode. Intuitively you can understand that if the agent is smarter and makes better predictions, the game length becomes shorter.\nTraining progress in TensorBoard\nSince this is a game that needs instantaneous responses from the agent, we want to deploy the model on mobile devices instead of servers. After training the model, we use the TFLite converter to convert the Keras model into a TFLite model, and integrate it into our Android app.\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\nThe exported model is very fast and takes <1 ms to execute on Pixel phones. During the game play, at each step the agent looks at the user\u2019s board position and predicts its next strike position to achieve 8 red cells as fast as possible.\nconvertBoardStateToByteBuffer(board);\ntflite.run(boardData, outputProbArrays);\nfloat[] probArray = outputProbArrays[0];\nint agentStrikePosition = -1;\nfloat maxProb = 0;\nfor (int i = 0; i < probArray.length; i++) {\n  int x = i / Constants.BOARD_SIZE;\n  int y = i % Constants.BOARD_SIZE;\n  if (board[x][y] == BoardCellStatus.UNTRIED && probArray[i] > maxProb) {\n    agentStrikePosition = i;\n    maxProb = probArray[i];\n  }\n}\nTensorFlow Lite with a model trained with TensorFlow Agents\nWhile it\u2019s a good exercise to write our agent from scratch using TensorFlow API, it\u2019s better to leverage existing implementations of RL algorithms. TensorFlow Agents is a library for reinforcement learning in TensorFlow, and makes it easier to design, implement and test new RL algorithms by providing well tested modular components that can be modified and extended. TF Agents has implemented several state-of-the-art RL algorithms, including DQN, DDPG, REINFORCE, PPO, SAC and TD3. Trained policies by TF Agents can be converted to TFLite directly and deployed into mobile apps (note that this feature is only recently enabled so you will need the nightly builds of TensorFlow and TensorFlow Agents).\nWe use the TF Agents REINFORCE agent to train our agent. First, we need to define a TF Agents training environment as we did with the gym environment in the previous section. Then we can define an actor net as our policy network\nactor_net = tfa.networks.Sequential([\n    tfa.keras_layers.InnerReshape([BOARD_SIZE, BOARD_SIZE], [BOARD_SIZE**2]),\n    tf.keras.layers.Dense(FC_LAYER_PARAMS, activation='relu'),\n    tf.keras.layers.Dense(BOARD_SIZE**2),\n    tf.keras.layers.Lambda(lambda t: tfp.distributions.Categorical(logits=t)),\n],        input_spec=train_py_env.observation_spec(\n                                    ))\nWe are going to use the built-in REINFORCE agent that TF Agents has already implemented. The agent is built on top of the \u2018actor_net\u2019 defined above:\n  tf_agent = reinforce_agent.ReinforceAgent(\n      train_env.time_step_spec(),\n      train_env.action_spec(),\n      actor_network=actor_net,\n      optimizer=optimizer,\n      normalize_returns=True,\n      train_step_counter=train_step_counter)\nTo train the agent, we need to collect some trajectories as experience. We define a function just for that using DeepMind Reverb and TF Agent PyDriver:\ndef collect_episode(environment, policy, num_episodes, replay_buffer_observer):\n  \"\"\"Collect game episode trajectories.\"\"\"\n  initial_time_step = environment.reset()\n\n  driver = py_driver.PyDriver(\n      environment,\n      py_tf_eager_policy.PyTFEagerPolicy(policy, use_tf_function=True),\n      [replay_buffer_observer],\n      max_episodes=num_episodes)\n  initial_time_step = environment.reset()\n  driver.run(initial_time_step)\nNow we are ready to train the model:\nfor i in range(iterations):\n  # Collect a few episodes using collect_policy and save to the replay buffer.\n  collect_episode(train_py_env, collect_policy,\n                COLLECT_EPISODES_PER_ITERATION, replay_buffer_observer)\n\n  # Use data from the buffer and update the agent's network.\n  iterator = iter(replay_buffer.as_dataset(sample_batch_size=1))\n  trajectories, _ = next(iterator)\n  tf_agent.train(experience=trajectories)\n  replay_buffer.clear()\nYou can monitor the training progress using TensorBoard. In this case, we visualize both the average episode length and average return.\nTF Agents training progress in TensorBoard\nOnce the policy has been trained and exported as SavedModel, you can converted it into a TFLite model:\nconverter = tf.lite.TFLiteConverter.from_saved_model(\n    policy_dir, signature_keys=['action'])\nconverter.target_spec.supported_ops = [\n    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\n    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.\n]\ntflite_policy = converter.convert()\nwith open(os.path.join(model_dir, 'planestrike_tf_agents.tflite'), 'wb') as f:\n  f.write(tflite_policy)\nCurrently there are a few TensorFlow ops that are required during the conversion. The converted model is slightly different from the model we trained using TensorFlow directly, because it takes 4 tensors as the input. What really matters here is the \u2018observation\u2019 tensor. Our agent will look at this \u2018observation\u2019 tensor and predict its next move. The other 3 can be safely ignored at inference time.\nVisualizing TFLite model converted from TF Agents using Netron\nAlso, the model directly outputs the strike position instead of the probability distribution, so we no longer need to do argmax manually.\n@Override\nprotected void runInference() {\n  Map output = new HashMap<>();\n  // TF Agent directly returns the predicted action\n  int[][] prediction = new int[1][1];\n  output.put(0, prediction);\n  tflite.runForMultipleInputsOutputs(inputs, output);\n  agentStrikePosition = prediction[0][0];\nSo to summarize, in this post we showed you 2 paths of how to train a game agent, convert the trained model to TFLite and deploy it into an Android app. Hopefully this end-to-end tutorial helps you better understand how to leverage the TensorFlow ecosystem to build cool games.\nAnd lastly, if this little game looks interesting to you, we challenge you to install the app on your phone and see if you can beat the agent we have trained \ud83d\ude03.",
    "link": "https://blog.tensorflow.org/2021/10/building-board-game-app-with-tensorflow.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/a/AVvXsEj7K9sQBJ8GVQcgPNGxsMGJmgCkoPaOxvbTJttSqn0kRxpRDhVSW7gPzr93vcDWOhZDY73YKOSG0_ERiJHwcw9T08EuwUvUXUAt7bFC8giFOu-Shl6FWnWmILajHGZ2K41XSzCtKNoiEpMCc2WnxGavNPac2Ua8T2iGemtud1NQ_pVfRutYVlzv6HZt",
      "https://blog.tensorflow.org/2021/10/building-board-game-app-with-tensorflow.html",
      "https://blogger.googleusercontent.com/img/a/AVvXsEj7K9sQBJ8GVQcgPNGxsMGJmgCkoPaOxvbTJttSqn0kRxpRDhVSW7gPzr93vcDWOhZDY73YKOSG0_ERiJHwcw9T08EuwUvUXUAt7bFC8giFOu-Shl6FWnWmILajHGZ2K41XSzCtKNoiEpMCc2WnxGavNPac2Ua8T2iGemtud1NQ_pVfRutYVlzv6HZt",
      "https://blogger.googleusercontent.com/img/a/AVvXsEhFwzcS4F1vegsRm85Crg3_ArGRYFSfN90f-vVyd8FBnkwYI4oQEjgkh3gKhMLR2a2hD1moCEYDh4oNVA8uRxeNPIYkq2a7lKMqlUpLMihXw5b_8lixszKeFdaI6YbzS5JFJRppQHyaRmNASGhTYsZbJaUJFzkfEk0vZF3la1vQFn3Z2Awew6uyt0S1",
      "https://blogger.googleusercontent.com/img/a/AVvXsEjCWvoDmwSp-kJF04BjTiJCUlKlWOcGWFyLx8pq_ARTWugwFn1Zf37wZ9UISx4VVZMWyBwhr247Nl3tz3G5tAl8R65KxVwMpvQPZjDfUXLO8_CXnzVZZXkIz5uDkl3bizsa4yHvutqNifR4kfh3JPab9jRqxq2cwRKW8yFzXu3QVOpkjZ-Oxk2zNf3z",
      "https://blogger.googleusercontent.com/img/a/AVvXsEgeZ3sjfKckxLxzob4JcEZFGacUV-CZqCiHF7J2O7SLzVAzjxruYEfD3g67vtI8su3W0sHTh3JPL0toLdFe-2ozyxTirjKaS-lfv5YeHTYvmQV167K_7u8xN3CikYkTRdLhjy9i4TukCHwISVvqdusTVIOZz2TcXoXe0oSnF0aDjaAzlDOOQqwenKgF",
      "https://blogger.googleusercontent.com/img/a/AVvXsEgHNkvarGJnyJczW9AucI_4IlotCPqY7t6Zbhu8sAcFRnHqfZbm4AXx2dhF8_l3Ih0fI6IuK61ONX0_gJmMolZBFHiPFZMKNyh01scouRmkTHRmq9L811t2pL6QpWNygJdWC2PkbimUE9SHtPGIljTT4VU_IJ3L5wZeIfeg146WHZK6YGmsigeca3da",
      "https://blogger.googleusercontent.com/img/a/AVvXsEiw_3Ft1LElnfxLmnR65KW1q-jI-oqewgnYxoyjV840N1c-TICZOwWsQuaesliFoXyqPAmyVhdjCLmlTlLOsynptmBd5qxhXFo5NH2air88C6ABYkyBsPphG_68PDE3qaMRx5M833Gy6xBTDp8fXsGHlArRaGZU8BfQCUMFEWTABULHtt9osf6c304O"
    ],
    "time": "2023/12/04 00:58:43"
  },
  {
    "title": "Get Started with TensorFlow Lite Micro on Sony\u2019s Spresense",
    "content": "A guest post by Daniel Sandblom, Sony\nEditor's note: an earlier version of this article appeared on the Sony Developers.\nNow you can develop solutions with TensorFlow Lite Micro (TFLM) for the Spresense microcontroller board from Sony. TFLM is designed to run on microcontroller systems where the hardware resources are more limited compared to larger computerized systems. The footprint of TFLM is typically in the order of only 10\u2019s of kBs.\nWhat you get is a combination of a leading machine learning ecosystem with a high performance microcontroller running at super low power consumption. The Spresense board was designed with camera and hi-res audio inputs as core features which open up a substantial set of use cases. Pete Warden, a research engineer on the TensorFlow team, shares his view on that TFLM is now available for use with the Spresense board: \u201cIt\u2019s great to see this kind of compute capability tightly integrated into a low power sensor, the combination will help make machine learning accessible to developers in medical, agriculture, industrial monitoring and many other areas where a small form factor and energy are strong constraints.\u201d\nThe development of TFLM has been a tight collaboration between Google and Arm to optimize functionality while keeping the footprint to a minimum. Fredrik Knutsson, Team Lead at Arm, explains how TFLM has been optimized for the ARM processor architecture: \u201cArm\u2019s open source CMSIS-NN library provides high performance implementations of common neural network functions for Arm Cortex-M processors. Arm\u2019s engineers have worked closely with the TensorFlow team to develop optimized versions of the TensorFlow Lite kernels in the CMSIS-NN library, delivering extremely fast performance on Arm Cortex-M cores like Spresense.\u201d\nHow to get started with TensorFlow on Spresense\nThe easiest and quickest way to get started with TensorFlow on Spresense is to run one of the examples. There is one hello_world example that shows the basic steps and functionality. There is also a micro_speech example using Spresense\u2019s audio abilities, and there\u2019s a person_detection example utilizing the Spresense camera. The latter two examples demonstrate how to link visual and audio sensors to the inputs of TensorFlow models.\nBelow are the general steps to run the examples:\nSet up the Spresense SDK: Getting started with TensorFlow for Spresense\nDownload the Spresense repository including the examples\nBuild and Flash the binary into Spresense main board\nRun the example\nHeads-up: we will run an upcoming webinar for \u201cTensorFlow on Spresense\u201d on October 14 \u2013 register here!\nCheck out these links for more info:\nSpresense developer website\nTensorFlow tutorials for Spresense\nTensorFlow Lite for Microcontrollers web site\nArm\nEdge Impulse for embedded machine learning",
    "link": "https://blog.tensorflow.org/2021/10/TF-Lite-Sony-Spresense.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-L3xPayUgXFg/YVcUh5VA9RI/AAAAAAAAEjg/8uiM3H3BQswCgtTlb0of5T3yJiyjRzy8QCLcBGAsYHQ/s0/sony%2Btensorflow.png"
    ],
    "time": "2023/12/04 00:58:43"
  },
  {
    "title": "TensorFlow Model Optimization Toolkit \u2014 Collaborative Optimization API",
    "content": "A guest post by Mohamed Nour Abouelseoud and Elena Zhelezina at Arm.\nThis blog post introduces collaborative techniques for machine learning model optimization for edge devices, proposed and contributed by Arm to the TensorFlow Model Optimization Toolkit, available starting from release v0.7.0.\nThe main idea of the collaborative optimization pipeline is to apply the different optimization techniques in the TensorFlow Model Optimization Toolkit one after another while maintaining the balance between compression and accuracy required for deployment. This leads to significant reduction in the model size and could improve inference speed given framework and hardware-specific support such as that offered by the Arm Ethos-N and Ethos-U NPUs.\nThis work is a part of the toolkit's roadmap to support the development of smaller and faster ML models. You can see previous posts on post-training quantization, quantization-aware training, sparsity, and clustering for more background on the toolkit and what it can do.\nWhat is Collaborative Optimization? And why?\nThe motivation behind collaborative optimization remains the same as that behind the Model Optimization Toolkit (TFMOT) in general, which is to enable model conditioning and compression for improving deployment to edge devices. The push towards edge computing and endpoint-oriented AI creates high demand for such tools and techniques. The Collaborative Optimization API stacks all of the available techniques for model optimization to take advantage of their cumulative effect and achieve the best model compression while maintaining required accuracy.\nGiven the following optimization techniques, various combinations for deployment are possible:\nWeight pruning\nWeight clustering\nQuantization\nPost-training quantization\nQuantization aware training (QAT)\nIn other words, it is possible to apply one or both of pruning and clustering, followed by post-training quantization or QAT before deployment.\nThe challenge in combining these techniques is that the APIs don\u2019t consider previous ones, with each optimization and fine-tuning process not preserving the results of the preceding technique. This spoils the overall benefit of simultaneously applying them; i.e., clustering doesn't preserve the sparsity introduced by the pruning process and the fine-tuning process of QAT loses both the pruning and clustering benefits. To overcome these problems, we introduce the following collaborative optimization techniques:\nSparsity preserving clustering: clustering API that ensures a zero cluster, preserving the sparsity of the model.\nSparsity preserving quantization aware training (PQAT): QAT training API that preserves the sparsity of the model.\nCluster preserving quantization aware training (CQAT): QAT training API that does re-clustering and preserves the same number of centroids.\nSparsity and cluster preserving quantization aware training (PCQAT): QAT training API that preserves the sparsity and number of clusters of a model trained with sparsity-preserving clustering.\nConsidered together, along with the option of post-training quantization instead of QAT, these provide several paths for deployment, visualized in the following deployment tree, where the leaf nodes are deployment-ready models, meaning they are fully quantized and in TFLite format. The green fill indicates steps where retraining/fine-tuning is required and a dashed red border highlights the collaborative optimization steps. The technique used to obtain a model at a given node is indicated in the corresponding label.\nThe direct, quantization-only (post-training or QAT) deployment path is omitted in the figure above.\nThe idea is to reach the fully optimized model at the third level of the above deployment tree; however, any of the other levels of optimization could prove satisfactory and achieve the required inference latency, compression, and accuracy target, in which case no further optimization is needed. The recommended training process would be to iteratively go through the levels of the deployment tree applicable to the target deployment scenario and see if the model fulfils the optimization requirements and, if not, use the corresponding collaborative optimization technique to compress the model further and repeat until the model is fully optimized (pruned, clustered, and quantized), if needed.\nTo further unlock the improvements in memory usage and speed at inference time associated with collaborative optimization, specialized run-time or compiler software and dedicated machine learning hardware is required. Examples include the Arm ML Ethos-N driver stack for the Ethos-N processor and the Ethos-U Vela compiler for the Ethos-U processor. Both examples currently require quantizing and converting optimized Keras models to TensorFlow Lite first.\nThe figure below shows the density plots of a sample weight kernel going through the full collaborative optimization pipeline.\nThe result is a quantized deployment model with a reduced number of unique values as well as a significant number of sparse weights, depending on the target sparsity specified at training time. This results in substantial model compression advantages and significantly reduced inference latency on specialized hardware.\nCompression and accuracy results\nThe tables below show the results of running several experiments on popular models, demonstrating the compression benefits vs. accuracy loss incurred by applying these techniques. More aggressive optimizations can be applied, but at the cost of accuracy. Though the table below includes measurements for TensorFlow Lite models, similar benefits are observed for other serialization formats.\nSparsity-preserving Quantization aware training (PQAT)\nModel\nMetric\nBaseline\nPruned Model (50% sparsity)\nQAT Model\nPQAT Model\nDS-CNN-L\nFP32 Top-1 Accuracy\n95.23%\n94.80%\n(Fake INT8) 94.721%\n(Fake INT8) 94.128%\nINT8 Top-1 Accuracy\n94.48%\n93.80%\n94.72%\n94.13%\nCompression\n528,128 \u2192 434,879 (17.66%)\n528,128 \u2192 334,154 (36.73%)\n512,224 \u2192 403,261 (21.27%)\n512,032 \u2192 303,997 (40.63%)\nMobileNet_v1 on ImageNet\nFP32 Top-1 Accuracy\n70.99%\n70.11%\n(Fake INT8) 70.67%\n(Fake INT8) 70.29%\nINT8 Top-1 Accuracy\n69.37%    \n67.82%\n70.67%    \n70.29%\nCompression\n4,665,520 \u2192 3,880,331 (16.83%)\n4,665,520 \u2192 2,939,734 (37.00%)\n4,569,416 \u2192 3,808,781 (16.65%)\n4,569,416 \u2192 2,869,600 (37.20%)\nNote: DS-CNN-L is a keyword spotting model designed for edge devices. More can be found in Arm\u2019s ML Examples repository.\nCluster-preserving Quantization aware training (CQAT)\n\nModel\nMetric\nBaseline\nClustered Model\nQAT Model\nCQAT Model\nMobileNet_v1 on CIFAR-10\nFP32 Top-1 Accuracy\n94.88%\n94.48% (16 clusters)    \n(Fake INT8) 94.80%\n(Fake INT8) 94.60%\nINT8 Top-1 Accuracy\n94.65%    \n94.41% (16 clusters)    \n94.77%\n94.52%\nSize\n3,000,000\n2,000,000\n2,840,000\n1,940,000\nMobileNet_v1 on ImageNet\nFP32 Top-1 Accuracy\n71.07%\n65.30% (32 clusters)\n(Fake INT8) 70.39%\n(Fake INT8) 65.35%\nINT8 Top-1 Accuracy\n69.34%    \n60.60% (32 clusters)\n70.35%\n65.42%\nCompression\n4,665,568 \u2192 3,886,277 (16.7%)\n4,665,568 \u2192 3,035,752 (34.9%)\n4,569,416 \u2192 3,804,871 (16.7%)\n4,569,472 \u2192 2,912,655 (36.25%)\nSparsity and cluster preserving quantization aware training (PCQAT)\n\nModel\nMetric\nBaseline\nPruned Model (50% sparsity)\nQAT Model\nPruned Clustered Model\nPCQAT Model\nDS-CNN-L\nFP32 Top-1 Accuracy\n95.06%\n94.07%    \n(Fake INT8) 94.85%\n93.76% (8 clusters)    \n(Fake INT8) 94.28%\nINT8 Top-1 Accuracy\n94.35%    \n93.80%    \n94.82%    \n93.21% (8 clusters)\n94.06%\nCompression\n506,400 \u2192 425,006 (16.07%)\n506,400 \u2192 317,937 (37.22%)\n507,296 \u2192 424,368 (16.35%)\n506,400 \u2192 205,333\n(59.45%)\n507,296 \u2192 201,744 (60.23%)\nMobileNet_v1 on ImageNet\nFP32 Top-1 Accuracy\n70.98%\n70.49%\n(Fake INT8) 70.88%\n67.64% (16 clusters)\n(Fake INT8) 67.80%\nINT8 Top-1 Accuracy\n70.37%\n69.85%    \n70.87%    \n66.89% (16 clusters)    \n68.63%\n\nCompression\n4,665,552 \u2192  3,886,236 (16.70%)\n4,665,552 \u2192  2,909,148 (37.65%)\n4,569,416 \u2192 3,808,781 (16.65%)\n4,665,552 \u2192  2,013,010 (56.85%)    \n4,569472 \u2192  1,943,957 (57.46%)\nApplying PCQAT\nTo apply PCQAT, you will need to first use the pruning API to prune the model, then chain it with clustering using the sparsity-preserving clustering API. After that, the QAT API is used along with the custom collaborative optimization quantization scheme. An example is shown below.\nimport tensorflow_model_optimization as tfmot\nmodel = build_your_model() \n\n# prune model\nmodel_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model, ...)\n\nmodel_for_pruning.fit(...) \n\n# pruning wrappers must be stripped before clustering\nstripped_pruned_model = tfmot.sparsity.keras.strip_pruning(pruned_model)\nAfter pruning the model, cluster and fit as below.\n# Sparsity preserving clustering\nfrom tensorflow_model_optimization.python.core.clustering.keras.experimental import (cluster)\n\n# Specify your clustering parameters along\n# with the `preserve_sparsity` flag\nclustering_params = {\n  ...,\n  'preserve_sparsity': True\n}\n\n# Cluster and fine-tune as usual\ncluster_weights = cluster.cluster_weights\nsparsity_clustered_model = cluster_weights(stripped_pruned_model_copy, **clustering_params)\nsparsity_clustered_model.compile(...)\nsparsity_clustered_model.fit(...)\n\n# Strip clustering wrappers before the PCQAT step\nstripped_sparsity_clustered_model = tfmot.clustering.keras.strip_clustering(sparsity_clustered_model)\nThen apply PCQAT.\npcqat_annotate_model = quantize.quantize_annotate_model(stripped_sparsity_clustered_model )\npcqat_model = quantize.quantize_apply(quant_aware_annotate_model,scheme=default_8bit_cluster_preserve_quantize_scheme.Default8BitClusterPreserveQuantizeScheme(preserve_sparsity=True))\npcqat_model.compile(...)\npcqat_model.fit(...)\nThe example above shows the training process to achieve a fully optimized PCQAT model, for the other techniques, please refer to the CQAT, PQAT, and sparsity-preserving clustering example notebooks. Note that the API used for PCQAT is the same as that of CQAT, the only difference being the use of the preserve_sparsity flag to ensure that the zero cluster is preserved during training. The PQAT API usage is similar but uses a different, sparsity preserving, quantization scheme.\nAcknowledgments\nThe features and results presented in this post are the work of many people including the Arm ML Tooling team and our collaborators in Google\u2019s TensorFlow Model Optimization Toolkit team.\nFrom Arm - Anton Kachatkou, Aron Virginas-Tar, Ruomei Yan, Saoirse Stewart, Peng Sun, Elena Zhelezina, Gergely Nagy, Les Bell, Matteo Martincigh, Benjamin Klimczak, Thibaut Goetghebuer-Planchon, Tam\u00e1s Ny\u00edri, Johan Gras.\nFrom Google - David Rim, Frederic Rechtenstein, Alan Chiao, Pulkit Bhuwalka",
    "link": "https://blog.tensorflow.org/2021/10/Collaborative-Optimizations.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-9ZZ9eIx3vTY/YVcOQUrehjI/AAAAAAAAEjQ/97cOzXvSm8s_Rp8yUDKIsRqXTXxSfmdxACLcBGAsYHQ/s0/image1.png",
      "https://1.bp.blogspot.com/-YmHwDvk6YeY/YVcOuNNQDKI/AAAAAAAAEjY/BxdQHlt7cFEAYM_t1kTxjJ1Iurn8BrJAACLcBGAsYHQ/s0/image2.png"
    ],
    "time": "2023/12/04 00:58:43"
  },
  {
    "title": "End-to-end tinyML audio classification with the Raspberry Pi RP2040",
    "content": "A guest post by Sandeep Mistry, Arm\nSome tools you\u2019ll need for this project (learn more below!)\nIntroduction\nMachine learning enables developers and engineers to unlock new capabilities in their applications. Instead of explicitly defining instructions and rules for a computer to execute, you can collect large amounts of data for a classification task that your application requires, and train an ML model to learn from the patterns in the data.\nTraining typically happens in the cloud on computers equipped with one or more GPUs. Once a model has been trained, depending on its size, it can be deployed for inference on a wide range of devices. These devices range from large computers in the cloud with gigabytes of memory, to tiny microcontrollers (or MCUs) which typically have just kilobytes of memory.\nMicrocontrollers are low-power, self-contained, cost-effective computer systems that are embedded in devices that you use everyday, such as your microwave, electric toothbrush, or smart door lock. Microcontroller based systems typically interact with their surrounding environment via one or more sensors (think buttons, microphones, motion sensors) and perform an action using one or more actuators (think LEDs, motors, speakers).\nMicrocontrollers also offer privacy advantages, and can perform inference locally on the device, without needing to send any data to the cloud. This can have power advantages too for devices running off batteries.\nIn this article, we will demonstrate how an Arm Cortex-M based microcontroller can be used for local on-device ML to detect audio events from its surrounding environment. This is a tutorial-style article, and we\u2019ll guide you through training a TensorFlow based audio classification model to detect a fire alarm sound.\nWe\u2019ll show you how to use TensorFlow Lite for Microcontrollers with Arm CMSIS-NN accelerated kernels to deploy the ML model to an Arm Cortex-M0+ based microcontroller board for local on-device ML inference. Arm\u2019s CMSIS-DSP library, which provides optimized Digital Signal Processing (DSP) function implementations for Arm Cortex-M processors, will also be used to extract features from the real-time audio data before inference.\nWhile this guide focuses on detecting a fire alarm sound, it can be adapted for other sound classification tasks. You may also need to adapt the feature extraction stages and/or adjust ML model architecture for your use case.\nAn interactive version of this tutorial is available on Google Colab and all technical assets for this guide can be found on GitHub.\nWhat you need to to get started\nDevelopment Environment\nGoogle Colab\nHardware\nYou\u2019ll need one of the following development boards that are based on Raspberry Pi\u2019s RP2040 MCU chip that was released early in 2021.\nSparkFun RP2040 MicroMod and MicroMod ML Carrier\nThis board is great for folks new to electronics and microcontrollers. It does not require a soldering iron, knowing how to solder, or how to wire up breadboards.\nSparkFun MicroMod RP2040 Processor. This is the brains of the operation. It contains Raspberry Pi\u2019s RP2040 MCU and 16MB of flash storage.\nSparkFun MicroMod Machine Learning Carrier Board. This enables USB connectivity, and provides a built-in microphone, IMU and camera connector.\nA USB-C cable to connect the board to your computer.\nA Phillips screwdriver.\nRaspberry Pi Pico and PDM microphone board\nThis option is great if you know how to solder (or would like to learn). It requires a soldering iron and knowledge of how to wire a breadboard with electronic components. You\u2019ll need:\nRaspberry Pi Pico.\nAdafruit PDM MEMS Microphone Breakout.\nHalf size or full size breadboard.\nJumper wires.\nA USB-B micro cable to connect the board to your computer.\nSoldering iron.\nBoth of the options above will allow you to collect real-time 16 kHz audio from a digital microphone and process the audio signal on the development board\u2019s Arm Cortex-M0+ processor, which operates at 125 MHz. The application running on the Arm Cortex-M0+ will have a Digital Signal Processing (DSP) stage to extract features from the audio signal. The extracted features will then be fed into a neural network to perform a classification task to determine if a fire alarm sound is present in the board\u2019s environment.\nDataset\nWe will start by training a sound classifier (for many events) with TensorFlow using the ESC-50: Dataset for Environmental Sound Classification. After training on this broad dataset, we will use Transfer Learning to fine tune it for our specific audio classification task.\nThis model will be trained on the ESC-50 dataset, which contains 50 types of sounds. Each sound category has 40 audio files that are 5 seconds each in length. Each audio file will be split into 1 second soundbites, and any soundbites that contain pure silence will be discarded.\nA sample waveform from the data set of a dog barking.\nSpectrograms\nRather than passing in the time series data directly into our TensorFlow model, we will transform the audio data into an audio spectrogram representation. This will create a 2D representation of the audio signal\u2019s frequency content over time.\nThe input audio signal we will use will have a sampling rate of 16kHz, this means one second of audio will contain 16,000 samples. Using TensorFlow\u2019s tf.signal.stft(...) function we can transform a 1 second audio signal into a 2D tensor representation. We will choose a frame length of 256 and a frame step of 128, so the output of this feature extraction stage will be a Tensor that has a shape of (124, 129).\nAn audio spectrogram representation of a dog barking.\nThe ML model\nNow that we have the features extracted from the audio signal, we can create a model using TensorFlow\u2019s Keras API. You can find the complete code linked above. The model will consist of 8 layers:\nAn input layer.\nA preprocessing layer, that will resize the input tensor from 124x129x1 to 32x32x1.\nA normalization layer, that will scale the input values between -1 and 1\nA 2D convolution layer with: 8 filters, a kernel size of 8x8, and stride of 2x2, and ReLU activation function.\nA 2D max pooling layer with size of 2x2\nA flatten layer to flatten the 2D data to 1D\nA dropout layer, that will help reduce overfitting during training\nA dense layer with 50 outputs and a softmax activation function, which outputs the likelihood of the sound category (between 0 and 1).\nThe model summary can be found below:\nNotice that this model only has about 15K parameters (this is quite small!)\nFine tuning\nNow we will use transfer learning and change the classification head (the last Dense layer) of the model to train a binary classification model for fire alarm sounds. We have collected 10 fire alarm clips from freesound.org and BigSoundBank.com. Background noise clips from the SpeechCommands dataset will be used for non-fire alarm sounds. This dataset is small, and enough for us to get started. Data augmentation techniques will be used to supplement the training data we\u2019ve collected.\nFor real-world applications, it\u2019s important to collect a much larger dataset (you can learn more about best practices on TensorFlow\u2019s Responsible AI website).\nData Augmentation\nData augmentation is a set of techniques used to increase the size of a dataset. This is done by slightly modifying samples from the dataset or by creating synthetic data. In this situation we are using audio and we will create a few functions to augment different samples. We will use three techniques:\nAdding white noise to the audio samples.\nAdding random silence to the audio.\nMixing two audio samples together.\nAs well as increasing the size of the dataset, data augmentation also helps to reduce overfitting by training the model on different (not perfect) data samples. For example, on a microcontroller you are unlikely to have perfect high quality audio, and so a technique like adding white noise can help the model work in situations where your microphone might every so often have noise in there.\nA gif showing how data augmentation slightly changes the spectrogram by adding noise (watch it closely, it can be a bit hard to see).\nFeature Extraction\nTensorFlow Lite for Microcontroller (TFLu) provides a subset of TensorFlow operations, so we are unable to use the tf.signal.sft(...) API we\u2019ve used for feature extraction of the baseline model on our MCU. However, we can leverage Arm\u2019s CMSIS-DSP library to generate spectrograms on the MCU. CMSIS-DSP contains support for both floating-point and fixed-point DSP operations which are optimized for Arm Cortex-M processors, including the Arm Cortex-M0+ that we will be deploying the ML model to. The Arm Cortex-M0+ does not contain a floating-point unit (FPU) so it would be better to leverage a 16-bit fixed-point DSP based feature extraction pipeline on the board.\nWe can leverage CMSIS-DSP\u2019s Python Wrapper in the notebook to perform the same operations on our training pipeline using 16-bit fixed-point math. At a high level we can replicate the TensorFlow SFT API with the following CMSIS-DSP based operations:\nManually creating a Hanning Window of length 256 using the Hanning Window formula along with CMSIS-DSP\u2019s arm_cos_f32 API.\nCreating a CMSIS-DSP arm_rfft_instance_q15 instance and initializing it using CMSIS-DSP\u2019s arm_rfft_init_q15 API.\nLooping through the audio data 256 samples at a time, with a stride of 128 (this matches the parameters we\u2019ve passed into the TF sft API)\nMultiplying the 256 samples by the Hanning Window, using CMSIS-DSP\u2019s arm_mult_q15 API\nCalculating the FFT of the output of the previous step, using CMSIS-DSP\u2019s arm_rfft_q15 API\nCalculating the magnitude of the previous step, using CMSIS-DSP\u2019s arm_cmplx_mag_q15 API\nEach audio soundbites\u2019s FFT magnitude represents the one column of the spectrogram.\nSince our baseline model expects a floating point input, instead of the 16-bit quantized value we were using, the CMSIS-DSP arm_q15_to_float API can be used to convert the spectrogram data from a 16-bit fixed-point value to a floating-point value for training.\nThe complete Python code for this is a bit long, but can be found in the \u201cTransfer Learning -> Load dataset\u201d section of the Google Colab notebook.\nWaveform and audio spectrogram of a smoke alarm sound.\nFor an in-depth description of how to create audio spectrograms using fixed-point operations with CMSIS-DSP, please see Towards Data Science \u201cFixed-point DSP for Data Scientists\u201d guide.\nLoading the baseline model and changing the classification head\nThe model we previously trained on the ESC-50 dataset, predicted the presence of 50 sound types, and which resulted in the final dense layer of the model having 50 outputs. The new model we would like to create is a binary classifier, and needs to have a single output value.\nWe will load the baseline model, and swap out the final dense layer to match our needs:\n# We need a new head with one neuron.\nmodel_body = tf.keras.Model(inputs=model.input, outputs=model.layers[-2].output)\n\nclassifier_head = tf.keras.layers.Dense(1, activation=\"sigmoid\")(model_body.output)\n\nfine_tune_model = tf.keras.Model(model_body.input, classifier_head)\nThis results in the following model.summary():\nTransfer Learning\nTransfer Learning is the process of retraining a model that has been developed for a task to complete a new similar task. The idea is that the model has learned transferable \"skills\" and the weights and biases can be used in other models as a starting point.\nAs humans we use transfer learning too. The skills you developed to learn to walk could also be used to learn to run later on.\nIn a neural network, the first few layers of a model start to perform a \"feature extraction\" such as finding shapes, edges and colours. The layers later on are used as classifiers; they take the extracted features and classify them.\nBecause of this, we can assume the first few layers have learned quite general feature extraction techniques that can be applied to similar tasks, and so we can freeze all these layers and use them on a new task in the future. The classifier layer will need to be trained based on the new task.\nTo do this, we break the process into two steps:\nFreeze the \"backbone\" of the model and train the head with a fairly high learning rate. We slowly reduce the learning rate.\nUnfreeze the \"backbone\" and fine-tune the model with a low learning rate.\nTo freeze a layer in TensorFlow we can set layer.trainable=False. Let's loop through all the layers and do this:\nfor layer in fine_tune_model.layers:\n  layer.trainable = False\nand now unfreeze the last layer (the head):\nfine_tune_model.layers[-1].trainable = True\nWe can now train the model using a binary crossentropy loss function. Keras callbacks for early stopping (to avoid overfitting) and a dynamic learning rate scheduler will also be used.\nAfter we\u2019ve trained with the frozen layers, we can unfreeze them:\nfor layer in fine_tune_model.layers:\n  layer.trainable = True\nAnd train again for up to 10 epochs. You can find the complete code for this in the \u201cTransfer Learning -> \u201dTrain Model\u201d section of Colab notebook.\nRecording your own training data\nWe now have an ML model which can classify the presence of fire alarm sound. However this model was trained on publicly available sound recordings which might not match the sound characteristics of the hardware microphone we will use for inferencing.\nThe Raspberry Pi RP2040 MCU has a native USB feature that allows it to act like a custom USB device. We can flash an application to the board to enable it to act like a USB microphone to our PC. Then we can extend Google Colab\u2019s capabilities with the Web Audio API on a modern Web browser like Google Chrome to collect live data samples (all from within Google Colab!)\nHardware Setup\nSparkFun MicroMod RP2040\nFor assembly, remove the screw on the carrier board, at an angle, slide in the MicroMod RP2040 Processor board into the socket and secure it in place with the screw. See the MicroMod Machine Learning Carrier Board Hookup Guide for more details.\nRaspberry Pi Pico\nFollow the instructions from the Hardware Setup section of the \u201cCreate a USB Microphone with the Raspberry Pi Pico\u201d guide for assembly instructions.\nTop: Fritzing wiring diagram Bottom: Assembled breadboard\nSetting up the firmware applications toolchains\nRather than setting up the Raspberry Pi Pico\u2019s SDK on your personal computer. We can leverage Colab\u2019s built-in Linux shell command feature to set up the Pico SDK development environment with CMake and GNU Arm Embedded Toolchain.\nThe pico-sdk will also have to be downloaded to the Colab instance using git:\n%%shell\ngit clone https://github.com/raspberrypi/pico-sdk.git\ncd pico-sdk\ngit submodule init\ngit submodule update\nCompiling and flashing the USB microphone application\nNow we can use the USB microphone example from the Microphone Library for Pico. The example application can be compiled using cmake and make. Then we can flash the example application to the board over USB by putting the board into \u201cboot ROM mode\u201d which will allow us to upload an application to the board.\nSparkFun\nPlug the USB-C cable into the board and your PC to power the board.\nWhile holding down the BOOT button on the board, tap the RESET button.\nRaspberry Pi Pico\nPlug the USB Micro cable into your PC, but do NOT plug in the Pico side.\nWhile holding down the white BOOTSEL button, plug in the micro USB cable to the Pico.\nIf you are using a WebUSB API enabled browser like Google Chrome, you can directly flash the image onto the board from within Google Collab!\nDownloading USB microphone application to the board from within Google Colab and WebUSB.\nOtherwise, you can manually download the .uf2 file to your computer and then drag it onto the USB disk for the RP2040 board.\nCollecting training data\nNow that you have flashed the USB microphone application to the board, it will appear as a USB audio input on your PC.\nWe can now use Google Colab to record a fire alarm sound, select \u201cMicNode '' as the audio input source in the drop down. Then while pressing the test button on a smoke alarm, click the record button on Google Colab to record a 1 second audio clip. Repeat this process a few times.\nSimilarly, we can also do the same to collect background audio samples in the next code cell in Google Colab. Repeat this a few times for non-fire alarm sounds like silence, yourself talking, or any other normal sounds for the environment.\nFinal model training\nNow that we\u2019ve collected additional samples with the microphone that will be used during inference. We can tune the model again with the new data.\nConverting the Model to run on the MCU\nWe will need to convert the Keras model we\u2019ve used to TensorFlow Lite format so that we can use it for inference on the device.\nQuantization\nTo optimize the model to run on the Arm Cortex-M0+ processor, we will use a process called model quantization. Model quantization converts the model\u2019s weights and bias from 32-bit floating point values to 8-bit values. The pico-tflmicro library, which is a port of TFLu for the RP2040\u2019s Pico SDK contains Arm\u2019s CMSIS-NN library, which supports optimized kernel operations for quantized 8-bit weights on Arm Cortex-M processors.\nWe can use TensorFlow\u2019s Quantization Aware Training (QAT) feature to easily convert the floating-point model to quantized.\nConverting the model to TF Lite format\nWe will now use the tf.lite.TFLiteConverter.from_keras_model(...) API to convert the quantized Keras model to TF Lite format, and then save it to disk as a .tflite file.\nconverter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\ntrain_ds = train_ds.unbatch()\n\ndef representative_data_gen():\n  for input_value, output_value in train_ds.batch(1).take(100):\n    # Model has only one input so each data point has one element.\n    yield [input_value]\n    \nconverter.representative_dataset = representative_data_gen\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n# Set the input and output tensors to uint8 (APIs added in r2.3)\nconverter.inference_input_type = tf.int8\nconverter.inference_output_type = tf.int8\ntflite_model_quant = converter.convert()\n\nwith open(\"tflite_model.tflite\", \"wb\") as f:\n  f.write(tflite_model_quant)\nSince TensorFlow also supports loading TF Lite models using tf.lite, we can also verify the functionality of the quantized model and compare its accuracy with the regular unquantized model inside Google Colab.\nThe RP2040 MCU on the boards we are deploying to, does not have a built-in file system, which means we cannot use the .tflite file directly on the board. However, we can use the Linux `xxd` command to convert the .tflite file to a .h file which can then be compiled in the inference application in the next step.\n%%shell\necho \"alignas(8) const unsigned char tflite_model[] = {\" > tflite_model.h\ncat tflite_model.tflite | xxd -i                        >> tflite_model.h\necho \"};\" \nDeploy the model to the device\nWe now have a model that is ready to be deployed to the device. We\u2019ve created an application template for inference which can be compiled with the .h file that we\u2019ve generated for the model.\nThe C++ application uses the pico-sdk as the base, along with the CMSIS-DSP, pico-tflmicro, and Microphone Library for Pico libraries. It\u2019s general structure is as follows:\nInitialization\nConfigure the board's built-in LED for output. The application will map the brightness of the LED to the output of the model. (0.0 LED off, 1.0 LED on with full brightness)\nSetup the TF Lite library and TF Lite model for inference\nSetup the CMSIS-DSP based DSP pipeline\nSetup and start the microphone for real-time audio\nInference loop\nWait for 128 * 4 = 512 new audio samples from the microphone\nShift the spectrogram array over by 4 columns\nShift the audio input buffer over by 128 * 4 = 512 samples and copy in the new samples\nCalculate 4 new spectrogram columns for the updated input buffer\nPerform inference on the spectrogram data\nMap the inference output value to the on-board LED\u2019s brightness and output the status to the USB port\nIn-order to run in real-time each cycle of the inference loop must take under (512 / 16000) = 0.032 seconds or 32 milliseconds. The model we\u2019ve trained and converted takes 24 ms for inference, which gives us ~8 ms for the other operations in the loop.\n128 was used above to match the stride of 128 used in the training pipeline for the spectrogram. We used a shift of 4 in the spectrogram to fit within the real-time constraints we had.\nCompiling the Firmware\nNow we can use CMake to generate the build files required for compilation followed by make to compile.\nThe \u201ccmake ..\u201d line will have to be changed based on the board you are using:\nSparkFun: cmake .. -DPICO_BOARD=sparkfun_micromod\nRaspberry Pi Pico: cmake .. -DPICO_BOARD=pico\nFlashing the Inference Application to the board\nYou\u2019ll need to put the board into \u201cboot ROM mode\u201d again to load the new application to it.\nSparkFun\nPlug the USB-C cable into the board and your PC to power the board.\nWhile holding down the BOOT button on the board, tap the RESET button.\nRaspberry Pi Pico\nPlug the USB Micro cable into your PC, but do NOT plug in the Pico side.\nWhile holding down the white BOOTSEL button, plug in the micro USB cable to the Pico.\nIf you are using a WebUSB API enabled browser like Google Chrome, you can directly flash the image onto the board from within Google Colab. Otherwise, you can manually download the .uf2 file to your computer and then drag it onto the USB disk for the RP2040 board.\nMonitoring the Inference on the board\nNow that the inference application is running on the board you can observe it in action in two ways:\nVisually by observing the brightness of the LED on the board. It should remain off or dim when no fire alarm sound is present - and be on when a fire alarm sound is present:\nConnecting to the board\u2019s USB serial port to view output from the inference application. If you are using a Web Serial API enabled browser like Google Chrome, this can be done directly from Google Colab:\nImproving the model\nYou now have the first version of the model deployed to the board, and it is performing inference on live 16,000 kHz audio data!\nTest out various sounds to see if the model has the expected output. Maybe the fire alarm sound is being falsely detected (false positive) or not detected when it should be (false negative).\nIf this occurs, you can record more new audio data for the scenario(s) by flashing the USB microphone application firmware to the board, recording the data for training, re-training the model and converting to TF lite format, and re-compiling + flashing the inference application to the board.\nSupervised machine learning models can generally only be as good as the training data they are trained with, so additional training data for these scenarios might help. You can also try to experiment with changing the model architecture or feature extraction process - but keep in mind that your model must be small enough and fast enough to run on the RP2040 MCU.\nConclusion\nThis article covered an end-to-end flow of how to train a custom audio classifier model to run locally on a development board that uses an Arm Cortex-M0+ processor. TensorFlow was used to train the model using transfer learning techniques along with a smaller dataset and data augmentation techniques. We also collected our own data from the microphone that is used at inference time by loading an USB microphone application to the board, and extending Colab\u2019s features with the Web Audio API and JavaScript.\nThe training side of the project combined Google\u2019s Colab service and Chrome browser, with the open source TensorFlow library. The inference application captured audio data from a digital microphone, used Arm\u2019s CMSIS-DSP library for the feature extraction stage, then used TensorFlow Lite for Microcontrollers with Arm CMSIS-NN accelerated kernels to perform inference with a 8-bit quantized model that classified a real-time 16 kHz audio input on an Arm Cortex-M0+ processor.\nThe Web Audio API, Web USB API, and Web Serial API features of Google Chrome were used to extend Google Colab\u2019s functionality to interact with the development board. This allowed us to experiment with and develop our application entirely with a web browser and deploy it to a constrained development board for on-device inference.\nSince the ML processing was performed on the development boards RP2040 MCU, no audio data left the device at inference time.\nLearn more\nYou can learn more and get hands-on experience using TinyML at the upcoming Arm DevSummit, a 3-day virtual event between October 19 - 21. The event includes workshops on tinyML computer vision for real-world embedded devices and building large vocabulary voice control with Arm Cortex-M based MCUs. We hope to see you there!",
    "link": "https://blog.tensorflow.org/2021/09/TinyML-Audio-for-everyone.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-r04Uz1ouKDo/YVzmaJWpAxI/AAAAAAAAEnI/xq1XXOcQPAgHym44n2ZJJwAHP1wxR3oaQCLcBGAsYHQ/s0/ezgif.com-gif-maker%2B%25282%2529.gif",
      "https://1.bp.blogspot.com/-A_RPT-81Ae0/YVzDdLByRcI/AAAAAAAAEkI/59uQW5SrniUch48nWw2OZAaMalzQPjWmQCLcBGAsYHQ/s0/image1%2Band%2B2.jpg",
      "https://1.bp.blogspot.com/-A_RPT-81Ae0/YVzDdLByRcI/AAAAAAAAEkI/59uQW5SrniUch48nWw2OZAaMalzQPjWmQCLcBGAsYHQ/s0/image1%2Band%2B2.jpg",
      "https://1.bp.blogspot.com/-ZupWT8MV3GQ/YVzF_lamtKI/AAAAAAAAEkY/VECPsVkVGoU8s4nD4ydq5N-d5pLH8FTwQCLcBGAsYHQ/s0/image%2B3.jpg",
      "https://1.bp.blogspot.com/-bxMovman5O0/YVzGoGx9iDI/AAAAAAAAEkg/E4tJKRNjMDEyVkknJmN8HWYTWUFynko5QCLcBGAsYHQ/s0/image%2B4.png",
      "https://1.bp.blogspot.com/-18Nxxx_MDoI/YVzHCABPXwI/AAAAAAAAEko/7AEC1ZflbSA3qLP4w7ueZ79kGKigyKlzACLcBGAsYHQ/s0/image%2B5.png",
      "https://1.bp.blogspot.com/-5E-fQn-0kaI/YVzHjK7mkdI/AAAAAAAAEkw/ja0CfXPnsUg_WQEHD0Cbq4tCEumnv9NfwCLcBGAsYHQ/s0/image%2B6.png",
      "https://1.bp.blogspot.com/-WRLpssx8a5U/YVzIGovCdhI/AAAAAAAAEk4/fKZBihD9b60JzJJjOt2AsXFJQCCUXwVHACLcBGAsYHQ/s0/image%2B7.gif",
      "https://1.bp.blogspot.com/-YLnHC3F2U9E/YVzIk6rP96I/AAAAAAAAElA/UJ6Wb4H6hbU2SWAWz3rhOOXmgBQzA5f5ACLcBGAsYHQ/s0/image%2B8.png",
      "https://1.bp.blogspot.com/-d3KME3Fq35E/YVzP_A92C0I/AAAAAAAAElQ/IVon2a9Jmx0sJRF3NYaPKTn1M50BuOKawCLcBGAsYHQ/s0/image%2B9.png",
      "https://1.bp.blogspot.com/-cD9SUWhPH3o/YVzgcPBI1YI/AAAAAAAAEnA/uK0jzlOIptc5VpHiJm_BJhFP6sqFxVeHwCLcBGAsYHQ/s0/image%2B10v2.png",
      "https://1.bp.blogspot.com/-r4Oime32_sA/YVzRIzuVrzI/AAAAAAAAElg/ozX_og12FcEUytqB___hcgk4qCd3F0TogCLcBGAsYHQ/s0/image%2B11.jpg",
      "https://1.bp.blogspot.com/-MER2VNeUSsU/YVzWIUuEt9I/AAAAAAAAEmI/AiwPT4xVEmIYvXdGqXLdT_OsHqZMhNmqQCLcBGAsYHQ/s0/image%2B12.png",
      "https://1.bp.blogspot.com/-rpvV8VJ1G7Y/YVzWVXStUxI/AAAAAAAAEmM/jpMUA4HB5U8vbyViLJURgrJ0fXkpQuYmACLcBGAsYHQ/s0/image%2B13.jpg",
      "https://1.bp.blogspot.com/-wmHgErQV828/YVzTXKyNgsI/AAAAAAAAEmA/acHY5-JX9eE5IFgHw1ufd8nzslZUT_ElQCLcBGAsYHQ/s0/image%2B14.gif",
      "https://1.bp.blogspot.com/-WWW13cZvQwM/YVzZUMbhpXI/AAAAAAAAEmY/sjayw-fhUHg3WizCGrMuMUbakfVxzd1OQCLcBGAsYHQ/s0/image%2B15.gif",
      "https://1.bp.blogspot.com/-99bDB-RYLRw/YVzZ-6MNLXI/AAAAAAAAEmg/RpZoudRbSmsxXu5tdDHGDXJpZ0G5Tj7ZACLcBGAsYHQ/s0/image%2B16.gif",
      "https://1.bp.blogspot.com/-MGUv75OnE_0/YVzaX_lNMOI/AAAAAAAAEmo/od_DaTiuzswSI3P_sGIQxpwXg08jOMBJQCLcBGAsYHQ/s0/image%2B17.gif",
      "https://1.bp.blogspot.com/-Lo8-IjZ3JtQ/YVza5GCM86I/AAAAAAAAEmw/xNu-ia6wIS4Fif0tHwP_VR-BVWDJSLXDgCLcBGAsYHQ/s0/image%2B18.gif",
      "https://1.bp.blogspot.com/-MdXwNsmLpx0/YVzbdv3OD3I/AAAAAAAAEm4/HnQuk1U1kvYCy6tH5JhhltqGlR-8CfMewCLcBGAsYHQ/s0/image%2B19.gif"
    ],
    "time": "2023/12/04 00:58:43"
  },
  {
    "title": "Optical character recognition with TensorFlow Lite: A new example app",
    "content": "Posted by Wei Wei, TensorFlow Developer Advocate\nAs the old adage goes, \u201ca picture is worth a thousand words.\u201d Images are rich in visual information, but sometimes the key is with the text within. While it is easy for literate human beings to read words embedded in images, how do we use computer vision and machine learning to teach computers to do so?\nToday, we are going to show you how to use TensorFlow Lite to extract text from images on Android devices. We will walk you through the key steps of the Optical Character Recognition (OCR) Android app that we recently open sourced here, which you can refer to for the complete code. You can see how the app extracts the product names from three Google product logos in the animation below.\nThe process of recognizing text from images is called Optical Character Recognition and is widely used in many domains. For example, Google Maps uses OCR technology to automatically extract information from the geo-located imagery to improve Google Maps.\nGenerally speaking, OCR is a pipeline with multiple steps. Usually they consist of text detection and text recognition:\nUse a text detection model to find out bounding boxes around text;\nDo some post-processing to transform the bounding boxes;\nTransform the images within those bounding boxes into grayscale, so that a text recognition model can map out the words and numbers.\nIn our case, we are going to leverage the text detection and text recognition models from TensorFlow Hub. There are several different model versions for speed / accuracy tradeoffs; we use the float16 quantized models here. For more information on model quantization, please refer to the TensorFlow Lite quantization section. We also use OpenCV, which is a widely used computer vision library for Non-Maximum Suppression (NMS) and perspective transformation (we\u2019ll expand on this later) to post-process detection results. In addition, we use the TFLite Support Library to grayscale and normalize the images.\nOCR pipeline from text detection, perspective transformation, to recognition.\nFor text detection, since the detection model accepts a fixed size of 320x320, we use the TFLite Support Library to resize and normalize the input image:\nval imageProcessor =\n ImageProcessor.Builder()\n   .add(ResizeOp(height, width, ResizeOp.ResizeMethod.BILINEAR))\n   .add(NormalizeOp(means, stds))\n   .build()\nvar tensorImage = TensorImage(DataType.FLOAT32)\n\ntensorImage.load(bitmapIn)\ntensorImage = imageProcessor.process(tensorImage)\nThen we use TFLite to run the detection model:\ndetectionInterpreter.runForMultipleInputsOutputs(detectionInputs, detectionOutputs)\nThe output of the detection model is a number of rotated bounding boxes which contain the text in the image. We run Non-Maximum Suppression to identify one bounding box for each text block with OpenCV:\nNMSBoxesRotated(\n  boundingBoxesMat,\n  detectedConfidencesMat,\n  detectionConfidenceThreshold.toFloat(),\n  detectionNMSThreshold.toFloat(),\n  indicesMat\n)\nSometimes texts inside images are distorted (e.g., the \u2018kubernetes\u2019 sticker on my laptop) with a perspective angle:\nPerspective transformation demo\nIf we just feed the raw rotated bounding box into the recognition model, the model is unlikely to correctly identify the characters. In this case, we need to use OpenCV to do perspective transformation:\nval rotationMatrix = getPerspectiveTransform(srcPtsMat, targetPtsMat)\n\nwarpPerspective(\n  srcBitmapMat,\n  recognitionBitmapMat,\n  rotationMatrix,\n  Size(recognitionImageWidth.toDouble(), recognitionImageHeight.toDouble())\n)\nAfter that, we use the TFLite Support Library again to resize, grayscale, and normalize the transformed images inside the bounding boxes:\nval imageProcessor =\n  ImageProcessor.Builder()\n    .add(ResizeOp(height, width, ResizeOp.ResizeMethod.BILINEAR))\n    .add(TransformToGrayscaleOp())\n    .add(NormalizeOp(mean, std))\n    .build()\nFinally, we run the text recognition model, map out the characters and numbers from the model output, and update the app UI:\nrecognitionInterpreter.run(recognitionTensorImage.buffer, recognitionResult)\n\nvar recognizedText = \"\"\nfor (k in 0 until recognitionModelOutputSize) {\n  var alphabetIndex = recognitionResult.getInt(k * 8)\n  if (alphabetIndex in 0..alphabets.length - 1)\n    recognizedText = recognizedText + alphabets[alphabetIndex]\n}\nLog.d(\"Recognition result:\", recognizedText)\nif (recognizedText != \"\") {\n  ocrResults.put(recognizedText, getRandomColor())\n}\nThat's it. We are now able to extract text from input images using TFLite within our app.\nFinally, if you just want a ready-to-use OCR SDK, Google also offers on-device OCR functionality through ML Kit, which uses TFLite underneath and should be sufficient for most OCR use cases. There are some situations where you may want to build your own OCR solution with TFLite such as:\nYou have your own text detection / recognition TFLite models that you would like to use;\nYou have special business requirements (e.g. recognizing upside-down text) and need to customize the OCR pipeline;\nYou want to support languages not covered by ML Kit;\nYour target user devices that don\u2019t necessarily have Google Play services installed;\nYou want to have control over hardware backends (CPU / GPU / etc.) used to run your models.\n\nIn these cases, I hope that this tutorial and our example implementation can help you get started on building your own OCR functionality in your app.\nYou can learn more about OCR with the resources below.\nOpenCV text detection / recognition example\nOCR TFLite community project by @Tulasi123789 and @risingsayak\nOpenCV Text Detection (EAST text detector)\nKeras OCR\nDeep Learning based Text Detection Using OpenCV\nOCR model for reading Captchas with Keras\nAcknowledgements\nThe author would like to thank Tian Lin for the helpful feedback and community contributors @Tulasi123789 and @risingsayak for their prior work on OCR using TFLite (creating and uploading the models to TF Hub, providing accompanying notebooks, and etc.).",
    "link": "https://blog.tensorflow.org/2021/09/blog.tensorflow.org202109optical-character-recognition.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-evZXm9aNW_A/YUt5gmbBJ8I/AAAAAAAAEgE/LS3Li9WxxmobhGAE9lqQQvY8-b65ZjYHACLcBGAsYHQ/s0/demo.gif",
      "https://1.bp.blogspot.com/-CgPPD5GpEFg/YUt6dmwpPLI/AAAAAAAAEgQ/cxnNPDNY6Ng10SE_b6ak0Fna-4bjax3YgCLcBGAsYHQ/s0/image2.png",
      "https://1.bp.blogspot.com/-xPEt9nCop2c/YUt8XfUk_JI/AAAAAAAAEgY/0TcFiweGbb8Uv7A5q6T_F27Vg76LVQwLQCLcBGAsYHQ/s0/image1.png"
    ],
    "time": "2023/12/04 00:58:43"
  },
  {
    "title": "Faster Quantized Inference with XNNPACK",
    "content": "Posted by Marat Dukhan and Frank Barchard, software engineers\nQuantization is among the most popular methods to speedup neural network inference on CPUs. A year ago TensorFlow Lite increased performance for floating-point models with the integration of XNNPACK backend. Today, we are extending the XNNPACK backend to quantized models with, on average across computer vision models, 30% speedup on ARM64 mobile phones, 5X speedup on x86-64 laptop and desktop systems, and 20X speedup for in-browser inference with WebAssembly SIMD compared to the default TensorFlow Lite quantized kernels.\nQuantized inference in XNNPACK is optimized for symmetric quantization schemas used by the TensorFlow Model Optimization Toolkit. XNNPACK supports both the traditional per-tensor quantization schema and the newer accuracy-optimized schema with per-channel quantization of weights and per-tensor quantization of activations. Additionally, XNNPACK supports the asymmetric quantization schema, albeit with reduced efficiency.\nPerformance improvements\nWe evaluated XNNPACK-acclerated quantized inference on a number of edge devices and neural network architectures. Below, we present benchmarks on four public and two internal quantized models covering common computer vision tasks:\nEfficientNet-Lite0 image classification [download]\nEfficientDet-Lite0 object detection [download]\nDeepLab v3 segmentation with MobileNet v2 feature extractor [download]\nCartoonGAN image style transfer [download]\nQuantized version of the Face Mesh landmarks\nQuantized version of the Video Segmentation\nSpeedup from XNNPACK on single-threaded inference of quantized computer vision models on Android/ARM64 mobile phones.\nAcross the six Android ARM64 mobile devices XNNPACK delivers, on average, 30% speedup over the default TensorFlow Lite quantized kernels.\nSpeedup from XNNPACK on single-threaded inference of quantized computer vision models on x86-64 laptop and desktop systems.\nXNNPACK offers even greater improvements on laptop and desktop systems with x86 processors. On the 5 x86 processors in our benchmarks XNNPACK accelerated inference on average by 5 times. Notably, low-end and older processors which don\u2019t support AVX instructions see over 20X speedup from switching quantized inference to XNNPACK: while the previous TensorFlow Lite inference backend had optimized implementations only for AVX, AVX2, and AVX512 instruction sets, XNNPACK provides optimized implementations for all x86-64 processors.\nSpeedup from XNNPACK on single-threaded WebAssembly SIMD inference of quantized computer vision models on mobile phones, laptops, and desktops when running through V8.\nBesides the traditional mobile and laptop/desktop platforms, XNNPACK brings accelerated quantized inference to the Web platform through the TensorFlow Lite Web API. The above plot demonstrates a geomean speedup of 20X over the default TensorFlow Lite implementation when running WebAssembly SIMD benchmarks through the V8 JavaScript engine on 3 x86-64 and 2 ARM64 systems.\nTwo years of optimizations\nXNNPACK started its life as a fork of QNNPACK library, but as the first version of XNNPACK focused on floating-point inference and QNNPACK focused on quantized inference, it was not possible to compare the two. Now with XNNPACK introducing support for quantized inference, we can directly evaluate and attribute the two further years of performance optimizations.\nTo compare the two quantized inference backends, we ported randomized MobileNet v1 and MobileNet v2 models from XNNPACK API to QNNPACK API, and benchmarked their single-threaded performance on two ARM64 Android phones and two x86-64 systems. The results are presented in the plot above, and the progress made by XNNPACK in two years is striking. XNNPACK is 50% faster on the older Pixel 3a phone and 4-5X faster on the newer Pixel 4a phone, 2.5X faster on the x86-64 laptop, and over 3X faster on the x86-64 workstation. These improvements are the result of a multiple optimizations XNNPACK gained in the two years since it forked from QNNPACK:\nXNNPACK retained the optimizations in QNNPACK, like the Indirect Convolution algorithm and microarchitecture-specific microkernel selection, and further augmented them with Indirect Deconvolution algorithm, and more flexible capabilities, like built-in numpy-like broadcasting in the quantized addition and quantized multiplication operators.\nConvolution, Deconvolution, and Fully Connected operators accumulate products of 8-bit activations and weights into a 32-bit number, and in the end this number needs to be converted back, or requantized, to an 8-bit number. There are multiple ways how requantization can be implemented, but QNNPACK adapted schema from the GEMMLOWP library, which pioneered quantized computations for neural network inference. However, it has since been discovered that GEMMLOWP requantization schema is suboptimal in terms of both accuracy and performance, and XNNPACK replaced it with more performant and accurate alternatives\nWhereas QNNPACK targeted asymmetric quantization schema, where both activations and weights are represented as unsigned integers with zero point and scale quantization parameters, XNNPACK\u2019s optimizations focus on symmetric quantization, where both activations and weights are signed integers, and weights have additional restrictions: the zero point of the weights is always zero and the quantized weights elements are limited to the [-127, 127] range (-128 is excluded even though it can be represented as a signed 8-bit integer). Symmetric quantization offers two computational advantages exploited in XNNPACK. First, when the filter weights are static, the results of accumulating the product of input zero point by the filter weights can be completely fused into the bias term in the Convolution, Deconvolution, and Fully Connected operators. Thus, zero point parameters are completely absent from the inference computations. Secondly, the product of a signed 8-bit input element by the weight element restricted to [-127, 127] fits into 15 bits. This enables the microkernels for Convolution, Deconvolution, and Fully Connected operators to do half of the accumulations on 16-bit variables rather than always extending the products to 32 bits.\nQNNPACK microkernels were optimized NEON SIMD instructions on ARM and SSE2 SIMD instructions on x86, but XNNPACK supports a much wider set of instruction set-specific optimizations. Most quantized microkernels in XNNPACK are optimized for SSE2, SSE4.1, AVX, XOP, AVX2, and AVX512 instructions on x86/x86-64, for NEON, NEON V8, and NEON dot product instructions on ARM/ARM64, and for WebAssembly SIMD instructions. Additionally, XNNPACK provides scalar implementations for WebAssembly 1.0 and pre-NEON ARM processors.\nQNNPACK introduced the idea of specialized assembly microkernels for high-end ARM and low-end ARM cores, but XNNPACK takes this idea much further. XNNPACK not only includes specialized expert-tuned software pipelined assembly microkernels for Cortex-A53, Cortex-A55, and high-end cores with and without NEON dot product instructions, but even supports switching between them on the fly. When a thread doing inference migrates from a big to a little core, XNNPACK automatically adapts from using a microkernel optimized for the big core to the one optimized for the little core.\nQNNPACK mainly focused on multi-threaded inference and organized computations as a large number of small tasks, each computing a tiny tile of the output tensor. XNNPACK reworked parallelisation and made the tasks flexible: they can be fine-grained or coarse-grained depending on the number of threads participating in the parallelization. Through dynamic adjustment of task granularity, XNNPACK archives low overhead in single-threaded execution and high parallelization efficiency for multi-threaded inference.\nTaken together, these optimizations make XNNPACK the new state-of-art for quantized inference, and turn TensorFlow Lite into the most versatile quantized inference solution, covering systems from Raspberry Pi Zero to Chromebooks to workstations with server-class processors.\nHow can you use it?\nQuantized XNNPACK inference is enabled by default in the CMake builds of TensorFlow Lite for all platforms, in the Bazel builds of TensorFlow Lite for the Web platform, and will be available in TensorFlow Lite Web API in the 2.7 release. In Bazel builds for other platforms, quantized XNNPACK inference is enabled via a build-time opt-in mechanism. When building TensorFlow Lite with Bazel, add --define tflite_with_xnnpack=true --define xnn_enable_qs8=true, and the TensorFlow Lite interpreter will use the XNNPACK backend by default for supported operators with symmetric quantization. Limited support for operators with asymmetric quantization is available via the --define xnn_enable_qu8=true Bazel option.\nWhich operations are accelerated?\nThe XNNPACK backend currently supports a subset of quantized TensorFlow Lite operators (see documentation for details and limitations). XNNPACK supports models produced by the Model Optimization Toolkit through post-training integer quantization and quantization-aware training, but not post-training dynamic range quantization.\nFuture work\nThis is the third version of the XNNPACK integration into TensorFlow Lite following the initial release of the floating-point implementation and the subsequent release that brought sparse inference support. In the following versions we plan to add the following improvements:\nHalf-precision inference on the recent ARM processors\nSparse quantized inference.\nEven faster dense inference.\nWe encourage you to leave your thoughts and comments on our GitHub and StackOverflow pages, and you can ask questions on discuss.tensorflow.org",
    "link": "https://blog.tensorflow.org/2021/09/faster-quantized-inference-with-xnnpack.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-4zpxrFI20RA/YTo8SP1eMNI/AAAAAAAAEe4/u6ko0nLcJ5U3kriplvWNKkSrSD1AlpHigCLcBGAsYHQ/s0/image_1_xnn.jpeg",
      "https://1.bp.blogspot.com/-VXmu1C9nuDQ/YTfk-enyl2I/AAAAAAAAEeY/uQgStN5oTYgjfwbcLM8Jr45WajzNCVwPwCLcBGAsYHQ/s0/image%2B1%2Bxnn.png",
      "https://1.bp.blogspot.com/-h2c4iatmdD4/YTflkgYKrpI/AAAAAAAAEeg/CzBBB4wXlEksHY_Les9QvIwJ6iAbgoJKQCLcBGAsYHQ/s0/image%2B2%2Bxnn.png",
      "https://1.bp.blogspot.com/-Q-WqKLJbBUU/YTfl7qeXoZI/AAAAAAAAEeo/UjUxZwIu6IoC4BqORv9u-fzdLOuh05qRwCLcBGAsYHQ/s0/image%2B3%2Bxnn.png",
      "https://1.bp.blogspot.com/-xRb2EX_Ms3o/YTfmTyb-sJI/AAAAAAAAEew/bkWu53uV0T8IJR8gYxc8e9LsQcup9VXCgCLcBGAsYHQ/s0/image%2B4%2Bxnn.png"
    ],
    "time": "2023/12/04 00:58:43"
  },
  {
    "title": "Pose estimation and classification on edge devices with MoveNet and TensorFlow Lite",
    "content": "Posted by Khanh LeViet, TensorFlow Developer Advocate and Yu-hui Chen, Software Engineer\nThe MoveNet iOS sample has been released. Check it out on GitHub.\nSince MoveNet\u2019s announcement at Google I/O earlier this year, we have received a lot of positive feedback and feature requests. Today, we are excited to share several updates with you:\nThe TensorFlow Lite version of MoveNet is now available on TensorFlow Hub. This includes a few updates to improve accuracy and make it compatible with hardware accelerators including GPUs and other accelerators available via the Android NN API.\nWe\u2019ve released a new Android, Raspberry Pi pose estimation sample that lets you try out MoveNet on mobile and IoT devices. (iOS is coming soon)\nWe\u2019ve also released a Colab notebook that teaches you how to do custom pose classification (e.g. recognize different yoga poses) with MoveNet. You can try pose classification on the Android, iOS and Raspberry Pi apps mentioned earlier.\nWhat is pose estimation?\nPose estimation is a machine learning task that estimates the pose of a person from an image or a video by estimating the spatial locations of specific body parts (keypoints). MoveNet is the state-of-the-art pose estimation model that can detect these 17 key-points:\nNose\nLeft and right eye\nLeft and right ear\nLeft and right shoulder\nLeft and right elbow\nLeft and right wrist\nLeft and right hip\nLeft and right knee\nLeft and right ankle\nWe have released two versions of MoveNet:\nMoveNet.Lightning is smaller, faster but less accurate than the Thunder version. It can run in realtime on modern smartphones.\nMoveNet.Thunder is the more accurate version but also larger and slower than Lightning.\nThe MoveNet models outperform Posenet (paper, blog post, model), our previous TensorFlow Lite pose estimation model, on a variety of benchmark datasets (see the evaluation/benchmark result in the table below).\nThese MoveNet models are available in both the TensorFlow Lite FP16 and INT8 quantized formats, allowing maximum compatibility with hardware accelerators.\nThis version of MoveNet can recognize a single pose from the input image. If there is more than one person in the image, the model along with the cropping algorithm will try its best to focus on the person who is closest to the image center. We have also implemented a smart cropping algorithm to improve the detection accuracy on videos. In short, the model will zoom into the region where there\u2019s a pose detected in the previous frame, so that the model can see the finer details and make better predictions in the current frame.\nIf you are interested in a deep-dive into MoveNet\u2019s implementation details, check out an earlier blog post including its model architecture and the dataset it was trained on.\nSample app for Android and Raspberry Pi\nWe have released new pose estimation sample apps for these platforms so that you can quickly try out different pose estimation models (MoveNet Lightning, MoveNet Thunder, Posenet) on the platform of your choice.\nAndroid sample\niOS sample\nRaspberry Pi sample\nIn the Android and iOS sample, you can also choose an accelerator (GPU, NNAPI, CoreML) to run the pose estimation models.\nScreenshot of the Android sample app. The image is from Pixabay.\nMoveNet performance\nWe have optimized MoveNet to run well on hardware accelerators supported by TensorFlow Lite, including GPU and accelerators available via the Android NN API. This performance benchmark result may help you choose the runtime configurations that are most suitable for your use cases.\nModel\nSize (MB)\nmAP*\nLatency (ms) **\nPixel 5 - \nCPU 4 threads\nPixel 5 - GPU\nRaspberry Pi 4 - CPU 4 threads\nMoveNet.Thunder (FP16 quantized)\n12.6MB\n72.0\n155ms\n45ms\n594ms\nMoveNet.Thunder (INT8 quantized)\n7.1MB\n68.9\n100ms\n52ms\n251ms\nMoveNet.Lightning (FP16 quantized)\n4.8MB\n63.0\n60ms\n25ms\n186ms\nMoveNet.Lightning (INT8 quantized)\n2.9MB\n57.4\n52ms\n28ms\n95ms\nPoseNet\n(MobileNetV1 backbone, FP32)\n13.3MB\n45.6\n80ms\n40ms\n338ms\n* mAP was measured on a subset of the COCO keypoint dataset where we filter and crop each image to contain only one person.\n** Latency was measured end-to-end using the Android and Raspberry Pi sample apps with TensorFlow 2.5 under sustained load.\nHere are some tips when deciding which model and accelerator to use:\nChoose Lightning or Thunder. Firstly, you should see whether the accuracy of the Lightning version is enough for your use case.\nIf the Lightning INT8 model\u2019s accuracy is good enough, then go with it because it\u2019s the smallest and fastest model in the lineup. A faster model also means less battery consumed.\nIf having good accuracy is critical for your use case, go with the Thunder FP16 model.\nChoose the accelerator. Accelerator performance varies a lot between Android devices from different manufacturers.\nCPU is the safest and simplest choice because you can know for sure that it will work on practically any Android device that can run TensorFlow Lite. However, it is usually slower and consumes more power than running the model on accelerators. All MoveNet models can run well on CPU so you should choose a model based on your accuracy needs.\nGPU is the most widely available accelerator and provides a decent performance boost. Choose the FP16 quantized models if you want to leverage GPUs.\nAndroid NNAPI is the convenient way to access additional ML accelerators on Android devices. If you are already using the CPU or GPU for other workloads and your user\u2019s device runs Android 10 or a newer version, you can choose a model that suits your accuracy needs, and let NNAPI choose the path that it thinks works best for your model.\nIf you are an IoT developer, you may want to use Coral to increase inference speed. See the benchmark numbers for Coral here.\nDeploy the model over-the-air rather than bundle it in the app binary. Due to the variety of the Android ecosystem, there\u2019s no single model that is optimal for all of your users. For users with lower-end devices, the Lightning INT8 model might be optimal for them because it\u2019s the fastest and consumes the least battery. However, for users with high-end devices, you may want to deliver better performance using the Thunder FP16 model. If you want to change models according to the user device, consider using the free Firebase ML to host your models instead of bundling all the models you intend to use into your app. You can write a logic to download an optimal model for each of your user\u2019s device when the user starts using a feature in your app that requires the TFLite model.\nPose classification\nWhile the pose estimation model tells you where the pose key points are, in many fitness applications, you may want to go further and classify the pose, for example whether it\u2019s a yoga goddess pose or a plank pose, to deliver relevant information to your users.\nTo make pose classification easier to implement, we\u2019ve also released a Colab notebook that teaches you how to use MoveNet and TensorFlow Lite to train a custom pose classification model from your custom pose dataset. It means that if you want to recognize yoga poses, all you need is to collect images of poses that you want to recognize, label them, and follow the tutorial to train and deploy a yoga pose classifier into your applications.\nThe pose classifier consists of two stages:\nUse MoveNet to detect keypoints from the input image.\nUse a small TensorFlow Lite model to classify the pose from the detected keypoints.\nAn example of pose classification using MoveNet. The input image is from Pixabay.\nIn order to train a custom pose classifier, you need to prepare the pose images and put them into a folder structure as below. Each subfolder name is the name of the class you want to recognize. Then you can run the notebook to train a custom pose classifier and convert it to the TensorFlow Lite format.\nyoga_poses\n|__ downdog\n    |______ 00000128.jpg\n    |______ 00000181.bmp\n    |______ ...\n|__ goddess\n    |______ 00000243.jpg\n    |______ 00000306.jpg\n    |______ ...\n...\nThe pose classification TensorFlow Lite model is very small, only about 30KBs. It takes the landmarks output from MoveNet, normalizes the pose coordinates and feeds it through a few fully connected layers. The model output is a list of probabilities that the pose is each of the known pose types.\nOverview of the pose classification TensorFlow Lite model.\nYou can try your pose classification model in any of the pose estimation sample apps for Android or Raspberry Pi that we have just released.\nWhat\u2019s next\nOur goal is to provide the core pose estimation and action recognition engine so that developers can build creative applications on top of it. Here are some of the directions that we are actively working on:\nAn improved version of MoveNet that can detect multiple poses in one forward path.\nAction recognition based on the detected poses on multiple frames.\nPlease let us know via tflite@tensorflow.org or the TensorFlow Forum if you have any feedback or suggestions!\nAcknowledgements\nWe would like to thank the other contributors to MoveNet: Ronny Votel, Ard Oerlemans, Francois Belletti along with those involved with the TensorFlow Lite: Tian Lin, Lu Wang.",
    "link": "https://blog.tensorflow.org/2021/08/pose-estimation-and-classification-on-edge-devices-with-MoveNet-and-TensorFlow-Lite.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-CtOy_PWv_uQ/YRV3lPjJohI/AAAAAAAAEck/-OX0MFUkHNsPur0sLpFsZeRmKvKvm_Y5ACLcBGAsYHQ/s0/three_pane_aligned%2B%25281%2529.gif",
      "https://1.bp.blogspot.com/-CtOy_PWv_uQ/YRV3lPjJohI/AAAAAAAAEck/-OX0MFUkHNsPur0sLpFsZeRmKvKvm_Y5ACLcBGAsYHQ/s0/three_pane_aligned%2B%25281%2529.gif",
      "https://1.bp.blogspot.com/-5q8wKgO8QwY/YRV6O_TNEgI/AAAAAAAAEc0/oJg_cIkbfNEuUKsjCV3V-KQhIPHPvpL3gCLcBGAsYHQ/s16000/unnamed%2B%25281%2529.png",
      "https://1.bp.blogspot.com/-9GXquaL0KnY/YRV8BkPH5ZI/AAAAAAAAEc8/V4jNeg4IN5EyFt617hXeLFu93DL_xxrEgCLcBGAsYHQ/s0/TF_blog_pose_estimation_3.jpeg",
      "https://1.bp.blogspot.com/-SJHcKMbc3Lg/YRV9VeBJ2_I/AAAAAAAAEdE/j7gc8f4Bp94ye7qIA3zDXk0VwPp2d-vWACLcBGAsYHQ/s0/TF_Blog__Next_generation_pose_estimation_4.jpeg"
    ],
    "time": "2023/12/04 00:58:43"
  },
  {
    "title": "Easier object detection on mobile with TensorFlow Lite",
    "content": "Posted by Khanh LeViet, Developer Advocate on behalf of the TensorFlow Lite team\nAt Google I/O this year, we are excited to announce several product updates that simplify training and deployment of object detection models on mobile devices:\nOn-device ML learning pathway: a step-by-step tutorial on how to train and deploy a custom object detection model on mobile devices with no machine learning expertise required.\nEfficientDet-Lite: a state-of-the-art object detection model architecture optimized for mobile devices.\nTensorFlow Lite Model Maker for object detection: train custom models in just a few lines of code.\nTensorFlow Lite Metadata Writer API: simplify metadata creation to generate custom object detection models compatible with TFLite Task Library.\nDespite being a very common ML use case, object detection can be one of the most difficult to do. We've worked hard to make it easier for you, and in this blog post we'll show you how to leverage the latest offerings from TensorFlow Lite to build a state-of-the-art mobile object detector using your own domain data.\nOn-device ML learning pathway: learn how to train and deploy custom TensorFlow Lite object detection model in 12 minutes.\nTraining a custom object detection model and deploying it to an Android app has become super easy with TensorFlow Lite. We released a learning pathway that teaches you step-by-step how to do it.\nIn the video, you can learn the steps to build a custom object detector:\nPrepare the training data.\nTrain a custom object detection model using TensorFlow Lite Model Maker.\nDeploy the model on your mobile app using TensorFlow Lite Task Library.\nThere\u2019s also a codelab with source code on GitHub for you to run through the code yourself. Please try it out and let us know your feedback!\nEfficientDet-Lite: the state-of-the-art model architecture for object detection on mobile devices\nRunning machine learning models on mobile devices means we always need to consider the trade-off between model accuracy vs. inference speed and model size. The state-of-the-art mobile-optimized model doesn\u2019t only need to be more accurate, but it also needs to run faster and be smaller. We adapted the neural architecture search technique published in the EfficientDet paper, then optimized the model architecture for running on mobile devices and came up with a novel mobile object detection model family called EfficientDet-Lite.\nEfficientDet-Lite has 5 different versions: Lite0 to Lite4. The smaller version runs faster but is not as accurate as the larger version. You can experiment with multiple versions of EfficientNet-Lite and choose the one that is most suitable for your use case.\nModel architecture\nSize(MB)*\nLatency (ms)**\nAverage Precision***\nEfficientDet-Lite0\n4.4\n37\n25.69%\nEfficientDet-Lite1\n5.8\n49\n30.55%\nEfficientDet-Lite2\n7.2\n69\n33.97%\nEfficientDet-Lite3\n11.4\n116\n37.70%\nEfficientDet-Lite4\n19.9\n260\n41.96%\nSSD MobileNetV2 320x320\n6.7\n24\n20.2%\nSSD MobileNetV2 FPNLite 640x640\n4.3\n191\n28.2%\n* Size of the integer quantized models.\n** Latency measured on Pixel 4 using 4 threads on CPU.\n*** Average Precision is the mAP (mean Average Precision) on the COCO 2017 validation dataset.\nWe have released the EfficientDet-Lite models trained on the COCO dataset to TensorFlow Hub. You also can train EfficientDet-Lite custom models using your own training data with TensorFlow Lite Model Maker.\nTensorFlow Lite Model Maker: train a custom object detection using transfer learning in a few lines of code\nTensorFlow Lite Model Maker is a Python library that significantly simplifies the process of training a machine learning model using a custom dataset. It leverages transfer learning to enable training high quality models using just a handful of images.\nModel Maker accepts datasets in the PASCAL VOC format and the Cloud AutoML\u2019s CSV format. As you can create your own dataset using open-source GUI tools such as LabelImg or makesense.ai, everyone can create training data for Model Maker without writing a single line of code.\nOnce you have your training data, you can start training a TensorFlow Lite custom object detectors.\n# Step 1: Choose the model architecture\nspec = model_spec.get('efficientdet_lite2')\n\n# Step 2: Load your training data\ntrain_data, validation_data, test_data = object_detector.DataLoader.from_csv('gs://cloud-ml-data/img/openimage/csv/salads_ml_use.csv')\n\n# Step 3: Train a custom object detector\nmodel = object_detector.create(train_data, model_spec=spec, validation_data=validation_data)\n\n# Step 4: Export the model in the TensorFlow Lite format\nmodel.export(export_dir='.')\n\n# Step 5: Evaluate the TensorFlow Lite model\nmodel.evaluate_tflite('model.tflite', test_data)\nCheck out this notebook to learn more.\nTensorFlow Lite Task Library: deploying object detection models on mobile in a few lines of code\nTensorFlow Lite Task Library is a cross-platform library which simplifies TensorFlow Lite model deployments on mobile. Custom object detection models trained with TensorFlow Lite Model Maker can be deployed to an Android app in just a few lines of Kotlin code:\n// Step 1: Load the TensorFlow Lite model\nval detector = ObjectDetector.createFromFile(context, \"model.tflite\")\n\n// Step 2: Convert the input Bitmap into a TensorFlow Lite's TensorImage object\nval image = TensorImage.fromBitmap(bitmap)\n\n// Step 3: Feed given image to the model and get the detection result\nval results = detector.detect(image)\nSee our documentation to learn more about the customization options in Task Library, including how to configure the minimum detection threshold or the maximum number of detected objects.\nTensorFlow Lite Metadata Writer API: simplify deployment of custom models trained with TensorFlow Object Detection API\nTask Library relies on the model metadata bundled in the TensorFlow Lite model to execute the preprocessing and postprocessing logic required to run inference using the model. They include how to normalize the input image, or how to map the class id to human readable labels. Models trained using Model Maker have these metadata by default, making them compatible with Task Library. But if you train a TensorFlow Lite object detection model using a training pipeline other than Model Maker, you can add the metadata using TensorFlow Lite Metadata Writer API.\nFor example, if you train a model using TensorFlow Object Detection API, you can add metadata to the TensorFlow Lite model using this Python code:\nLABEL_PATH = 'label_map.txt'\nMODEL_PATH = \"ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/model.tflite\"\nSAVE_TO_PATH = \"ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/model_with_metadata.tflite\"\n\n# Step 1: Specify the preprocessing parameters and label file\nwriter = object_detector.MetadataWriter.create_for_inference(\n    writer_utils.load_file(MODEL_PATH), input_norm_mean=[0],\n    input_norm_std=[255], label_file_paths=[LABEL_PATH])\n\n# Step 2: Export the model with metadata\nwriter_utils.save_file(writer.populate(), SAVE_TO_PATH)\nHere we specify the normalization parameters (input_norm_mean=[0], input_norm_std=[255]) so that the input image will be normalized into the [0..1] range. You need to specify normalization parameters to be the same as in the preprocessing logic used during the model training.\nSee this notebook for a full tutorial on how to convert models trained with the TensorFlow Object Detection API to TensorFlow Lite and add metadata.\nWhat\u2019s next\nOur goal is to make machine learning easier to use for every developer, with or without machine learning expertise. We are working with the Model Garden team to bring more object detection model architectures to Model Maker. We will also continue to work with researchers in Google to make future state-of-the-art object detection models available via Model Maker, shortening the path from cutting-edge research to production for everyone. Stay tuned for more updates!",
    "link": "https://blog.tensorflow.org/2021/06/easier-object-detection-on-mobile-with-tf-lite.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-DyzSg_BCVFA/YMfS6Yu9XRI/AAAAAAAAEUc/QrsAyeXgEKIY5VikTHqtGXtwPVQ4gZLWwCLcBGAsYHQ/s0/tf%2Bpothole.png",
      "https://1.bp.blogspot.com/-DyzSg_BCVFA/YMfS6Yu9XRI/AAAAAAAAEUc/QrsAyeXgEKIY5VikTHqtGXtwPVQ4gZLWwCLcBGAsYHQ/s0/tf%2Bpothole.png"
    ],
    "time": "2023/12/04 00:58:43"
  },
  {
    "title": "How TensorFlow helps Edge Impulse make ML accessible to embedded engineers",
    "content": "Posted by Daniel Situnayake, Founding TinyML Engineer, Edge Impulse.\nMicrocontrollers that run our world\nNo matter where you are reading this right now\u2014your home, your office, or sitting in a vehicle\u2014you are likely surrounded by microcontrollers. They are the tiny, low-power computers that animate our modern world: from smart watches and kitchen appliances to industrial equipment and public transportation. Mostly hidden inside other products, microcontrollers are actually the most numerous type of computer, with more than 28 billion of them shipped in 2020.\nThe software that powers all these devices is written by embedded software engineers. They\u2019re some of the most talented, detail-oriented programmers in the industry, tasked with squeezing every last drop of efficiency from tiny, inexpensive processors. A typical mid-range microcontroller\u2014based around Arm\u2019s popular Cortex-M4 architecture\u2014might have a 32-bit processor running at just 64Mhz, with 256KB of RAM and 1MB of flash memory for storing a program. That doesn\u2019t leave a lot of room for waste.\nSince microcontrollers interface directly with sensors and hardware, embedded engineers are often experts in signal processing and electrical engineering\u2014and they tend to have a lot of domain knowledge in their area of focus. One engineer might be an expert on the niche sensors used for medical applications, while another might focus on analyzing audio signals.\nEmbedded machine learning\nIn the past few years, a set of technologies have been developed that make it possible to run miniature, highly optimized machine learning models on low-power microcontrollers like the one described above. By using machine learning to interpret sensor data right at the source, embedded applications can become smarter, faster, and more energy efficient, making their own decisions rather than having to stream data to the cloud and wait for a response. This concept is known as embedded machine learning, or TinyML.\nWith their deep signal processing and domain expertise, embedded engineers are ideally placed to design this new generation of smart applications. However, embedded engineers tend to have highly specialized skill sets and use development toolchains that are often far removed from the Python-heavy stack preferred by data scientists and machine learning engineers.\nIt isn\u2019t reasonable to expect domain experts to retrain as data scientists, or for data scientists to learn the embedded development skills required to work with microcontrollers. Instead, a new generation of tooling is required that will allow those with domain expertise to capture their knowledge and insight as machine learning models and deploy them to embedded devices\u2014with help from machine learning experts an optional extra.\nThe TinyML development process is similar to the traditional machine learning workflow. It starts with collecting, exploring, and evaluating a dataset. Next up, feature engineering takes the form of sophisticated digital signal processing, often using the types of algorithms that embedded engineers are already familiar with. Once features have been extracted from the data, a machine learning model is trained and evaluated\u2014with a critical eye on its size, to make sure it will fit on a tiny microcontroller and run fast enough to be useful.\nAfter the training, the model is optimized for size and efficiency. This often involves quantization, reducing the precision of the model\u2019s weights so that they take up less precious memory. Once the model is ready, it must be deployed as a C++ library (the language of choice for the majority of embedded platforms) that includes all of the operator kernels required to run it. The embedded engineer can then write and tune an application that interprets the model\u2019s output and uses it to make decisions.\nThroughout this process, it\u2019s important to carefully evaluate the model and application to ensure that it functions in the way that it is intended to when used in a real world environment. Without adequate monitoring and review, it\u2019s possible to create models that seem superficially accurate but that fail in harmful ways when exposed to real world data.\nEdge Impulse and TensorFlow\nThe Edge Impulse team has created an end-to-end suite of tooling that helps embedded engineers and domain experts build and test machine learning applications. Edge Impulse is designed to integrate beautifully with the tools that embedded engineers use every day, providing a high-level interface for incorporating machine learning into projects.\nEdge Impulse makes use of the TensorFlow ecosystem for training, optimizing, and deploying deep learning models to embedded devices. While it was designed with non-ML engineers in mind, the philosophy behind Edge Impulse is that it should be extensible by machine learning experts and flexible enough to incorporate their insights and additions\u2014from hand-tuned model architectures and loss functions to custom operator kernels.\nThis extensibility is made possible by the TensorFlow ecosystem, which provides a set of standards and integration points that experts can use to make their own improvements.\nTraining a tiny model\nThis process starts during training. Novice ML developers using Edge Impulse can use a library of preset deep learning model architectures designed to work well with embedded devices. For example, this simple convolutional model is intended for classifying ambient noise:\nUnder the hood, Edge Impulse generates a Python implementation of the model using TensorFlow\u2019s Keras APIs. More experienced developers can customize the layers of the deep learning network, tweaking parameters and adding new layers that are reflected in the underlying Keras model. And expert developers have access to edit the training code itself, directly within the UI:\nSince Edge Impulse uses TensorFlow libraries and APIs, it\u2019s incredibly simple to extend the built-in training code with your own logic. For example, the tf.data.Dataset class is used to provide an efficient pipeline to the training and validation datasets. This pipeline can easily be extended to add transformations, such as the data augmentation function seen in the following screenshot from an image classification project:\nFor in-depth experiments, developers can download a Jupyter Notebook containing all of the dependencies required to run their training script locally.\nAny custom model code using the TensorFlow APIs fits seamlessly into the end-to-end pipeline hosted by Edge Impulse. Training is run in the cloud, and trained models are automatically optimized for embedded deployment using a combination of TensorFlow utilities and Edge Impulse\u2019s own open source technologies.\nModel optimization\nQuantization is the most common form of optimization used when deploying deep learning models to embedded devices. Edge Impulse uses TensorFlow\u2019s Model Optimization Toolkit to quantize models, reducing their weights\u2019 precision from float32 to int8 with minimal impact on accuracy.\nUsing TensorFlow Lite for Microcontrollers along with the emulation software Renode, Edge Impulse provides developers with an accurate estimate of the latency and memory usage of their model once it is deployed to the target embedded device. This makes it easy to determine the impact of optimizations such as quantization across different slices of the dataset:\nA comparison between int8 quantized and unoptimized versions of the same mode, showing the difference in performance and results.\nFor maximum flexibility and compatibility with developers\u2019 existing workflows, the trained model is available for download in multiple formats. Developers can choose to export the original model as a TensorFlow SavedModel, or download one of several optimized models using the portable TensorFlow Lite flatbuffer format:\nDownload links for models serialized using TensorFlow\u2019s SavedModel and TensorFlow Lite formats.\nDeployment\nOnce a model has been trained and tested there are multiple ways to deploy it to the target device. Embedded engineers work heavily with C++, so the standard option is to export a C++ SDK: a library of optimized source code that implements both the signal processing pipeline and the deep learning model. The SDK has a permissive open source license, so developers are free to use it in any project or share it with others.\nThere are two main options for running deep learning models, both of which make use of TensorFlow technologies. The first, Edge Impulse\u2019s EON Compiler, is a code generation tool that converts TensorFlow Lite models into human readable C++ programs.\nEnabling EON Compiler can reduce memory usage by up to 50% with no impact on model accuracy.\nEON Compiler makes use of the operator kernels implemented in TensorFlow Lite for Microcontrollers, invoking them in an efficient manner that doesn\u2019t require the use of an interpreter. This results in memory savings of up to 50%. It automatically applies any available optimized kernels for the target device, meaning libraries such as Arm\u2019s CMSIS-NN will be used where appropriate.\nSome projects benefit from additional flexibility. In these cases, developers can choose to export a library that uses the TensorFlow Lite for Microcontrollers interpreter to run the model. This can be useful for developers who wish to experiment with custom kernel implementations for their specific hardware, or who are working within an environment that has TensorFlow Lite for Microcontrollers built in.\nIn addition to the C++ SDK, developers can choose to target specific environments. For example, a TensorRT library provides optimized support for NVidia\u2019s Jetson Nano embedded Linux developer kit. This interoperability is enabled by the extensive TensorFlow ecosystem and open source community, which has tooling for numerous platforms and targets.\nModels can be optimized and exported for targets in the broader TensorFlow ecosystem, such as NVidia\u2019s Jetson Nano.\nEnabling new technologies\nTensorFlow is unique amongst deep learning frameworks due to its broad, mature, and extensible set of technologies for training and deploying models to embedded devices. TensorFlow formats, such as the TensorFlow Lite flatbuffer, have become de-facto standards amongst companies bringing deep learning models to the edge.\nThe TensorFlow ecosystem has been key to enabling the growth of embedded machine learning, enabling companies like Edge Impulse to put artificial intelligence in the hands of domain experts who are building the next generation of consumer and industrial technologies.\nIf you\u2019d like to learn more about embedded machine learning using Edge Impulse and TensorFlow, there are many options. Take a look at the Introduction to Embedded Machine Learning course on Coursera, or jump right in with the Getting Started guide or Recognize sounds from audio tutorial. You can even check out a public Edge Impulse project that you can clone and customize with a single click.\nDaniel Situnayake\nFounding TinyML Engineer, Edge Impulse.",
    "link": "https://blog.tensorflow.org/2021/06/how-tensorflow-helps-edge-impulse-make-ml-accessible.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-sNOWmv1oIm0/YLansaz14II/AAAAAAAAERY/3-j0qt8XOnwta1qftb3bIvKokAl7L5b4gCLcBGAsYHQ/s0/Tensorflow%2Band%2BEdge%2BImpulse.png",
      "https://1.bp.blogspot.com/-sNOWmv1oIm0/YLansaz14II/AAAAAAAAERY/3-j0qt8XOnwta1qftb3bIvKokAl7L5b4gCLcBGAsYHQ/s0/Tensorflow%2Band%2BEdge%2BImpulse.png",
      "https://1.bp.blogspot.com/--STWoo5ykhs/YLaSTOuSy9I/AAAAAAAAEQY/8tbQrPTkiEgMpHZs6ZUkuWrBVphBodg7gCLcBGAsYHQ/s0/image%2B1%2Btf.png",
      "https://1.bp.blogspot.com/-OfkuFl29YC8/YLetziQyiDI/AAAAAAAAERg/qGws1ETRQzA1GIgx6G37K9YrTXTj1HGIgCLcBGAsYHQ/s0/impulse_screenshot1.png",
      "https://1.bp.blogspot.com/-9VR8l88Z-5M/YLet9JVl9rI/AAAAAAAAERk/mJau3BQDhp8pqn_MQBL637heDgLFynA2gCLcBGAsYHQ/s0/impulse_screenshot2.png",
      "https://1.bp.blogspot.com/-5UZ5I4ttP0o/YLaTd3FnTuI/AAAAAAAAEQs/y-uXthSYkpA5FA7EEkHLXAUtLIPVH8QGQCLcBGAsYHQ/s0/image%2B4%2Btf.png",
      "https://1.bp.blogspot.com/-WwWrpkoQXds/YLaT1WZQndI/AAAAAAAAEQ4/sHMumh9SBlIebDZP1j_eURok482W8u6vACLcBGAsYHQ/s0/image%2B5%2Btf.png",
      "https://1.bp.blogspot.com/-gvJybGlScS8/YLaVuV0mxRI/AAAAAAAAERA/O-nLxOFFY_UlhjnL1nvbcR14FiaxEekgwCLcBGAsYHQ/s0/image%2B6%2Btf.png",
      "https://1.bp.blogspot.com/-5lJCDD0ODwQ/YLaWGild1uI/AAAAAAAAERI/Jtg9m8oVB4QaQrv28Msk3-TwI2aY0VZSgCLcBGAsYHQ/s0/image%2B7%2Btf.png",
      "https://1.bp.blogspot.com/-ECoGj6p7s4A/YLaWhG2nV-I/AAAAAAAAERQ/Io2RuUy474AZmIZo2R_56RQMPSC-nTv4wCLcBGAsYHQ/s0/image%2B8%2Btf.png"
    ],
    "time": "2023/12/04 00:58:43"
  },
  {
    "title": "Building a TinyML Application with TF Micro and SensiML",
    "content": "A guest post by Chris Knorowski, SensiML CTO\nTinyML reduces the complexity of adding AI to the edge, enabling new applications where streaming data back to the cloud is prohibitive. Some examples of applications that are making use of TinyML right now are :\nVisual and audio wake words that trigger an action when a person is detected in an image or a keyword is spoken .\nPredictive maintenance on industrial machines using sensors to continuously monitor for anomalous behavior.\nGesture and activity detection for medical, consumer, and agricultural devices, such as gait analysis, fall detection or animal health monitoring.\nOne common factor for all these applications is the low cost and power usage of the hardware they run on. Sure, we can detect audio and visual wake words or analyze sensor data for predictive maintenance on a desktop computer. But, for a lot of these applications to be viable, the hardware needs to be inexpensive and power efficient (so it can run on batteries for an extended time).\nFortunately, the hardware is now getting to the point where running real-time analytics is possible. It is crazy to think about, but the Arm Cortex-M4 processor can do more FFT\u2019s per second than the Pentium 4 processor while using orders of magnitude less power. Similar gains in power/performance have been made in sensors and wireless communication. TinyML allows us to take advantage of these advances in hardware to create all sorts of novel applications that simply were not possible before.\nAt SensiML our goal is to empower developers to rapidly add AI to their own edge devices, allowing their applications to autonomously transform raw sensor data into meaningful insight. We have taken years of lessons learned in creating products that rely on edge optimized machine learning and distilled that knowledge into a single framework, the SensiML Analytics Toolkit, which provides an end-to-end development platform spanning data collection, labeling, algorithm development, firmware generation, and testing.\nSo what does it take to build a TinyML application?\nBuilding a TinyML application touches on skill sets ranging from hardware engineering, embedded programming, software engineering, machine learning, data science and domain expertise about the application you are building. The steps required to build the application can be broken into four parts:\nCollecting and annotating data\nApplying signal preprocessing\nTraining a classification algorithm\nCreating firmware optimized for the resource budget of an edge device\nThis tutorial will walk you through all the steps, and by the end of it you will have created an edge optimized TinyML application for the Arduino Nano 33 BLE Sense that is capable of recognizing different boxing punches in real-time using the Gyroscope and Accelerometer sensor data from the onboard IMU sensor.\nWhat you need to get started\nWe will use the SensiML Analytics Toolkit to handle collecting and annotating sensor data, creating a sensor preprocessing pipeline, and generating the firmware. We will use TensorFlow to train our machine learning model and TensorFlow Lite Micro for inferencing. Before you start, we recommend signing up for SensiML Community Edition to get access to the SensiML Analytics Toolkit.\nThe Software\nWe will use the SensiML Open Gateway, an open-source python application to stream data from edge devices.\nWe will use the SensiML Data Capture Lab (Windows 10) to record and label the sensor data.\nWe will use Google Colab to train our model using TensorFlow Lite for Microcontrollers\nWe will use the SensiML Analytics Studio for offline validation and code generation of the firmware\nWe will use Visual Studio Code with the Platform IO extension to flash the firmware.\nThe Hardware\nArduino Nano 33 BLE Sense\nAdafruit Li-Ion Backpack Add-On (optional)\nLithium-Ion Polymer Battery ( 3.7v 100mAh)\nZebra Byte Case\nGlove and Double Sided Tape\nThe Arduino Nano 33 BLE Sense has an Arm Cortex-M4 microcontroller running at 64 MHz with 1MB Flash memory and 256 KB of RAM. If you are used to working with cloud/mobile this may seem tiny, but many applications can run in such a resource-constrained environment.\nThe Nano 33 BLE Sense also has a variety of onboard sensors which can be used in your TinyML applications. For this tutorial, we are using the motion sensor which is a 9-axis IMU (accelerometer, gyroscope, magnetometer).\nFor wireless power, we used the Adafruit Li-Ion Battery Pack. If you do not have the battery pack, you can still walk through this tutorial using a suitably long micro USB cable to power the board. Though collecting gesture data is not quite as fun when you are wired. See the images below hooking up the battery to the Nano 33 BLE Sense.\nBuilding Your Data Set\nFor every machine learning project, the quality of the final product depends on the quality of your data set. Time-series data, unlike image and audio, are typically unique to each application. Because of this, you often need to collect and annotate your datasets. The next part of this tutorial will walk you through how to connect to the Nano 33 BLE Sense to stream data wirelessly over BLE as well as label the data so it can be used to train a TensorFlow model.\nFor this project we are going to collect data for 5 different gestures as well as some data for negative cases which we will label as Unknown. The 5 boxing gestures we are going to collect data for are Jab, Overhand, Cross, Hook, and Uppercut.\nWe will also collect data on both the right and left glove. Giving us a total of 10 different classes. To simplify things we will build two separate models one for the right glove, and one for the left. This tutorial will focus on the left glove.\nStreaming sensor data from the Nano 33 over BLE\nThe first challenge of a TinyML project is often to figure out how to get data off of the sensor. Depending on your needs you may choose Wi-Fi, BLE, Serial, or LoRaWAN. Alternatively, you may find storing data to an internal SD card and transferring the files after is the best way to collect data. For this tutorial, we will take advantage of the onboard BLE radio to stream sensor data from the Nano 33 BLE Sense.\nWe are going to use the SensiML Open Gateway running on our computer to retrieve the sensor data. To download and launch the gateway open a terminal and run the following commands:\ngit clone https://github.com/sensiml/open-gateway\n\ncd open-gateway\npip3 install -r requirements.txt\npython3 app.py\nThe gateway should now be running on your machine.\nNext, we need to connect the gateway server to the Nano 33 BLE Sense. Make sure you have flashed the Data Collection Firmware to your Nano 33. This firmware implements the Simple Streaming Interface specification which creates two topics used for streaming data. The /config topic returns a JSON describing the sensor data and /stream topic streams raw sensor data as a byte array of Int16 values.\nTo configure the gateway to connect to your sensor:\nGo to the gateway address in your browser (defaults to localhost:5555)\nClick on the Home Tab\nSet Device Mode: Data Capture\nSet Connection Type: BLE\nClick the Scan button, and select the device named Nano 33 DCL\nClick the Connect to Device button\nThe gateway will pull the configuration from your device, and be ready to start forwarding sensor data. You can verify it is working by going to the Test Stream tab and clicking the Start Stream button.\nSetting up the Data Capture Lab Project\nNow that we can stream data, the next step is to record and label the boxing gestures. To do that we will use the SensiML Data Capture Lab. If you haven\u2019t already done so, download and install the Data Capture Lab to record sensor data.\nWe have created a template project to get you started. The project is prepopulated with the gesture labels and metadata information, along with some pre-recorded example gestures files. To add this project to your account:\nDownload and unzip the Boxing Glove Gestures Demo Project\nOpen the Data Capture Lab\nClick Upload Project\nClick Browse which will open the file explorer window\nNavigate to the Boxing Glove Gestures Demo folder you just unzipped and select the Boxing Glove Gestures Demo.dclproj file\nClick Upload\nConnecting to the Gateway\nAfter uploading the project, you can start capturing sensor data. For this tutorial we will be streaming data to the Data Capture Lab from the gateway over TCP/IP. To connect to the Nano 33 BLE Sense from the Data Capture Lab through the gateway:\nOpen the Project Boxing Glove Gestures Demo\nClick Switch Modes -> Capture Mode\nSelect Connection Method: Wi-Fi\nClick the Find Devices button\nEnter the IP Address of your gateway machine, and the port the server is running on (typically 127.0.0.1:5555)\nClick Add Device\nSelect the newly added device\nClick the Connect button\nYou should see sensor data streaming across the screen. If you are having trouble with this step, see the full documentation here for troubleshooting.\nCapturing Boxing Gesture Sensor Data\nThe Data Capture Lab can also play videos that have been recorded alongside your sensor data. If you want to capture videos and sync them up with sensor data see the documentation here. This can be extremely helpful during the annotation phase to help interpret what is happening at a given point in the time-series sensor waveforms.\nNow that data is streaming into the Data Capture Lab, we can begin capturing our gesture data set.\nSelect \u201cJab\u201d from the Label dropdown in the Capture Properties screen. (this will be the name of the file)\nSelect the Metadata which captures the context (subject, glove, experience, etc.)\nThen click the Begin Recording button to start recording the sensor data\nPerform several \u201cJab\u201d gestures\nClick the Stop Recording button when you are finished\nAfter you hit stop recording, the captured data will be saved locally and synced with the cloud project. You can view the file by going to the Project Explorer and double-clicking on the newly created file.\nThe following video walks through capturing sensor data.\nAnnotating Sensor Data\nTo classify sensor data in real-time, you need to decide how much and which portion of the sensor stream to feed to the classifier. On edge devices, it gets even more difficult as you are limited to a small buffer of data due to the limited RAM. Identifying the right segmentation algorithm for an application can save on battery life by limiting the number of classifications performed as well as improving the accuracy by identifying the start and end of a gesture.\nSegmentation algorithms work by taking the input from the sensor and buffering the data until they determine a new segment has been found. At that point, they pass the data buffer down to the result of the pipeline. The simplest segmentation algorithm is a sliding window, which continually feeds a set chunk of data to the classifier. However, there are many drawbacks to the sliding window for discrete gesture recognition, such as performing classifications when there are no events. This wastes battery and runs the risk of having events split across multiple windows which can lower accuracy.\nSegmenting in the Data Capture Lab\nWe identify events in the Data Capture Lab by creating Segments around the events in your sensor data. Segments are displayed with a pair of blue and red lines when you open a file and define where an event is located.\nThe Data Capture Lab has two methods for labeling your events: Manual and Auto. In manual mode you can manually drag and drop a segment onto the graph to identify an event in your sensor data. Auto mode uses a segmentation algorithm to automatically detect events based on customizable parameters. For this tutorial, we are going to use a segmentation algorithm in Auto mode. The segmentation algorithms we use for determining events will also be compiled as part of the firmware so that the on-device model will be fed the same segments of data it was trained against.\nWe have already created a segmentation algorithm for this project based on the dataset we have collected so far. To perform automatic event detection on newly captured data file:\nSelect the file from the Project Explorer\nClick on the Detect Segments button\nThe segmentation algorithm will be run against the capture and the segments it finds will be added to the file\nNote: If the events are not matching the real segments in your file, you may need to adjust the parameters of the segmentation algorithm.\nLabeling Events in the Data Capture Lab\nKeep in mind that automatic event detection only detects that an event has occurred, it does not determine what type of event has occurred. For each event that was detected, you will need to apply a label to them. To do that:\nSelect one or more of the segments from the graph\nClick the Edit button or (Ctrl+E)\nSpecify which label is associated with that event\nRepeat steps 1-3 for all segments in the capture\nClick Save\nBuilding a TinyML Model\nWe are going to use Google Colab to train our machine learning model using the data we collected from the Nano 33 BLE Sense in the previous section. Colab provides a Jupyter notebook that allows us to run our TensorFlow training in a web browser. Open the Google Colab notebook and follow along to train your model.\nOffline Model Validation\nAfter saving the model, go to the Analytic Studio to perform offline validation. To test the model against any of the captured data files\nOpen the Boxing Glove Gestures Demo project in the Summary Tab\nGo to Test Model Tab\nSelect your model from the Model Name dropdown\nSelect one or more of the capture files by clicking on them\nClick the Compute Accuracy Button to classify the captures using the selected model\np> When you click the Compute Accuracy button, the segmentation algorithm, preprocessing steps, and TensorFlow model are compiled into a single Knowledge Pack. Then the classification results and accuracy for each of the captures you selected are computed using the compiled Knowledge Pack. Click the Results button for the individual capture to see the classifications for all of the detected events and how they compared with the ground truth labels.\nDeploy and Test on the Nano 33 BLE Sense\nDownloading the model as firmware\nNow that you validated the model offline, it's time to see how it performs at the edge. To do that we download and flash the model to the Nano 33 BLE Sense.\nGo to the Download Model tab of the Analytics Studio\nSelect the HW Platform: Arduino CortexM4\nSelect Format: Library\nClick the Download button\nThe compiled library file should download to your computer\nFlashing the Firmware\nAfter downloading the library, we will build and upload the firmware to the Nano 33 BLE Sense. For this step, you will need the Nano 33 Knowledge Pack Firmware. To compile the firmware, we are using Visual Studio Code with the Platform IO plugin. To compile your model and Flash the Nano 33 BLE Sense with this firmware:\nOpen your terminal and run\ngit clone https://github.com/sensiml/nano33_knowledge_pack/\nUnzip the downloaded Knowledge Pack.\nIn the folder, you will find the following directories:\nknowledgepack_project/\nlibsensiml/\nCopy the files from libsensiml to nano33_knowledge_pack/lib/sensiml. You will overwrite the files included in the repository.\nCopy the files from knowledgepack_project to nano33_knowledge_pack/src/\nSwitch to the Platform I/O extension tab in VS Code\nConnect your Nano 33 BLE Sense to your computer using the micro USB cable.\nClick Upload and Monitor under the nano33ble_with_tensorflow in the PlatformI/O tab.\nWhen the device restarts, it will boot up and your model will be running automatically. The video below walks through these steps.\nViewing Classification Results\nTo see the classification results in real-time connect to your device over BLE using the Android TestApp or the SensiML Open Gateway. The device will show up with the name Nano33 SensiML KP when you scan for devices. We have trained two models, one for the left glove and one for the right glove. You can see a demo of both models running at the same time in the following video.\nConclusion\nWe hope this blog has given you the tools you need to start building an end-to-end TinyML application using TensorFlow Lite For Microcontrollers and the SensiML Analytics Toolkit. For more tutorials and examples of TinyML applications checkout the application examples in our documentation. Follow us on LinkedIn or get in touch with us, we love hearing about all of the amazing TinyML applications the community is working on!",
    "link": "https://blog.tensorflow.org/2021/05/building-tinyml-application-with-tf-micro-and-sensiml.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-3FkWRPQ2kNk/YJL8e1TCMKI/AAAAAAAAEJ0/yceZFKdjrsgXz4NBXyviyC1S_SqL-s5mACLcBGAsYHQ/s0/boxing_gestures_rawTestApp%2B%25281%2529.gif",
      "https://1.bp.blogspot.com/-k0ucobaq0Cg/YJL9c3v0taI/AAAAAAAAEJ8/009xCFPr3XwupVZiij7qWt3GkFnZZtc-gCLcBGAsYHQ/s0/Li-Ion%2BBattery%2Bpack.jpg",
      "https://1.bp.blogspot.com/-itZg34sXfmU/YJL-DFpL5dI/AAAAAAAAEKE/YUrqqYtsp18dsPUUDZ4qtMYdc0_cetbLACLcBGAsYHQ/s0/wiring_diagram_nano33.jpeg",
      "https://1.bp.blogspot.com/-0BTdz0iqpxQ/YJL-mEMpMeI/AAAAAAAAEKM/2cAAbzmgs6Y4ZDotspZ3UhIHMwl77W_dwCLcBGAsYHQ/s0/Boxing%2BGestures.png",
      "https://1.bp.blogspot.com/-iJm7uN-9wPg/YJMAxPmgCaI/AAAAAAAAEKU/_MTk70QoTRgGGB9SJ00JPvD_MMsJbpqagCLcBGAsYHQ/s0/powersheel.PNG",
      "https://1.bp.blogspot.com/-zLj88JP8djA/YJMBTfpKn0I/AAAAAAAAEKc/1qZgLfITG5Alcuxa1uXIDHOcv_KirUQVgCLcBGAsYHQ/s0/pasted%2Bimage%2B0%2B%25286%2529.png",
      "https://1.bp.blogspot.com/-b2Rj6uqG_5c/YJMBrdzt84I/AAAAAAAAEKk/daKrYkLdSMgKxuZnVYqFEIstGKtI4Qv3ACLcBGAsYHQ/s0/pasted%2Bimage%2B0%2B%25287%2529.png",
      "https://1.bp.blogspot.com/-uD65Bqt7w7E/YJMCm-u2AZI/AAAAAAAAEKs/q6LhmngrxCMOHORaKm2As9GPWYlQhuzpwCLcBGAsYHQ/s0/wifi-connection.PNG",
      "https://1.bp.blogspot.com/-HNWPi3QzDIE/YJMEP8zRifI/AAAAAAAAEK0/i2b364IUNU04drcS1rsNjdaGKRvZE-lQwCLcBGAsYHQ/s0/unnamed%2B%25283%2529.png",
      "https://1.bp.blogspot.com/-9ttXXo5dBJ4/YJMFnGMNLfI/AAAAAAAAEK8/qMQvWzmnauQXa65v_4PxRsuepT8xcZcpwCLcBGAsYHQ/s0/data_catpure_giff_small.gif",
      "https://1.bp.blogspot.com/-2Mm7LehRVXE/YJMGUjMKs0I/AAAAAAAAELE/VGya1XsK4qkQiMI-er_8GdHMoxCNoe0gwCLcBGAsYHQ/s0/pasted%2Bimage%2B0%2B%25288%2529.png",
      "https://1.bp.blogspot.com/-_VUSOMOfYaw/YJMMm70ue0I/AAAAAAAAELM/UTbg16udff8vIyDN1mzZxIpoBsK8ZtmEwCLcBGAsYHQ/s0/auto_segmentation.gif",
      "https://1.bp.blogspot.com/-4LfTiiF0eLM/YJMNOfC1MFI/AAAAAAAAELU/Llz8S8EOYxQYLsHMYAFiG0AKuILsyJiqgCLcBGAsYHQ/s0/labeling_data.gif",
      "https://1.bp.blogspot.com/-Sgi9KHHrCHs/YJMNkT0l4cI/AAAAAAAAELc/EmmrNxq9W-csUiAHFvgTV4AHe0Ay0FqMACLcBGAsYHQ/s0/as_screen.PNG",
      "https://1.bp.blogspot.com/-w209k2BRMgo/YJMOAyBSX1I/AAAAAAAAELk/VZ8HHw1oIfs-XDVHuA1AOl8ioCeTXflswCLcBGAsYHQ/s0/compute_accuracy.PNG",
      "https://1.bp.blogspot.com/-1WmS-Z2xK5U/YJMOb2gN9pI/AAAAAAAAELs/rqUR5irl_JsKgTvXxic_9JPf_LeKKzvugCLcBGAsYHQ/s0/pasted%2Bimage%2B0%2B%25289%2529.png",
      "https://1.bp.blogspot.com/-k1WFf6UfbyE/YJMPgtvz1SI/AAAAAAAAEL0/DH3gymgTgdkQaiFM9teQDif1EoZDJ5btQCLcBGAsYHQ/s0/pasted%2Bimage%2B0%2B%252810%2529.png",
      "https://1.bp.blogspot.com/-hxJgmmw2Jik/YJMQBELjGlI/AAAAAAAAEL8/HbGmPivIluwQcAid588KPoKtKSsPt1pDwCLcBGAsYHQ/s0/pasted%2Bimage%2B0%2B%252811%2529.png"
    ],
    "time": "2023/12/04 00:58:43"
  },
  {
    "title": "Adaptive Framework for On-device Recommendation",
    "content": "Posted by Ellie Zhou, Tian Lin, Shuangfeng Li and Sushant Prakash\nIntroduction & Motivation\nWe are excited to announce an adaptive framework to build on-device recommendation ML solutions with your own data and advanced user modeling architecture.\nAfter the previously open-sourced on-device recommendation solution, we received a lot of interest from the community on introducing on-device recommender AI. Motivated and inspired by the feedback, we considered various use cases, and created a framework that could generate TensorFlow Lite recommendation models accommodating different kinds of data, features, and architectures to improve the previous models.\nBenefits of this framework:\nFlexible: The adaptive framework allows users to create a model in a configurable way.\nBetter model representation: To improve the previous model, our new recommendation models can utilize multiple kinds of features than a single feature.\nPersonalized recommendations play an increasingly important role in digital life nowadays. With more and more user actions being moved to edge devices, supporting recommenders on-device becomes an important direction. Compared with conventional pure server-based recommenders, the on-device solution has unique advantages, such as protecting users\u2019 privacy, providing fast reaction to on-device user actions, leveraging lightweight TensorFlow Lite inference, and bypassing network dependency. We welcome you to try out this framework and create recommendation experience in your applications.\nIn this article, we will\nIntroduce the improved model architecture and framework adaptivity.\nWalk you through how to utilize the framework step-by-step.\nProvide insights based on research done with a public dataset.\nPlease find more details on TensorFlow website.\nModel\nA recommendation model typically predicts users\u2019 future activities, based on users\u2019 previous activities. Our framework supports models using context information to do the prediction, which can be described in the following architecture:\nFigure 1: An illustration of the configurable recommendation model. Each module is created according to the user-defined configuration.\nAt the context side, representations of all user activities are aggregated by the encoder to generate the context embedding. We support three different types of encoders: 1) bag-of-words (a.k.a. BOW), 2) 1-D convolution (a.k.a. CNN), and 3) LSTM. At the label side, the label item as positive and all other items in the vocabulary as negatives will be encoded to vectors as well. Context and label embeddings are combined with a dot product and fed to the loss of softmax cross entropy.\nInside the framework, we encapsulate tf.keras layers for ContextEncoder, LabelEncoder and DotProductSimilarity as key components in RecommendationModel.\nTo model each user activity, we could use the ID of the activity item (called ID-based), or multiple features of the item (called feature-based), or a combination of both. The feature-based model utilizing multiple features to collectively encode users\u2019 behavior. With our framework, you could create either ID-based or feature-based models in a configurable way.\nSimilar to the last version, a TensorFlow Lite model will be exported after training which can directly provide top-K predictions among the recommendation candidates.\nStep-by-step\nTo demonstrate the new adaptive framework, we trained a on-device movie recommendation model with MovieLens dataset using multiple features, and integrated it in the demo app. (Both the model and the app are for demonstration purposes only.) The MovieLens 1M dataset contains ratings from 6039 users across 3951 movies, with each user rating only a small subset of movies.\nLet\u2019s look at how to use the framework step-by-step in this notebook.\n(a) Environment preparation\ngit clone https://github.com/tensorflow/examples\ncd examples/lite/examples/recommendation/ml/\npip install -r requirements.txt\n(b) Prepare training data\nPlease prepare your training data reference to the movielens example generation file. Would like to note that TensorFlow Lite input features are expected to be FixedLenFeature, please pad or truncate your features, and set up feature lengths in input configuration. Feel free to use the following command to process the example dataset.\npython -m data.example_generation_movielens \\\n --data_dir=data/raw \\\n --output_dir=data/examples \\\n --min_timeline_length=3 \\\n --max_context_length=10 \\\n --max_context_movie_genre_length=32 \\\n --min_rating=2 \\\n --train_data_fraction=0.9 \\\n --build_vocabs=True\nMovieLens data contains ratings.dat (columns: UserID, MovieID, Rating, Timestamp), and movies.dat (columns: MovieID, Title, Genres). The example generation script takes both files, only keep ratings higher than 2, form user movie interaction timelines, sample activities as labels and previous user activities as the context for prediction. Please find the generated tf.Example:\n0 : {\n  features: {\n    feature: {\n      key  : \"context_movie_id\"\n      value: { int64_list: { value: [ 1124, 2240, 3251, ..., 1268 ] } }\n    }\n    feature: {\n      key  : \"context_movie_rating\"\n      value: { float_list: {value: [ 3.0, 3.0, 4.0, ..., 3.0 ] } }\n    }\n    feature: {\n      key  : \"context_movie_year\"\n      value: { int64_list: { value: [ 1981, 1980, 1985, ..., 1990 ] } }\n    }\n    feature: {\n      key  : \"context_movie_id\"\n      value: { int64_list: { value: [ 1124, 2240, 3251, ..., 1268 ] } }\n    }\n    feature: {\n      key  : \"context_movie_genre\"\n      value: { bytes_list: { value: [ \"Drama\", \"Drama\", \"Mystery\", ..., \"UNK\" ] } }\n    }\n    feature: {\n      key  : \"label_movie_id\"\n      value: { int64_list: { value: [ 3252 ] }  }\n    }\n  }\n}\n(c) Create input config\nOnce data prepared, please set up input configuration, e.g. this is one example configuration for movielens movie recommendation model.\nactivity_feature_groups {\n  features {\n    feature_name: \"context_movie_id\"\n    feature_type: INT\n    vocab_size: 3953\n    embedding_dim: 8\n    feature_length: 10\n  }\n  features {\n    feature_name: \"context_movie_rating\"\n    feature_type: FLOAT\n    feature_length: 10\n  }\n  encoder_type: CNN\n}\nactivity_feature_groups {\n  features {\n    feature_name: \"context_movie_genre\"\n    feature_type: STRING\n    vocab_name: \"movie_genre_vocab.txt\"\n    vocab_size: 19\n    embedding_dim: 4\n    feature_length: 32\n  }\n  encoder_type: CNN\n}\nlabel_feature {\n  feature_name: \"label_movie_id\"\n  feature_type: INT\n  vocab_size: 3953\n  embedding_dim: 8\n  feature_length: 1\n}\n(d) Train model\nThe model trainer will construct the recommendation model based on the input config, with a simple interface.\npython -m model.recommendation_model_launcher -- \\\n --training_data_filepattern \"data/examples/train_movielens_1m.tfrecord\" \\\n --testing_data_filepattern \"data/examples/test_movielens_1m.tfrecord\"\\\n --model_dir \"model/model_dir\" \\\n --vocab_dir \"data/examples\" \\\n --input_config_file \"configs/sample_input_config.pbtxt\" \\\n --batch_size 32 \\\n --learning_rate 0.01 \\\n --steps_per_epoch 2 \\\n --num_epochs 2 \\\n --num_eval_steps 2 \\\n --run_mode \"train_and_eval\" \\\n --gradient_clip_norm 1.0 \\\n --num_predictions 10 \\\n --hidden_layer_dims \"32,32\" \\\n --eval_top_k \"1,5\" \\\n --conv_num_filter_ratios \"2,4\" \\\n --conv_kernel_size 4 \\\n --lstm_num_units 16\nInside the recommendation model, core components are packaged up to keras layers (context_encoder.py, label_encoder.py and dotproduct_similarity.py), each of which could be utilized by itself. The following diagram illustrates the code structure:\nFigure 2: An example of model architecture using context information to predict the next movie. The inputs are the history of (a) movie IDs, (b) ratings and (c) genres, which is specified by the config mentioned above.\nWith the framework, you can directly execute the model training launcher with command:\npython -m model.recommendation_model_launcher \\\n --input_config_file \"configs/sample_input_config.pbtxt\" \\\n --vocab_dir \"data/examples\" \\\n --run_mode \"export\" \\\n --checkpoint_path \"model/model_dir/ckpt-1000\" \\\n --num_predictions 10 \\\n --hidden_layer_dims \"32,32\" \\\n --conv_num_filter_ratios \"2,4\" \\\n --conv_kernel_size 4 \\\n --lstm_num_units 16\nThe inference code after exporting to TensorFlow Lite can be found in the notebook, and we refer readers to check out the details there.\nFramework Adaptivity\nOur framework provides a protobuf interface, through which feature groups, types and other information can be configured to build models accordingly. With the interface, you can configure:\nFeatures\nThe framework generically categorizes features into 3 types: integer, string, and float. Embedding spaces will be created for both integer and string features, hence, embedding dimension, vocabulary name and size need to be specified. Float feature values will be directly used. Besides, for on-device models, we suggest to use fixed length features which can be configured directly.\nmessage Feature {\n  optional string feature_name = 1;\n\n  // Supported feature types: STRING, INT, FLOAT.\n  optional FeatureType feature_type = 2;\n\n  optional string vocab_name = 3;\n\n  optional int64 vocab_size = 4;\n\n  optional int64 embedding_dim = 5;\n\n  optional int64 feature_length = 6;\n}\nFeature groups\nOne feature for one user activity may have multiple values. For instance, one movie can belong to multiple categories, each movie will have multiple genre feature values. To handle the different feature shapes, we introduced the \u201cfeature group\u201d to combine features as a group . The features with the same length can be put in the same feature group to be encoded together. Inside input config, you can set up global feature groups and activity feature groups.\nmessage FeatureGroup {\n  repeated Feature features = 1;\n\n  // Supported encoder types: BOW, CNN, LSTM.\n  optional EncoderType encoder_type = 2;\n}\nInput config\nYou can use the input config interface to set up all the features and feature groups together.\nmessage InputConfig {\n  repeated FeatureGroup global_feature_groups = 1;\n\n  repeated FeatureGroup activity_feature_groups = 2;\n\n  optional Feature label_feature = 3;\n}\nThe input config is utilized by both input_pipeline.py and recommendation_model.py to process training data to tf.data.Dataset and construct the model accordingly. Inside ContexEncoder, FeatureGroupEncoders will be created for all feature groups, and used to compute feature group embeddings from input features. Concatenated feature group embeddings will be fed through top hidden layers to get the final context embedding. Worth noting that the final context embedding and label embedding dimensions should be equal.\nPlease check out the different model graphs produced with the different input configurations in the Appendix section.\nExperiments and Analysis\nWe take this opportunity to analyze the performance of ID-based and feature-based models with various configurations, and provide some empirical results.\nFor the ID-based model, only movie_id is used as the input feature. And for the feature-based model, both movie_id and movie_genre features are used. Both types of models are experimented with 3 encoder types (BOW/CNN/LSTM) and 3 context history lengths (10/50/100).\nComparison between ID-based and Feature-based models. We compare them on BOW/CNN/LSTM encoders and context history lengths 10/50/100.\nSince MovieLens dataset is an experimental dataset with ~4000 candidate movies and 19 movie genres, hence we scaled down embedding dimensions in the experiments to simulate the production scenario. For the above experiment result chart, ID embedding dimension is set to 8, and movie genre embedding dimension is set to 4. If we take the context10_cnn as an example, the feature-based model outperforms the ID-based model by 58.6%. Furthermore, the on average results show that feature-based models outperforms by 48.35%. Therefore, In this case, the feature-based model outperforms the ID-based model, because movie_genre feature introduces additional information to the model.\nBesides, underlying features of candidate items mostly have a smaller vocabulary size, hence smaller embedding spaces as well. For instance,the movie genre vocabulary is much smaller than the movie ID vocabulary. In this case, utilizing underlying features could reduce the memory size of the model, which is more on-device friendly.\nAcknowledgement\nSpecial thanks to Cong Li, Josh Gordon, Khanh LeViet, Arun Venkatesan and Lawrence Chan for providing valuable suggestions to this work.",
    "link": "https://blog.tensorflow.org/2021/04/adaptive-framework-for-on-device-recommendation.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-uY-8BS4p9b8/YIsQND_h8cI/AAAAAAAAEJE/xgCrN_6t30YQ4Bwy8nW5ZLlvrogs-zW9ACLcBGAsYHQ/s0/feature-based_model.jpeg",
      "https://1.bp.blogspot.com/-RPFuuA88vpU/YInAyCfiHiI/AAAAAAAAEIk/4rKGnVMC5nwqzKGhiN1Vcui0bM3AI0algCLcBGAsYHQ/s0/reco%2Bcode%2Bstructure.jpg",
      "https://1.bp.blogspot.com/-vADglbJmaw4/YInCma-3p0I/AAAAAAAAEIs/aDeSNuSBvHwGqBp_jVe0nEzM_ADfHqFLgCLcBGAsYHQ/s0/pasted%2Bimage%2B0%2B%25284%2529.png",
      "https://1.bp.blogspot.com/-KIeFKVlgA7I/YInEHtY7jRI/AAAAAAAAEI0/bRsetRpUpWUZ53r_Sxg2akhZ4RzlVC_XACLcBGAsYHQ/s0/chart%2B%25281%2529.png"
    ],
    "time": "2023/12/04 00:58:43"
  },
  {
    "title": "Accelerated inference on Arm microcontrollers with TensorFlow Lite for Microcontrollers and CMSIS-NN",
    "content": "A guest post by Fredrik Knutsson of Arm\n\nThe MCU universe\nMicrocontrollers (MCUs) are the tiny computers that power our technological environment. There are over 30 billion of them manufactured every year, embedded in everything from household appliances to fitness trackers. If you\u2019re in a house right now, there are dozens of microcontrollers all around you. If you drive a car, there are dozens riding with you on every drive. Using TensorFlow Lite for Microcontrollers (TFLM), developers can deploy TensorFlow models to many of these devices, enabling entirely new forms of on-device intelligence.\nWhile ubiquitous, microcontrollers are designed to be inexpensive and energy efficient, which means they have small amounts of memory and limited processing power. A typical microcontroller might have a few hundred kilobytes of RAM, and a 32-bit processor running at less than 100 MHz. With advances in machine learning enabled by TFLM, it has become possible to run neural networks on these devices.\nWith minimal computational resources, it is important that microcontroller programs are optimized to run as efficiently as possible. This means making the most of the features of their microprocessor hardware, which requires carefully tuned application code.\nMany of the microcontrollers used in popular products are built around Arm\u2019s Cortex-M based processors, which are the industry leader in 32-bit microcontrollers, with more than 47 billion shipped. Arm\u2019s open source CMSIS-NN library provides optimized implementations of common neural network functions that maximize performance on Cortex-M processors. This includes making use of DSP and M-Profile Vector Extension (MVE) instructions for hardware acceleration of operations such as matrix multiplication.\nBenchmarks for key use cases\nArm\u2019s engineers have worked closely with the TensorFlow team to develop optimized versions of the TensorFlow Lite kernels that use CMSIS-NN to deliver blazing fast performance on Arm Cortex-M cores. Developers using TensorFlow Lite can use these optimized kernels with no additional work, just by using the latest version of the library. Arm has made these optimizations in open source, and they are free and easy for developers to use today!\nThe following benchmarks show the performance uplift when using CMSIS-NN optimized kernels versus reference kernels for several key use cases featured in the TFLM example applications. The tests have been performed on an Arm Cortex-M4 based FPGA platform:\nThe Arm Cortex-M4 processor supports DSP extensions, that enables the processor to execute DSP-like instructions for faster inference. To improve the inference performance even further, the new Arm Cortex-M55 processor supports MVE, also known as Helium technology.\nImproving performance with CMSIS-NN\nSo far, the following optimized CMSIS-NN kernels have been integrated with TFLM:\nThere will be regular updates to the CMSIS-NN library to expand the support of optimized kernels, where the key driver for improving support is that it should give a significant performance increase for a given use case. For discussion regarding kernel optimizations, a good starting point is to raise a ticket on the TensorFlow or CMSIS Github repository describing your use case.\nMost of the optimizations are implemented specifically for 8-bit quantized (int8) operations, and this will be the focus of future improvements.\nIt\u2019s easy to try the optimized kernels yourself by following the instructions that accompany the examples. For example, to build the person detection example for the SparkFun Edge with CMSIS-NN kernels, you can use the following command:\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=sparkfun_edge OPTIMIZED_KERNEL_DIR=cmsis_nn person_detection_int8_bin\nThe latest version of the TensorFlow Lite Arduino library includes the CMSIS-NN optimizations, and includes all of the example applications, which are compatible with the Cortex-M4 based Arduino Nano 33 BLE Sense.\nNext leap in neural processing\nLooking ahead into 2021 we can expect a dramatic increase in neural processing from the introduction of devices including a microNPU (Neural Processing Unit) working alongside a microcontroller. These microNPUs are designed to accelerate ML inference within the constraints of embedded and IoT devices, with devices using the Arm Cortex-M55 MCU coupled with the new Ethos-U55 microNPU delivering up to a 480x performance increase compared to previous microcontrollers.\nThis unprecedented level of ML processing capability within smaller, power constrained devices will unlock a huge amount of innovation across a range of applications, from smart homes and cities to industrial, retail, and healthcare. The potential for innovation within each of these different areas is huge, with hundreds of sub segments and thousands of potential applications that will make a real difference to people\u2019s lives.",
    "link": "https://blog.tensorflow.org/2021/02/accelerated-inference-on-arm-microcontrollers-with-tensorflow-lite.html",
    "imgSource": [
      "https://1.bp.blogspot.com/--hE_UFwKPCs/YCG_W47amDI/AAAAAAAAD9c/FarmUarBfyMu1W8N71S4XIqLS_9Geqb8gCLcBGAsYHQ/s0/arm_logo.jpeg",
      "https://1.bp.blogspot.com/--6WS0iSyHPs/YB2Q08ivXgI/AAAAAAAAD8w/qhci8ezYOLIGHcQiadNp2kEt0gMxkVDbwCLcBGAsYHQ/s0/arm%2Blogo.png",
      "https://1.bp.blogspot.com/-vouE2QUxsEQ/YCG9doRvYhI/AAAAAAAAD9A/q4JdVenKa0MNXldbr7iJRcyTmsO6pkgZgCLcBGAsYHQ/s0/Arm%2BImage%2B1.jpg",
      "https://1.bp.blogspot.com/-5aAZH2ieY0Y/YCG95eRJrDI/AAAAAAAAD9I/G-j83Vbp8_sKjeipdAFdqHx-4YnJMJCYgCLcBGAsYHQ/s0/Arm%2BImage%2B2.png",
      "https://1.bp.blogspot.com/--t-vyiFNvZw/YCG-RfcSnYI/AAAAAAAAD9Q/Sv2JVor3AwYVht2WowWB9cIjpAhk0uSegCLcBGAsYHQ/s0/Arm%2BImage%2B3.png"
    ],
    "time": "2023/12/04 00:58:43"
  },
  {
    "title": "How to generate super resolution images using TensorFlow Lite on Android",
    "content": "Posted by Wei Wei, TensorFlow Developer Advocate\n\nThe task of recovering a high resolution (HR) image from its low resolution counterpart is commonly referred to as Single Image Super Resolution (SISR). While interpolation methods, such as bilinear or cubic interpolation, can be used to upsample low resolution images, the quality of resulting images is generally less appealing. Deep learning, especially Generative Adversarial Networks, has successfully been applied to generate more photo-realistic images, for example, SRGAN and ESRGAN. In this blog, we are going to use a pre-trained ESRGAN model from TensorFlow Hub and generate super resolution images using TensorFlow Lite in an Android app. The final app looks like below and the complete code has been released in TensorFlow examples repo for reference.\nFirst, we can conveniently load the ESRGAN model from TFHub and easily convert it to a TFLite model. Note that here we are using dynamic range quantization and fixing the input image dimensions to 50x50. The converted model has been uploaded to TFHub but we want to demonstrate how to do it just in case you want to convert it yourself (for example, try a different input size in your own app):\nmodel = hub.load(\"https://tfhub.dev/captain-pool/esrgan-tf2/1\")\nconcrete_func = model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\nconcrete_func.inputs[0].set_shape([1, 50, 50, 3])\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\ntflite_model = converter.convert()\n\n# Save the TF Lite model.\nwith tf.io.gfile.GFile('ESRGAN.tflite', 'wb') as f:\n  f.write(tflite_model)\n\nesrgan_model_path = './ESRGAN.tflite'\nYou can also convert the model without hardcoding the input dimensions at conversion time and later resize the input tensor at runtime, as TFLite now supports inputs of dynamic shapes. Please refer to this example for more information.\nOnce the model is converted, we can quickly verify that the ESRGAN TFLite model does generate a much better image than bicubic interpolation. We also have another tutorial on ESRGAN if you want to better understand the model.\nlr = cv2.imread(test_img_path)\nlr = cv2.cvtColor(lr, cv2.COLOR_BGR2RGB)\nlr = tf.expand_dims(lr, axis=0)\nlr = tf.cast(lr, tf.float32)\n\n# Load TFLite model and allocate tensors.\ninterpreter = tf.lite.Interpreter(model_path=esrgan_model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output tensors.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Run the model\ninterpreter.set_tensor(input_details[0]['index'], lr)\ninterpreter.invoke()\n\n# Extract the output and postprocess it\noutput_data = interpreter.get_tensor(output_details[0]['index'])\nsr = tf.squeeze(output_data, axis=0)\nsr = tf.clip_by_value(sr, 0, 255)\nsr = tf.round(sr)\nsr = tf.cast(sr, tf.uint8)\nLR: low resolution input image cropped from an butterfly image in DIV2K dataset. ESRGAN (x4): super resolution output image generated using ESRGAN model with upscale_ratio=4. Bicubic: output image generated using bicubic interpolation. As can be seen here, bicubic interpolation-generated image is much blurrier than the ESRGAN-generated one. PSNR is also higher on the ESRGAN-generated image.\nAs you may already know, TensorFlow Lite is the official framework to run inference with TensorFlow models on edge devices and is deployed on more than 4 billions edge devices worldwide, supporting Android, iOS, Linux-based IoT devices and microcontrollers. You can use TFLite in Java, C/C++ or other languages to build Android apps. In this blog we will use TFLite C API since many developers have asked for such an example.\nWe distribute TFLite C header files and libraries in prebuilt AAR files (the core library and the GPU library). To set up the Android build correctly, the first thing we need to do is download the AAR files and extract the header files and shared libraries. Have a look at how this is done in the download.gradle file.\nSince we are using Android NDK to build the app (NDK r20 is confirmed to work), we need to let Android Studio know how native files should be handled. This is done in CMakeList.txt file:\nWe included 3 sample images in the app, so a user may easily run the same model multiple times, which means we need to cache the interpreter for better efficiency. This is done by passing the interpreter pointer from C++ code to Java code, after the interpreter is successfully created:\nextern \"C\" JNIEXPORT jlong JNICALL\nJava_org_tensorflow_lite_examples_superresolution_MainActivity_initWithByteBufferFromJNI(JNIEnv *env, jobject thiz, jobject model_buffer, jboolean use_gpu) {\n  const void *model_data = static_cast<void *>(env->GetDirectBufferAddress(model_buffer));\n  jlong model_size_bytes = env->GetDirectBufferCapacity(model_buffer);\n  SuperResolution *super_resolution = new SuperResolution(model_data, static_cast<size_t>(model_size_bytes), use_gpu);\n  if (super_resolution->IsInterpreterCreated()) {\n    LOGI(\"Interpreter is created successfully\");\n    return reinterpret_cast<jlong>(super_resolution);\n   } else {\n    delete super_resolution;\n    return 0;\n  }\n}\nOnce the interpreter is created, running the model is fairly straightforward as we can follow the TFLite C API documentation. We first need to carefully extract RGB values from each pixel. Now we can run the interpreter:\n// Feed input into model\nstatus = TfLiteTensorCopyFromBuffer(input_tensor, input_buffer, kNumberOfInputPixels * kImageChannels * sizeof(float));\n\u2026...\n\n// Run the interpreter\nstatus = TfLiteInterpreterInvoke(interpreter_);\n\u2026...\n\n// Extract the output tensor data\nconst TfLiteTensor* output_tensor = TfLiteInterpreterGetOutputTensor(interpreter_, 0);\nfloat output_buffer[kNumberOfOutputPixels * kImageChannels];\nstatus = TfLiteTensorCopyToBuffer(output_tensor, output_buffer, kNumberOfOutputPixels * kImageChannels * sizeof(float));\n\u2026...\nWith the models results at hand, we can pack the RGB values back into each pixel.\nThere you have it. A reference Android app using TFLite to generate super resolution images on device. More details can be found in the code repository. Hopefully this is useful as a reference to Android developers who are getting started to use TFLite in C/C++ to build amazing ML apps.\nFeedback\nWe are looking forward to seeing what you have built with TensorFlow Lite, as well as hearing your feedback. Share your use cases with us directly or on Twitter with hashtags #TFLite and #PoweredByTF. To report bugs and issues, please reach out to us on GitHub.\nAcknowledgements\nThe author would like to thank @captain__pool for uploading the ESRGAN model to TFHub, and Tian Lin and Jared Duke for the helpful feedback.\n[1] Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi. 2016. Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network.\n[2] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Chen Change Loy, Yu Qiao, Xiaoou Tang. 2018. ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks.\n[3] Tensorflow 2.x based implementation of EDSR, WDSR and SRGAN for single image super-resolution\n[4] @captain__pool\u2019s ESGRAN code implementation\n[5] Eirikur Agustsson, Radu Timofte. 2017. NTIRE 2017 Challenge on Single Image Super-Resolution: Dataset and Study.",
    "link": "https://blog.tensorflow.org/2020/12/how-to-generate-super-resolution-images-using-tensorflow-lite-on-android.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-obzaAHxrRrM/X9qQnEzbD9I/AAAAAAAAD5A/vMo0W3qCTo40x8tO2gZfqyPHuLFSAJWOQCLcBGAsYHQ/s0/unnamed%2B%25281%2529.png",
      "https://1.bp.blogspot.com/-aYoM9OpirIY/X9qOm0aJCnI/AAAAAAAAD40/gfAxU2oBOZUs6xBuidxikhQtOVvsMs8ZACLcBGAsYHQ/s0/screencap.gif",
      "https://1.bp.blogspot.com/-obzaAHxrRrM/X9qQnEzbD9I/AAAAAAAAD5A/vMo0W3qCTo40x8tO2gZfqyPHuLFSAJWOQCLcBGAsYHQ/s0/unnamed%2B%25281%2529.png"
    ],
    "time": "2023/12/03 00:58:43"
  },
  {
    "title": "How CEVA uses TensorFlow Lite for Always-On Speech Recognition on the Edge",
    "content": "A guest article by Ido Gus of CEVA\nCEVA is a leading licensor of wireless connectivity and smart sensing technologies. Our products help OEMs design power-efficient, intelligent and connected devices for a range of end markets, including mobile, consumer, automotive, robotics, industrial and IoT.\nIn this article, we'll describe how we used TensorFlow Lite for Microcontrollers (TFLM) to deploy a speech recognition engine and frontend, called WhisPro, on a bare-metal development board based on our CEVA-BX DSP core. WhisPro detects always-on wake words and speech commands efficiently, on-device.\nFigure 1 CEVA Multi-microphone DSP Development Board\nAbout WhisPro\nWhisPro is a speech recognition engine and frontend targeted to run on low power, resource constrained edge devices. It is designed to handle the entire data flow from processing audio samples to detection.\nWhisPro supports two use cases for edge devices:\nAlways-on wake word detection engine. In this use case, WhisPro\u2019s role is to wake a device in sleep mode when a predefined phrase is detected.\nSpeech commands. In this use case, WhisPro\u2019s role is to enable a voice-based interface. Users can control the device using their voice. Typical commands can be: volume up, volume down, play, stop, etc.\nWhisPro enables voice interface on any SoC that has a CEVA BX DSP core integrated into it, lowering entry barriers to OEMs and ODM interested in joining the voice interface revolution.\nOur Motivation\nOriginally, WhisPro was implemented using an in-house neural network library called CEVA NN Lib. Although that implementation achieved excellent performance, the development process was quite involved. We realized that, if we ported the TFLM runtime library and optimized it for our target hardware, the entire model porting process would become transparent and more reliable (far fewer lines of code would need to be written, modified, and maintained).\nBuilding TFLM for CEVA-BX DSP Family\nThe first thing we had to do is to figure out how to port TFLM to our own platform. We found that following this porting to a new platform guide to be quite useful.\nFollowing the guide, we:\nVerified DebugLog() implementation is supported by our platform.\nCreated a TFLM runtime library project in CEVA\u2019s Eclipse-based IDE:\nCreated a new CEVA-BX project in CEVA\u2019s IDE\nAdded all the required source files to the project\nBuilt the TFLM runtime library for the CEVA-BX core.\nThis required the usual fiddling with compiler flags, including paths (not all required files are under the \u201cmicro\u201d directory), linker script, and so on.\nModel Porting Process\nOur starting point is a Keras implementation of our model. Let\u2019s look at the steps we took to deploy our model on our bare-metal target hardware:\nConverted theTensorFlow model to TensorFlow Lite using the TF built-in converter:\n$ python3 -m tensorflow_docs.tools.nbfmt [options] notebook.ipynb\n```\nconverter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\nconverter.experimental_new_converter = True\ntflite_model = converter.convert()\nopen(\"converted_to_tflite_model.tflite\", \"wb\").write(tflite_model)\n```\nUsed quantization:\n$ python3 -m tensorflow_docs.tools.nbfmt [options] notebook.ipynb\n\n```\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.representative_dataset = representative_data_gen\n```\nConverted the TensorFlow Lite model to TFLM using xxd:\n$ python3 -m tensorflow_docs.tools.nbfmt [options] notebook.ipynb\n  \n\n```\n$> xxd \u2013I model.tflite > model.cc\n```\n\nHere we found that some of the model layers (for example, GRU) were not properly supported (at the time) by TFLM. It is very reasonable to assume that, as TFLM continues to mature and Google and the TFLM community invest more in it, issues like this will become rarer. \nIn our case, though, we opted to re-implement the GRU layers in terms of Fully Connected layers, which was surprisingly easy. \nIntegration\nThe next step was to integrate the TFLM runtime library and the converted model into our existing embedded C frontend, which handles audio preprocessing and feature extraction.\nEven though our frontend was not written with TFLM in mind, it was modular enough to allow easy integration by implementation of a single simple wrapper function, as follows:\nLinked the TFLM runtime library into our embedded C application (WhisPro frontend)\nImplemented a wrapper-over-setup function for mapping the model into a usable data structure, allocating the interpreter and tensors\nImplemented a wrapper-over-execute function for mapping data passed from the WhisPro frontend into tflite tensors used by the actual execute function\nReplaced the call to the original model execute function with a call to the TFLM implementation\nProcess Visualization\nThe process we described is performed by two components:\nThe microcontroller supplier, in this case, CEVA \u2013 is responsible for optimizing TFLM for its hardware architecture.\nThe microcontroller user, in this case, CEVA WhisPro developer \u2013 is responsible for deploying a neural network based model, using an optimized TFLM runtime library, on the target microcontroller.\nWhat\u2019s Next\nThis work has proven the importance of the TFLM platform to us, and the significant value supporting TFLM can add to our customers and partners by enabling easy neural network model deployment on edge devices. We are committed to further support TFLM on the CEVA-BX DSP family by:\nActive contribution to the TFLM project, with the goal of improving layer coverage and overall platform maturity.\nInvesting in TFLM operator optimization for execution on CEVA-BX cores, aiming for full coverage.\nFinal Thoughts\nWhile the porting process had some bumps along the way, at the end it was a great success, and took about 4-5 days' worth of work. Implementing a model in C from scratch, and handcrafting model conversion scripts from Python to C, could take 2-3 weeks (and lots of debugging).\nCEVA Technology Virtual Seminar\nTo learn more, you are welcome to watch CEVA\u2019s virtual seminar - Wireless Audio session, covering TFLM, amongst other topics.",
    "link": "https://blog.tensorflow.org/2020/10/how-ceva-uses-tensorflow-lite-for.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-AQPbkSaN_BU/X4ioWLTjZwI/AAAAAAAADp8/NtS7ce8XougXhuZQi7XHErWdavGtuC3SACLcBGAsYHQ/s0/image2.jpg",
      "https://1.bp.blogspot.com/-tcRP9bFlXnQ/X4ipJiJxF3I/AAAAAAAADqE/STVwCll0UHAFNct75F6pjGn1ZuiQ__DcgCLcBGAsYHQ/s0/unnamed%2B%25282%2529.png"
    ],
    "time": "2023/12/03 00:58:43"
  },
  {
    "title": "Optimizing TensorFlow Lite Runtime Memory",
    "content": "Posted by Juhyun Lee and Yury Pisarchyk, Software Engineers\nRunning inference on mobile and embedded devices is challenging due to tight resource constraints; one has to work with limited hardware under strict power requirements. In this article, we want to showcase improvements in TensorFlow Lite's (TFLite) memory usage that make it even better for running inference at the edge.\nIntermediate Tensors\nTypically, a neural network can be thought of as a computational graph consisting of operators, such as CONV_2D or FULLY_CONNECTED, and tensors holding the intermediate computation results, called intermediate tensors. These intermediate tensors are typically pre-allocated to reduce the inference latency at the cost of memory space. However, this cost, when implemented naively, can't be taken lightly in a resource-constrained environment; it can take up a significant amount of space, sometimes even several times larger than the model itself. For example, the intermediate tensors in MobileNet v2 take up 26MB memory (Figure 1) which is about twice as large as the model itself.\nFigure 1. The intermediate tensors of MobileNet v2 (top) and a mapping of their sizes onto a 2D memory space (bottom). If each intermediate tensor uses a dedicated memory buffer (depicted with 65 distinct colors), they take up ~26MB of runtime memory.\nThe good news is that these intermediate tensors don't have to co-exist in memory thanks to data dependency analysis. This allows us to reuse the memory buffers of the intermediate tensors and reduce the total memory footprint of the inference engine. If the network has the shape of a simple chain, two large memory buffers are sufficient as they can be swapped back and forth interchangeably throughout the network. However, for arbitrary networks forming complicated graphs, this NP-complete resource allocation problem requires a good approximation algorithm.\nWe have devised a number of different approximation algorithms for this problem, and they all perform differently depending on the neural network and the properties of memory buffers, but they all use one thing in common: tensor usage records. A tensor usage record of an intermediate tensor is an auxiliary data structure that contains information about how big the tensor is and when it is used for the first and the last time in a given execution plan of the network. With the help of these records, the memory manager is able to compute the intermediate tensor usages at any moment in the network's execution and optimize its runtime memory for the smallest footprint possible.\nShared Memory Buffer Objects\nIn TFLite GPU OpenGL backend, we employ GL textures for these intermediate tensors. These come with a couple of interesting restrictions: (a) A texture's size can't be modified after its creation, and (b) only one shader program gets exclusive access to the texture object at a given time. In this Shared Memory Buffer Objects mode, the objective is to minimize the sum of the sizes of all created shared memory buffer objects in the object pool. This optimization is similar to the well-known register allocation problem, except that it's much more complicated due to the variable size of each object.\nWith the aforementioned tensor usage records, we have devised 5 different algorithms as shown in Table 1. Except for Min-Cost Flow, they are greedy algorithms, each using a different heuristic, but still reaching or getting very close to the theoretical lower bound. Some algorithms perform better than others depending on the network topology, but in general, GREEDY_BY_SIZE_IMPROVED and GREEDY_BY_BREADTH produce the object assignments with the smallest memory footprint.\nTable 1. Memory footprint of Shared Objects strategies (in MB; best results highlighted in green). The first 5 rows are our strategies, and the last 2 serve as a baseline (Lower Bound denotes an approximation of the best number possible which may not be achievable, and Naive denotes the worst number possible with each intermediate tensor assigned its own memory buffer).\nComing back to our opening example, GREEDY_BY_BREADTH performs best on MobileNet v2 which leverages each operator's breadth, i.e. the sum of all tensors in the operator's profile. Figure 2, especially when compared to Figure 1, highlights how big of a gain one can get when employing a smart memory manager.\nFigure 2. The intermediate tensors of MobileNet v2 (top) and a mapping of their sizes onto a 2D memory space (bottom). If the intermediate tensors share memory buffers (depicted with 4 distinct colors), they only take up ~7MB of runtime memory.\nMemory Offset Calculation\nFor TFLite running on the CPU, the memory buffer properties applicable to GL textures don't apply. Thus, it is more common to allocate a huge memory arena upfront and have it shared among all readers and writers which access it by a given offset that does not interfere with other read and writes. The objective in this Memory Offset Calculation approach is to minimize the size of the memory arena.\nWe have devised 3 different algorithms for this optimization problem and have also explored prior work (Strip Packing by Sekiyama et al. 2018). Similar to the Shared Objects approach, some algorithms perform better than others depending on the network as shown in Table 2. One takeaway from this investigation is that the Offset Calculation approach has a smaller footprint than the Shared Objects approach in general, and thus, one should prefer the former over the latter if applicable.\nTable 2. Memory footprint of Offset Calculation strategies (in MB; best results highlighted in green). The first 3 rows are our strategies, the next 1 is prior work, and the last 2 serve as baseline (Lower Bound denotes an approximation of the best number possible which may not be achievable, and Naive denotes the worst number possible with each intermediate tensor assigned its own memory buffer).\nThese memory optimizations, for both CPU and GPU, have shipped by default with the last few stable TFLite releases, and have proven valuable in supporting more demanding, state-of-the-art models like MobileBERT. You can find more details about the implementation by looking at the GPU implementation and CPU implementation directly.\nAcknowledgements\nMatthias Grundmann, Jared Duke, Sarah Sirajuddin, and special thanks to Andrei Kulik for initial brainstorming and Terry Heo for the final implementation in TFLite.",
    "link": "https://blog.tensorflow.org/2020/10/optimizing-tensorflow-lite-runtime.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-eZ-pDCwizjo/X3Z7CV83t7I/AAAAAAAADoA/9CsVRyIRYwseS8PofJaEQKe8_Q72M5FYQCLcBGAsYHQ/s0/fig1.png",
      "https://1.bp.blogspot.com/-j8kp4Tw3d_4/X3Z7eXy0jAI/AAAAAAAADoI/CjBBofKf4e8R2FtM8Mgt6lR4z6ZBbJTsACLcBGAsYHQ/s0/table1.png",
      "https://1.bp.blogspot.com/-DIyo4TLPeSo/X3Z7mUNc8zI/AAAAAAAADoM/-kiCGKXxa5sAWUHsHXDEq-fSBLM5DVRlACLcBGAsYHQ/s0/fig2.png",
      "https://1.bp.blogspot.com/-rnJ6BYpBFCs/X3Z7w85IP3I/AAAAAAAADoU/pPwLTFvQc0ovXJiKjw7-mGhTVQcV62QsgCLcBGAsYHQ/s0/table2.png"
    ],
    "time": "2023/12/03 00:58:43"
  },
  {
    "title": "What\u2019s new in TensorFlow Lite for NLP",
    "content": "Posted by Tian Lin, Yicheng Fan, Jaesung Chung and Chen Cen\n\nTensorFlow Lite has been widely adopted in many applications to provide machine learning features on edge devices such as mobile phones, microcontroller units, and Edge TPUs. Among all popular applications that make people\u2019s life easier and more productive, Natural Language Understanding is one of the key areas that attracts much attention from both the research community and the industry. After the demo of the on-device question-answering use case at TensorFlow World in 2019, we got a lot of interest and feedback from the community on making more such NLP models available for on-device inference.\n\nInspired by that feedback, today we are delighted to announce an end-to-end support for NLP tasks based on TensorFlow Lite. With this infrastructure work, more and more NLP models are able to run on mobile phones, and users can enjoy the advantage of NLP models, while keeping their personal data on-device. In this blog, we will introduce the new features that allow: (1) Using new pre-trained NLP Models, (2) Creating your own NLP models, (3) Better support for converting TensorFlow NLP Models to TensorFlow Lite format and (4) Deploying these models on mobile devices.\nUsing new pre-trained NLP models\nReference apps\nReference apps are a set of open-source mobile applications that encapsulate pretrained machine learning models, inference code and runnable demos. We provide a series of NLP reference apps that are integrated with Android Studio and XCode, so developers can build with just one click and deploy on Android or iOS phones.\n\nUsing the NLP reference apps below, mobile developers can learn the end to end flow of integrating existing NLP models (powered by BERT, MobileBERT or ALBERT), transforming raw text data, and connecting the model\u2019s inputs and outputs to generate prediction results,\nText classification: The model predicts labels based on given text data.\nQuestion answering app: Given an article and a user question, the model can answer the question within the article.\nSmart reply app: Given previous context, the model can predict and generate potential auto replies.\nThe pretrained models used in the above reference apps are available in TensorFlow Hub. The chart below shows a comparison of the latency, size and F1 score between the models.\nBenchmark on Pixel 4 CPU, 4 Threads, March 2020\nModel hyper parameters: Sequence length 128, Vocab size 30K\nOptimizing NLP Models for on-device use cases\nOn-device models have different constraints compared to server-side models. They run on devices with less memories and slower chips and hence need to be optimized for size and inference speed.Here are several examples of how we optimize models for NLP tasks.\nQuantized MobileBERT\nMobileBERT is a compact BERT model open sourced on GitHub. It is 4.3x smaller & 5.5x faster than BERT base (float32) while achieving comparable results on GLUE and SQuAD datasets. \\\n\nAfter the initial release, we further improved the model by using quantization to optimize its model size and performance, so that it can utilize accelerators like GPU/DSP if available. The quantized MobileBERT is 16x smaller & 8x faster than the BERT base, with little accuracy loss. The MLPerf for Mobile community is leveraging the quantized MobileBERT model for mobile inference benchmarking, and the model can also run in Chrome using TensorFlow.js.\n\nCompared with the original BERT base model (416MB), the below table shows the performance of quantized MobileBERT under the same setting.\nEmbedding-free NLP models with projection methods\nLanguage Identification is a type of problems to classify the language of a given text. Recently we open source two models using projection methods, namely SGNN and PRADO.\n\nWe used SGNN to show how easy and efficient to use Tensorflow Lite for NLP Tasks. SGNN projects texts to fixed-length features followed by fully connected layers. With annotations to tell TensorFlow Lite converter to fuse TF.Text API, we can get a more efficient model for inference on TensorFlow Lite. Previously, the model took 1332.87 \u03bcs to run on benchmark; and after fusion, we see 64.06 \u03bcs on the same machine. This brings the magic of 20x speed-up.\n\nWe also demonstrate a model architecture called PRADO. PRADO first computes trainable projected features from the sequence of word tokens, then applies convolution and attention to map features to a fixed-length encoding. By combining a projection layer, a convolutional and attention encoder mechanism, PRADO achieves similar accuracy as LSTM, but with 100x smaller model size.\n\nThe idea behind these models is to use projection to compute features from texts, so that the model does not need to maintain a big embedding table to convert text features to embeddings. In this way, we\u2019ve proven the model will be much smaller than embedding based models, while maintaining similar performance and inference latency.\nCreating your own NLP Models\nIn addition to using pre-trained models, TensorFlow Lite also provides you with tools such as Model Maker to customize existing models for your own data.\nTensorFlow Lite Model Maker: Transfer Learning Toolkit for machine learning beginners\nTensorFlow Lite Model Maker is an easy-to-use transfer learning tool to adapt state-of-the-art machine learning models to your dataset. It allows mobile developers to create a model without any machine learning expertise, reduces the required training data and shortens the training time through transfer learning.\n\nAfter the initial release focusing on vision tasks, we recently added two new NLP tasks to Model Maker. You can follow the colab and guide for Text Classification and Question Answer. To install Model Maker:\npip install tflite-model-maker\nTo customize the model, developers need to write a few lines of python code as follow:\n# Loads Data.\ntrain_data = TextClassifierDataLoader.from_csv(train_csv_file, mode_spec=spec)\ntest_data = TextClassifierDataLoader.from_csv(test_csv_file, mode_spec=spec)\n\n# Customize the TensorFlow model.\nmodel = text_classifier.create(train_data, model_spec=spec)\n\n# Evaluate the model.\nloss, acc = model.evaluate(test_data)\n\n# Export as a TensorFlow Lite model.\nmodel.export(export_dir, quantization_config=config)\nConversion: Better support to convert NLP models to TensorFlow Lite\nSince the TensorFlow Lite builtin operator library only supports a subset of TensorFlow operators, you may have run into issues while converting your NLP model to TensorFlow Lite, either due to missing ops or unsupported data types (like RaggedTensor support, hash table support, and asset file handling, etc.). Here are a few tips on how to resolve the conversion issues in such cases.\nRun TensorFlow ops and TF.text ops in TensorFlow Lite\nWe have enhanced Select TensorFlow ops to support various cases. With Select TF ops, developers can leverage TensorFlow ops to run models on TensorFlow Lite, when there are no built-in TensorFlow Lite equivalent ops. For example, it\u2019s common to use TF.Text ops and RaggedTensor when training TensorFlow models, and now those models can be easily converted to TensorFlow Lite and run with necessary ops.\n\nFurthermore, we provide the solution of using op selectively building, so that we get a trimmed binary for mobile deployment. It can select a small set of used ops in the final build target, and thus reduces the binary size in deployment.\nMore efficient and friendly custom ops\nIn TensorFlow Lite, we provide a few new mobile-friendly ops for NLP, such as Ngram, SentencePieceTokenizer, WordPieceTokenizer and WhitespaceTokenizer.\n\nPreviously, there were several restrictions blocking models with SentencePiece from being converted to TensorFlow Lite. The new SentencePieceTokenizer API for mobile resolves these challenges, and simultaneously optimizes the implementation to make it run faster.\n\nSimilarly, Ngram and WhitespaceTokenizer are now not only supported, but will also be executed more efficiently on devices.\n\nTensorFlow Lite recently announced operation fusion with MLIR. We used the same mechanism to fuse TF.Text APIs into custom TensorFlow Lite ops, improving inference efficiency significantly. For example, the WhitespaceTokenizer API was made up of multiple ops, and took 0.9ms to run in the original graph in TensorFlow Lite. After fusing these ops into a single op, it finishes in 0.04ms, a 23x speed-up. This approach has been proven to bring a huge gain in inference latency in the SGNN model mentioned above.\nHash table support\nHash table is important for many NLP models, since we usually need to utilize numeric computation in the language model by transforming words into token IDs and vice versa. Hash table will be enabled in TensorFlow Lite soon. It is supported by handling asset files natively in the TensorFlow Lite format and delivering op kernels as TensorFlow Lite built-in operators.\nDeployment: How to run NLP models on-device\nRunning inference with TensorFlow Lite is now much easier than before. You can use pre-built inference APIs to integrate your model within 5 lines of code, or use utilities to build your own Android/iOS inference APIs.\nSimple model deployment using TensorFlow Lite Task Library\nThe TensorFlow Lite Task Library is a powerful and easy-to-use task-specific library that provides out of the box pre- and post-processing utilities required for ML inference, enabling app developers to easily create machine learning features with TensorFlow Lite. There are three text APIs supported in the Task Library, which correspond to the use cases and models mentioned above:\nNLClassifier: classifies the input text to a set of known categories.\nBertNLClassifier: classifies text optimized for BERT-family models.\nBertQuestionAnswerer: answers questions based on the content of a given passage with BERT-family models.\nThe Task Library works cross-platform on both Android and iOS. The following example shows inference with a BertQA model in Java/Swift:\n// Initialization\nBertQuestionAnswerer answerer = BertQuestionAnswerer.createFromFile(androidContext, modelFile);\n// Answer a question\nList answers = answerer.answer(context, question);\nJava code for Android\n// Initialization\nlet mobileBertAnswerer =   TFLBertQuestionAnswerer.mobilebertQuestionAnswerer(modelPath: modelPath)\n// Answer a question\nlet answers = mobileBertAnswerer.answer(context: context, question: question)\nSwift code for iOS\nCustomized Inference APIs\nIf your use case is not supported by the existing task libraries, you can also leverage the Task API Infrastructure and build your own C++/Android/iOS inference APIs using common NLP utilities such as Wordpiece and Sentencepiece tokenizers in the same repo.\nConclusion\nIn this article, we introduced the new support for NLP tasks in TensorFlow Lite. With the latest update of TensorFlow Lite, developers can easily create, convert and deploy NLP models on-device. We will continue providing more useful tools, and accelerate the development of on-device NLP models from research to production. We would love to hear your feedback, and suggestions for newer NLP tools and utilities. Please email tflite@tensorflow.org or create a TensorFlow Lite support GitHub issue.\nAcknowledgments\nWe like to thank Khanh LeViet, Arun Venkatesan, Max Gubin, Robby Neale, Terry Huang, Peter Young, Gaurav Nemade, Prabhu Kaliamoorthi, Ping Yu, Renjie Liu, Lu Wang, Xunkai Zhang, Yuqi Li, Sijia Ma, Thai Nguyen, Xinying Song, Chung-Ching Chang, Shuangfeng Li to contribute to the blogpost.",
    "link": "https://blog.tensorflow.org/2020/09/whats-new-in-tensorflow-lite-for-nlp.html",
    "imgSource": [
      "https://3.bp.blogspot.com/-DDX5HNAlS6k/X2ESuMU7LbI/AAAAAAAADlY/WtTmT5vmM98PGIm5SoQOxqpbQLXdGReKwCLcBGAsYHQ/s1600/imageLikeEmbed.png",
      "https://1.bp.blogspot.com/-XgEX2LzBwKo/X2ETBhTLVtI/AAAAAAAADlg/19th2xRRoRsQrissxZTSQGscG18QJNIlwCLcBGAsYHQ/s1600/Screen%2BShot%2B2020-09-15%2Bat%2B12.16.14%2BPM.png"
    ],
    "time": "2023/12/03 00:58:43"
  },
  {
    "title": "Introduction to TFLite On-device Recommendation",
    "content": "Posted by Ellie Zhou, Tian Lin, Cong Li, Shuangfeng Li and Sushant Prakash\nIntroduction & Motivation\nWe are excited to open source an end-to-end solution for TFLite on-device recommendation tasks. We invite developers to build on-device models using our solution that provides personalized, low-latency and high-quality recommendations, while preserving users\u2019 privacy.\n\nGenerating personalized high-quality recommendations is crucial to many real-world applications, such as music, videos, merchandise, apps, news, etc. Currently, a typical recommender system is fully constructed at the server side, including collecting user activity logs, training recommendation models using the collected logs, and serving recommendation models.\n\nWhile purely server-based recommender systems have been proven to be powerful, we explore and showcase a more lightweight approach to serve an recommendation model by deploying it on device. We demonstrate that such an on-device recommendation solution enjoys low latency inference that is orders of magnitude faster than server-side models. It enables user experiences that cannot be achieved by traditional server-based recommender systems, such as updating rankings and UI responding to every user tap or interaction.\n\nMoreover, on-device model inference respects user privacy without sending user data to a server to do predictions, instead keeping all needed data on the device. It is possible to train the model on public data or via an existing proxy dataset to avoid collecting user data for each new use case, which is demonstrated in our solution. For on-device training, we would refer interested readers to Federated Learning or TFLite model personalization as an alternative.\n\nPlease find our solution includes the following components:\nSource code that constructs and trains high quality personalized recommendation models for on-device scenarios.\nA movie recommendation demo app that runs the model on device.\nWe also provided source code for preparing training examples and a pretrained model in Github repo.\nModel\nRecommendation problems are typically formulated as future-activity prediction problems. A recommendation model is therefore trained to predict the user\u2019s future activities, given their previous activities happened before. Our published model is constructed with the following architecture:\nAt the context side, each user activity, such as a movie watch, is embedded into an embedding vector. Embedding vectors from past user activities are aggregated by the encoder to generate the context embedding. We support three different types of encoders:\nBag-of-Words: activity embeddings are simply averaged.\nCNN: 1-D convolution is applied to activity embeddings followed by max-pooling.\nRNN: LSTM is applied to activity embeddings.\nAt the label side, the label item, such as the next movie that the user watched or is interested in, is considered as \u201cpositive\u201d, while all other items (e.g. all other movies the user didn\u2019t watch) are considered as \u201cnegative\u201d through negative sampling. Both positive and negative items are embedded, and the dot product combines the context embedding to produce logits and feed to the loss of softmax cross entropy. Other modeling situations where labels are not binary will be supported in future. After training, the model can be exported and deployed on device for serving. We take the top-K recommendations which are simply the K-highest logits between the context embedding and all label embeddings.\nExample\nTo demonstrate the quality and the user experience of an on-device recommendation model, we trained an example movie recommendation model using the MovieLens dataset and developed a demo app. (Both the model and the app are for demonstration purposes only.) The MovieLens 1M dataset contains ratings from 6039 users across 3951 movies, with each user rating only a small subset of movies. For simplification, we ignore the rating score, and train a model to predict which movies will get rated given N previous movies, where N is referred to as the history length.\n\nThe model\u2019s performance on all the three encoders with different history lengths is shown below:\nWe can find that all models achieve high recall metric, while CNN and RNN models usually perform better for a longer history length. In practice, developers may conduct experiments with different history lengths and encoder types, and find out the best for the specific recommendation problem they want to solve.\n\nWe want to highlight that all the published on-device models have very low inference latency. For example, for the CNN model with N=10 which we integrated with our demo app, the inference latency on Pixel 4 phones is only 0.05ms in our experiment. As stated in the introduction, such a low latency allows developing immediate and smooth response to every user interaction on the phone, as is demonstrated in our app:\nFuture Work\nWe welcome different kinds of extensions and contributions. The currently open sourced model does not support more than one feature column to represent each user\u2019s activity. In the next version, we are going to support multiple features as the activity representation. Moreover, we are planning more advanced user encoders, such as Transformer-based (Vaswani, A., et al., 2017).\nReferences\nVaswani, A., et al. \"Attention is all you need. arXiv 2017.\" arXiv preprint arXiv:1706.03762 (2017), https://arxiv.org/abs/1706.03762.",
    "link": "https://blog.tensorflow.org/2020/09/introduction-to-tflite-on-device-recommendation.html",
    "imgSource": [
      "https://4.bp.blogspot.com/-t6zYUbKQwvU/X1vDVv4T5dI/AAAAAAAADk0/evRqa0cAbYIc_El47Z_x_jDH5hm3WUaogCLcBGAsYHQ/s1600/model.png",
      "https://3.bp.blogspot.com/-QdfPvUKPkn4/X1vEKndiADI/AAAAAAAADk8/yChUUr6wKm4BcYFjPkrzvDP9VuEzUFsFgCLcBGAsYHQ/s1600/savedmodel.png",
      "https://2.bp.blogspot.com/-S7h2XuAF4xg/X1vFD95iwOI/AAAAAAAADlE/48VbAUMfIkot1BIUMeIaHUoS1n22rwogwCLcBGAsYHQ/s1600/table.png",
      "https://2.bp.blogspot.com/-GM7TxKWcynE/X1vFZqR13mI/AAAAAAAADlM/y6q2eVA6VFcqExmA_DSJWMr7SQybXPuMACLcBGAsYHQ/s1600/TFLrec.gif"
    ],
    "time": "2023/12/03 00:58:43"
  },
  {
    "title": "Easy ML mobile development with TensorFlow Lite Task Library",
    "content": "Posted by Lu Wang, Chen Cen, Arun Venkatesan, Khanh LeViet\nOverview\nRunning inference with TensorFlow Lite models on mobile devices is much more than just interacting with a model, but also requires extra code to handle complex logic, such as data conversion, pre/post processing, loading associated files and more.\n\nToday, we are introducing the TensorFlow Lite Task Library, a set of powerful and easy-to-use model interfaces, which handles most of the pre- and post-processing and other complex logic on your behalf. The Task Library comes with support for popular machine learning tasks, including Image Classification and Segmentation, Object Detection and Natural Language Processing. The model interfaces are specifically designed for each task to achieve the best performance and usability - inference on pre trained and custom models for supported tasks can now be done with just 5 lines of code! The Task Library has been widely used in production for many Google products.\nSupported ML Tasks\nThe TensorFlow Lite Task Library currently supports six ML tasks including Vision and NLP use cases. Here is the brief introduction for each of them.\nImageClassifier\nImage classification is a common use of machine learning to identify what an image represents. For example, we might want to know what type of animal appears in a given picture. The ImageClassifier API supports common image processing and configurations. It also allows displaying labels in specific supported locales and filtering results based on label allowlist and denylist.\nObjectDetector\nObject detectors can identify which of a known set of objects might be present and provide information about their positions within the given image or a video stream. The ObjectDetector API supports similar image processing options as ImageClassifer. The output is a list of the top-k detected objects with label, bounding box, and probability.\nImageSegmenter\nImage segmenters predict whether each pixel of an image is associated with a certain class. This is in contrast to object detection, which detects objects in rectangular regions, and image classification, which classifies the overall image. Besides image processing, ImageSegmenter also supports two types of output masks, category mask and confidence mask.\nNLClassifier & BertNLClassifier\nNLClassifier classifies input text into different categories. This versatile API can be configured to load any TFLite model with text input and score output.\n\nBertNLClassifier is similar to NLClassifier, except that this API is specially tailored for BERT related models that requires Wordpiece and Sentencepiece tokenizations outside the TFLite model.\nBertQuestionAnswerer\nBertQuestionAnswerer loads a BERT model and answers question based on the content of a given passage. It currently supports MobileBERT and ALBERT. Similar to BertNLClassifier, BertQuestionAnswerer encapsulates complex tokenization processing for input text. You can simply pass in contexts and questions in string to BertQuestionAnswerer.\nSupported Models\nThe Task Library is compatible with the following known sources of models:\nTensorFlow Hub Task Library model collections (image classification / object detection / image segmentation / question and answer).\nModels created by TensorFlow Lite Model Maker.\nModels created by AutoML Vision Edge.\nThe Task Library also supports custom models that fit the model compatibility requirements of each Task API. The associated files (i.e. label maps and vocab files) and processing parameters, if applicable, should be properly populated into the Model Metadata. See the documentation on TensorFlow website for each API for more details.\nRun inference with the Task Library\nThe Task Library works cross-platform and is supported on Java, C++ (experimental), and Swift (experimental). Running inference with the Task Library can be as easy as just writing a few lines of code. For example, you can use the DeepLab v3 TFLite model to segment an airplane image (Figure 1) in Android as follows:\n// Create the API from a model file and options\nString modelPath = \"path/to/model.tflite\"\nImageSegmenterOptions options = ImageSegmenterOptions.builder().setOutputType(OutputType.CONFIDENCE_MASK).build();\n\nImageSegmenter imageSegmenter = ImageSegmenter.createFromFileAndOptions(context, modelPath, options);\n\n// Segment an image\nTensorImage image = TensorImage.fromBitmap(bitmap);\nList results = imageSegmenter.segment(image);\nFigure 1. ImageSegmenter input image.\nFigure 2. Segmented mask.\nYou can then use the colored labels and category mask in the results to construct the segmented mask image as shown in Figure 2.\n\nSwift is supported for the three text APIs. To perform Question and Answer in iOS with the SQuAD v1 TFLite model on a given context and a question, you could run:\nlet modelPath = \"path/to/model.tflite\"\n\n// Create the API from a model file\nlet mobileBertAnswerer =   TFLBertQuestionAnswerer.mobilebertQuestionAnswerer(modelPath: modelPath)\n\nlet context = \"\"\"\nThe Amazon rainforest, alternatively, the Amazon Jungle, also known in \\\nEnglish as Amazonia, is a moist broadleaf tropical rainforest in the \\\nAmazon biome that covers most of the Amazon basin of South America. This \\\nbasin encompasses 7,000,000 square kilometers(2,700,000 square miles), of \\\nwhich 5,500,000 square kilometers(2,100,000 square miles) are covered by \\\nthe rainforest. This region includes territory belonging to nine nations.\n\"\"\"\nlet question = \"Where is Amazon rainforest?\"\n// Answer a question\nlet answers = mobileBertAnswerer.answer(context: context, question: question)\n// answers.[0].text could be \u201cSouth America.\u201d\nBuild a Task API for your use case\nIf your use case is not supported by the existing Task libraries, you can leverage the Task API infrastructure and build your custom C++/Android/iOS inference APIs. See this guide for more details.\nFuture Work\nWe will continue improving the user experience for the Task Library. Here is the roadmap for the near future:\nImprove the usability of the C++ Task Library, such as providing prebuilt binaries and creating user-friendly workflows for users who want to build from source code.\nPublish reference examples using the Task Library.\nEnable more machine learning use cases via new task types.\nImprove cross-platform support and enable more tasks for iOS.\n\nFeedback\nWe would love to hear your feedback, and suggestions for newer use cases to be supported in the Task Library. Please email tflite@tensorflow.org or create a TensorFlow Lite support GitHub issue.\nAcknowledgments\nThis work would not have been possible without the efforts of\nC\u00e9dric Deltheil and Maxime Br\u00e9non, the main contributors for the Task Library Vision API.\nChen Cen, the main contributor for the Task Library native/Android/iOS infrastructure and Text API.\nXunkai and YoungSeok Yoon, the main contributors for the dev infra and releasing process.\nWe would like to thank Tian Lin, Sijia Ma, YoungSeok Yoon, Yuqi Li, Hsiu Wang, Qifei Wang, Alec Go, Christine Kaeser-Chen, Yicheng Fan, Elizabeth Kemp, Willi Gierke, Arun Venkatesan, Amy Jang, Mike Liang, Denis Brul\u00e9, Gaurav Nemade, Khanh LeViet, Luiz GUStavo Martins, Shuangfeng Li, Jared Duke, Erik Vee, Sarah Sirajuddin, Tim Davis for their active support of this work.",
    "link": "https://blog.tensorflow.org/2020/09/introducing-tensorflow-lite-task-library.html",
    "imgSource": [
      "https://3.bp.blogspot.com/-C6nsjcOmXxo/X1lZ-3s4i5I/AAAAAAAADkg/F_vaI4yE5zwBCVkH3meewLWkKIUGtXHIQCLcBGAsYHQ/s1600/plane.jpg",
      "https://3.bp.blogspot.com/-bfgcPOnkVjM/X1laBlg-eoI/AAAAAAAADkk/SxhoHpfk46861ycrDO2UXt_OeLP7ycGUwCLcBGAsYHQ/s1600/segmentation-output.png"
    ],
    "time": "2023/12/03 00:58:43"
  },
  {
    "title": "How to Create a Cartoonizer with TensorFlow Lite",
    "content": "A guest post by ML GDEs Margaret Maynard-Reid (Tiny Peppers) and Sayak Paul (PyImageSearch)\n\nThis is an end-to-end tutorial on how to convert a TensorFlow model to TensorFlow Lite (TFLite) and deploy it to an Android app for cartoonizing an image captured by the camera.\n\nWe created this end-to-end tutorial to help developers with these objectives:\nProvide a reference for the developers looking to convert models written in TensorFlow 1.x to their TFLite variants using the new features of the latest (v2) converter \u2014 for example, the MLIR-based converter, more supported ops, and improved kernels, etc.\n(In order to convert TensorFlow 2.x models in TFLite please follow this guide.)\nHow to download the .tflite models directly from TensorFlow Hub if you are only interested in using the models for deployment.\nUnderstand how to use the TFLite tools such as the Android Benchmark Tool, Model Metadata, and Codegen.\nGuide developers on how to create a mobile application with TFLite models easily, with ML Model Binding feature from Android Studio.\nPlease follow along with the notebooks here for model saving/conversion, populating metadata; and the Android code on GitHub here. If you are not familiar with the SavedModel format, please refer to the TensorFlow documentation for details. While this tutorial discusses the steps of how to create the TFLite models , feel free to download them directly from TensorFlow Hub here and get started using them in your own applications.\n\nWhite-box CartoonGAN is a type of generative adversarial network that is capable of transforming an input image (preferably a natural image) to its cartoonized representation. The goal here is to produce a cartoonized image from an input image that is visually and semantically aesthetic. For more details about the model check out the paper Learning to Cartoonize Using White-box Cartoon Representations by Xinrui Wang and Jinze Yu. For this tutorial, we used the generator part of White-box CartoonGAN.\nCreate the TensorFlow Lite Model\nThe authors of White-box CartoonGAN provide pre-trained weights that can be used for making inference on images. However, those weights are not ideal if we were to develop a mobile application without having to make API calls to fetch them. This is why we will first convert these pre-trained weights to TFLite which would be much more suitable to go inside a mobile application. All of the code discussed in this section is available on GitHub here.\nHere is a step-by-step summary of what we will be covering in this section:\nGenerate a SavedModel out of the pre-trained model checkpoints.\nConvert SavedModel with post-training quantization using the latest TFLiteConverter.\nRun inference in Python with the converted model.\nAdd metadata to enable easy integration with a mobile app.\nRun model benchmark to make sure the model runs well on mobile.\nGenerate a SavedModel from the pre-trained model weights\nThe pre-trained weights of White-box CartoonGAN come in the following format (also referred to as checkpoints) -\n\u251c\u2500\u2500 checkpoint\n\u251c\u2500\u2500 model-33999.data-00000-of-00001\n\u2514\u2500\u2500 model-33999.index\nAs the original White-box CartoonGAN model is implemented in TensorFlow 1, we first need to generate a single self-contained model file in the SavedModel format using TensorFlow 1.15. Then we will switch to TensorFlow 2 later to convert it to the lightweight TFLite format. To do this we can follow this workflow -\nCreate a placeholder for the model input.\nInstantiate the model instance and run the input placeholder through the model to get a placeholder for the model output.\nLoad the pre-trained checkpoints into the current session of the model.\nFinally, export to SavedModel.\nNote that the aforementioned workflow will be based on TensorFlow 1.x.\n\nThis is how all of this looks in code in TensorFlow 1.x:\nwith tf.Session() as sess:\n   input_photo = tf.placeholder(tf.float32, [1, None, None, 3], name='input_photo')\n \n   network_out = network.unet_generator(input_photo)\n   final_out = guided_filter.guided_filter(input_photo, network_out,           r=1, eps=5e-3)\n   final_out = tf.identity(final_out, name='final_output') \n  \n   all_vars = tf.trainable_variables()\n   gene_vars = [var for var in all_vars if 'generator' in var.name]\n   saver = tf.train.Saver(var_list=gene_vars)\n   sess.run(tf.global_variables_initializer())\n   saver.restore(sess, tf.train.latest_checkpoint(model_path))\n  \n   # Export to SavedModel\n   tf.saved_model.simple_save(\n  sess,\n      saved_model_directory,\n      inputs={input_photo.name: input_photo},\n      outputs={final_out.name: final_out}\n   )\nNow that we have the original model in the SavedModel format, we can switch to TensorFlow 2 and proceed toward converting it to TFLite.\nConvert SavedModel to TFLite\nTFLite provides support for three different post-training quantization strategies -\nDynamic range\nFloat16\nInteger\nBased on one\u2019s use-case a particular strategy is determined. In this tutorial, however, we will be covering all of these different quantization strategies to give you a fair idea.\nTFLite models with dynamic-range and float16 quantization\nThe steps to convert models to TFLite using these two quantization strategies are almost identical except during float16 quantization, you need to specify an extra option. The steps for model conversion are demonstrated in the code below -\n# Create a concrete function from the SavedModel \nmodel = tf.saved_model.load(saved_model_dir)\nconcrete_func = model.signatures[\n    tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\n\n# Specify the input shape\nconcrete_func.inputs[0].set_shape([1, IMG_SHAPE, IMG_SHAPE, 3])\n\n# Convert the model and export \nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.target_spec.supported_types = [tf.float16] # Only for float16\ntflite_model = converter.convert()\nopen(tflite_model_path, 'wb').write(tflite_model)\nA couple of things to note from the code above -\nHere, we are specifying the input shape of the model that will be converted to TFLite. However, note that TFLite supports dynamic shaped models from TensorFlow 2.3. We used fixed-shaped inputs in order to restrict the memory usage of the models running on mobile devices.\nIn order to convert the model using dynamic-range quantization, one just needs to comment this line converter.target_spec.supported_types = [tf.float16].\nTFLite models with integer quantization\nIn order to convert the model using integer quantization, we need to pass a representative dataset to the converter so that the activation ranges can be calibrated accordingly. TFLite models generated using this strategy are known to sometimes work better than the other two that we just saw. Integer quantized models are generally smaller as well.\n\nFor the sake of brevity, we are going to skip the representative dataset generation part but you can refer to it in this notebook.\n\nIn order to let the TFLiteConverter take advantage of this strategy, we need to just pass converter.representative_dataset = representative_dataset_gen and remove converter.target_spec.supported_types = [tf.float16].\n\nSo after we generated these different models here\u2019s how we stand in terms of model size -\nYou might feel tempted to just go with the model quantized with integer quantization but you should also consider the following things before finalizing this decision -\nQuality of the end results of the models.\nInference time (the lower the better).\nHardware accelerator compatibility.\nMemory usage.\nWe will get to these in a moment. If you want to dig deeper into these different quantization strategies refer to the official guide here.\n\nThese models are available on TensorFlow Hub and you can find them here.\nRunning inference in Python\nAfter you have generated the TFLite models, it is important to make sure that models perform as expected. A good way to ensure that is to run inference with the models in Python before integrating them in mobile applications.\n\nBefore feeding an image to our White-box CartoonGAN TFLite models it\u2019s important to make sure that the image is preprocessed well. Otherwise, the models might perform unexpectedly. The original model was trained using BGR images, so we need to account for this fact in the preprocessing steps as well. You can find all of the preprocessing steps in this notebook.\n\nHere is the code to use a TFLite model for making inference on a preprocessed input image -\ninterpreter = tf.lite.Interpreter(model_path=tflite_model_path)\ninput_details = interpreter.get_input_details()\n\ninterpreter.allocate_tensors()\ninterpreter.set_tensor(input_details[0]['index'],                \npreprocessed_source_image)\ninterpreter.invoke()\n\nraw_prediction = interpreter.tensor(\n    interpreter.get_output_details()[0]['index'])()\nAs mentioned above, the output would be an image but with BGR channel ordering which might not be visually appropriate. So, we would need to account for that fact in the postprocessing steps.\n\nAfter the postprocessing steps are incorporated here is how the final image would look like alongside the original input image -\nAgain, you can find all of the postprocessing steps in this notebook.\nAdd metadata for easy integration with a mobile app\nModel metadata in TFLite makes the life of mobile application developers much easier. If your TFLite model is populated with the right metadata then it becomes a matter of only a few keystrokes to integrate that model into a mobile application. Discussing the code to populate a TFLite model with metadata is out of the scope for this tutorial, and please refer to the metadata guide. But in this section, we are going to provide you with some of the important pointers about metadata population for the TFLite models we generated. You can follow this notebook to refer to all the code. Two of the most important parameters we discovered during metadata population are mean and standard deviation with which the results should be processed. In our case, mean and standard deviation need to be used for both preprocessing postprocessing. For normalizing the input image the metadata configuration should be like the following -\ninput_image_normalization.options.mean = [127.5]\ninput_image_normalization.options.std = [127.5]\nThis would make the pixel range in an input image to [-1, 1]. Now, during postprocessing, the pixels need to be scaled back to the range of [0, 255]. For this, the configuration would go as follows -\noutput_image_normalization.options.mean = [-1]\noutput_image_normalization.options.std = [0.00784313] # 1/127.5\nThere are two files created from the \u201cadd metadata process\u201d:\nA .tflite file with the same name as the original model, with metadata added, including model name, description, version, input and output tensor, etc.\nTo help to display metadata, we also export the metadata into a .json file so that you can print it out. When you import the model into Android Studio, metadata can be displayed as well.\nThe models that have been populated with metadata make it really easy to import in Android Studio which we will discuss later under the \u201cModel deployment to an Android\u201d section.\nBenchmark models on Android (Optional)\nAs an optional step, we used the TFLite Android Model Benchmark tool to get an idea of the runtime performance on Android before deploying it.\n\nThere are two options of using the benchmark tool, one with a C++ binary running in background and another with an Android APK running in foreground.\n\nHere ia a high-level summary using the benchmark C++ binary:\n\n1. Configure Android SDK/NDK prerequisites\n\n2. Build the benchmark C++ binary with bazel\nbazel build -c opt \\\n      --config=android_arm64 \\\n      tensorflow/lite/tools/benchmark:benchmark_model\n3. Use adb (Android Debug Bridge) to push the benchmarking tool binary to device and make executable\nadb push benchmark_model /data/local tmp\n    adb shell chmod +x /data/local/tmp/benchmark_model\n4. Push the whitebox_cartoon_gan_dr.tflite model to device\nadb push whitebox_cartoon_gan_dr.tflite /data/local/tmp\n5. Run the benchmark tool\nadb shell /data/local/tmp/android_aarch64_benchmark_model \\       \n      --graph=/data/local/tmp/whitebox_cartoon_gan_dr.tflite \\\n      --num_threads=4\nYou will see a result in the terminal like this:\nRepeat above steps for the other two tflite models: float16 and int8 variants.\n\nIn summary, here is the average inference time we got from the benchmark tool running on a Pixel 4:\nRefer to the documentation of the benchmark tool (C++ binary | Android APK) for details and additional options such as how to reduce variance between runs and how to profile operators, etc. You can also see the performance values of some of the popular ML models on the TensorFlow official documentation here.\nModel deployment to Android\nNow that we have the quantized TensorFlow Lite models with metadata by either following the previous steps (or by downloading the models directly from TensorFlow Hub here), we are ready to deploy them to Android. Follow along with the Android code on GitHub here.\n\nThe Android app uses Jetpack Navigation Component for UI navigation and CameraX for image capture. We use the new ML Model Binding feature for importing the tflite model and then Kotlin Coroutine for async handling of the model inference so that the UI is not blocked while waiting for the results.\n\nLet\u2019s dive into the details step by step:\nDownload Android Studio 4.1 Preview.\nCreate a new Android project and set up the UI navigation.\nSet up the CameraX API for image capture.\nImport the .tflite models with ML Model Binding.\nPutting everything together.\nDownload Android Studio 4.1 Preview\nWe need to first install Android Studio Preview (4.1 Beta 1) in order to use the new ML Model Binding feature to import a .tflite model and auto code generation. You can then explore the tfllite models visually and most importantly use the generated classes directly in your Android projects.\n\nDownload the Android Studio Preview here. You should be able to run the Preview version side by side with a stable version of Android Studio. Make sure to update your Gradle plug-in to at least 4.1.0-alpha10; otherwise the ML Model Binding menu may be inaccessible.\nCreate a new Android Project\nFirst let\u2019s create a new Android project with an empty Activity called MainActivity.kt which contains a companion object that defines the output directory where the captured image will be stored.\n\nUse Jetpack Navigation Component to navigate the UI of the app. Please refer to the tutorial here to learn more details about this support library.\n\nThere are 3 screens in this sample app:\nPermissionsFragment.kt handles checking the camera permission.\nCameraFragment.kt handles camera setup, image capture and saving.\nCartoonFragment.kt handles the display of input and cartoon image in the UI.\nThe navigation graph in nav_graph.xml defines the navigation of the three screens and data passing between CameraFragment and CartoonFragment.\nSet up CameraX for image capture\nCameraX is a Jetpack support library which makes camera app development much easier.\n\nCamera1 API was simple to use but it lacked a lot of functionality. Camera2 API provides more fine control than Camera1 but it\u2019s very complex \u2014 with almost 1000 lines of code in a very basic example.\n\nCameraX on the other hand, is much easier to set up with 10 times less code. In addition, it\u2019s lifecycle aware so you don\u2019t need to write the extra code to handle the Android lifecycle.\n\nHere are the steps to set up CameraX for this sample app:\nUpdate build.gradle dependencies\nUse CameraFragment.kt to hold the CameraX code\nRequest camera permission\nUpdate AndroidManifest.ml\nCheck permission in MainActivity.kt\nImplement a viewfinder with the CameraX Preview class\nImplement image capture\nCapture an image and convert it to a Bitmap\nCameraSelector is configured to be able to take use of the front facing and rear facing camera since the model can stylize any type of faces or objects, and not just a selfie.\n\nOnce we capture an image, we convert it to a Bitmap which is passed to the TFLite model for inference. Navigate to a new screen CartoonFragment.kt where both the original image and the cartoonized image are displayed.\nImport the TensorFlow Lite models\nNow that the UI code has been completed. It\u2019s time to import the TensorFlow Lite model for inference. ML Model Binding takes care of this with ease. In Android Studio, go to File > New > Other > TensorFlow Lite Model:\nSpecify the .tflite file location.\n\u201cAuto add build feature and required dependencies to gradle\u201d is checked by default.\nMake sure to also check \u201cAuto add TensorFlow Lite gpu dependencies to gradle\u201d since the GAN models are complex and slow, and so we need to enable GPU delegate.\nThis import accomplishes two things:\nautomatically create a ml folder and place the model file .tflite file under there.\nauto generate a Java class under the folder: app/build/generated/ml_source_out/debug/[package-name]/ml, which handles all the tasks such as model loading, image pre-preprocess and post-processing, and run model inference for stylizing the input image.\nOnce the import completes, we see the *.tflite display the model metadata info as well as code snippets in both Kotlin and Java that can be copy/pasted in order to use the model:\nRepeat the steps above to import the other two .tflite model variants.\nPutting everything together\nNow that we have set up the UI navigation, configured CameraX for image capture, and the tflite models are imported, let\u2019s put all the pieces together!\nModel input: capture a photo with CameraX and save it\nRun inference on the input image and create a cartoonized version\nDisplay both the original photo and the cartoonized photo in the UI\nUse Kotlin coroutine to prevent the model inference from blocking UI main thread\nFirst we capture a photo with CameraX in CameraFragment.kt under imageCaptue?.takePicture(), then in ImageCapture.OnImageSavedCallback{}, onImageSaved() convert the .jpg image to a Bitmap, rotate if necessary, and then save it to an output directory defined in MainActivity earlier.\n\nWith the JetPack Nav Component, we can easily navigate to CartoonFragment.kt and pass the image directory location as a string argument, and the type of tflite model as an integer. Then in CartoonFragment.kt, retrieve the file directory string where the photo was stored, create an image file then convert it to be Bitmap which can be used as the input to the tflite model.\n\nIn CartoonFragment.kt, also retrieve the type of tflite model that was chosen for inference. Run model inference on the input image and create a cartoon image. We display both the original image and the cartoonized image in the UI.\n\nNote: the inference takes time so we use Kotlin coroutine to prevent the model inference from blocking the UI main thread. Show a ProgressBar till the model inference completes.\n\nHere is what we have once all pieces are put together and here are the cartoon images created by the model:\nThis brings us to the end of the tutorial. We hope you have enjoyed reading it and will apply what you learned to your real-world applications with TensorFlow Lite. If you have created any cool samples with what you learned here, please remember to add it to awesome-tflite - a repo with TensorFlow Lite samples, tutorials, tools and learning resources.\nAcknowledgments\nThis Cartoonizer with TensorFlow Lite project with end-to-end tutorial was created with the great collaboration by ML GDEs and the TensorFlow Lite team. This is the one of a series of end-to-end TensorFlow Lite tutorials. We would like to thank Khanh LeViet and Lu Wang (TensorFlow Lite), Hoi Lam (Android ML), Trevor McGuire (CameraX) and Soonson Kwon (ML GDEs Google Developers Experts Program), for their collaboration and continuous support.\n\nAlso thanks to the authors of the paper Learning to Cartoonize Using White-box Cartoon Representations: Xinrui Wang and Jinze Yu.\n\nWhen developing applications, it\u2019s important to consider recommended practices for responsible innovation; check out Responsible AI with TensorFlow for resources and tools you can use.",
    "link": "https://blog.tensorflow.org/2020/09/how-to-create-cartoonizer-with-tf-lite.html",
    "imgSource": [
      "https://3.bp.blogspot.com/-h-z7dkHMmIQ/X1ey-p5oHUI/AAAAAAAADjA/VtZqX51ZZTgsh2dCeS5iCEyO5Fjp0ks5QCLcBGAsYHQ/s1600/cartoonizer.png",
      "https://4.bp.blogspot.com/-mKNhXweFFYo/X1e57mxzj7I/AAAAAAAADjM/vxEpHSwlKJQ7QxXMDba6oR54jTQVN6bgQCLcBGAsYHQ/s1600/table.png",
      "https://4.bp.blogspot.com/-KndNlgruerk/X1e7Bo4ZY5I/AAAAAAAADjY/wCRm9KwSLfAfb_ULhC9ANENhcDTpAp-pACLcBGAsYHQ/s1600/cartoonizedimage.png",
      "https://4.bp.blogspot.com/-sngMiEbHSJ4/X1e8v8gBlbI/AAAAAAAADjk/k8IkCHRh6AM2Ev0bdHhWEIztPz2pfbh_wCLcBGAsYHQ/s1600/code.png",
      "https://2.bp.blogspot.com/-OfW3DgVWVYo/X1e9NiR-UfI/AAAAAAAADjs/IR6Qf8egWf0te4_vSZkHwIUx71WrXYUkwCLcBGAsYHQ/s1600/table2.png",
      "https://2.bp.blogspot.com/-_QQc9FJ6DXY/X1fOPA25FzI/AAAAAAAADj4/BTRm62y1Rd4as4RTk1LYlbLwMuunNZY4ACLcBGAsYHQ/s1600/importmodels.png",
      "https://3.bp.blogspot.com/-tLDrvQyILzQ/X1fOedrrqXI/AAAAAAAADj8/8QP_AhdkWjkSUVUsmUSKvRqBOviBO-hdgCLcBGAsYHQ/s1600/importmodel2.png",
      "https://4.bp.blogspot.com/-Sn4trYBgDc8/X1fPNnAgyoI/AAAAAAAADkI/nBivTcu94kUJufg3OIiRGUkCahk4tlD6wCLcBGAsYHQ/s1600/samplecode.png",
      "https://2.bp.blogspot.com/-wkfUzighAUc/X1fQlnphIyI/AAAAAAAADkU/cD5ijoogoAAjgeMhQFyQgsgFK-ZDAyqMACLcBGAsYHQ/s1600/mobilexamples.jpg"
    ],
    "time": "2023/12/03 00:58:43"
  },
  {
    "title": "Announcing TensorFlow Lite Micro support on the ESP32",
    "content": "A guest article by Vikram Dattu, Aditya Patwardhan, Kedar Sovani of Espressif Systems\nIntroducing ESP32: The Wi-Fi MCU\nWe are glad to announce TensorFlow Lite Micro support for the ESP32 chipset.\n\nThe ESP32 is a Wi-Fi/BT/BLE enabled MCU (micro-controller) that is widely used by hobbyists and makers to build cool and interesting projects that sense or modify real world data/object, and also commonly deployed in smart home appliances like light bulbs, switches, refrigerators, and air conditioners to provide connectivity.\nThe interesting part of ESP32 is that it\u2019s a unique SoC that can be used right from quick prototypes to high-volume production. A wide community, numerous development kits, and plethora of tutorials/SDKs make it a great vehicle for quick prototypes with almost any vertical you might be interested in. The all-in-one package (Wi-Fi/BT/MCU) and existing high volume deployments in the field make it ideal for building end-products with.\n\nESP32 is already being used in a number of smart-home/connected-device projects with a variety of sensors and actuators connected to the microcontroller to sense the environment and act accordingly. With TensorFlow Lite for Microcontrollers executing on ESP32, this opens up scenarios for all kinds of use-cases that are triggered by local inference. ESP32 has 2 CPU cores and a bunch of optimizations, making it easier to run heavy TF Micro workfloads. The Wi-Fi backhaul helps to raise remote events and trigger actions based on the inferences made.\nPerson Detection or a Door-Bell Camera?\nAs an example, we have modified the person_detection example that you all might be familiar with to make it a smart door-bell camera. We use the ESP-EYE developer kit for this demonstration. Note that this example uses person detection (it detects when a face is in front of the camera), and not person identification (identifying who the person is).\n\nThe ESP-EYE dev-kit includes the ESP32 Wi-Fi/BT MCU coupled with a 2MP camera.\nIn Action\nIn our example, we will use this camera to observe and send out an email notification if we detect a person in the vicinity.\nBuilding it for yourself\nOrder the ESP-EYE: You can get the ESP-EYE Development Kit from your favourite distributor, or from here. You will need a USB to micro-USB cable for connecting this to your Windows/Linux/macOS host.\nClone the repository: https://github.com/espressif/tensorflow/\nSetup your development host: Setup your development host with toolchains and utilities required to cross-build for ESP32. Follow the instructions of the ESP-IDF get started guide to set up the toolchain and the ESP-IDF itself.\nGenerate the example: The example project can be generated with the following command:\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_doorbell_camera_esp_project\nBuild the example:\n\na. Go to the example project directory\ncd tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/doorbell_camera/esp-idf\n\nb. Clone the esp32-camera component with following command:\n$ git clone https://github.com/espressif/esp32-camera components/esp32-camera\n\nc. Configure the camera and the email address:\nidf.py menuconfig\n\nd. Enter the Camera Pins configuration and SMTP Configuration menus to select the camera details, and also the email details.\n\ne. Build the example:\nidf.py build\nFlash and Run the program: Use the following command to flash and run the program:\nidf.py --port /dev/ttyUSB0 flash monitor\nNow, whenever a person\u2019s face is detected, the program will send out an email to the configured email address.\nWhat Next?\nNow that you have tried the door bell camera example, you may try the other applications that are part of the TF Micro repository: hello_world and micro_speech.\n\nESP32 is pretty powerful for a microcontroller. Clocked at 240MHz, with just a single core it can do the detection well under 1 second (roughly ~700ms; additional optimizations are on the way to reduce this even further). This leaves the second core free for other tasks from your application.\n\nThe TinyML book is an excellent resource for a thorough understanding of TensorFlow Lite for Microcontrollers.",
    "link": "https://blog.tensorflow.org/2020/08/announcing-tensorflow-lite-micro-esp32.html",
    "imgSource": [
      "https://1.bp.blogspot.com/--5pgg5SDWPs/X0gGeraXKNI/AAAAAAAADhA/6wy7ULeBzygFwb-VsP2PR-cJ3OBf8rzNQCLcBGAsYHQ/s1600/ESP-EYE-Final.jpg",
      "https://2.bp.blogspot.com/-n_s9uJZmCg4/X0gHYYFpBsI/AAAAAAAADhI/49EKmeNvzHA5TEjO55Q00kNLAOEv59DsACLcBGAsYHQ/s1600/example.gif"
    ],
    "time": "2023/12/03 00:58:43"
  },
  {
    "title": "Even Faster Mobile GPU Inference with OpenCL",
    "content": "Posted by Juhyun Lee and Raman Sarokin, Software Engineers\n\nWhile the TensorFlow Lite (TFLite) GPU team continuously improves the existing OpenGL-based mobile GPU inference engine, we also keep investigating other technologies. One of those experiments turned out quite successful, and we are excited to announce the official launch of OpenCL-based mobile GPU inference engine for Android, which offers up to ~2x speedup over our existing OpenGL backend, on reasonably sized neural networks that have enough workload for the GPU.\nFigure 1. Duo's AR effects are powered by our OpenCL backend.\nImprovements over the OpenGL Backend\nHistorically, OpenGL is an API designed for rendering vector graphics. Compute shaders were added with OpenGL ES 3.1, but its backward compatible API design decisions were limiting us from reaching the full potential of the GPU. OpenCL, on the other hand, was designed for computation with various accelerators from the beginning and is thus more relevant to our domain of mobile GPU inference. Therefore, we have looked into an OpenCL-based inference engine, and it brings quite a lot of features that let us optimize our mobile GPU inference engine.\n\nPerformance Profiling: Optimizing the OpenCL backend was much easier than OpenGL, because OpenCL offers good profiling features and Adreno supports them well. With these profiling APIs, we are able to measure the performance of each kernel dispatch very precisely.\n\nOptimized Workgroup Sizes: We have observed that the performance of TFLite GPU on Qualcomm Adreno GPUs is very sensitive to workgroup sizes; picking the right workgroup size can boost the performance, whereby picking the wrong one can degrade the performance by an equal amount. Unfortunately, picking the right workgroup size is not trivial for complex kernels with complicated memory access patterns. With the help of the aforementioned performance profiling features in OpenCL, we were able to implement an optimizer for workgroup sizes, which resulted in up to 50% speedup over the average.\n\nNative 16-bit Precision Floating Point (FP16): OpenCL supports FP16 natively and requires the accelerator to specify the data type's availability. Being a part of the official spec, even some of the older GPUs, e.g. Adreno 305 from 2012, can operate at their full capabilities. OpenGL, on the other hand, relies on hints which the vendors can choose to ignore in their implementations, leading to no performance guarantees.\n\nConstant Memory: OpenCL has a concept of constant memory. Qualcomm added a physical memory that has properties that makes it ideal to be used with OpenCL's constant memory. This turned out to be very efficient for certain special cases, e.g. very thin layers at the beginning or at the end of the neural network. OpenCL on Adreno is able to greatly outperform OpenGL's performance by having a synergy with this physical constant memory and the aforementioned native FP16 support.\nPerformance Evaluation\nBelow, we show the performance of TFLite on the CPU (single-threaded on a big core), on the GPU using our existing OpenGL backend, and on the GPU using our new OpenCL backend. Figure 2 and Figure 3 depict the performance of the inference engine on select Android devices with OpenCL on a couple of well-known neural networks, MNASNet 1.3 and SSD MobileNet v3 (large), respectively. Each group of 3 bars are to be observed independently which shows the relative speedup among the TFLite backends on a device. Our new OpenCL backend is roughly twice as fast as the OpenGL backend, but does particularly better on Adreno devices (annotated with SD), as we have tuned the workgroup sizes with Adreno's performance profilers mentioned earlier. Also, the difference between Figure 2 and Figure 3 visualizes that OpenCL performs even better on larger networks.\nFigure 2. Inference latency of MNASNet 1.3 on select Android devices with OpenCL.\nFigure 3. Inference latency of SSD MobileNet v3 (large) on select Android devices with OpenCL.\nSeamless Integration through the GPU Delegate\nOne major hurdle in employing the OpenCL inference engine is that OpenCL is not a part of the standard Android distribution. While major Android vendors include OpenCL as part of their system library, it is possible that OpenCL is not available for some users. For these devices, one needs to fall back to the OpenGL backend which is available on every Android device.\n\nTo make developers' life easy, we have added a couple of modifications to the TFLite GPU delegate. We first check the availability of OpenCL at runtime. If it is available, we employ the new OpenCL backend as it is much faster than the OpenGL backend; if it is unavailable or couldn't be loaded, we fall back to the existing OpenGL backend. In fact, the OpenCL backend has been in the TensorFlow repository since mid 2019 and seamlessly integrated through the TFLite GPU delegate v2, so you might be already using it through the delegate's fallback mechanism.\nAcknowledgements\nAndrei Kulik, Matthias Grundmann, Jared Duke, Sarah Sirajuddin, and special thanks to Sachin Joglekar for his contributions to this blog post.",
    "link": "https://blog.tensorflow.org/2020/08/faster-mobile-gpu-inference-with-opencl.html",
    "imgSource": [
      "https://2.bp.blogspot.com/-afLl3xBw7DY/XzcHL2u_9tI/AAAAAAAADdw/KT3D3vIIvgM0VJyYc_KaEJEHPlarO501QCLcBGAsYHQ/s1600/gpu.gif",
      "https://4.bp.blogspot.com/-by7IjvNomi8/XzcHYiEgtCI/AAAAAAAADd0/bVnIQgzIFzAnKOAmN5qIK-n5bEM_k4LyACLcBGAsYHQ/s1600/figure2.png",
      "https://4.bp.blogspot.com/-6nj5bVjrUfc/XzcHcup6zUI/AAAAAAAADd8/JQ2nV3-eCjA0putBfZtuUYf4X7x90u47QCLcBGAsYHQ/s1600/figure3.png"
    ],
    "time": "2023/12/03 00:58:43"
  },
  {
    "title": "The Future of Machine Learning is Tiny and Bright",
    "content": "Posted by Josh Gordon, Developer Advocate\nA new HarvardX TinyML course on edX.org\nProf. Vijay Janapa Reddi of Harvard, the TensorFlow Lite Micro team, and the edX online learning platform are sharing a series of short TinyML courses this fall that you can observe for free, or sign up to take and receive a certificate. In this article, I\u2019ll share a bit about TinyML, what you can do with it, and the upcoming HarvardX program.\n\nAbout TinyML\nTinyML is one of the fastest-growing areas of Deep Learning. In a nutshell, it\u2019s an emerging field of study that explores the types of models you can run on small, low-power devices like microcontrollers.\n\nTinyML sits at the intersection of embedded-ML applications, algorithms, hardware and software. The goal is to enable low-latency inference at edge devices on devices that typically consume only a few milliwatts of battery power. By comparison, a desktop CPU would consume about 100 watts (thousands of times more!). Such extremely reduced power draw enables TinyML devices to operate unplugged on batteries and endure for weeks, months and possibly even years --- all while running always-on ML applications at the edge/endpoint.\nTinyML powering a simple speech recognizer. Learn how to build your own here.\nAlthough most of us are new to TinyML, it may surprise you to learn that TinyML has served in production ML systems for years. You may have already experienced the benefits of TinyML when you say \u201cOK Google\u201d to wake up an Android device. That\u2019s powered by an always-on, low-power keyword spotter, not dissimilar in principle from the one you can learn to build here.\n\nThe difference now is that TinyML is becoming rapidly more accessible, thanks in part to TensorFlow Lite Micro and educational resources like this upcoming HarvardX course.\n\nTinyML unlocks many applications for embedded ML developers, especially when combined with sensors like accelerometers, microphones, and cameras. It is already proving useful in areas such as wildlife tracking for conservation and detecting crop diseases for agricultural needs, as well as predicting wildfires.\n\nTinyML can also be fun! You can develop smart game controllers such as controlling a T-Rex dinosaur using a neural-network-based motion controller or enable a variety of other games. Using the same ML principles and technical chops, you could then imagine collecting accelerator data in a car to detect various scenarios (such as a wobbly tire) and alert the driver.\nChrome\u2019s T-Rex dinosaur controlled using TensorFlow Lite for Microcontrollers.\nFun and games aside, as with any ML application--- and especially when you are working with sensor data---it\u2019s essential to familiarize yourself with Responsible AI. TinyML can support a variety of private ML applications because inference can take place entirely at the edge (data never needs to leave the device). In fact, many tiny devices have no internet connection at all.\nMore About the Short Courses\nThe HarvardX course is designed to be widely accessible to developers. You will learn what TinyML is, how it can serve in the world, and how to get started.\n\nThe courses begin with ML basics, including how to collect data, how to train basic models (think: linear regression), and so on. Next, they introduce deep learning basics (think: MNIST), then Tiny ML models for computer vision, and how to deploy them using TensorFlow Lite for Microcontrollers. Along the way, the courses cover case studies and important papers, and increasingly advanced applications.\n\nIn one workflow, you\u2019ll build a TensorFlow model using Python in Colab (as always), then convert it to run in C on a microcontroller. The course will show how to optimize the ML models for severely resource-constrained devices (e.g., those with less than 100 KB of storage). And it includes various case studies that examine the challenges of deploying TinyML \u201cinto the wild.\u201d\nTake TinyML Home\nWe\u2019re excited to work closely with Arduino and HarvardX to make this experience possible.\nArduino is preparing a TinyML kit, especially for the course.\nAn off-the-shelf TinyML kit from Arduino will be available to edX learners for purchase. It includes an Arm Cortex-M4 microcontroller with onboard sensors, a camera and a breadboard with wires\u2014everything needed to unlock the initial suite of TinyML application capabilities, such as image, sound and gesture detection. Students will have the opportunity to invent the future.\n\nWe\u2019ll feature the best student projects from the course right here on the TensorFlow blog.\n\nWe\u2019re excited to see what you\u2019ll create!\n\nSign-up here.",
    "link": "https://blog.tensorflow.org/2020/08/the-future-of-ml-tiny-and-bright.html",
    "imgSource": [
      "https://4.bp.blogspot.com/-PKCcVJ7liNc/Xyi8xP2lVJI/AAAAAAAADbc/jPVJaVa9cCY1YIZsibT0Qe9YWTJbV2zWACLcBGAsYHQ/s1600/harvardx.png",
      "https://3.bp.blogspot.com/-ENFI39-Lb_4/XyjA7I3-LrI/AAAAAAAADbw/oa72PX1IKTgZWHTu00QupltJGyFwHQV9gCLcBGAsYHQ/s1600/demo.gif",
      "https://2.bp.blogspot.com/-5qxa3-_b-Aw/XyjAIIFo4TI/AAAAAAAADbo/eS7RkgkjQGw-COSnFc0EW4FU3aCUP0fKgCLcBGAsYHQ/s1600/chromedino.gif",
      "https://3.bp.blogspot.com/-2fMVJmxsBDg/XyjBJ7h3KnI/AAAAAAAADb0/cwR7ORlehRwiTx-pysFeheKryxDSSpLUACLcBGAsYHQ/s1600/arduino.png"
    ],
    "time": "2023/12/03 00:58:43"
  },
  {
    "title": "Accelerating TensorFlow Lite with XNNPACK Integration",
    "content": "Posted by Marat Dukhan, Google Research\n\nLeveraging the CPU for ML inference yields the widest reach across the space of edge devices. Consequently, improving neural network inference performance on CPUs has been among the top requests to the TensorFlow Lite team. We listened and are excited to bring you, on average, 2.3X faster floating-point inference through the integration of the XNNPACK library into TensorFlow Lite.\n\nTo achieve this speedup, the XNNPACK library provides highly optimized implementations of floating-point neural network operators. It launched earlier this year in the WebAssembly backend of TensorFlow.js, and with this release we are introducing additional optimizations tailored to TensorFlow Lite use-cases:\nTo deliver the greatest performance to TensorFlow Lite users on mobile devices, all operators were optimized for ARM NEON. The most critical ones (convolution, depthwise convolution, transposed convolution, fully-connected), were tuned in assembly for commonly-used ARM cores in mobile phones, e.g. Cortex-A53/A73 in Pixel 2 and Cortex-A55/A75 in Pixel 3.\nFor TensorFlow Lite users on x86-64 devices, XNNPACK added optimizations for SSE2, SSE4, AVX, AVX2, and AVX512 instruction sets.\nRather than executing TensorFlow Lite operators one-by-one, XNNPACK looks at the whole computational graph and optimizes it through operator fusion. For example, convolution with explicit padding is represented in TensorFlow Lite via a combination of PAD operator and a CONV_2D operator with VALID padding mode. XNNPACK detects this combination of operators and fuses the two operators into a single convolution operator with explicitly specified padding.\nThe XNNPACK backend for CPU joins the family of TensorFlow Lite accelerated inference engines for mobile GPUs, Android\u2019s Neural Network API, Hexagon DSPs, Edge TPUs, and the Apple Neural Engine. It provides a strong baseline that can be used on all mobile devices, desktop systems, and Raspberry Pi boards.\n\nWith the TensorFlow 2.3 release, XNNPACK backend is included with the pre-built TensorFlow Lite binaries for Android and iOS, and can be enabled with a one-line code change. XNNPACK backend is also supported in Windows, macOS, and Linux builds of TensorFlow Lite, where it is enabled via build-time opt-in mechanism. Following wider testing and community feedback, we plan to enable it by default on all platforms in an upcoming release.\nPerformance Improvements\nXNNPACK-accelerated inference in TensorFlow Lite has already been used in Google products in production, and we observed significant speedups across a wide variety of neural network architectures and mobile processors. The XNNPACK backend boosted background segmentation in Pixel 3a Playground by 5X and delivered 2X speedup on neural network models in Augmented Faces API in ARCore.\nWe found that TensorFlow Lite benefits the most from the XNNPACK backend on small neural network models and low-end mobile phones. Below, we present benchmarks on nine public models covering common computer vision tasks:\nMobileNet v2 image classification [download]\nMobileNet v3-Small image classification [download]\nDeepLab v3 segmentation [download]\nBlazeFace face detection [download]\nSSDLite 2D object detection [download]\nObjectron 3D object detection [download]\nFace Mesh landmarks [download]\nMediaPipe Hands landmarks [download]\nKNIFT local feature descriptor [download]\nSingle-threaded inference speedup with TensorFlow Lite with the XNNPACK backend compared to the default backend across 5 mobile phones. Higher numbers are better.\nSingle-threaded inference speedup with TensorFlow Lite with the XNNPACK backend compared to the default backend across 5 desktop, laptop, and embedded devices. Higher numbers are better.\nHow Can I Use It?\nThe XNNPACK backend is already included in pre-built TensorFlow Lite 2.3 binaries, but requires an explicit runtime opt-in to enable it. We\u2019re working to enable it by default in a future release.\nOpt-in to XNNPACK backend on Android/Java\nPre-built TensorFlow Lite 2.3 Android archive (AAR) already include XNNPACK, and it takes only a single line of code to enable it in Interpreter.Options object:\nInterpreter.Options interpreterOptions = new Interpreter.Options();\ninterpreterOptions.setUseXNNPACK(true);\nInterpreter interpreter = new Interpreter(model, interpreterOptions);\nOpt-in to XNNPACK backend on iOS/Swift\nPre-built TensorFlow Lite 2.3 CocoaPods for iOS similarly include XNNPACK, and a mechanism to enable it in the InterpreterOptions class:\nvar options = InterpreterOptions()\noptions.isXNNPackEnabled = true\nvar interpreter = try Interpreter(modelPath: \"model/path\", options: options)\nOpt-in to XNNPACK backend on iOS/Objective-C\nOn iOS XNNPACK inference can be enabled from Objective-C as well via a new property in the TFLInterpreterOptions class:\nTFLInterpreterOptions *options = [[TFLInterpreterOptions alloc] init];\noptions.useXNNPACK = YES;\nNSError *error;\nTFLInterpreter *interpreter =\n    [[TFLInterpreter alloc] initWithModelPath:@\"model/path\"\n                                      options:options\n                                        error:&error];\nOpt-in to XNNPACK backend on Windows, Linux, and Mac\nXNNPACK backend on Windows, Linux, and Mac is enabled via a build-time opt-in mechanism. When building TensorFlow Lite with Bazel, simply add --define tflite_with_xnnpack=true, and the TensorFlow Lite interpreter will use the XNNPACK backend by default.\nTry out XNNPACK with your TensorFlow Lite model\nYou can use the TensorFlow Lite benchmark tool and measure your TensorFlow Lite model performance with XNNPACK. You only need to enable XNNPACK by the --use_xnnpack=true flag as below, even if the benchmark tool is built without the --define tflite_with_xnnpack=true Bazel option.\nadb shell /data/local/tmp/benchmark_model \\\n  --graph=/data/local/tmp/mobilenet_quant_v1_224.tflite \\\n  --use_xnnpack=true \\\n  --num_threads=4\nWhich Operations Are Accelerated?\nThe XNNPACK backend currently supports a subset of floating-point TensorFlow Lite operators (see documentation for details and limitations). XNNPACK supports both 32-bit floating-point models and models using 16-bit floating-point quantization for weights, but not models with fixed-point quantization in weights or activations. However, you do not have to constrain your model to the operators supported by XNNPACK: any unsupported operators would transparently fall-back to the default implementation in TensorFlow Lite.\nFuture Work\nThis is just the first version of the XNNPACK backend. Along with community feedback, we intend to add the following improvements:\nIntegration of the Fast Sparse ConvNets algorithms\nHalf-precision inference on the recent ARM processors\nQuantized inference in fixed-point representation\n\nWe encourage you to leave your thoughts and comments on our GitHub and StackOverflow pages.\nAcknowledgements\nWe would like to thank Frank Barchard, Chao Mei, Erich Elsen, Yunlu Li, Jared Duke, Artsiom Ablavatski, Juhyun Lee, Andrei Kulik, Matthias Grundmann, Sameer Agarwal, Ming Guang Yong, Lawrence Chan, Sarah Sirajuddin.",
    "link": "https://blog.tensorflow.org/2020/07/accelerating-tensorflow-lite-xnnpack-integration.html",
    "imgSource": [
      "https://2.bp.blogspot.com/-Fq-o5JrAw6g/XxnJDHEagOI/AAAAAAAADZQ/CUVEpTdONn4JOw8Ffp9FMK8vMZEuSX9_wCLcBGAsYHQ/s1600/mobilephones.png",
      "https://3.bp.blogspot.com/-N7hbMVkZnLg/XxnJMyP-aeI/AAAAAAAADZU/1d6AajiGYd8wc_OimsNGsfXczUhcxMsewCLcBGAsYHQ/s1600/desktop.png"
    ],
    "time": "2023/12/03 00:58:43"
  },
  {
    "title": "Sharing Pixelopolis, a self-driving car demo from Google I/O built with TF-Lite",
    "content": "Posted by Miguel de Andr\u00e9s-Clavera, Product Manager, Google PI\n\nIn this post, I\u2019d like to share with you a demo we built for (and had planned to show at) Google I/O this year with TensorFlow Lite. I wish we had the opportunity to meet in person, but I hope you find this article interesting nonetheless!\nPixelopolis\nPixelopolis is an interactive installation that showcases self-driving miniature cars powered by TensorFlow Lite. Each car is outfitted with its own Pixel phone, which used its camera to detect and understand signals from the world around it. In order to sense lanes, avoid collisions and read traffic signs, the phone uses machine learning running on the Pixel Neural Core, which contains a version of an Edge TPU.\n\nAn edge computing implementation is a good option to make projects like this possible. Processing video and detecting objects are much more difficult using Cloud-based methods - due to latency. If you can, doing it on-device is much faster.\n\nUsers can interact with Pixelopolis via a \u201cstation\u201d (an app running on a phone), where they can select the destination the car will drive to. The car will navigate to the destination, and during the journey, the app shows real-time streaming video from the Car -- this allows the user to see what the car sees and detects. As you may notice from the gifs below, Pixelopolis has multilingual support built-in as well.\nStation App\nCar App\nHow it works\nUsing the front camera on a mobile device, we perform lane-keeping, localization and object detection right on the device in real-time. Not only that, in our case, the Pixel 4 also controls the motors and other electronic components via USB-C, so the car can stop when it detects other cars or turn at a right intersection when it needs to.\n\nIf you\u2019re interested in technical details, the remainder of this article describes the major components of the car, and our journey building it.\nLane-keeping\nWe explored a variety of models for Lane-keeping. As a baseline, we used a CNN to detect the traffic lines in each frame and adjust the steering wheel every frame, which works fine. We improved this by adding an LSTM and using multiple previous frames. After experimenting a bit more, we followed a similar model architecture to this paper.\nCNN model input and output\nModel Architecture\nnet_in = Input(shape = (80, 120, 3))\nx = Lambda(lambda x: x/127.5 - 1.0)(net_in)\nx = Conv2D(24, (5, 5), strides=(2, 2),padding=\"same\", activation='elu')(x)  \nx = Conv2D(36, (5, 5), strides=(2, 2),padding=\"same\", activation='elu')(x)\nx = Conv2D(48, (5, 5), strides=(2, 2),padding=\"same\", activation='elu')(x)\nx = Conv2D(64, (3, 3), padding=\"same\",activation='elu')(x)   \nx = Conv2D(64, (3, 3), padding=\"same\",activation='elu')(x)\nx = Dropout(0.3)(x)\nx = Flatten()(x)\nx = Dense(100, activation='elu')(x)\nx = Dense(50, activation='elu')(x)\nx = Dense(10, activation='elu')(x) \nnet_out = Dense(1, name='net_out')(x)\nmodel = Model(inputs=net_in, outputs=net_out)\nData Collection\nBefore we are able to use this model, we need to find a way to collect the image data from the car to train. The problem is we didn\u2019t have a car or a track to use at the time. So, we decided to use a simulator. We chose Unity and this simulator project from Udacity for lane-keeping data collection.\nMultiple waypoints on the track in the simulator\nBy setting multiple waypoints on the track, the car bot is able to drive to different locations and also collects data for us. In this simulator, we collect image data and steering angle every 50ms.\nImage Augmentation\nData Augmentation with various environments\nSince we do all data collection within the simulator, we need to create various environments in the scene because we want our model to be able to handle different lighting, background environment and other noises. We added these variables to the scene: random HDRI sphere ( with different rotation and exposure values), random brightness and color, and random cars.\nTraining\nOutput from the first Neural Network layer\nTraining the ML model using only the simulator doesn\u2019t mean it will actually work in the real-world situation, at least not the first try. The car ran on the tracks for a few seconds and then just went off the track for various reasons.\n\n\n\n\n\n\nEarly versions of the toy car running off the track/td>\nLater, we found out that we only trained the model using mostly straight tracks. To fix this imbalance data issue, we added various shapes of curves.\n(Left) square shape track, (Right) Curvy track\nAfter fixing the imbalanced dataset, the car began to correctly navigate corners.\nCar successfully turn at the corners\nTraining with the final track design\nFinal track design\nWe started creating more complex situations for the car, such as adding multiple intersections to the tracks. We also added more routing paths to make the car handle these new conditions. However, we ran into new problems right away which is the car turned and hit the side track when it tried to turn at the intersection because it saw some random objects outside the track.\nTraining the model with additional routing\nWe tested out many solutions and went with the one that was most simple and effective. We cropped only the bottom \u00bc of the image and fed it to the lane keeping model, then adjusted the model input size to 120x40 and it works like a charm.\nCropping bottom part of the image for lane-keeping\nObject Detection\nWe use object detection for two purposes. One is for localization. Each car needs to know where it is in the city by detecting objects in its environment (in this case, we detect the traffic signs in the city). The other purpose is to detect other cars, so they won\u2019t bump into each other.\n\nFor choosing the object detector model there are many models already available in TensorFlow object detection model zoo. But, for the Pixel 4 edge TPU, we use the ssd_mobilenet_edgetpu model.\n\nssd_mobilenet_edgetpu model on Pixel 4\u2019s \u201cNeural Core\u201d Edge TPU is currently the fastest mobilenet object detection. It takes only 6.6 ms per frame, which is more than enough for using with real-time applications.\nPixel 4 Edge TPU model performance\nData labelling and Simulation\nWe use image data from both simulation and real scenes to train the model. Next, we developed our own simulator for this using Unreal Engine 4. The simulator generates random objects with random background and also an annotation file in a Pascal VOC format that is used in TensorFlow object detection API.\nObject detection simulator using UE4\nFor images that were taken from the real scene We have to do manual labeling using the labelImg tool.\nData labeling with labelImg\nTraining\nLoss report\nWe used TensorBoard to monitor training progress. We use it to evaluate mAP (mean Average Precision), which normally you have to do it manually.\nTensorBoard\nDetection result and the groundtruth\nTensorFlow Lite\nSince we want to run our ML model on the Pixel 4, which is running Android, we need to convert all the models to .tflite. Of course, you can use TensorFlow lite to target iOS and other devices as well (including microcontrollers). Here are the steps we did:\nLane keeping\nFirst, we convert the lane keeping model from .h5 to .tflite by using\nimport tensorflow as tf\nconverter = tf.lite.TFLiteConverter.from_keras_model_file(\"lane_keeping.h5\")\nmodel = converter.convert()\nfile = open(\"lane_keeping.tflite\",'wb') \nfile.write( model )\nfile.close()\nNow, we have the model ready for the Android project. Next, we build a lane keeping class in our app. We began with an example android project from here.\nObject detection\nWe have to convert the model checkpoint (.ckpt) to tensorflow lite format (.tflite)\nUsing export_tflite_ssd_graph.py script to convert .ckpt to .pb file (the script already provided in Tensorflow object detector API)\nUsing toco: TensorFlow Lite Converter to convert .pb to .tflite format\n\nUsing Neural Core\nWe use an Android sample project from here. Then we modified the delegate to use Pixel 4 Edge TPU with the following code.\nInterpreter.Options tfliteOptions = new Interpreter.Options();\nnnApiDelegate = new NnApiDelegate();\ntfliteOptions.addDelegate(nnApiDelegate);\ntfLite = new Interpreter(loadModelFile(assetManager, modelFilename),tfliteOptions);\nReal-time Video Streaming\nAfter a user selects a destination, the car will start driving itself. While it\u2019s driving, the car will stream what it sees to the station phone as a video feed. When we started implementing this part, we knew right away that streaming a raw video feed wouldn\u2019t be possible due to the amount of data that we need to transfer between several car phones and station phones. The solution that we use is, first, compress a raw image frame to a JPEG format to reduce the amount of that data, then stream the JPEG buffer via http protocol using multipart/x-mixed-replace as an HTTP Content-type. This way we can achieve several video streams at the same time with unnoticeable lag between the devices.\nServer App\nServer Stack\nWe use NodeJS for the server app and MongoDB for the database.\nHail a Car\nSince we have multiple stations and cars, we need to find a way to connect these two together. We built a booking system similar to popular car apps. Our booking system has 3 steps. First, the car connects to the server and tells the server that it\u2019s ready to be booked. Second, the station connects to the server and asks the server for a car. Third, the server looks for the car that\u2019s ready and connects these two together and also stores the device_id from both station and car apps.\nNavigation\nNode/Edge\nSince we will have a fleet of cars running around in the city, we need to find a way to navigate them. We use the Node/Edge concept. Node is a place on the map and Edge is the path between two Nodes. We then map each node to the actual signs in the city.\nTop view of the tracks and sign locations\nWhen the destination is selected on the station app, the station will send node_id to the server and the server will return an object which indicates a list of nodes and their properties so the car knows where to drive to and the expected sign it will see.\nElectronics\nParts\nWe started off with NUCLEO-F411RE as our development board. We chose Dynamixel for the motors.\nNUCLEO-F411RE\nWe designed and developed a shield for additional components such as motors to reduce the number of wires inside the car chassis.There are three parts in the shield: 1) Battery measurement in voltage, 2) On/off switch with MOSFET, 3) Buttons.\n(Left) Shield and Motors, (Right) Power socket, power switch, Enable motor button, Reset Motor button, Board status LED, Motor status LED\nIn the later phase, we would like to make the car a lot smaller, so we moved from NUCLEO-F411RE to NUCLEO-L432KC because it has a lot smaller footprint.\nNUCLEO-L432KC\nCar Chassis & Exterior\nMark I\nMark I Design\nWe designed and 3D printed the car chassis with PLA material. The front wheels are castor wheels.\nMark II\nMark II Design\nWe added a battery measurement circuit to the board and cut off the power when the phone detached from the board.\nMark III\nMark III Design\nWe added status LEDs so we can easily debug the state of the board. From the previous version, we encountered a motor overheating issue, so in this version we improved the ventilation by adding a fan to the motor. We also added a USB Type-C power delivery to the board so the phone can use the car battery.\nMark IV\nMark IV Design\nWe moved all the control buttons and status LEDs to the back of the car for an easy access.\nMark V\nMark V Design\nThis is the final version and we need to reduce the car footprint as much as possible. First, we changed the board from NUCLEO-F411RE to NUCLEO-L432KC to achieve a smaller footprint. Second, the front wheel has been changed to ball caster wheels. Third, we rearranged the board location to the top of the car and stacked the battery underneath the board. Lastly, we removed the USB Type-C power delivery because we want to prolong the driving time by giving all battery power to the board and motors instead of the phone.\nPerformance metrics\nRoadmap\nThere are many areas that we plan to improve this experience.\nBattery\nCurrently, the motor and the controller board are powered by three packs of 3000mAh lithium-ion batteries and we have a charging circuit to handle the charging process. When we want to charge the battery, we would need to move the car to the charging station and plug the power adapter to the back of the car to charge. This has a lot of downsides because the car won\u2019t be able to run on the track if it\u2019s charging and the charging time is a few hours which is quite long.\n3000mAh Li-ion Battery (left), 18650 Li-ion Battery (right)\nWe would like to reduce this process by changing the battery to an 18650 battery cell instead. This type of battery is used in electronics such as laptops, tools, and e-bikes, due to the high capacity in a small form factor. This way we can swap the battery easily by popping in the new ones and let the empty ones charge in the battery charger without leaving the car at the charging station.\nLocalization\nLocalization with SLAM\nLocalization is a very important process for this installation and we would like to make it more robust by adding SLAM to our app. We believe that this would improve the turning mechanism significantly.\nLearning more\nThanks so much for reading! It's incredible what you can do with a phone camera, TensorFlow and a bit of imagination. Hopefully, this post gave you ideas for your own projects - we learned a lot working on this one, and hope you will in yours as well. The article provides links to resources for you to delve deeper into all the different areas and you can find plenty of ML models and tutorials by the developer community to learn from on TensorFlow hub.\n\nIf you\u2019re really passionate about building self-driving cars and want to learn more about how machine learning and deep learning are powering the autonomous vehicle industry check out Udacity\u2019s Self Driving Cars Nanodegree Program. It\u2019s perfect for engineers & students looking for complete training in all aspects of self-driving cars, including computer vision, sensor fusion & localization.\nAcknowledgements\nThis project would not have been possible without the following awesome and talented group of people: Sina Hassani, Ashok Halambi, Pohung Chen, Eddie Azadi, Shigeki Hanawa, Clara Tan Su Yi, Daniel Bactol, Kiattiyot Panichprecha Praiya Chinagarn, Pittayathorn Nomrak, Nonthakorn Seelapun, Jirat Nakarit, Phatchara Pongsakorntorn, Tarit Nakavajara, Witsarut Buadit, Nithi Aiempongpaiboon, Witaya Junma, Taksapon Jaionnom and Watthanasuk Shuaytong.",
    "link": "https://blog.tensorflow.org/2020/07/pixelopolis-self-driving-car-demo-tensorflow-lite.html",
    "imgSource": [
      "https://4.bp.blogspot.com/-7Gu5RteEQrU/Xwix-QKxCEI/AAAAAAAADRg/9RUeZzIER8wm4QQDsVe1_u-b5IGGnuQ_QCLcBGAsYHQ/s1600/106.gif",
      "https://4.bp.blogspot.com/-ZCRCjUikPRg/Xwi9A9mPPWI/AAAAAAAADSM/XULVkHR_VXoEbRx8ja_2YNMTruD3mHTSQCLcBGAsYHQ/s1600/stationapp.gif",
      "https://4.bp.blogspot.com/-hLrwbfBo1Bg/Xwi9NFalNdI/AAAAAAAADSQ/Df3wWgO1zZYwnVpZArwEyzUpaFZyqO8AACLcBGAsYHQ/s1600/carapp.gif",
      "https://2.bp.blogspot.com/--tyQ_e33YAc/XwjBCgfMbXI/AAAAAAAADSc/fCaL0Hy3s3c-MMnrpG05Tr7b-67XgTFngCLcBGAsYHQ/s1600/cnnmodel.png",
      "https://4.bp.blogspot.com/-Tc6HEwmEZ5Q/XwjDB_80b_I/AAAAAAAADSw/OT6e_PF19vIGNXolrR2E8-AYxW4zn9YTACLcBGAsYHQ/s1600/waypoints.gif",
      "https://2.bp.blogspot.com/-dyDMuJ6mzFA/XwjDq1rSLXI/AAAAAAAADS4/gP408u0N_MQhOS1lX59BIRiJ4syy36PIACLcBGAsYHQ/s1600/out4.gif",
      "https://1.bp.blogspot.com/-RYLgFssF9P0/XwjE5YU6FhI/AAAAAAAADTE/oGWfIkvDWecSxa0CpE4FH8M8CJa7xOMxACLcBGAsYHQ/s1600/training.png",
      "https://1.bp.blogspot.com/-85Z3VaN928o/XwjFkC2uMnI/AAAAAAAADTM/JtmQ3G8pPnkTlK7UGkvOHqUFVTLWxdt9wCLcBGAsYHQ/s1600/car.gif",
      "https://3.bp.blogspot.com/-nM5HEGWnxfY/XwjFulsyAFI/AAAAAAAADTQ/GUm4mlyioo4DITtoqvDjB0d99BdjX662wCLcBGAsYHQ/s1600/tracks.png",
      "https://1.bp.blogspot.com/-coY08Q0zZ_Y/XwjG2zGZKbI/AAAAAAAADTc/NuTcIhrvkSgLyWhxbAhtC2sxwC3uuAc-QCLcBGAsYHQ/s1600/Comp%2B1_2.gif",
      "https://2.bp.blogspot.com/-VKfkE-0-tLw/XwjH_UnTLPI/AAAAAAAADTo/qwrm0Ki-dfEjbQZMyW0fKPmRACjMu0eiACLcBGAsYHQ/s1600/finaltrack.png",
      "https://2.bp.blogspot.com/-VT9T-Onck58/XwjIkYiT-mI/AAAAAAAADTw/gA2Qb2eDbtIiGxjU4-VNGl5_zayshfMIgCLcBGAsYHQ/s1600/additionalrouting.png",
      "https://2.bp.blogspot.com/-1XXUPPcBBq0/XwjI2CxncTI/AAAAAAAADT4/YD6IdjVMdsUC4GRTDiergzeAHthtJAT_gCLcBGAsYHQ/s1600/crop.png",
      "https://1.bp.blogspot.com/-iXj3QBjewiA/XwjJStAIJdI/AAAAAAAADUA/uYlG460dZBIqRrCto_K23zHB6kazJgIHQCLcBGAsYHQ/s1600/pixel4.png",
      "https://3.bp.blogspot.com/-Udeb3_dVtzw/XwjMuu45znI/AAAAAAAADUM/1NVluuT0LjITd-jnLb45dHgtxwcndQ1zACLcBGAsYHQ/s1600/Screen%2BShot%2B2020-07-10%2Bat%2B1.14.41%2BPM.png",
      "https://1.bp.blogspot.com/-bWqTUgTyHGk/XwjNH6gos2I/AAAAAAAADUU/HlxOwToby04SUWynUe5EGflkCY9lTvTyACLcBGAsYHQ/s1600/datalabeling.png",
      "https://2.bp.blogspot.com/-zSs8L9EjqXA/XwjNV_6NoWI/AAAAAAAADUY/m0zYdgLl0UIXXrhgz8ozjE0IPchPMrwKgCLcBGAsYHQ/s1600/lossreport.png",
      "https://2.bp.blogspot.com/-eYr0l6fI470/XwjN_CVir9I/AAAAAAAADUo/3xAT2KzVaG8sqmoPIL0vGY0tWOtCT0KrwCLcBGAsYHQ/s1600/tensorboard.png",
      "https://1.bp.blogspot.com/-oNfFX96n788/XwjOoT_GDKI/AAAAAAAADUw/wnpCw26ZczAxK-ZNWyadRNUN4PNAc4YYgCLcBGAsYHQ/s1600/detectionresult.gif",
      "https://3.bp.blogspot.com/-gbBVoyJ20sg/XwjT_eZHK2I/AAAAAAAADU8/7UhEruSVOOEGGu9P31AD7QW4jZB9Qh9owCLcBGAsYHQ/s1600/nodeedge.png",
      "https://3.bp.blogspot.com/-CDlSYv9CC-w/XwjUg6I4PhI/AAAAAAAADVE/ukjB6UT1tn8bj4MJSf9jZkWb5qkTc5DtQCLcBGAsYHQ/s1600/topviewtracks.png",
      "https://4.bp.blogspot.com/-hGgKqCfkCqM/XwjVFlPC3cI/AAAAAAAADVQ/EIxA58qZDQwxrplLZdecX83jVFJ5nEu-QCLcBGAsYHQ/s1600/nucleo.jpg",
      "https://3.bp.blogspot.com/-GVugwga1V3s/XwjZgA4NAwI/AAAAAAAADVc/JeBk9fhKUf8SeX2mIyY5ebD78ZaVdTu-wCLcBGAsYHQ/s1600/Screen%2BShot%2B2020-07-10%2Bat%2B1.56.51%2BPM.png",
      "https://1.bp.blogspot.com/-q9b-VCAaFfs/XwjeKD_gAkI/AAAAAAAADVo/IMwUuEHfd6w9uSmdv4g5xNt-jLEHPHzsQCLcBGAsYHQ/s1600/nucleo2.jpg",
      "https://2.bp.blogspot.com/-ROsVHRo9_FQ/XwjgWtgyAMI/AAAAAAAADV0/V_5ddPyR9_cdLbDge5l0L4UwjA12-yU7wCLcBGAsYHQ/s1600/mark1.JPG",
      "https://2.bp.blogspot.com/-JSto1qGUfLU/XwjiQkZBgsI/AAAAAAAADWA/fGhhTrYDLY4cu0t59oJxtrjkKsBIQ-l8ACLcBGAsYHQ/s1600/markII.JPG",
      "https://3.bp.blogspot.com/-z3RVmOmUPhU/Xwjig9SI3fI/AAAAAAAADWI/YaN5G0AnFDo9DORA4n4AWTNHgIyO91cnQCLcBGAsYHQ/s1600/markIII.JPG",
      "https://3.bp.blogspot.com/-XmKSFHegAXI/Xwji4xN0e5I/AAAAAAAADWQ/Y61dQzNucB4UKvKS1tL1tQAl9z3qLUhKwCLcBGAsYHQ/s1600/markIV.JPG",
      "https://4.bp.blogspot.com/-nJlBUKKaFSA/Xwjjc1fDEaI/AAAAAAAADWc/BpysunwV9LY6Xyqf1EaQAIIKrlXu90PHwCLcBGAsYHQ/s1600/markV.JPG",
      "https://3.bp.blogspot.com/-ZhfhdO6OJDc/XwjkDbz8LiI/AAAAAAAADWk/oVoXZKupJn8YI8F_eUCPUmBpuBPubd5rACLcBGAsYHQ/s1600/Screen%2BShot%2B2020-07-10%2Bat%2B2.56.20%2BPM.png",
      "https://1.bp.blogspot.com/-TP1NYhyDq6k/XwjluhRCpsI/AAAAAAAADWw/eCfp-HEYRZMQn5gbGVikmHNoDiNFTIl9gCLcBGAsYHQ/s1600/DSC_0549.JPG",
      "https://3.bp.blogspot.com/-L6nTcTLKmQM/XwjmTqM-fHI/AAAAAAAADW4/E3RH-0H-R6QfK9bZ2Yu8Jlyq07BG96YrwCLcBGAsYHQ/s1600/localizationslam.gif"
    ],
    "time": "2023/12/03 00:58:43"
  },
  {
    "title": "TensorFlow operation fusion in the TensorFlow Lite converter",
    "content": "Posted by Ashwin Murthy, Software Engineer, TensorFlow team @ Google\nOverview\nEfficiency and performance are critical for edge deployments. TensorFlow Lite achieves this by means of fusing and optimizing a series of more granular TensorFlow operations (which themselves are composed of composite operations, like LSTM) into a single executable TensorFlow Lite unit.\n\nMany users have asked us for more granular control of the way operations can be fused to achieve greater performance improvements. Today, we are delivering just that by providing users with the ability to specify how operations can be fused.\n\nFurthermore, this new capability allows for seamless conversion of TensorFlow Keras LSTM operations\u2014one of our most requested features. And to top it off, you can now plug in a user-defined RNN conversion to TensorFlow Lite!\nFused operations are more efficient\nAs mentioned earlier, TensorFlow operations are typically composed of a number of primitive, more granular operations, such as tf.add. This is important in order to have a level of reusability, enabling users to create operations that are a composition of existing units. An example of a composite operation is tf.einsum. Executing a composite operation is equivalent to executing each of its constituent operations.\n\nHowever, with efficiency in mind, it is common to \u201cfuse\u201d the computation of a set of more granular operations into a single operation.\n\nAnother use for fused operations is providing a higher level interface to define complex transformations like quantization, which would otherwise be infeasible or very hard to do at a more granular level.\n\nConcrete examples of fused operations in TensorFlow Lite include various RNN operations like Unidirectional and Bidirectional sequence LSTM, convolution (conv2d, bias add, relu), fully connected (matmul, bias add, relu) and more.\n\nFusing TensorFlow operations into TensorFlow Lite operations has historically been challenging until now!\nOut-of-the-box RNN conversion and other composite operation support\nOut-of-the-box RNN conversion\nWe now support conversion of Keras LSTM and Keras Bidirectional LSTM, both of which are composite TensorFlow operations. This is the simplest way to get RNN-based models to take advantage of the efficient LSTM fused operations in TensorFlow Lite. See this notebook for end-to-end keras LSTM to TensorFlow Lite conversion and execution via the TensorFlow Lite interpreter.\n\nFurthermore, we enabled conversion to any other TensorFlow RNN implementation by providing a convenient interface to the conversion infrastructure. You can see a couple of examples of this capability using lingvo\u2019s LSTMCellSimple and LayerNormalizedLSTMCellSimple RNN implementations.\n\nFor more information, please look at our RNN conversion documentation.\n\nNote: We are working on adding quantization support for TensorFlow Lite\u2019s LSTM operations. This will be announced in the future.\nExtending conversion to other composite operations\nWe extended the TensorFlow Lite converter to enable conversion of other composite TensorFlow operations into existing or custom TensorFlow Lite operations.\n\nThe following steps are needed to implement a TensorFlow operation fusion to TensorFlow Lite:\nWrap the composite operation in a tf.function. In the TensorFlow model source code, identify and abstract out the composite operation into a tf.function with the experimental_implements function annotation.\nWrite conversion code. Conceptually, the conversion code replaces the composite implementation of this interface with the fused one. In the prepare-composite-functions pass, plug in your conversion code.\nInvoke the TensorFlow Lite converter. Use the TFLiteConverter.from_saved_model API to convert to TensorFlow Lite.\nFor the overall architecture of this infrastructure, see here. For detailed steps with code examples, see here. To learn how operation fusion works under the hood, see the detailed documentation.\nFeedback\nPlease email tflite@tensorflow.org or create a GitHub issue with the component label \u201cTFLiteConverter\u201d.\nAcknowledgements\nThis work would not have been possible without the efforts of Renjie Liu, a key collaborator on this project since its inception. We would like to thank Raziel Alvarez for his leadership and guidance. We would like to thank Jaesung Chung, Scott Zhu, Sean Silva, Mark Sandler, Andrew Selle, Qiao Liang and River Riddle for important contributions. We would like to acknowledge Sarah Sirajuddin, Jared Duke, Lawrence Chan, Tim Davis and the TensorFlow Lite team as well as Tatiana Shpeisman, Jacques Pienaar and the Google MLIR team for their active support of this work.",
    "link": "https://blog.tensorflow.org/2020/07/tensorflow-operation-fusion-in-tensorflow-lite-converter.html",
    "imgSource": [
      "https://3.bp.blogspot.com/-3OwHpCMiYgQ/XvuE59vpabI/AAAAAAAADQ4/ayHWL6pICCkVHQXSJ0i1pU61zmRR5-grwCLcBGAsYHQ/s1600/op_fusion_banner.jpg"
    ],
    "time": "2023/12/03 00:58:43"
  },
  {
    "title": "Enhance your TensorFlow Lite deployment with Firebase",
    "content": "Posted by Khanh LeViet, TensorFlow Developer Advocate\n\n\nTensorFlow Lite is the official framework for running TensorFlow models on mobile and edge devices. It is used in many of Google\u2019s major mobile apps, as well as applications by third-party developers. When deploying TensorFlow Lite models in production, you may come across situations where you need some support features that are not provided out-of-the-box by the framework, such as:\nover-the-air deployment of TensorFlow Lite models\nmeasure model inference speed in user devices\nA/B test multiple model versions in production\nIn these cases, instead of building your own solutions, you can leverage Firebase to quickly implement these features in just a few lines of code.\n\nFirebase is the comprehensive app development platform by Google, which provides you infrastructure and libraries to make app development easier for both Android and iOS. Firebase Machine Learning offers multiple solutions for using machine learning in mobile applications.\n\nIn this blog post, we show you how to leverage Firebase to enhance your deployment of TensorFlow Lite models in production. We also have codelabs for both Android and iOS to show you step-by-step of how to integrate the Firebase features into your TensorFlow Lite app.\nDeploy model over-the-air instantly\nYou may want to deploy your machine learning model over-the-air to your users instead of bundling it into your app binary. For example, the machine learning team who builds the model has a different release cycle with the mobile app team and they want to release new models independently with the mobile app release. In another example, you may want to lazy-load machine learning models, to save device storage for users who don\u2019t need the ML-powered feature and reduce your app size for faster download from Play Store and App Store.\n\nWith Firebase Machine Learning, you can deploy models instantly. You can upload your TensorFlow Lite model to Firebase from the Firebase Console.\nYou can also upload your model to Firebase using the Firebase ML Model Management API. This is especially useful when you have a machine learning pipeline that automatically retrains models with new data and uploads them directly to Firebase. Here is a code snippet in Python to upload a TensorFlow Lite model to Firebase ML.\n# Load a tflite file and upload it to Cloud Storage.\nsource = ml.TFLiteGCSModelSource.from_tflite_model_file('example.tflite')\n\n# Create the model object.\ntflite_format = ml.TFLiteFormat(tflite_source=source)\nmodel = ml.Model(display_name=\"example_model\", model_format=tflite_format)\n\n# Add the model to your Firebase project and publish it.\nnew_model = ml.create_model(model)\nml.publish_model(new_model.model_id)\nOnce your TensorFlow Lite model has been uploaded to Firebase, you can download it in your mobile app at any time and initialize a TensorFlow Lite interpreter with the downloaded model. Here is how you do it on Android.\nval remoteModel = FirebaseCustomRemoteModel.Builder(\"example_model\").build()\n\n// Get the last/cached model file.\nFirebaseModelManager.getInstance().getLatestModelFile(remoteModel)\n  .addOnCompleteListener { task ->\n    val modelFile = task.result\n    if (modelFile != null) {\n      // Initialize a TF Lite interpreter with the downloaded model.\n      interpreter = Interpreter(modelFile)\n    }\n  }\nMeasure inference speed on user devices\nThere is a diverse range of mobile devices available in the market nowadays, from flagship devices with powerful chips optimized to run machine learning models to cheap devices with low-end CPUs. Therefore, your model inference speed on your users\u2019 devices may vary largely across your user base, leaving you wondering if your model is too slow or even unusable for some of your users with low-end devices.\n\nYou can use Performance Monitoring to measure how long your model inference takes across all of your user devices. As it is impractical to have all devices available in the market for testing in advance, the best way to find out about your model performance in production is to directly measure it on user devices. Firebase Performance Monitoring is a general purpose tool for measuring performance of mobile apps, so you also can measure any arbitrary process in your app, such as pre-processing or post-processing code. Here is how you do it on Android.\n// Initialize a Firebase Performance Monitoring trace\nval modelInferenceTrace = firebasePerformance.newTrace(\"model_inference\")\n\n// Run inference with TensorFlow Lite\ninterpreter.run(...)\n\n// End the Firebase Performance Monitoring trace\nmodelInferenceTrace.stop()\nPerformance data measured on each user device is uploaded to Firebase server and aggregated to provide a big picture of your model performance across your user base. From the Firebase console, you can easily identify devices that demonstrate slow inference, or see how inference speed differs between OS versions.\nA/B test multiple model versions\nWhen you iterate on your machine learning model and come up with an improved model, you may feel very eager to release it to a production right away. However, it is not rare that a model may perform well on test data but fail badly in production. Therefore, the best practice is to roll out your model to a smaller set of users, A/B test it with the original model and closely monitor how it affects your important business metrics before releasing it to all of your users.\n\nFirebase A/B Testing enables you to run this kind of A/B testing with minimal effort. The steps required are:\nUpload all TensorFlow Lite model versions that you want to test to Firebase, giving each one a different name.\nSetup Firebase Remote Config in the Firebase console to manage the TensorFlow Lite model name used in the app.\nUpdate the client app to fetch TensorFlow Lite model name from Remote Config and download the corresponding TensorFlow Lite model from Firebase.\nSetup A/B testing in the Firebase console.\nDecide the testing plan (e.g. how many percent of your user base to test each model version).\nDecide the metric(s) that you want to optimize for (e.g. number of conversions, user retention etc.).\nHere is an example of setting up an A/B test with TensorFlow Lite models. We deliver each of two versions of our model to 50% of our user base and with the goal of optimizing for multiple metrics.\nThen we change our app to fetch the model name from Firebase and use it to download the TensorFlow Lite model assigned to each device.\nval remoteConfig = Firebase.remoteConfig\nremoteConfig.fetchAndActivate()\n  .addOnCompleteListener(this) { task ->\n      // Get the model name from Firebase Remote Config\n      val modelName = remoteConfig[\"model_name\"].asString()\n      \n      // Download the model from Firebase ML\n      val remoteModel = FirebaseCustomRemoteModel.Builder(modelName).build()\n      val manager = FirebaseModelManager.getInstance()\n      manager.download(remoteModel).addOnCompleteListener {\n        // Initialize a TF Lite interpreter with the downloaded model\n        interpreter = Interpreter(modelFile)\n      }\n  }\nAfter you have started the A/B test, Firebase will automatically aggregate the metrics on how your users react to different versions of your model and show you which version performs better. Once you are confident with the A/B test result, you can roll out the better version to all of your users with just one click.\nNext steps\nCheck out this codelab (Android version or iOS version) to learn step by step how to integrate these Firebase features into your app. It starts with an app that uses a TensorFlow Lite model to recognize handwritten digits and show you:\nHow to upload a TensorFlow Lite model to Firebase via the Firebase Console and the Firebase Model Management API.\nHow to dynamically download a TensorFlow Lite model from Firebase and use it.\nHow to measure pre-processing, post processing and inference time on user devices with Firebase Performance Monitoring.\nHow to A/B test two versions of a handwritten digit classification model with Firebase A/B Testing.\nAcknowledgements\nAmy Jang, Ibrahim Ulukaya, Justin Hong, Morgan Chen, Sachin Kotwani",
    "link": "https://blog.tensorflow.org/2020/06/enhance-your-tensorflow-lite-deployment-with-firebase.html",
    "imgSource": [
      "https://2.bp.blogspot.com/-BqTV89AlTXs/XvVDQumnVbI/AAAAAAAADPQ/UExLLiuf0hwI7WPgbFXWOm1sWJy4IOIpwCLcBGAsYHQ/s1600/tensorflowandfirebase.png",
      "https://4.bp.blogspot.com/-nwjBE3isSAY/XvVDmpk39-I/AAAAAAAADPY/K8niux6qNl8gX8mA0wEwNJUWmgVpcn2jQCLcBGAsYHQ/s1600/machinelearning.png",
      "https://1.bp.blogspot.com/-qDOd8-i5EL4/XvVEcfLFf0I/AAAAAAAADPk/RfrICUwsiKQlB4N4f5dIOopkBpU7LiuXgCLcBGAsYHQ/s1600/inferencespeeds.png",
      "https://2.bp.blogspot.com/-4vGUgiYvuBE/XvVE8zYGPPI/AAAAAAAADPs/-FU-EtutvLojV7QB6HcxyEtbdJ7ZsT79ACLcBGAsYHQ/s1600/experimentoverview.png",
      "https://1.bp.blogspot.com/-4qrh6FxloI8/XvVFIUIVFkI/AAAAAAAADPw/0rRw79Nxo7gYKzhU6JDxdFDwfHIqVPlYACLcBGAsYHQ/s1600/clearimprovements.png"
    ],
    "time": "2023/12/02 00:58:43"
  },
  {
    "title": "Running and Testing TF Lite on Microcontrollers without hardware in Renode",
    "content": "A guest post by Michael Gielda of Antmicro\n\nEvery day more and more software developers are exploring the worlds of machine learning, embedded systems, and the Internet of Things. Perhaps one of the most exciting advances to come out of the most recent innovations in these fields is the incorporation of ML at the edge and into smaller and smaller devices - often referred to as TinyML.\n\nIn \u201cThe Future of Machine Learning is Tiny\u201d, Pete Warden predicted that machine learning would become increasingly available on tiny, low-power devices. Thanks to the work of the TensorFlow community, the power and flexibility of the framework is now also available on fairly resource-constrained devices like Arm Cortex-M MCUs, as per Pete\u2019s prediction.\n\nThousands of developers using TensorFlow can now deploy ML models for actions such as keyphrase detection or gesture recognition onto embedded and IoT devices. However, testing software at scale on many small and embedded devices can still be challenging. Whether it's difficulty sourcing hardware components, incorrectly setting up development environments or running into configuration issues while incorporating multiple unique devices into a multi-node network, sometimes even a seemingly simple task turns out to be complex.\nRenode 1.9 was released just last month\nEven experienced embedded developers find themselves trudging through the process of flashing and testing their applications on physical hardware just to accomplish simple test-driven workflows which are now commonplace in other contexts like Web or desktop application development.\n\nThe TensorFlow Lite MCU team also faced these challenges: how do you repeatedly and reliably test various demos, models, and scenarios on a variety of hardware without manually re-plugging, re-flashing and waving around a plethora of tiny boards?\n\nTo solve these challenges, they turned to Renode, an open source simulation framework from Antmicro that strives to do just that: allow hardware-less, Continuous Integration-driven workflows for embedded and IoT systems.\n\nIn this article, we will show you the basics of how to use Renode to run TensorFlow Lite on a virtual RISC-V MCU, without the need for physical hardware (although if you really want to, we\u2019ve also prepared instructions to run the same exact software on a Digilent Arty board).\n\nWhile this tutorial focuses on a RISC-V-based platform, Renode is able to simulate software targeting many different architectures, like Arm, POWER and others, so this approach can be used with other hardware as well.\nWhat\u2019s the deal with Renode?\nAt Antmicro, we pride ourselves on our ability to enable our customers and partners to create scalable and sustainable advanced engineering solutions to tackle complex technical challenges. For the last 10 years, our team has worked to overcome many of the same structural barriers and developer tool deficiencies now faced by the larger software developer community. We initially created the Renode framework to meet our own needs, but as proud proponents of open source, in 2015 we decided to release it under a permissive license to expand the reach and make embedded system design flexible, mobile and accessible to everyone.\n\nRenode, which has just released version 1.9, is a development framework which accelerates IoT and embedded systems development by letting you simulate physical hardware systems - including both the CPU, peripherals, sensors, environment and - in case of multi-node systems - wired or wireless medium between nodes. It\u2019s been called \u201cdocker for embedded\u201d and while the comparison is not fully accurate, it does convey the idea pretty well.\nRenode allows you to deterministically simulate entire systems and dynamic environments - including feeding modeled sample data to simulated sensors which can then be read and processed by your custom software and algorithms. The ability to quickly run unmodified software without access to physical hardware makes Renode an ideal platform for developers looking to experiment and build ML-powered applications on embedded and IoT devices with TensorFlow Lite.\nGetting Renode and demo software\nTo get started, you first need to install Renode as detailed in its README file - binaries are available for Linux, Mac and Windows.\n\nMake sure you download the proper version for your operating system to have the renode command available. Upon running the renode command in your terminal you should see the Monitor pop up in front of you, which is Renode\u2019s command-line interface.\nThe Renode \u201cMonitor\u201d CLI\nOnce Renode has started, you\u2019re good to go - remember, you don\u2019t need any hardware.\n\nWe have prepared all the files you will need for this demo in a dedicated GitHub repository.\n\nClone this repository with git (remember to get the submodules):\ngit clone --recurse-submodules https://github.com/antmicro/litex-vexriscv-tensorflow-lite-demo \nWe will need a demo binary to run. To simplify things, you can use the precompiled binary from the binaries/magic_wand directory (in \u201cBuilding your own application\u201d below we\u2019ll explain how to compile your own, but you only need to do that when you\u2019re ready).\nRunning TensorFlow Lite in Renode\nNow the fun part! Navigate to the renode directory:\ncd renode\nThe renode directory contains a model of the ADXL345 accelerometer and all necessary scripts and assets required to simulate the Magic Wand demo.\n\nTo start the simulation, first run renode with the name of the script to be loaded. Here we use \u201clitex-vexriscv-tflite.resc\u201c, which is a \u201cRenode script\u201d (.resc) file with the relevant commands to create the needed platform and load the application to its memory:\nrenode litex-vexriscv-tflite.resc\nYou will see Renode\u2019s CLI, called \u201cMonitor\u201d, from which you can control the emulation. In the CLI, use the start command to begin the simulation:\n(machine-0) start\nYou should see the following output on the simulated device\u2019s virtual serial port (also called UART - which will open as a separate terminal in Renode automatically):\nAs easy as 1-2-3\nWhat just happened?\nRenode simulates the hardware (both the RISC-V CPU but also the I/O and sensors) so that the binary thinks it\u2019s running on the real board. This is achieved by two Renode features: machine code translation and full SoC support.\n\nFirst, the machine code of the executed application is translated to the native host machine language.\n\nWhenever the application tries to read from or write to any peripheral, the call is intercepted and directed to an appropriate model. Renode models, usually (but not exclusively) written in C# or Python, implement the register interface and aim to be behaviorally consistent with the actual hardware. Thanks to the abstract nature of these models, you can interact with them programmatically from the Renode CLI or from script files.\n\nIn our example we feed the virtual sensor with some offline, pre-recorded angle and circle gesture data files:\n\ni2c.adxl345 FeedSample @circle.data\n\nThe TF Lite binary running in Renode processes the data and - unsurprisingly - detects the gestures.\n\nThis shows another benefit of running in simulation - we can be entirely deterministic should we choose to, or devise more randomized test scenarios, feeding specially prepared generated data, choosing different simulation seeds etc.\nBuilding your own application\nIf you want to build other applications, or change the provided demos, you can now build them yourself using the repository you have downloaded. You will need to install the following prerequisites (tested on Ubuntu 18.04):\nsudo apt update\nsudo apt install cmake ninja-build gperf ccache dfu-util device-tree-compiler wget python python3-pip python3-setuptools python3-tk python3-wheel xz-utils file make gcc gcc-multilib locales tar curl unzip\nSince the software is running the Zephyr RTOS, you will need to install Zephyr\u2019s prerequisites too:\nsudo pip3 install psutil netifaces requests virtualenv\n# install Zephyr SDK\nwget https://github.com/zephyrproject-rtos/sdk-ng/releases/download/v0.11.2/zephyr-sdk-0.11.2-setup.run\nchmod +x zephyr-sdk-0.11.2-setup.run\n./zephyr-sdk-0.11.2-setup.run -- -d /opt/zephyr-sdk\nOnce all necessary prerequisites are in place, go to the repository you downloaded earlier:\ncd litex-vexriscv-tensorflow-lite-demo\nAnd build the software with:\ncd tensorflow\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=zephyr_vexriscv \\\nmagic_wand_bin\nThe resulting binary can be found in the tensorflow/lite/micro/tools/make/gen/zephyr_vexriscv_x86_64/magic_wand/CMake/zephyr folder.\n\nCopy it into the root folder with:\nTF_BUILD_DIR=tensorflow/lite/micro/tools/make/gen/zephyr_vexriscv_x86_64\ncp ${TF_BUILD_DIR}/magic_wand/CMake/zephyr/zephyr.elf ../\ncp ${TF_BUILD_DIR}/magic_wand/CMake/zephyr/zephyr.bin ../\nYou can run it in Renode exactly as before.\n\nTo make sure the tutorial keeps working, and to showcase how simulation also enables you to do Continuous Integration easily, we also put together a Travis CI for the demo, and that is how the binary in the example is generated.\n\nWe will describe how the TensorFlow Lite team uses Renode for Continuous Integration and how you can do that yourself in a separate note soon - stay tuned for that!\nRunning on hardware\nNow that you have the binaries and you\u2019ve seen them work in Renode, let\u2019s see how the same binary behaves on physical hardware.\n\nYou will need a Digilent Arty A7 board and ACL2 PMOD, connected to the rightmost Pmod connector as in the picture.\nThe hardware setup\nThe system is a SoC-in-FPGA called LiteX, with a pretty capable RISC-V core and various I/O options.\n\nTo build the necessary FPGA gateware containing our RISC-V SoC, we will be using LiteX Build Environment, which is an FPGA oriented build system that serves as an easy entry into FPGA development on various hardware platforms.\n\nNow initialize the LiteX Build Environment:\ncd litex-buildenv\nexport CPU=vexriscv\nexport CPU_VARIANT=full\nexport PLATFORM=arty\nexport FIRMWARE=zephyr\nexport TARGET=tf\n\n./scripts/download-env.sh\nsource scripts/enter-env.sh\nThen build the gateware:\nmake gateware\nOnce you have built the gateware, load it onto the FPGA with:\nmake gateware-load\nWith the FPGA programmed, you can load the Zephyr binary on the device using the flterm program provided inside the environment you just initialized above:\nflterm --port=/dev/ttyUSB1 --kernel=zephyr.bin --speed=115200\nflterm will open the serial port. Now you can wave the board around and see the gestures being recognized in the terminal. Congratulations! You have now completed the entire tutorial.\nSummary\nIn this post, we have demonstrated how you can useTensorFlow Lite for MCUs without (and with) hardware. In the coming months, we will follow up with a description of how you can proceed from interactive development with Renode to doing Continuous Integration of your Machine Learning code, and then show the advantages of combining the strengths of TensorFlow Lite and the Zephyr RTOS.\n\nYou can find the most up to date instructions in the demo repository. The repository links to tested TensorFlow, Zephyr and LiteX code versions via submodules. Travis CI is used to test the guide.\n\nIf you\u2019d like to explore more hardware and software with Renode, check the complete list of supported boards. If you encounter problems or have ideas, file an issue on GitHub, and for specific needs, such as enabling TensorFlow Lite and simulation on your platform, you can contact us at contact@renode.io.",
    "link": "https://blog.tensorflow.org/2020/06/running-and-testing-tf-lite-on-microcontrollers.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-_ati_gUy35k/XuDxnkOpjaI/AAAAAAAADM8/2z1H2SsCDmcUYZFUYYCx7Z0qMJr4YYdlQCLcBGAsYHQ/s1600/tf-lite-in-renode_TF-blog.png",
      "https://3.bp.blogspot.com/-eLqq4QXd1OI/Xt6C5fwoqfI/AAAAAAAADMY/s6bpO6vCCnwmC1A_xfbCuogjA5AqmKSCgCLcBGAsYHQ/s1600/renodemonitor.png",
      "https://3.bp.blogspot.com/-m_CdMB6YzOE/Xt6Fqxemu9I/AAAAAAAADMk/SZAC_4Eys_ESPioONyewwkkD6rQDYMGWACLcBGAsYHQ/s1600/simulateddevice.gif",
      "https://3.bp.blogspot.com/-NUM7zU2I5aY/Xt6IeAFwv-I/AAAAAAAADMw/JyCN7KFjVHgOLzkDaKqfbV5wUno6j5KnwCLcBGAsYHQ/s1600/thehardwaresetup.jpg"
    ],
    "time": "2023/12/02 00:58:43"
  },
  {
    "title": "Simulated Spotify Listening Experiences for Reinforcement Learning with TensorFlow and TF-Agents",
    "content": "Posted by Surya Kanoria, Joseph Cauteruccio, Federico Tomasi, Kamil Ciosek, Matteo Rinaldi, and Zhenwen Dai \u2013 Spotify\nIntroduction\nMany of our music recommendation problems involve providing users with ordered sets of items that satisfy users\u2019 listening preferences and intent at that point in time. We base current recommendations on previous interactions with our application and, in the abstract, are faced with a sequential decision making process as we continually recommend content to users.\nReinforcement Learning (RL) is an established tool for sequential decision making that can be leveraged to solve sequential recommendation problems. We decided to explore how RL could be used to craft listening experiences for users. Before we could start training Agents, we needed to pick a RL library that allowed us to easily prototype, test, and potentially deploy our solutions.\nAt Spotify we leverage TensorFlow and the extended TensorFlow Ecosystem (TFX, TensorFlow Serving, and so on) as part of our production Machine Learning Stack. We made the decision early on to leverage TensorFlow Agents as our RL Library of choice, knowing that integrating our experiments with our production systems would be vastly more efficient down the line.\nOne missing bit of technology we required was an offline Spotify environment we could use to prototype, analyze, explore, and train Agents offline prior to online testing. The flexibility of the TF-Agents library, coupled with the broader advantages of TensorFlow and its ecosystem, allowed us to cleanly design a robust and extendable offline Spotify simulator.\nWe based our simulator design on TF-Agents Environment primitives and using this simulator we developed, trained and evaluated sequential models for item recommendations, vanilla RL Agents (PPG, DQN) and a modified deep Q-Network, which we call the Action-Head DQN (AH-DQN), that addressed the specific challenges imposed by the large state and action space of our RL formulation.\nThrough live experiments we were able to show that our offline performance estimates were strongly correlated with online results. This then opened the door for large scale experimentation and application of Reinforcement Learning across Spotify, enabled by the technological foundations unlocked by TensorFlow and TF-Agents.\nIn this post we\u2019ll provide more details about our RL problem and how we used TF-Agents to enable this work end to end.\nThe RL Loop and Simulated Users\nIn RL, Agents interact with the environment continuously. At a given time step the Agent consumes an observation from the environment and, using this observation, produces an action given its policy at time t. The environment then processes the action and emits both a reward and the next observation (note that although typically used interchangeably, State is the complete information required to summarize the environment post action, Observation is the portion of this information actually exposed to the Agent).\nIn our case the reward emitted from the environment is the response of a user to music recommendations driven by the Agent\u2019s action. In the absence of a simulator we would need to expose real users to Agents to observe rewards. We utilize a model-based RL approach to avoid letting an untrained Agent interact with real users (with the potential of hurting user satisfaction in the training process).\nIn this model-based RL formulation the Agent is not trained online against real users. Instead, it makes use of a user model that predicts responses to a list of tracks derived via the Agent\u2019s action. Using this model we optimize actions in such a way as to maximize a (simulated) user satisfaction metric. During the training phase the environment makes use of this user model to return a predicted user response to the action recommended by the Agent.\nWe use Keras to design and train our user model. The serialized user model is then unpacked by the simulator and used to calculate rewards during Agent training and evaluation.\nSimulator Design\nIn the abstract, what we needed to build was clear. We needed a way to simulate user listening sessions for the Agent. Given a simulated user and some content, instantiate a listening session and let the Agent drive recommendations in that session. Allow the simulated user to \u201creact\u201d to these recommendations and let the Agent adjust its strategy based on this result to drive some expected cumulative reward.\nThe TensorFlow Agents environment design guided us in developing the modular components of our system, each of which was responsible for different parts of the overall simulation.\nIn our codebase we define an environment abstraction that requires the following be defined for every concrete instantiation:\nclass AbstractEnvironment(ABC):\n    _user_model: AbstractUserModel = None\n    _track_sampler: AbstractTrackSampler = None\n    _episode_tracker: EpisodeTracker = None\n    _episode_sampler: AbstractEpisodeSampler = None\n\n    @abstractmethod\n    def reset(self) -> List[float]:\n      pass\n\n    @abstractmethod\n    def step(self, action: float) -> (List[float], float, bool):\n      pass\n\n    def observation_space(self) -> Dict:\n      pass\n\n    @abstractmethod\n    def action_space(self) -> Dict:\n      pass\nSet-Up\nAt the start of Agent training we need to instantiate a simulation environment that has representations of hypothetical users and the content we\u2019re looking to recommend to them. We base these instantiations on both real and hypothetical Spotify listening experiences. The critical information that defines these instantiations is passed to the environment via _episode_sampler. As mentioned, we also need to provide the simulator with a trained user model, in this case via _user_model.\nActions and Observations\nJust like any Agent environment, our simulator requires that we specify the action_spec and observation_spec. Actions in our case may be continuous or discrete depending both on our Agent selection and how we propose to translate an Agent\u2019s action into actual recommendations. We typically recommend ordered lists of items drawn from a pool of potential items. Formulating this action space directly would lead to it being combinatorially complex. We also assume the user will interact with multiple items, and as such previous work in this area that relies on single choice assumptions doesn\u2019t apply.\nIn the absence of a discrete action space consisting of item collections we need to provide the simulator with a method for turning the Agent\u2019s action into actual recommendations. This logic is contained in the via _track_sampler. The \u201cexample play modes\u201d proposed by the episode sampler contains information on items that can be presented to the simulated user. The track sampler consumes these and the agent\u2019s action and returns actual item recommendations.\nTermination and Reset\nWe also need to handle the episode termination dynamics. In our simulator, the reset rules are set by the model builder and based on empirical investigations of interaction data relevant to a specific music listening experience. As a hypothetical, we may determine that 92% of listening sessions terminate after 6 sequential track skips and we\u2019d construct our simulation termination logic to match. It also requires that we design abstractions in our simulator that allow us to check if the episode should be terminated after each step.\nWhen the episode is reset the simulator will sample a new hypothetical user listening session pair and begin the next episode.\nEpisode Steps\nAs with standard TF Agents Environments we need to define the step dynamics for our simulation. We have optional dynamics of the simulation that we need to make sure are enforced at each step. For example, we may desire that the same item cannot be recommended more than once. If the Agent\u2019s action indicates a recommendation of an item that was previously recommended we need to build in the functionality to pick the next best item based on this action.\nWe also need to call the termination (and other supporting functions) mentioned above as needed at each step.\nEpisode Storage and Replay\nThe functionality mentioned up until this point collectively created a very complex simulation setup. While the TF Agents replay buffer provided us with the functionality required to store episodes for Agent training and evaluation, we quickly realized the need to be able to store more episode data for debugging purposes, and more detailed evaluations specific to our simulation distinct from standard Agent performance measures.\nWe thus allowed for the inclusion of an expanded _episode_tracker that would store additional information about the user model predictions, information noting the sampled users/content pairs, and more.\nCreating TF-Agent Environments\nOur environment abstraction gives us a template that matches that of a standard TF-Agents Environment class. Some inputs to our environment need to be resolved before we can actually create the concrete TF-Agents environment instance. This happens in three steps.\nFirst we define a specific simulation environment that conforms to our abstraction. For example:\nclass PlaylistEnvironment(AbstractEnvironment):\n    def __init__(\n        self,\n        user_model: AbstractUserModel,\n        track_sampler: AbstractTrackSampler,\n        episode_tracker: EpisodeTracker,\n        episode_sampler: AbstractEpisodeSampler,\n  ....\n    ):\n\n...\nNext we use an Environment Builder Class that takes as input a user model, track sampler, etc. and an environment class like PlaylistEnvironment. The builder creates a concrete instance of this environment:\nself.playlist_env: PlaylistEnvironment = environment_ctor(\n            user_model=user_model,\n            track_sampler=track_sampler,\n            episode_tracker=episode_tracker,\n            episode_sampler=self._eps_sampler,\n        )\nLastly, we utilize a conversion class that constructs a TF-Agents Environment from a concrete instance of ours:\nclass TFAgtPyEnvironment(py_environment.PyEnvironment):\n      def __init__(self, environment: AbstractEnvironment):\n          super().__init__()\n          self.env = environment\nThis is then executed internally to our Environment Builder:\nclass EnvironmentBuilder(AbstractEnvironmentBuilder):\n\n      def __init__(self, ...):\n          ...\n\n      def get_tf_env(self):\n          ...\n          tf_env: TFAgtPyEnvironment = TFAgtPyEnvironment(\n              self.playlist_env\n              )\n          return tf_env\n\nThe resulting TensorFlow Agents environment can then be used for Agent training.\nThis simulator design allows us to easily create and manage multiple environments with a variety of different configurations as needed.\nWe next discuss how we used our simulator to train RL Agents to generate Playlists.\nA Customized Agent for Playlist Generation\nAs mentioned, Reinforcement Learning provides us with a method set that naturally accommodates the sequential nature of music listening; allowing us to adapt to users\u2019 ever evolving preferences as sessions progress.\nOne specific problem we can attempt to use RL to solve is that of automatic music playlist generation. Given a (large) set of tracks, we want to learn how to create one optimal playlist to recommend to the user in order to maximize satisfaction metrics. Our use case is different from standard slate recommendation tasks, where usually the target is to select at most one item in the sequence. In our case, we assume we have a user-generated response for multiple items in the slate, making slate recommendation systems not directly applicable. Another complication is that the set of tracks from which recommendations are drawn is ever changing.\nWe designed a DQN variant capable of handling these constraints that we called an Action Head DQN (AHDQN).\nThe AH-DQN network takes as input the current state and an available action to produce a single Q value for the input action. This process is repeated for every possible item in the input. Finally, the item with the highest Q value is selected and added to the slate, and the process continues until the slate is full.\nExperiments In Brief\nWe tested our approach both offline and online at scale to assess the ability of the Agent to power our real-world recommender systems. In addition to testing the Agent itself we were also keen to assess the extent to which our offline performance estimates for various policies returned by our simulator matched (or at least directionally aligned) with our online results.\n\nWe observed this directional alignment for numerous naive, heuristic, model driven, and RL policies.\n\n\n\n\n\nPlease refer to our KDD paper for more information on the specifics of our model-based RL approach and Agent design.\n\nAutomatic Music Playlist Generation via Simulation-based Reinforcement Learning\nFederico Tomasi, Joseph Cauteruccio, Surya Kanoria, Kamil Ciosek, Matteo Rinaldi, and Zhenwen Dai\nKDD 2023\n\nAcknowledgements\nWe\u2019d like to thank all our Spotify teammates past and present who contributed to this work. Particularly, we\u2019d like to thank Mehdi Ben Ayed for his early work in helping to develop our RL codebase. We\u2019d also like to thank the TensorFlow Agents team for their support and encouragement throughout this project (and for the library that made it possible).",
    "link": "https://blog.tensorflow.org/2023/10/simulated-spotify-listening-experiences-reinforcement-learning-tensorflow-tf-agents.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhP2qHWHXHUPcXak3sVnIQK59EBep2I4SxODdtyTmFFgkOujyn5Si1AHZTh9-xru72ZPZ_mFziQQX11NGawnhMh1EikNP1hTPgSKCX1EbPOeNU0Y1_UV7ZCQ8XFNROKmlffOcv5IXDzM6b3ckRYy1PaDKTUNgMttlHiU4CWOQwaHlh-Y3EbX8nrtxOZXJg/s1600/TensorFlowSimulated-Spotify-Listening-Experiences-social-V2.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8rdkHFZg7bozT_90hTvkhtGf5w03xXaSnmIPq-v1phY0UK3QXvxyfSjvtepuHA3-32CUbdowZsTSx3QqMy37kh1t2iEj_uusIQgSd2z_7UuuoSH2A5qxNrwIFWub3oItpU12ZKj65PWnfbGfsxUp9DPTJ0jZaKPMYEO_CuhV2EQpIXsHQHwJzQwyPCdk/s1600/TensorFlowSimulated-Spotify-Listening-Experiences-header-V2.png",
      null,
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbXOZI7T4iiAGjYNVId68JaJLtGqjk4g7a1058T3zuagJlm-cEBd2gNGUFYwnDu369omim5LVQL-1p0-SmbTRP6MsmziihreYJtH4trTsvkF50h2HyQMFZ-fkhkx_shg6JPZp2SMMCsC7fMreQkNxIxMBnnhl3pe-UaqAHU69IaWQsvBSePq_EDs1DzaA/s1600/TensorFlowSimulated%20Spotify%20Listening%20Experiences-charts-02.png",
      null,
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMM94Ox1CCXpjzz9O4z922a6nVp89j-1gpPxob5myKJ3oywvkUqy9zhktuRGc0042_Tebmi9sU1rMe_8lbDnrBkulgMzgJ_95X0-rUKPzUmrh8U6QpysOMSaODqc-ZznPq8Api7PSOYfCAH5rKeyy8qqG7iGSQOzOFgOXxS65IQ15K-uhEuZr6OtNVFkU/s1600/TensorFlowSimulated-Spotify-Listening-Experiences-charts-03-V2.png",
      null,
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiTslPiZ4ITRLSNrcBCLxVGyfmbDu0mxRj-iKGfNqnJoNw472CQg0YTdZHF2iE_pYwfUSw8kPtKCdMl4lCLXX0nYffYDMXU2l8wlnUfdlV_z9KTqPA8l6p3DOqAqL-ks2ZcwruY6f0qt5cXOVme9udBpsIsbqgCNlt4bKN71UnTOq4-9i9BHVA3O_xzUUo/s1600/TensorFlowSimulated%20Spotify%20Listening%20Experiences-charts-01.png",
      null,
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBJEpC0OzUfjzQTfbHYiqJ_cmQXg6gIxXkQKbZu7a-ER7ciyDcRu4ZTimAzlex7hxRnVwgNGOLDS7kxidsoqObJfYIkA6G3PLZ-01vj5Uw22T7FWxTiaDIzFUROU4X5bzFwPakxVLDhb6v5V6q728Q8xEbs13TYDzpkCw1rqOarUWGn0g4CD9rL6S7iwM/s1600/TensorFlowSimulated%20Spotify%20Listening%20Experiences-charts-04.png",
      null,
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhNP6pB_bKP170hXMVf20NPQjUzkLattEawfZwtUcXrDAl2mkcBT1Bqp1eMcnEdnT78D3m2IQQjOjKoZfje8cXgXh1ZV0BgQ66hS96rM3RRkeFWQfUWnRs8VCtiv0NFU_aqWiye3W5GVFaArVaisiaiU0yJ6tF_KkzBkOg89B4qLzgZ2f0VyYR0x5MQzgk/s1600/image3.gif",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj6CsH-cNX-i8iHq9Yif6iFykOfd5YY8Z5vbx9yu4bfhbN4oZdiL_EBC9Fxa9PLXh6JOzFF8FeH5lzuCagEAFymWTtueBD6MVhhCUTTwS4-lgulTRjHeq28lTT0U64cVw88m9zY5UEnNbkqyDzZk4HGWughW7Ju-pekqH59uITWKIhwoDvcHk4YeBRXn9A/w400-h270/image5.png"
    ],
    "time": "2023/12/02 00:58:43"
  },
  {
    "title": "Serving With TF and GKE: Stable Diffusion",
    "content": "Posted by Chansung Park and Sayak Paul (ML and Cloud GDEs)\nGenerative AI models like Stable Diffusion1 that lets anyone generate high-quality images from natural language text prompts enable different use cases across different industries. These types of models allow people to generate these images not only from images but also condition them with other inputs such as segmentation maps, other images, depth maps, etc. In many ways, an end Stable Diffusion system (such as this) is often very complete. One gives a free-form text prompt to start the generation process, and in the end, an image (or any data in the continuous modality) gets generated.\nIn this post, we discuss how TensorFlow Serving (TF Serving) and Google Kubernetes Engine (GKE) can serve such a system with online deployment. Stable Diffusion is just one example of many such systems that TF and GKE can serve with online deployment. We start by breaking down Stable Diffusion into main components and how they influence the subsequent consideration for deployment. Then we dive deep into the deployment-specific bits such as TF Serving deployment and k8s cluster configuration. Our code is open-sourced in this repository.\nLet\u2019s dive in.\nStable Diffusion in a nutshell\nStable Diffusion, is comprised of three sub-models:\nCLIP\u2019s text tower as the Text Encoder,\nDiffusion Model (UNet), and\nDecoder of a Variational Autoencoder\nWhen generating images from an input text prompt, the prompt is first embedded into a latent space with the text encoder. Then an initial noise is sampled, which is fed to the Diffusion model along with the text embeddings. This noise is then denoised using the Diffusion model in a continuous manner \u2013 the so-called \u201cdiffusion\u201d process. The output of this step is a denoise latent, and it is fed to the Decoder for final image generation. Figure 1 provides an overview.\n(For a more complete overview of Stable Diffusion, refer to this post.)\nFigure 1. Stable Diffusion Architecture\nAs mentioned above, three sub-models of Stable Diffusion work in a sequential manner. It\u2019s common to run all three models on a single server (which constructs the end Stable Diffusion system) and serve the system as a whole.\nHowever, because each component is a standalone deep learning model, each one could be served independently. This is particularly useful because each component has different hardware requirements. This can also have potentially improved resource utilization. The text encoder can still be run on moderate CPUs, whereas the other two should be run on GPUs, especially the UNet should be served with larger size GPUs (~3.4 GBs in size).\nFigure 2. Decomposing Stable Diffusion in three parts\nFigure 2 shows the Stable Diffusion serving architecture that packages each component into a separate container with TensorFlow Serving, which runs on the GKE cluster. This separation brings more control when we think about local compute power and the nature of fine-tuning of Stable Diffusion as shown in Figure 3.\nNOTE: TensorFlow Serving is a flexible, high-performance serving system for machine learning models, designed for production environments, which is widely adopted in industry. The benefits of using it include GPU serving support, dynamic batching, model versioning, RESTful and gRPC APIs, to name but a few.\nIn modern personal devices such as desktops and mobile phones, it is common that they are equipped with moderate CPUs and sometimes GPU/NPUs. In this case, we could selectively run the UNet and/or Decoder in the cloud using high capacity GPUs while running the text encoder locally on the user\u2019s device. In general, this approach allows us to flexibly architect the Stable Diffusion system in a way to maximize the resource utilization.\nFigure 3. Flexible serving structure of Stable Diffusion\nOne more scenario to consider is fine-tuned Stable Diffusion. Many variations such as DreamBooth, Textual Inversion, or style transfer have shown that modifying only one or two components (usually Text Encoder and UNet) can generate images with new concepts or different styles. In this case, we could selectively deploy more of certain fine-tuned models on separate instances or replace existing models without touching other parts.\nWrapping Stable Diffusion in SavedModels\nIn order to serve a TensorFlow/Keras model with TF Serving, it should be saved in the SavedModel format. After that, the model can be served by TF Serving, a high-performance serving system for machine learning models, specially designed for production environments. The potentially non-trivial parts of making a SavedModel could be divided into three parts:\ndefining an appropriate input signature specification of the underlying model,\nperforming computations with the underlying model so that everything can be compiled in native TensorFlow, and\nincluding most of the pre and post-processing operations within the SavedModel graph itself to reduce training/serving skew (this is optional, but highly recommended).\nTo make the Stable Diffusion class shipped in KerasCV compatible with TF Serving, we need to first isolate the sub-networks (as mentioned above) of the class. Recall that we have got three sub-networks here: text encoder, diffusion model, and a decoder. We then have to serialize these networks as SavedModels.\nA diffusion system also involves iterative sampling where a noise vector is gradually turned into an image. KerasCV\u2019s Stable Diffusion class implements the sampling process with non-TensorFlow operations. So, we need to eliminate those operations and ensure that it\u2019s implemented in pure TensorFlow so that there is end-to-end compatibility. This was the single most challenging aspect for us in the whole project.\nSince the serialization of the text encoder and the decoder is straightforward, we\u2019ll skip that in this post and instead, focus on the serialization of the diffusion model, including the sampling process. You can find an end-to-end notebook here.\nDiffusion Model and Iterative Sampling\nWe start by defining an input signature dictionary for the SavedModel to be serialized. In this case, the inputs consist:\ncontext, that denotes embeddings of the input text prompt extracted with the text encoder\nunconditional_context, that denotes the embeddings of a so-called \u201cnull prompt\u201d (see classifier-free guidance)\nnum_steps, that denotes the number of sampling steps for the reverse diffusion process\nbatch_size, that denotes the number of images to be returned\nfrom keras_cv.models.stable_diffusion.constants import ALPHAS_CUMPROD_TF\nimport tensorflow as tf\n\nIMG_HEIGHT = 512\nIMG_WIDTH = 512\nMAX_PROMPT_LENGTH = 77\nALPHAS_CUMPROD_TF = tf.constant(ALPHAS_CUMPROD_TF)\nUNCONDITIONAL_GUIDANCE_SCALE = 7.5\nHIDDEN_DIM = 768\nSEED = None\n\n\nsignature_dict = {\n    \"context\": tf.TensorSpec(shape=[None, MAX_PROMPT_LENGTH, HIDDEN_DIM], dtype=tf.float32, name=\"context\"),\n    \"unconditional_context\": tf.TensorSpec(\n        shape=[None, MAX_PROMPT_LENGTH, HIDDEN_DIM], dtype=tf.float32, name=\"unconditional_context\"\n    ),\n    \"num_steps\": tf.TensorSpec(shape=[], dtype=tf.int32, name=\"num_steps\"),\n    \"batch_size\": tf.TensorSpec(shape=[], dtype=tf.int32, name=\"batch_size\"),\n}\nNext up, we implement the iterative reverse diffusion process that involves the pre-trained diffusion model. diffusion_model_exporter() takes this model as an argument. serving_fn() is the function we use for exporting the final SavedModel. Most of this code is taken from the original KerasCV implementation here, except it has got all the operations implemented in native TensorFlow.\ndef diffusion_model_exporter(model: tf.keras.Model):\n    @tf.function\n    def get_timestep_embedding(timestep, batch_size, dim=320, max_period=10000):\n        ...\n    @tf.function(input_signature=[signature_dict])\n    def serving_fn(inputs):\n        img_height = tf.cast(tf.math.round(IMG_HEIGHT / 128) * 128, tf.int32)\n        img_width = tf.cast(tf.math.round(IMG_WIDTH / 128) * 128, tf.int32)\n\n        batch_size = inputs[\"batch_size\"]\n        num_steps = inputs[\"num_steps\"]\n\n        context = inputs[\"context\"]\n        unconditional_context = inputs[\"unconditional_context\"]\n\n        latent = tf.random.normal((batch_size, img_height // 8, img_width // 8, 4))\n\n        timesteps = tf.range(1, 1000, 1000 // num_steps)\n        alphas = tf.map_fn(lambda t: ALPHAS_CUMPROD_TF[t], timesteps, dtype=tf.float32)\n        alphas_prev = tf.concat([[1.0], alphas[:-1]], 0)\n\n        index = num_steps - 1\n        latent_prev = None\n        for timestep in timesteps[::-1]:\n            latent_prev = latent\n            t_emb = get_timestep_embedding(timestep, batch_size)\n            unconditional_latent = model(\n                [latent, t_emb, unconditional_context], training=False\n            )\n            latent = model([latent, t_emb, context], training=False)\n            latent = unconditional_latent + UNCONDITIONAL_GUIDANCE_SCALE * (\n                latent - unconditional_latent\n            )\n            a_t, a_prev = alphas[index], alphas_prev[index]\n            pred_x0 = (latent_prev - tf.math.sqrt(1 - a_t) * latent) / tf.math.sqrt(a_t)\n            latent = (\n                latent * tf.math.sqrt(1.0 - a_prev) + tf.math.sqrt(a_prev) * pred_x0\n            )\n            index = index - 1\n\n        return {\"latent\": latent}\n\n    return serving_fn\nThen, we can serialize the diffusion model as a SavedModel like so:\ntf.saved_model.save(\n    diffusion_model,\n    path_to_serialize_the_model,\n    signatures={\"serving_default\": diffusion_model_exporter(diffusion_model)},\n)\nHere, diffusion_model is the pre-trained diffusion model initialized like so:\nfrom keras_cv.models.stable_diffusion.diffusion_model import DiffusionModel\ndiffusion_model = DiffusionModel(IMG_HEIGHT, IMG_WIDTH, MAX_PROMPT_LENGTH)\nDeploy Stable Diffusion to GKE\nOnce you have successfully created TensorFlow SavedModels, it is quite straightforward to deploy them with TensorFlow Serving to a GKE cluster in the following steps.\nWrite Dockerfiles which are based on the TensorFlow Serving base image\nCreate a GKE cluster with accelerators attached\nApply NVIDIA GPU driver installation daemon to install the driver on each node\nWrite deployment manifests with GPU allocation\nWrite service manifests to expose the deployments\nApply all the manifests\nThe easiest way to wrap a SavedModel in TensorFlow Serving is to leverage the pre-built TensorFlow Serving Docker images. Depending on the configuration of the machine that you\u2019re deploying to, you should choose either tensorflow/serving:latest or tensorflow/serving:latest-gpu. Because all the steps besides GPU-specific configuration are the same, we will explain this section with an example of the Diffusion Model part only.\nBy default, TensorFlow Serving recognizes embedded models under /models, so the entire SavedModel folder tree should be placed inside /models/{model_name}/{version_num}. A single TensorFlow Serving instance can serve multiple versions of multiple models, so that is why we need such a {model_name}/{version_num} folder structure. A SavedModel can be exposed as an API by setting a special environment variable MODEL_NAME, which is used for TensorFlow Serving to look for which model to serve.\nFROM tensorflow/serving:latest-gpu\n...\nRUN mkdir -p /models/text-encoder/1\nRUN cp -r tfs-diffusion-model/* /models/diffusion-model/1/\nENV MODEL_NAME=diffusion-model\n...\nNext step is to create a GKE cluster. You can do this by using either Google Cloud Console or gcloud container CLI as below. If you want accelerators available on each node, you can specify how many of which GPUs to be attached with --accelerator=type={ACCEL_TYPE}, count={ACCEL_NUM} option.\n$ gcloud container clusters create {CLUSTER_NAME} \\\n  --machine-type={MACHINE_TYPE} \\                     # n1-standard-4\n  --accelerator=type={GPU_TYPE},count={GPU_NUM} \\     # nvidia-tesla-v100, 1\n  ...\nOnce the cluster is successfully created, and if the nodes in the cluster have accelerators attached, an appropriate driver for them should be installed correctly. This is done by running a special DaemonSet, which tries to install the driver on each node. If the driver has not been successfully installed, and if you try to apply Deployment manifests requiring accelerators, the status of the pod remains as Pending.\n$ DRIVER_URL = https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded.yaml\n\n$ kubectl apply -f $DRIVER_URL\nMake sure all the pods are up and running with kubectl get pods -A command. Then, we are ready to apply prepared Deployment manifests. Below is an example of the Deployment manifest for the Diffusion Model. The only consideration you need to take is to specify which resource the pods of the Deployment should consume. Because the Diffusion Model needs to be run on accelerators, resources:limits:nvidia.com/gpu: {ACCEL_NUM} should be set.\nFurthermore, if you want to expose gRPC and RestAPI at the same time, you need to set containerPort for both. TensorFlow Serving exposes the two endpoints via 8500 and 8501, respectively, by default, so both ports should be specified.\napiVersion: apps/v1\nkind: Deployment\n...\n    spec:\n      containers:\n      - image: {IMAGE_URI}\n...\n        args: [\"--rest_api_timeout_in_ms=1200000\"]\n        ports:\n        - containerPort: 8500\n          name: grpc\n        - containerPort: 8501\n          name: restapi\n        resources:\n          limits:\n            nvidia.com/gpu: 1\nOne more thing to note is that --rest_api_timeout_in_ms flag is set in args with a huge number. It takes a long time for heavy models to run inference. Since the flag is set to 5,000ms by default which is 5 seconds, sometimes timeout occurs before the inference is done. You can experimentally find out the right number, but we simply set this with a high enough number to demonstrate this project smoothly.\nThe final step is to apply prepared manifest files to the provisioned GKE cluster. This could be easily done with the kubectl apply -f command. Also, you could apply Service and Ingress depending on your needs. Because we simply used vanilla LoadBalancer type of Service for demonstration purposes, it is not listed in this blog. You can find all the Dockerfiles, and the Deployment and Service manifests in the accompanying GitHub repository.\nLet\u2019s generate images!\nOnce all the TensorFlow Serving instances are deployed, we could generate images by calling their endpoints. We will show how to do it through RestAPI, but you could do the same with the gRPC channel as well. The image generation process could be done in the following steps:\nPrepare tokens for the prompt of your choice\nSend the tokens to the Text Encoder endpoint\nSend context and unconditional context obtained from the Text Encoder to the Diffusion Model endpoint\nSend latent obtained from the Diffusion Model to the Decoder endpoint\nPlot the generated images\nSince it is non-trivial to embed a tokenizer into the Text Encoder itself, we need to prepare the tokens for the prompt of your choice. KerasCV library provides SimpleTokenizer in the keras_cv.models.stable_diffusion.clip_tokenizer module, so you could simply pass the prompt to it. Since the Diffusion Model is designed to accept 77 tokens, the tokens are padded with MAX_PROMPT_LENGTH up to 77 long.\nNOTE: Since KerasCV comes with lots of modules that we don\u2019t need for tokenization, it is not recommended to import the entire library. Instead, you could simply copy the codes for the SimpleTokenizer in your environment. Due to incompatibility issues, the current tokenizer cannot be shipped as a part of the Text Encoder SavedModel.\nfrom keras_cv.models.stable_diffusion.clip_tokenizer import SimpleTokenizer\n\nMAX_PROMPT_LENGTH = 77\nPADDING_TOKEN = 49407\n\ntokenizer = SimpleTokenizer()\n\nprompt = \"photograph of an astronaut riding a horse in a green desert\"\ntokens = tokenizer.encode(prompt)\ntokens = tokens + [PADDING_TOKEN] * (MAX_PROMPT_LENGTH - len(tokens))\nOnce the tokens are prepared, we could simply pass it to the Diffusion Model\u2019s endpoint. The headers and the way to call the all endpoints are identical as below, so we will omit it in the following steps. Just keep in mind you set the ADDRESS and the MODEL_NAME correctly, which is identical to the one we set in each Dockerfile.\nimport requests\n\nADDRESS = ENDPOINT_IP_ADDRESS\n\nheaders = {\"content-type\": \"application/json\"}\npayload = ENDPOINT_SPECIFIC\n\nresponse = requests.post(\n    f\"http://{ADDRESS}:8501/v1/models/{MODEL_NAME}:predict\", \n    data=payload, headers=headers\n)\nAs you see, each payload is dependent on the upstream tasks. For instance, we pass tokens to the Text Encoder\u2019s endpoint, context and unconditional_context retrieved from the Text Encoder to the Diffusion Model\u2019s endpoint, and latent retrieved from the Diffusion Model to Decoder\u2019s endpoint. The signature_name should be the same as when we created SavedModel with the signatures argument.\nimport json\n\n\nBATCH_SIZE = 4\n\npayload_to_text_encoder = json.dumps(\n    {\n        \"signature_name\": \"serving_default\", \n        \"inputs\": {\n            \"tokens\": tokens,\n            \"batch_size\": BATCH_SIZE\n        }\n})\n\n# json_response is from the text_encoder's response\n# json_response = json.loads(response.text)\npayload_to_diffusion_model = json.dumps(\n    {\n        \"signature_name\": \"serving_default\", \n        \"inputs\": {\n            \"batch_size\": BATCH_SIZE,\n            \"context\": json_response['outputs']['context'],\n            \"num_steps\": num_steps,\n            \"unconditional_context\": json_response['outputs']['unconditional_context']\n        }\n})\n\n# json_response is from the diffusion_model's response\n# json_response = json.loads(response.text)\npayload_to_decoder = json.dumps(\n    {\n        \"signature_name\": \"serving_default\", \n        \"inputs\": {\n            \"latent\": json_response['outputs'],\n        }\n})\nThe final response from the Decoder\u2019s endpoint contains a full of pixel values in a list, so we need to convert those into a format that the environment of your choice could understand as images. For demonstration purposes, we used the tf.convert_to_tensor() utility function that turns the Python list into TensorFlow\u2019s Tensor. However, you could plot the images in different languages, too, with your most familiar methods.\nimport matplotlib.pyplot as plt\n\ndef plot_images(images):\n    plt.figure(figsize=(20, 20))\n    for i in range(len(images)):\n        ax = plt.subplot(1, len(images), i + 1)\n        plt.imshow(images[i])\n        plt.axis(\"off\")\n\nplot_images(\n    tf.convert_to_tensor(json_response['outputs']).numpy()\n)\nFigure 4. Generated images with three TensorFlow Serving endpoints\nNote on XLA compilation\nWe can obtain a speed-up of 17 - 25% by incorporating compiling the SavedModels to be XLA compatible. Note that the individual sub-networks of the Stable Diffusion class are fully XLA compatible. But in our case, the SavedModels also contain important operations that are in native TensorFlow, such as the reverse diffusion process.\nFor deployment purposes, this speed-up could be impactful. To know more, check out the following repository: https://github.com/sayakpaul/xla-benchmark-sd.\nConclusion\nIn this blog post, we explored what Stable Diffusion is, how it could be decomposed into the Text Encoder, Diffusion Model, and Decoder, and why it might be beneficial for better resource utilization. Also, we touched upon the concrete demonstration about the deployment of the decomposed Stable Diffusion by creating SavedModels, containerizing them in TensorFlow Serving, deploying them on the GKE cluster, and running image generations. We used the vanilla Stable Diffusion, but feel free to try out replacing the only Diffusion Model with in-painting or pokemon fine-tuned diffusion models.\nReferences\nCLIP: Connecting text and images, OpenAI, https://openai.com/research/clip.\nThe Illustrated Stable Diffusion, Jay Alammar, https://jalammar.github.io/illustrated-stable-diffusion/.\nStable Diffusion, Stability AI, https://stability.ai/stable-diffusion.\nAcknowledgements\nWe are grateful to the ML Developer Programs team that provided Google Cloud credits to support our experiments. We thank Robert Crowe for providing us with helpful feedback and guidance.\n___________\n1 Stable Diffusion is not owned or operated by Google. It is made available by Stability AI. Please see their site for more information: https://stability.ai/blog/stable-diffusion-public-release.",
    "link": "https://blog.tensorflow.org/2023/04/serving-with-tf-and-gke-stable-diffusion.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEguJhniX2FvspHjYxJweJYNTFf1DAveLJuABYaoET7jnheNVQpnXN_twr3CuvTjMUZprSwoqF9QNipAjD985qwRfNpIg1HkPU2iUUfGeGFRkKTJOxpv9l1BWGd11bsrr6spltyp93eiW6XxGtl3foLcZ6zplnnGL617JyimdhzVxxwAhoJuDKtqpsV8/s1600/TF%20Stable%20Diffusion%20%282%29.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQ1OsQLLUKIwVt9yPv0pEuJ7PVJ-QWWUV2AvGkBX5gtHksVNkzlFITMLvV7VmrNTsAP6WL80Tn3X6Z3zdkBs4m1mjqUHletya9b0Ef5wgAvNqVndqKONvmwjWkwPQZsu7Q7OlQRpEKkefJ1ZveKCvVAKzDh4xyzVYIhqgw7SCIsjeqf1Bj4OOmlxwJ/s1600/image3.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1YVyDjjKWeQYsnna9cWkty6buXuJql2SA3xjLqji0teVuA_748WQl67s3--TJiusFbApgIW6z_FYMV3WoGCia0RijpaprQfD3pMBRMsVcZBPOB8xhfVdNAR9eWGHkYVGMNpU51S5myEFlp_HOFaM7V_xpwBy0pWLUXSR68LUQmtqOmeEi9Wv50vmz/s1600/image4.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEitUhiVzFClwCI5lUlN6wcIi3d-OkA8mogPW6B5WTSjuq535ELkDR3ziPZHySFk9tNq05Ps_kn7l3gFFP1LL8GAqg4t8_M9fP7ht4SXhX8smPO0k3PTq2DJ3EvJ6wSMHP912xM642eIfr0oiH_FXMBeV5MW8hHHOLrmpcVLVyUKlepKJqMaZz_mmTAE/s1600/image2.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8I4T52Q1T_QQBucX3djNEXFlkSWYXp1nvoQDzH61M3eDD54Z9AlUvxNTsnJheY0CSxX_W6zddB0vHBiXzsO4eF2RbD2mcjRb2fZanp3hR2lTXUMuZ_JB5N7pkJV1bCWk2fGNHvbepj0j4Q05kDj8gud3B1cHWL3oPBFQIkOnv9uPDGW-D_-pZ70zL/s1600/image1.png"
    ],
    "time": "2023/12/02 00:58:43"
  },
  {
    "title": "How Vodafone Uses TensorFlow Data Validation in their Data Contracts to Elevate Data Governance at Scale",
    "content": "Posted by Amandeep Singh (Vodafone), Max V\u00f6kler (Google Cloud)\nVodafone leverages Google Cloud to deploy AI/ML use cases at scale\nAs one of the largest telecommunications companies worldwide, Vodafone is working with Google Cloud to advance their entire data landscape, including their data lake, data warehouse (DWH), and in particular AI/ML strategies. While Vodafone has used AI/ML for some time in production, the growing number of use cases has posed challenges for industrialization and scalability. For Vodafone, it is key to rapidly build and deploy ML use cases at scale in a highly regulated industry. While Vodafone\u2019s AI Booster Platform \u2013 built on top of Google Cloud\u2019s Vertex AI \u2013 has provided a huge step to achieve that, this blog post will dive into how TensorFlow Data Validation (TFDV) helps advance data governance at scale.\nHigh-quality Data is a Prerequisite for ML Use Cases, yet not Easily Achieved\nExcelling in data governance is a key enabler to utilize the AI Booster Platform at scale. As Vodafone works in distributed teams and has shared responsibilities when developing use cases, it is important to avoid disruptions across the involved parties:\nMachine Learning Engineer at Vodafone Group level (works on global initiatives and provides best practices on productionizing ML at scale)\nData Scientist in local market (works on a concrete implementation for their specific country, needs to ensure proper data quality and feature engineering)\nData Owner in local market (needs to ensure data schemas do not change)\nAn issue that often arises is that table schemas are modified, or feature names and data types change. This could be due to a variety of reasons. For example, the data engineering process, which is owned by IT teams, is revised.\nData Contracts Define the Expected Form and Shape of Data\nData Contracts in machine learning are a set of rules that define the structure, data types, and constraints of the data that your models are trained on. The contracts provide a way to specify the expected schema and statistics of your data. The following can be included as part of your Data Contract:\nFeature names\nData types\nExpected distribution of values in each column.\nIt can also include constraints on the data, such as:\nMinimum and maximum values for numerical columns\nAllowed values for categorical columns.\nBefore a model is productionized, the Contract is agreed upon by the stakeholders working on the pipeline, such as the ML Engineers, Data Scientists and Data Owners. Once the Data Contract is agreed upon, it cannot change. If a pipeline breaks due to a change, the error can be traced back to the responsible party. If the Contract needs amending, it needs to go to a review between the stakeholders, and once agreed upon, the changes can be implemented into the pipeline. This helps ensure the quality of data going into our model in production.\nVodafone Benefits from TFDV Data Contracts as a Way to Streamline Data Governance\nAs part of Vodafone\u2019s efforts to streamline data governance, we made use of Data Contracts. A Data Contract ensures all teams work in unison, helping to maintain quality throughout the data lifecycle. These contacts are a powerful tool for managing and validating data used for machine learning. They provide a way to ensure that data is of high quality, free of errors and has the expected distribution. This blog post covers the basics of Data Contracts, discusses how they can be used to validate and understand your data better, and shows you how to use them in combination with TFDV to improve the accuracy and performance of your ML models. Whether you're a data scientist, an ML engineer, or a developer working with machine learning, understanding Data Contracts is essential for building high-quality, accurate models.\nHow Vodafone Uses Data Contracts\nUtilizing such a Data Contract, both in training and prediction pipelines, we can detect and diagnose issues such as outliers, inconsistencies, and errors in the data before they can cause problems with the models. Another great use of using Data Contracts is that it helps us detect data drift. Data drift is the most common reason for performance degradation in ML Models. Data drift is when the input data to your model changes to what it was trained on, leading to errors and inaccuracies in your predictions. Using Data Contracts can help you identify this issue.\nData Contracts are just one example of the many KPIs we have within Vodafone regarding AI Governance and Scalability. Since the development and release of AI Booster, more and more markets are using the platform to productionize their use case, and as part of this, we have the resources to scale components vertically. Examples of this, apart from Data Contracts, can be specialized logging, agreed-upon ways of calculating Model Metrics and Model Testing strategies, such as Champion/Challenger and A/B Testing.\nHow TensorFlow Data Validation (TFDV) Brings Data Contracts to Life\nTFDV is a library provided by the TensorFlow team, for analyzing and validating machine learning data. It provides a way to check that the data conforms to a Data Contract. TFDV also provides visualization options to help developers understand the data, such as histograms and summary statistics. It allows the user to define data constraints and detect errors, anomalies, and drift between datasets. This can help to detect and diagnose issues such as outliers, inconsistencies, and errors in your data before they can cause problems with your models.\nWhen you use TFDV to validate your data, it will check that the data has the expected schema. If there are any discrepancies, such as a missing column or a column with the wrong datatype, TFDV will raise an error and provide detailed information about the problem.\nAt Vodafone, before a pipeline is put into production, a schema is agreed upon for the input data. The agreement is between the Product Manager/Use Case Owner, Data Owner, Data Scientist and ML Engineer. The first thing we do in our pipeline, as seen in Figure 1, is to generate statistics about our data.\nFigure 1: Components of a Data Contract in a typical Vodafone training pipeline\nThe code below uses TFDV to generate statistics for the training dataset and visualizes them (step 2), making it easy to understand the distribution of the data and how it's been transformed. The output of this step is an HTML file, displaying general statistics about our input dataset. You can also choose a range of different functionalities on the HTML webpage to play around with the statistics and get a deeper understanding of the data.\n# generate training statistics\ngen_statistics = generate_statistics(\n dataset=train_dataset.output,\n file_pattern=file_pattern,\n).set_display_name(\"Generate data statistics\")\n\n# visualise statistics\nvisualised_statistics = visualise_statistics(\n statistics=gen_statistics.output,\n statistics_name=\"Training Statistics\"\n).set_display_name(\"Visualise data statistics\")\nStep 3 is concerned with validating the schema. Within our predefined schema, we also define some thresholds for certain data fields. We can specify domain constraints on our Data Contract, such as minimum and maximum values for numerical columns or allowed values for categorical columns. When you validate your data, TFDV will check that all the values in the dataset are within the specified domain. If any values are out of range, TFDV will provide a warning and give you the option to either discard or correct the data. There is also the possibility to specify the expected distribution of values in each feature of the Data Contract. TFDV will compute the actual statistics of your data, as shown in Figure 2, and compare them to the expected distribution. If there are any significant discrepancies, TFDV will provide a warning and give you the option to investigate the data further.\nFurthermore, this allows us to detect outliers and anomalies in the data (step 4) by comparing the actual statistics of your data to the expected statistics. It can flag any data points that deviate significantly from the expected distribution and provide visualizations to help you understand the nature of the anomaly.\nFigure 2: Example visualization of the dataset statistics created by TFDV\nThis code below is using the TFDV library to validate the data schema and detect any anomalies. The validate_schema function takes two arguments, statistics, and schema_path. Statistics argument is the output of a previous step which is generating statistics, and schema_path is the path to the schema file that was constructed in the first line. This function checks if the data conforms to the schema specified in the schema file.\n# Construct schema_path from base GCS path + filename\ntfdv_schema_path = (\n f\"{pipeline_files_gcs_path}/{tfdv_schema_filename}\"\n)\n\n# validate data schema\nvalidated_schema = validate_schema(\n statistics=gen_statistics.output,\n schema_path=tfdv_schema_path\n).set_display_name(\"Validate data schema\")\n\n# show anomalies and fail if any anomalies were detected\nanomalies = show_anomalies(\n anomalies=validated_schema.output,\n fail_on_anomalies=True\n).set_display_name(\"Show anomalies\")\nThe next block calls the show_anomalies function which takes two arguments, anomalies and fail_on_anomalies. The anomalies argument is the output of the previous validate_schema function, which includes the detected anomalies if any. The fail_on_anomalies argument is a flag that when set to true, will fail the pipeline if any anomalies are detected. This function will display the anomalies if any were detected, which looks something like this.\nanomaly_info {\n  key: \"producer_used\"\n  value {\n    description: \"Examples contain values missing from the schema: Microsoft (<1%), Sony Ericsson (<1%), Xiaomi (<1%), Samsung (<1%), IPhone (<1%). \"\n    severity: ERROR\n    short_description: \"Unexpected string values\"\n    reason {\n      type: ENUM_TYPE_UNEXPECTED_STRING_VALUES\n      short_description: \"Unexpected string values\"\n      description: \"Examples contain values missing from the schema: Microsoft (<1%), Sony Ericsson (<1%), Xiaomi (<1%), Samsung (<1%), IPhone (<1%). \"\n    }\n    path {\n      step: \"producer_used\"\n    }\n  }\n}\nAll the above components were developed internally using Custom KFP components and TFDV.\nHow Vodafone Industrialized the Approach on its AI Booster Platform\nAs part of the AI Booster platform, we have also provided templates for different Modeling Libraries such as XGBoost, TensorFlow, AutoML and BigQuery ML. These templates, which are based on Kubeflow Pipelines (KFP) pipelines, offer a wide range of customizable components that can be easily integrated into your machine learning workflow.\nOur templates provide a starting point for our Data Scientists and ML Engineers, but they are fully customizable to fit their specific needs. However, we do enforce the inclusion of certain components in the pipeline when it is being productionized. As shown in Figure 1, we require that all production pipelines include Data Contract components. These components are not specific to a particular model and are intended to be used whenever data is being ingested for training or prediction.\nAutomating this step helps with our data validation process, making it more efficient and less prone to human error. It gives all stakeholders the confidence that whenever the model is in production, the data being used by the Model is always up to standard and not full of surprises. In addition, it helps with reproducibility of use cases in different markets, using local data. But most importantly it helps with Compliance and Privacy. It ensures us that our data is being used in compliance with company policies and regulations, and provides a framework for tracking and monitoring the usage of the data to make sure that it is being used appropriately.\nData Contracts with TFDV Helped Vodafone Industrialize their ML Workflow\nData Contracts play a critical role in ensuring the quality and integrity of the data used in machine learning models. Data Contracts provide:\na set of rules and guidelines for how data should be collected, stored, and used, and help to ensure that the data is of high quality and free of errors\na framework for identifying and resolving issues with the data, such as outliers, inconsistencies, and errors, before they can cause problems with the models\na way to ensure compliance with company policies and regulations\na way to trace back the origin and history of the data, which can be useful for auditing and troubleshooting purposes\nThey also help to ensure that the data is being used consistently and in a reproducible way, which can help to improve the accuracy and performance of the models and reduce the risk of errors and inaccuracies in the predictions. Data contracts used in conjunction with tools like TFDV help automate the data validation process, making it more efficient and less prone to human error. Applying this concept in AI Booster helped us at Vodafone to make a key step forward in industrializing our AI/ML use cases.\nFind Out More\nFor more information about TFDV, see the user guide and tutorials on tensorflow.org. Special thanks to Amandeep Singh of Vodafone and Max V\u00f6kler of Google Cloud for their work to create this design and for writing this post.",
    "link": "https://blog.tensorflow.org/2023/03/how-vodafone-uses-tensorflow-data-validation-in-their-data-contracts-to-elevate-data-governance-at-scale.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjbhnD5DO3M7_Co3jXEp4eX-XF1ole4VJUUwXwzJ0rGm4_yZP9LLxllEP7CmfEHnCQanb4eQSGTQ56bRiYLaNcrkaay-a3RUsRxEJYPV_eKqnoNVZgU9TFh3RfWjo_dx1Nixa-O7B06q4iEE1B-OdzKTncyu3IlQ5butiqSFSdTOqUeIV0IOB7HOSUH/s1600/Social%20-%20TensorFlow%20-%20How%20Vodafone%20Uses%20Data%20Contracts.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiomVwhyjNc1aBF2Gv6ilaPITeplo4aY1fE3--gPdfzexHhYAUDMIv4YIoAeRLACLqbtMxFmtKiCLrMFpu-MH9IXUeOkI8hcOzbkQuV7Tq3nlV2lC9-c618KlZ6fVi9Vbk4QvGaXZL-ZR59JWUzPbnP-KkMOL6Oro0L3SKCuAT5risd_JygViOyknaT/s1600/Header%20-%20TensorFlow%20-%20How%20Vodafone%20Uses%20Data%20Contracts.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtyKCQNdlOm3kFDZ0bHM2QjAHatKERsNKOIR5lUSxpZsyZgsLjfMuR5TxTcF36PSNy8pBwKIuCdKXZlB3FvUrr9uut5QwqzqwPxVrFDzaLxIJUkSTJCj6LyZ9Fjb4wrvT71M_wY11cnQV5vOnk5BM986QgelsxAj3h0wGJtYCJj18-g4qMEw2awdbh/s1600/Vodafone%20Figure%201.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjpymwKCU3F_JhheVxi3vsNY3V3rB0NDpY3sns25d2w_jKSu4Ek-KrQdnaa5wmJPtM1QV2COwCrIxT-LJs1jmi31zGX0JEQw37wDizDNEC8yFKl0qpbI9jU9bPcwBiTKmAhnMtW0N_pDcD4E-BnE5rCCQ_WAmCtop88gY_HEVXR-YRkSJWIB1_VOSxA/s1600/image1.png"
    ],
    "time": "2023/12/02 00:58:43"
  },
  {
    "title": "Extend your TFX pipeline with TFX-Addons",
    "content": "Posted by Hannes Hapke and Robert Crowe\nTo produce production-level machine learning models, TensorFlow provides a portfolio of libraries under the umbrella of TensorFlow Extended (TFX). With just a pip install, TFX already includes a number of versatile pipeline components - referred to as the \u201cstandard components\u201d - which provide most of the basic functionality for training and batch inference. The standard components will get most developers started, but developers often find the need for additional functionality, which can be added by developing custom components. Any TFX pipeline, regardless of which components are included, can be used with a number of pipeline orchestrators like Google Cloud Vertex AI Pipelines, Apache Beam, Apache Airflow, or Kubeflow Pipelines.\nWhile the standard TFX components are great, a community of machine learning engineers from a number of companies including Twitter, Spotify, Digits, and Apple formed a TFX special interest group and started contributing new components, libraries, and examples to an extension of TFX called TFX-Addons.\nWhat is TFX?\nTFX is an end-to-end platform for deploying production ML pipelines. When you're ready to move your models from research to production, you can use TFX to create and manage an automated production pipeline for both training and/or batch inference. A TFX pipeline is a sequence of components that implement an ML pipeline which is specifically designed for scalable, high-performance machine learning tasks. Components can often be built using the TFX libraries - TensorFlow Data Validation, TensorFlow Transform, and Tensorflow Model Analysis - which can also be used individually. Components can also be built to run completely customized code, and even to distribute processing across a compute cluster through Apache Beam.\nTFX provides the following:\nA toolkit for building ML pipelines. TFX pipelines let you orchestrate your ML workflow on several platforms, such as: Apache Airflow, Apache Beam, and Kubeflow Pipelines. Learn more about TFX pipelines.\nA set of standard components that you can use as a part of a pipeline, or as a part of your ML training script. TFX standard components provide proven functionality to help you get started building an ML process easily. Learn more about TFX standard components.\nLibraries which provide the base functionality for many of the standard components. You can optionally use the TFX libraries to add this functionality to your own custom components, or use them separately. Learn more about the TFX libraries.\nTFX is a planet-scale production learning toolkit based on TensorFlow. It provides a configuration framework and shared libraries to integrate common components needed to define, launch, and monitor your machine learning system.\nWhat is TFX-Addons?\nTFX-Addons is a special interest group (SIG) for TFX users who are extending the standard set of components provided by Google\u2019s TensorFlow team. The addons are implementations by other machine learning companies and developers which rely heavily on TFX for their production machine learning operations.\nCommon MLOps patterns, for example ingesting data into machine learning pipelines, are solved through TFX components. As an example, members of TFX-Addons developed and open-sourced a TFX component to ingest data from a Feast feature store, a component maintained by machine learning engineers at Twitter and Apple.\nHow can you use the TFX-Addons components or examples?\nThe TFX-Addons components and examples are accessible via a simple pip installation. To install the latest version, run the following:\npip install tfx-addons\nTo ensure you have a compatible version of dependencies for any given project, you can specify the project name as an extra requirement during install:\npip install tfx-addons[feast_examplegen]\nTo use TFX-Addons:\nfrom tfx import v1 as tfx\nimport tfx_addons as tfxa\n\n# Then you can easily load projects tfxa.{project_name}. Ex:\n\ntfxa.feast_examplegen.FeastExampleGen(...)\nThe TFX-Addons components can be used in any TFX pipeline. Most components support all TFX orchestrators including Google Cloud\u2019s Vertex Pipelines, Apache Beam, Apache Airflow, or Kubeflow Pipelines.\nWhich additional components are currently available?\nThe list of components, libraries, and examples is constantly growing, with several new projects currently in development. As of this writing, these are the currently available components.\nFeast Component\nThe Example Generator allows you to ingest data samples from a Feast Feature Store.\nMore information: tfxa.feast_examplegen\nMessage Exit Handler\nThis component provides an exit handler for TFX pipelines which notifies the user about the final state of the pipeline (failed or succeeded) via a Slack message. If the pipeline fails, the component will provide the error message. The message component supports a number of message providers (e.g. Slack, stdout, logging providers) and can easily be extended to support Twilio. It also serves as an example of how to write exit handlers for TFX pipelines.\nMore information: tfxa.message_exit_handler\nSchema Curation Component\nThis component allows its users to update/change the schema produced by the SchemaGen component, and curate it based on domain knowledge. The curated schema can be used to stop pipelines if a feature drift is detected.\nMore information: tfxa.schema_curation\nFeature Selection Component\nThis component allows users to select features from datasets. This component is useful if you want to select features based on statistical feature selection metrics.\nMore information: tfxa.feature_selection\nXGBoost Evaluator Component\nThis component extends the standard TFX Evaluator component to support trained XGBoost models, in order to do deep analysis of model performance.\nMore information: tfxa.xgboost_evaluator\nSampling Component\nThis component allows users to balance their training datasets by randomly undersampling or oversampling, reducing the data to the lowest- or highest-frequency class.\nMore information: tfxa.sampling\nPandas Transform Component\nThis component can be used instead of the standard TFX Transform component, and allows you to work with Pandas dataframes for your feature engineering. Processing is distributed using Beam for scalability.\nMore information: tfxa.pandas_transform\nFirebase Publisher\nThis project helps users to publish trained models directly from a TFX pipeline to Firebase ML.\nMore information: tfxa.firebase_publisher\nHuggingFace Model Pusher\nThe HuggingFace Model Pusher (HFModelPusher) pushes a blessed model to the HuggingFace Model Hub. Also, it optionally pushes an application to HuggingFace Space Hub.\nMore information: tfxa.huggingface_pusher\nHow can you participate?\nThe TFX-Addons SIG is all about sharing reusable components and best practices. If you are interested in MLOps, join our bi-weekly conference calls. It doesn\u2019t matter if you are new to TFX or an experienced ML engineer, everyone is welcome and the SIG accepts open source contributions from all participants.\nIf you want to join our next meeting, sign up to our list group sig-tfx-addons@tensorflow.org.\nOther resources:\nTFX-Addons Slack - join here\nTFX-Addons Repository\nAlready using TFX-Addons?\nIf you\u2019re already using TFX-Addons we\u2019d love to hear from you! Use this form to send us your story!\nThanks to all Contributors\nBig thanks to all the open-source component contributions from following members:\nBadrul Chowdhury, Daniel Kim, Fatimah Adwan, Gerard Casas Saez, Hannes Hapke, Marcus Chang, Kshitijaa Jaglan, Pratishtha Abrol, Robert Crowe, Nirzari Gupta, Thea Lamkin, Wihan Booyse, Michael Hu, Vulko Milev, Sayak Paul, Chansung Park, and all the other contributors! Open-source only happens when people like you contribute!",
    "link": "https://blog.tensorflow.org/2023/02/extend-your-tfx-pipeline-with-tfx-addons.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQoDFwtP8Px9xqz5f2ChaLrYROmP7mD0loxn0RkCSspbnFfXhhnHAz4dnTtTVLXyvppu_nqD-kLoXrJHIGSvAS9h7hCmTHb2uzweTbvNQYX7Xaz9R8TLyyA4r6CUJ2Jw-KQa0Zl6co9WYnAyu0hiJkz2zHqdafdNgnT-n3fOyf1_PvGVZlBpz30LUw/s1600/Tensorflow-new-state-of-the-art-model-garden.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEid5UnPcbdH7N587INIjOsRBJwWBTpZsLIccR_ub_Z_b9-aZ-_aQMXLBTxaVapwvg00tggoG4HVvOUPQprwVXFx8w8Rd4l_IcK7Agl93vLCtcc387sPSF65N_YZQF5dLFWW_37Et8lULTk3-je9uWmfpCWduNEF605SG2_Hq6EyQsMtNmAREe3p0Rg7/s1600/Tensorflow-new-state-of-the-art-model-garden-4209x1253.png"
    ],
    "time": "2023/12/02 00:58:43"
  },
  {
    "title": "End-to-End Pipeline for Segmentation with TFX, Google Cloud, and Hugging Face",
    "content": "Posted by Chansung Park, Sayak Paul (ML and Cloud GDEs)\nTensorFlow Extended (TFX) is a flexible framework allowing Machine Learning (ML) practitioners to iterate on production-grade ML workflows faster with reliability and resiliency. TFX\u2019s power lies in its flexibility to run ML pipelines across different compatible orchestrators such as Kubeflow, Apache Airflow, Vertex AI Pipelines, etc., both locally and on the cloud.\nIn this blog post, we discuss the crucial details of building an end-to-end ML pipeline for Semantic Segmentation tasks with TFX and various Google Cloud services such as Dataflow, Vertex Pipelines, Vertex Training, and Vertex Endpoint. The pipeline also uses a custom TFX component that is integrated with Hugging Face \ud83e\udd17 Hub - HFPusher. Finally, you will see how we implemented CI/CD into the mix by leveraging GitHub Actions.\nAlthough we won\u2019t go over all the bits of the pipeline, you can still find the code of the underlying project in this GitHub repository.\nArchitectural Overview\nThe system architecture of the project is divided into three main parts. The first part is all about the core TFX pipeline handling all the steps from data ingestion to model deployment. The second part concerns the integration between the pipeline and the external Hugging Face \ud83e\udd17 Hub service. The last one is about automation and implementing CI/CD using GitHub Actions.\nFigure 1. Overall system architecture (original)\nIt is common to open Pull Requests when proposing new features or code refactorings in separate branches. When it comes to ML projects, these changes usually affect the model and/or data. Besides running basic validation on the proposed changes (code quality, tests, etc.), we should also ensure that the changes produce a model that is better enough to replace the currently deployed model before merging (if the changes pertain to modeling). In this project, we developed a GitHub Action that is manually triggered on the merging branch with configurable parameters. This way, project stakeholders can validate performance-related changes and reliably ship the changes to production. In reality, there might be more critical measurements here, but we hope this GitHub Action proves to be a good starting point.\nAt the heart of any MLOps project, there is an ML pipeline. We built a simple yet complete ML pipeline with support for automatic data ingestion, data preprocessing, model training, model evaluation, and model deployment in TFX. The TFX pipeline could be run on a local environment, but we also ran it on the Vertex AI platform to replicate real-world production-grade environments.\nFinally, the trained and qualified model from the ML pipeline is deployed to the Vertex AI Endpoint. The \u201cblessed\u201d model is also pushed to the Hugging Face Hub alongside an interactive demo via a custom HFPusher TFX component. Hugging Face Hub is a very popular place to store models and publish a fully working ML-powered interactive application for free. It is useful to showcase an application with the latest model to audit if it works as expected before going on a full production deployment.\nBelow, we discuss each of these components in a little more detail, discussing our design considerations and non-trivial technical aspects.\nTFX Pipeline\nThe ML pipeline is written entirely in TFX, from data ingestion to model deployment. Specifically, we used standard TFX components such as ExampleGen, ImportSchemaGen, Transform, Trainer, Evaluator, and Pusher, along with the custom HFPusher component. Let\u2019s briefly look at the roles of each component in the context of our project.\nFigure 2. Overview of the ML pipeline (original)\nExampleGen\nIn this project, we have prepared Pets dataset in TFRecord format with these scripts and stored them in Google Cloud Storage(GCS). ExampleGen brings the data files from GCS, splits them into training and evaluation datasets according to glob patterns, and stores them as TFRecords in GCS. Note that ExampleGen could take different data types such as CSV, TFRecord, or Parquet, then it generates datasets in a uniform format in TFRecord. It lets us handle the data uniformly inside the entire TFX pipeline. Note that since the Pets dataset is available from TF Datasets, you could also use a custom TFDS ExampleGen for this task.\nExampleGen can be integrated with Dataflow out of the box. All you need to do to benefit from Dataflow is to call with_beam_pipeline_args method with appropriate parameters such as machine type, disk size, the number of workers, and so on. For context, Dataflow is a managed service provided by Google Cloud that allows us to run Apache Beam pipelines efficiently in a fully distributed manner.\nImportSchemaGen\nImportSchemaGen imports a Protocol Buffer Text Format file that was previously automatically inferred by SchemaGen. It can also be hand-tuned to define the structure of the output data from ExampleGen.\nIn our case, the prepared Pets dataset has two features - image and segmentation map (label), and the size of each feature is 128x128. Therefore, we could define a schema like the one below.\nfeature {\n  name: \"image\" \n  type: FLOAT\n\n  float_domain {\n    min: 0\n    max: 255\n  }\n\n  shape {\n    dim { size: 128 }\n    dim { size: 12 }\n    dim { size: 3 }\n  }\n}\n\nfeature {\n  name: \"label\"\n  type: FLOAT\n\n  float_domain {\n    min: 0\n    max: 2\n  }\n\n  shape {\n    dim { size: 128 }\n    dim { size: 128 }\n  }  \n}\nAlso note that in the float_domain section, we can set the value restrictions. In this project, the input data is standard RGB images, so each pixel value should be between 0 and 255. On the other hand, the pixel value of the label should be 0, 1, or 2, meaning outer, inner, and border of an object in an image, respectively.\nTransform\nWith the help of ImportSchemaGen, the data is already shaped correctly in Transform and validated. Without ImportSchemaGen, we would have to write code to parse TFRecords and shape each feature manually inside Transform. Therefore, one line of code below is sufficient for the data preprocessing since the model in this project is built on top of MobileNetV2.\n# IMAGE_KEY is \"image\" which matches the name of feature in the ImportSchemaGen\n\nimage_features = mobilenet_v2.preprocess_input(inputs[IMAGE_KEY])\nSince data preprocessing is a CPU and memory-intensive job, Transform also can be integrated with Dataflow. Just like in ExampleGen, the job could be seamlessly delegated to Dataflow by calling the with_beam_pipeline_args method.\nTrainer\n(Vertex) Trainer simply trains a model. We used a UNet architecture built on top of MobileNetV2 from the TensorFlow official tutorial. Since the model architecture is nothing new, let\u2019s take a look at how it is modularized and some of the key pieces of code.\npipeline/\n\u251c\u2500 ...\n\u251c\u2500 models/\n    \u251c\u2500 common.py\n    \u251c\u2500 hyperparams.py\n    \u251c\u2500 signatures.py\n    \u251c\u2500 train.py\n    \u251c\u2500 unet.py\nYou place your modeling code in a separate file, which is supplied as a parameter to the Trainer. In this case, that file is named train.py. When the Trainer component is run, it looks for a starting point function with the name run_fn which is defined in train.py. The run_fn() function basically pulls in the training and evaluation datasets from the output of Transform, trains the UNet model ( defined in unet.py), then saves the trained model with appropriate signatures. The training process simply follows the standard Keras way \u2013 model.compile(), model.fit().\nThe Trainer component can be integrated with Vertex AI Training out of the box, which is a managed service to train models in a distributed system. By specifying how you would want to configure the training server clusters in the custom_config parameter of the Trainer, the training job is handled by Vertex AI Training automatically.\nIt is also important to notice which signatures the model exports in TensorFlow. Consider the following code snippet that saves a trained model (of the tf.keras.Model instance) into a SavedModel resource.\nmodel.save(\n    fn_args.serving_model_dir,\n    save_format=\"tf\",\n    signatures={\n        \"serving_default\": model_exporter(model),\n        \"transform_features\": transform_features_signature(\n            model, tf_transform_output\n        ),\n        \"from_examples\": tf_examples_serving_signature(\n            model, tf_transform_output\n        ),\n    },\n)\nThe signatures are functions that define how to handle given input data. For example, we have defined three different signatures. While serving_default is used during serving time, the other two are used during the model evaluation time.\nserving_default transforms a single or a batch of data points from user requests which is usually marshaled in JSON (base64 encoded) for HTTP or serialized Protocol Buffer messages for gRPC, then runs the model prediction on the data.\ntransform_features applies a transformation graph obtained from the Transform component to the data produced by ExampleGen. This function will be used in the Evaluator component, so the raw evaluation inputs from ExampleGen can be appropriately transformed that the model could understand.\nfrom_examples performs data transformation and model prediction in a sequential manner. How data transformation is done is identical to the process of the transform_features function.\nNote that the transform_features and from_examples signatures are used internally in the Evaluator component. In the next section, we explain their connections.\nEvaluator\nThe performance of the trained model should be evaluated by certain criteria or metrics. Evaluator lets us define such metrics that not only evaluates the trained model itself but also compares the trained model to the last best model retrieved by Resolver. In other words, the trained model will be deployed only if it achieves performance above the baseline threshold and it is better than the previously deployed model. The full configurations for this project can be found here.\nEVAL_CONFIGS = tfma.EvalConfig(\n    model_specs=[\n        tfma.ModelSpec(\n            signature_name=\"from_examples\",\n            preprocessing_function_names=[\"transform_features\"],\n        )\n    ],\n    ...\n)\nThe reason that we had transform_features and from_examples signatures that are doing the same data preprocessing is that they are used in different situations. Evaluator runs the evaluate() method on an existing model while it runs a function (signature) specified in the signature_name on the currently trained model. Therefore, we not only need a function that transforms a given sample but also runs the evaluate() method at the same time.\nPusher\nWhen the trained model is evaluated to be deployed, (Vertex) Pusher pushes the model to the Model Registry in Vertex AI. It also optionally creates an Endpoint and deploys the model to the endpoint out of the box. You can specify a number of different deployment-specific configurations to Pusher: machine type, GPU type, the number of GPUs, traffic splits etc.\nIntegration with Hugging Face \ud83e\udd17 Hub\nHugging Face Hub offers ML practitioners a powerful way to store and share models, datasets, and ML applications. Since it supports seamless support for storing model artifacts with automatic version control, we developed a custom TFX component named HFPusher that:\ntakes a model artifact (in the SavedModel format) and pushes that to the Hub in a separate branch for better segregation. The branch name is determined by time.time().\ncreates and pushes a model card that includes attributes of the model enabling d\u0131scovery of the models on the Hugging Face Hub platform.\nhosts an application with the model using Hugging Face Spaces given an application template referencing the branch where the model artifact was pushed to.\nYou can use this component anywhere after the Trainer component, but it\u2019s recommended to use it at the end of a TFX pipeline. The HFPusher component only requires a handful of arguments consisting of two TFX artifacts and four Hugging Face specific configurations:\nHugging Face user name\nHugging Face access token for creating and modifying repositories on the Hugging Face Hub, which is automatically injected with GitHub Action (see the next section)\nName of the repository to which the model artifacts will be pushed\nModel artifact as an output of a previous component such as Trainer\nHugging Face Space specific configurations (optional)\nApplication template to host a Space application\nName of the repository to which the Space application will be pushed. It has the same name as the name of the model repository by default.\nSpace SDK. The default value is gradio, but it could be set to streamlit\nModel blessing artifact as an output of a previous component such as Evaluator (optional)\nThe Hugging Face Hub is primarily based on Git and Git-LFS. The Hugging Face team provides an easy-to-use huggingface_hub API toolkit to interact with it. That is how it provides seamless support for version control, large file storage, and interaction.\nIn Figures 3 and 4, we show how the model repository and the application repository (which were automatically created from a TFX pipeline) look like on the Hugging Face Hub.\nFigure 3. Model versioning in Hugging Face Model Hub (original)\nFigure 4. Automatically published application in Hugging Face Space Hub (original)\nHFPusher has been contributed to the official TFX-Addons tfx-addons package. HFPusher will be available in version 0.4.0 and later in the tfx-addons package.\nAutomation with GitHub Actions\nIn the DevOps world, we usually run a number of tests on the changes introduced to ensure they\u2019re valid enough to hit production. If the tests pass, the changes are merged and a new deployment is shipped automatically.\nFor an ML codebase, the changes are usually either related to data or model on a broad level. Validating these changes is quite application dependent but there could still be common grounds:\nDo the changes introduced on the modeling side lead to better performance metrics?\nDo the changes lead to faster training throughput?\nDo the data-related changes reflect some distribution better?\nWe focused on the first point in this project. We designed a GitHub Action workflow that can:\n1. Google Cloud authentication and setup is done with google-github-actions/auth and google-github-actions/setup-gcloud GitHub Actions when a credential (JSON) is provided. In order to use appropriate credentials to the specified Google Cloud project ID, the workflow seeks for the credentials from GitHub Action Secret. Each credential is mapped to the name which is identical to the Google Cloud project ID.\n2. Some of the sensitive information is replaced with envsubst command. In this project, it is required to provide a Hugging Face \ud83e\udd17access token to the HFPusher component to create and update any repositories in Hugging Face \ud83e\udd17 Hub. The access token is stored in GitHub Action Secret.\n3. An environment variable enable_dataflow is set to \"true\" or \"false\" based on the specified parameter. By looking up the environment variable, the TFX pipeline conditionally defines dedicated parameters for Dataflow and passes them to ExampleGen and Transform components via with_beam_pipeline_args method.\n4. The last part of the workflow compiles and runs the TFX pipeline on Vertex AI with the TFX CLIs as below. The tfx pipeline create CLI creates the pipeline and registers it to the local system. Furthermore, it is capable of building and pushing a Docker Image to Google Container Registry(GCR) based on a custom Dockerfile in the pipeline. Then tfx run create CLI runs the pipeline on Vertex AI with the specified Google Cloud Project ID and region.\ntfx pipeline create \\\n  --pipeline-path kubeflow_runner.py \\\n  --engine vertex --build-image\n\ntfx run create \\\n  --engine vertex \\\n  --pipeline-name PIPELINE_NAME \\\n  --project GCP_PROJECT_ID --region GCP_REGION\nIn this case, we need to verify each PR if the suggested modification works well at the build and run times. Also, sometimes each collaborator wants to run the ML pipeline with their own Google Cloud account. Furthermore, it is better if we could conditionally delegate some heavy jobs in the ML pipeline to more dedicated Google Cloud services.\nFigure 5. GitHub Action for CI/CD of ML pipeline (original)\nAs you may notice from Figure 5, the GitHub Action runs a workflow based on five different parameters - branch, Google Cloud project ID, cloud region, the name of TFX pipeline, and enabling the Dataflow integration.\nConclusion\nIn this post, we discussed how to build an end-to-end ML pipeline for semantic segmentation tasks. We leveraged TensorFlow, TFX, and Google Cloud services such as Dataflow and Vertex AI, GitHub Actions, and Hugging Face \ud83e\udd17 Hub to develop a production-grade ML pipeline with external services along with semi-automatic CI/CD pipelines. We hope that you found this setup useful and reliable and that you will use this in your own ML pipeline projects.\nAs a future work, we will demonstrate a common MLOps scenario by extending this project. First, we\u2019ll add more complexities to the data to simulate model performance degradation. Second, we\u2019ll evaluate the currently deployed model to see if the model performance degradation actually happened. Last, we\u2019ll verify the model performance is recovered after replacing the current model architecture with better ones such as DeepLabV3+ or SegFormer.\n\nAcknowledgements\nWe are grateful to the ML Developer Programs team that provided Google Cloud credits to support our experiments. We thank Robert Crowe for providing us with helpful feedback and guidance. We also thank Merve Noyan who worked on integrating the model card utilities into the HFPusher component.",
    "link": "https://blog.tensorflow.org/2023/01/end-to-end-pipeline-for-segmentation-tfx-google-cloud-hugging-face.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJiBZPJrIB2hZFOsqnzo3ZUtOxA1-y395HlIjN34EGGgB1VS-Ue7NB14yO61DQhsk8ihGV5XGs1qVjz1AEs2Qpkt1lVL29_j5-69IgyWuUWUzmg2SRqBXn90-vH46hQXnNdiYbswEDPmhdDNPuAvAj1C_b0FgjxPKfK-mZcksh3Ky3gueE_uSP21Lt/s1600/ss-overview.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCc6CnkQQn5Dy0EBnGm-BS-RJlnl5df1X7QR7UVIu-4XrzlW-S499lMvBhgXUWX1iK1Og8N67CduetJDNkIjYDcNazFIG2mFYeEkEUJhUQogXi0CLJrN7eRUDaboFSOyN_upZeHsMAY5Qn0JCeCDUvyHjt26gSqz934VI9xrBkDqMflQmCv82D-Kg3/s1600/image5.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg6vcYbylwMNet0Tdb-r09ueyDXBadlNraJIWv4uRLrthy-R8Sq4EsrxJulF8e44tE9I-um4_WToI4IgXcap9p2wAB8HWKKKt2ZXDUSB9SJnUpVXwelY-icUaMO66igf-uQr_pbzsDxFDn3gScl4PD0x7i6Xc3W6Oj49mZB6__YbvMV5ewOIEdNFUDm/s1600/image2.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiZ7SqeVBH1QKJZsxR8cteywVpmf-movHdTj3NNy_lnKA8XGfJl5C2vT_StxquUEikvhXti3q8y2kwR7zo-OCXmCVcEpETCvH0iNFWwsW2tjF14g0gHfWrXVgT0NX_LzBme2oqOd5Hbc6XZnxMFEriujJKlSeHFk5cuD1nAq391iyhJ0I7BEbQG9s90/s1600/image1.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgHerwLBus9RVW6eVcfwNHPDUHOoGICBGx4tLRgW-bFb9a2qz7UzKVxYyxrCPV5_x7q1Nfn04SrsXN5nfqcd6tD3yTspZMubQ-52qf3ke8cKCKLKBkuAe6qnVY-P9gzptZIo3wH3fK8cof4-grr2NMKlPGu4eKE9A-2tt5tEp8zRyyqdcTSUfh5O5ua/s1600/image4.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiy79DSJYTYHNzXH683PFBZctZbIpayShA2Zv2KnD3pWIccX7bUFBC5wWDG1J2pHHMhsGkarJMEMlMLWZoqn6PFq9sjNaWBGx6mM40SDPPSutcAlLfeP6Scy5MkmZF1EvShl-5CdSUak7WquWDkm4p_vZo0MLtbkn2BXnE7TCDoe2iY2tLcr0VX90RV/s1600/image3.png"
    ],
    "time": "2023/12/02 00:58:43"
  },
  {
    "title": "How startups can benefit from TFX",
    "content": "Posted by Hannes Hapke and Robert Crowe\n\nStartup companies building Machine Learning-based services and products require production-level infrastructure for training and serving their models. This can be especially challenging for small teams that are spread thin and need to innovate and grow quickly. TFX (TensorFlow Extended) provides a range of options to mitigate these challenges. In this blog post, you will learn how the San Francisco-based FinTech startup Digits has benefitted from applying TFX early, how TFX helps Digits grow, and how other startups can benefit from TFX too.\nTFX is a set of libraries that streamline the development and deployment of production machine learning models, including implementing automated training pipelines. You might already be aware of major companies like Alphabet (including Google and Waze), Spotify, or Twitter successfully leveraging TFX to manage their machine learning pipelines. But TFX also has enormous benefits for medium-stage startups, like Digits.\nBefore we dive into how we are using TFX at Digits, let\u2019s introduce a conceptual software design question that every startup will face: Choosing between tactical and strategic programming (introduced by John Ousterhout in \u201cA Philosophy of Software Design\u201d). In his analysis, Ousterhout shows that strategic programming is a much more sustainable approach for long-term success: even though it takes more time to get to an initial release, strategic programming will help make the complexity of a growing codebase more manageable.\nSource: \u201cA Philosophy of Software Design\u201d, John Ousterhout, 2018\nAt Digits, we found that the same concept applies to machine learning. While we could train machine learning models in a minimal Jupyter notebooks-based setup, this system would become increasingly hard to manage as complexity increases. In this scenario, any initial wins of a rapidly trained machine learning model would dwindle as the company grows. Therefore, we invested heavily in our ML engineering setup from the start:\nWe developed ML-specific workflows and created a clear distinction between ML experiments and production-ready ML.\nWe invested heavily in ensuring we use tools like TFX, ML Metadata Store, and Google Cloud\u2019s Vertex AI as efficiently as possible.\nWe automated our model deployment processes to remove human shortcuts and errors.\nOusterhout found that strategic programming requires more upfront time, but developers will benefit from lower system complexity. For example, we have spent roughly 2-3 months setting up all the ML tooling and workflows, and we recognize that it is a substantial investment.\nWhile this might not be feasible for startups that are still trying to establish a product-market-fit, we believe that this ML strategy is the right path for startups with a growing customer base. Furthermore, it has been our experience that applying strategic programming to machine learning problems will add to the developers\u2019 job satisfaction and increase retention among the data team in the long run (fewer rushed hotfixes, systematic model retraining, etc.).\nGrowing our business with TFX, we have identified three key benefits that have allowed us to optimize our ML model training and deployment in ways that have been crucial to our success as a startup:\nKey benefit 1: Standardization\nAt Digits, we distinguish between machine learning experiments and production machine learning. The objective of an ML experiment is to develop a proof of concept model. Our engineers are free to use any framework and tooling for ML experiments as long as our security requirements are met.\nWhen we bring a model to production and customers rely on consistent predictions, we convert these experiments to production ML models. Every time we create a production ML model, we follow a consistent project structure and use the same steps for data and model analysis as well as feature engineering. TFX is crucial in standardizing those aspects.\nBecause each production model follows the same standards, we can detect potential synergies between projects early. This approach enables us to share code between projects even in the earliest development stages. Standardization has increased code reusability, and new projects have a much faster ramp-up time.\nAnother benefit of standardizing our workflows with TFX is that we can now apply our software engineering and DevOps principles to ML projects: Pipelines that run non-periodically can be triggered by our continuous integration system. TFX pipelines then register the newly produced model with our model registry. Based on this, the continuous integration system can also update our ML-serving endpoints and automatically deploy our ML models. This way, all changes to our ML systems are tracked in our Git repository.\nSystem components including CI\nKey benefit 2: Growth\nIn contrast to Keras\u2019 preprocessing layers, TFX supports feature engineering, model analysis, and data validation via Apache Beam tasks. This way we only need to implement the feature engineering once - with TFX, we can simply swap out the Apache Beam configuration when our datasets grow and we need more processing capabilities.\nStartups can begin with the TFX default setup based on Apache Beam\u2019s DirectRunner. The DirectRunner mode doesn\u2019t allow any parallelized execution of pipeline tasks but is available without any setup time. As the startup grows, the engineering team can swap out the underlying Apache Beam Runner for a more performant system like Google Cloud\u2019s Dataflow, Apache Spark, or Apache Flink, with minimal code changes - often only one line. While Dataflow is only available to Google Cloud customers, Apache Spark and Flink are open-source, and all major cloud providers offer managed services.\nWe successfully employed this strategy at Digits: We started out with Apache Beam\u2019s DirectRunner for our initial pipelines, a setup that helped us understand how TFX can improve our ML workflows. As our company grew, the volume of data to process grew as well. To handle the increasing volume of data, TFX allowed us to switch to a different Beam runner without any friction. By building our pipelines in two phases, we didn\u2019t have to implement TFX and the more performative and complex orchestration dependencies all at once, and saved our small initial team considerable strain.\nDifferent Beam Runner options, depending on the data volume\nAnother advantage that was useful to us is how easily TFX integrates with the Google Cloud ecosystem. Google Cloud\u2019s Vertex AI Pipeline natively supports TFX and provides all necessary pipeline infrastructure as a managed service. Instead of managing our own Kubernetes clusters, we can easily switch back and forth between pipeline runs in different Google Cloud projects. We are also not limited by cluster compute and memory limitations since we can access both GPUs and TPUs with Vertex Pipelines.\nKey benefit 3: Reproducibility & Repeatability\nKeeping track of all ML artifacts is key for the sustainable management of production ML models. Our goal was to track all relevant data points for all our production models. We needed to store artifacts like datasets, data splits, data validation results, feature transformations, trained models, and model analysis results. But we also didn\u2019t want to slow down the ML team with extensive record keeping.\nTFX is tightly integrated with the ML Metadata Store (MLMD) which helps us to keep track of all model details in one place. Under the hood, each TFX component in our ML pipelines records all intermediate pipeline results and metadata. We can generate model lineages for each model produced by our ML pipelines without any additional overhead. This has proven to be an indispensable tool when things move fast.\nModel lineage\nDigits\u2019 Lessons Learned\nWhile adapting TFX to our needs did take some time, we have seen this initial investment pay off over time. We are now able to convert machine learning experiments within minutes into production pipelines and continuously produce and deploy new versions of our models.\nTFX helps us to make our ML codebase more modular. We have developed several custom TFX components (e.g. for model deployments, model annotations, or model tracking). Due to the modularity of the TFX components, all projects can benefit from enhancements made in a single project.\nAt the same time, we benefited from standardizing our production ML codebase with TFX. As a growing startup company, we found this standardization especially useful as it helped us stay on track as complexity increased. New projects now follow a highly optimized cookie-cutter approach, which has resulted in major time and labor savings. Those standardizations also allowed us to automate large parts of the model deployment processes, which in turn helped free up engineering capacities. We have found that these savings are vital for the small, flexible ML teams which are common in startups. \nUsing TFX also has allowed us to future-proof our MLOps tooling. The fact that TFX uses Apache Beam under the hood gave us confidence that we don\u2019t need to reengineer our MLOps setup as the company grows. \nTFX, its metadata store, and its Google Cloud integrations have helped us reproduce models from given artifacts and made it much easier to accurately recreate any previous ML models whenever needed.\nThe experience of growing Digits with TFX has convinced us that any company that is serious about machine learning can benefit from TFX - at every step along the way, from small startups to large corporations.\nFor more information\nTo learn more about TFX, check out the TFX website, join the TFX discussion group, dive into other posts in the TFX blog, watch our TFX playlist on YouTube, or subscribe to the TensorFlow channel.",
    "link": "https://blog.tensorflow.org/2022/10/how-startups-can-benefit-from-tfx.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi7xiMAXfKtS58j3n7kbfrsT90qK_UYZwMefgNofvF-siXhoixAv7nBixozMzNKXnw4HydG2ZfyMYfijsS9T2y6ob3zlru74z2wvVPw2u1_zsKT9sPF9noFVP1pJwFROLC-uRtYQCTFttU4mctrkTU8TBbnix42nk22ZIFUupg57HpYO5UCtFaATnWK/s1600/image2.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi7xiMAXfKtS58j3n7kbfrsT90qK_UYZwMefgNofvF-siXhoixAv7nBixozMzNKXnw4HydG2ZfyMYfijsS9T2y6ob3zlru74z2wvVPw2u1_zsKT9sPF9noFVP1pJwFROLC-uRtYQCTFttU4mctrkTU8TBbnix42nk22ZIFUupg57HpYO5UCtFaATnWK/w663-h197/image2.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_rFkQwBda1_lOxYlprjVSUM1Q0eikThwXnmip1rlOHpDHSb0KY9Uw08ReDuWMdcZ7b4yWUAGiF5GuyX8f9RNyGDS4FZXeL6TRK_ReJBemchhDdBuuGD-qmxflvWCdjG1DCPef1ckEIdXZh-J1KH1EH13HxASkzXs4cWoAibDLOdJH51q0iovP3DCL/s16000/image3.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgSP039Ba8euVGrtWETDAggtwrALKQ2kzJlgvzE_PsFS7NKZtLIijf6WCjxK0aGmaHfOjW-Zs_Hhs0-csLimOLQSWAj9NvOFlHIMrkzbsc76v6oHiFASw2gXYhOtUr0CPTeqVPrdCmuwJqx2aARKEzTcHTL2oTufcFOeOVBcu34Fc4YuBpg_N1DLAOV/s1999/image1.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_YA4E8r6_NrkZoA-FLnD1koRnS-UX63u2XHPU7GF4bMW5hr4J9hKEvyW3rWxwmRRj_hNMMU1qW5Fs7IFWP2TWhTo6eEzSOivAdiBS6Pj4NgDlNmOeJFTNWHNfX1soXcVAtP9aT1i2iD3pBvyxK9XkIdW5XuNQtmu4ngEonAeAIIaWfWN1Sw-H85W6/s1999/image4.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQ08YSkyeONckJB55YCVF8jaTianiiqvrl3p_K9nUpHz9-COtonC824fzSGwqAJoqRV23xVm2mMhNzkfSbalKHrnp_6vYH6Kql3-waqxE3iAX6ae-oAlWLgQ5Oi-OJdEpe-4NsUgW7JUmBVIn-D-zR1nXw56-zt9NN08G9ilzMNSL3RYqO6Lwec5bQ/s1600/image5.png"
    ],
    "time": "2023/12/02 00:58:43"
  },
  {
    "title": "Automated Deployment of TensorFlow Models with TensorFlow Serving and GitHub Actions",
    "content": "Posted by Chansung Park and Sayak Paul (ML-GDEs)\n\nIf you are an applications developer, or if your organization doesn\u2019t have a dedicated ML Engineering team, it is common to deploy a machine learning model without worrying about the end to end machine learning pipeline or MLOps. TFX and TensorFlow Serving can help you create the heart of an MLOps infrastructure. \nIn this post, we will share how we serve a TensorFlow image classification model as RESTful and gRPC based services with TensorFlow Serving on a Kubernetes (k8s) cluster running on Google Kubernetes Engine (GKE) through a set of GitHub Actions workflows. \nOverview\nIn any GitHub project, you can make releases, with up to 2 GB of assets included in each release when using a free account. This is a good place to manage different versions of machine learning models for various reasons. One can also replace this with a more private component for managing model versions such as Google Cloud Storage buckets. For our purposes, the 2 GB space provided by GitHub Releases will be enough.\nFigure 1. Three steps to deploy TF Serving on GKE (original).\nThe basic idea is to:\nAutomatically detect a newly released version of a TensorFlow-based ML model in GitHub Releases\nBuild a custom TensorFlow Serving Docker image containing the released ML model\nDeploy it on a k8s cluster running on GKE through a set of GitHub Actions.\nThe entire workflow can be logically divided into three subtasks, so it\u2019s a good idea to write three separate composite GitHub Actions:\nPrerequisites\nThe GKE cluster should have been provisioned beforehand\nDownload the GCP service account key and include it as a GitHub Action Secret with the name of GCP_CREDENTIALS\nGrant IAM roles for Storage Admin, GKE Developer, and GCR Developer to the associated service account\nFirst subtask handles the environmental setup\nGCP Authentication (GCP credentials are injected from the GitHub Action Secret)\nInstall gcloud CLI toolkit to access the GKE cluster for the third subtask\nAuthenticate Docker to push images to the Google Cloud Registry (GCR)\nConnect to a designated GKE cluster for further accesses\nSecond subtask builds a custom TensorFlow Serving image\nDownload and extract your latest released SavedModel from your GitHub repository\nRun the official or a custom built TensorFlow Serving docker image\nCopy the extracted SavedModel into the running TensorFlow Serving docker container\nCommit the changes of the running container and give it a new name with the tags of special token to denote GCR, GCP project ID, and latest\nPush the committed image to the GCR\nThird subtask deploys the custom built TensorFlow Serving image to the GKE cluster\nDownload the Kustomize toolkit to handle overlay configurations\nPick one of the scenarios from the various experiments\nApply Deployment, Service, and ConfigMap according to the selected experiment to the currently connected GKE cluster\nConfigMap is used for batching-enabled scenarios to inject batching configurations dynamically into the Deployment.\nThere are a number of parameters that you can customize such as the GCP project ID, GKE cluster name, the repository where the ML model will be released, and so on. The full list of parameters can be found here. As noted above, the GCP credentials should be set as a GitHub Action Secret beforehand. If the entire workflow goes without any errors, you will see something similar to the output below.\n\nNAME         TYPE            CLUSTER-IP      EXTERNAL-IP      PORT(S)                            AGE\ntfs-server   LoadBalancer    xxxxxxxxxx      xxxxxxxxxx       8500:30869/TCP,8501:31469/TCP      23m\n\nThe combinations of the EXTERNAL-IP and the PORT(S) represent endpoints where external users can connect to the TensorFlow Serving pods in the k8s cluster. As you see, two ports are exposed, and 8500 and 8501 are for RESTful and gRPC services respectively. One thing to note is that we used LoadBalancer as the service type, but you may want to consider including Ingress controllers such as GKE Ingress for securing the k8s clusters with SSL/TLS and defining more flexible routing rules in production. You can check out the complete logs from the past runs.\nBuild a Custom TensorFlow Serving Image within a GitHub Action\nAs described in the overview and the official document, a custom TensorFlow Serving Docker image can be built in five steps. We also provide a notebook for local testing of these steps. In this section, we show how to write a composite GitHub Action for this partial subtask of the whole workflow (note that .inputs, .env, and ${{ }} for the environment variables are omitted for brevity).\n\nFirst, a model can be downloaded by an external robinraju/release-downloader GitHub Action with custom information about the URL of the GitHub repository and the filename in the list of assets from the latest release. The default filename is saved_model.tar.gz.\nSecond, the downloaded file should be decompressed to fetch the actual SavedModel that TensorFlow Serving can understand.\nruns:\n  using: \"composite\"\n  steps:\n      - name: Download the latest SavedModel release\n        uses: robinraju/release-downloader@v1.3\n        with:\n          repository: $MODEL_RELEASE_REPO\n          fileName: $MODEL_RELEASE_FILE\n          latest: true\n          \n      - name: Extract the SavedModel\n        run: |\n          mkdir MODEL_NAME\n          tar -xvf $MODEL_RELEASE_FILE --strip-components=1 --directory $MODEL_NAME\n    \n      - name: Run the CPU Optimized TensorFlow Serving container\n        run: |\n          docker run -d --name serving_base $BASE_IMAGE_TAG\n          \n      - name: Copy the SavedModel to the running TensorFlow Serving container\n        run: |\n          docker cp $MODEL_NAME serving_base:/models/$MODEL_NAME\n          \n      - id: push-to-registry\n        name: Commit and push the changed running TensorFlow Serving image\n        run: |\n          export NEW_IMAGE_NAME=tfserving-$MODEL_NAME:latest\n          export NEW_IMAGE_TAG=gcr.io/$GCP_PROJECT_ID/$NEW_IMAGE_NAME\n          echo \"::set-output name=NEW_IMAGE_TAG::$(echo $NEW_IMAGE_TAG)\"\n          docker commit --change \"ENV MODEL_NAME $MODEL_NAME\" serving_base $NEW_IMAGE_TAG\n          docker push $NEW_IMAGE_TAG\nThird, we can modify a running TensorFlow Serving Docker container by placing a custom SavedModel inside. In order to do this, we need to run the base TensorFlow Serving container instantiated either from the official image or a custom-built image. We have used the CPU-optimized version as the base image by compiling from source, and it is publicly available here.\n\nFourth, the SavedModel should be copied to the /models directory inside the running TensorFlow Serving container. In the last step, we set the MODEL_NAME environment variable to let TensorFlow Serving know which model to expose as services, and commit the two changes that we made to the base image. Finally, the updated TensorFlow Serving Docker image can be pushed into the designated GCR.\nNotes on the TensorFlow Serving Parameters\nWe consider three TensorFlow Serving specific parameters in this post: tensorflow_inter_op_parallelism, tensorlfow_inter_op_parallelism, and the batching option. Here, we provide brief overviews of each of them.\n\nParallelism threads: tesorflow_intra_op_parallelism controls the number of threads to parallelize the execution of an individual operation. tensorflow_inter_op_parallelism controls the number of threads to parallelize the execution of multiple independent operations. To know more, refer to this resource.\n\nBatching: As mentioned above, we can allow TensorFlow Serving to batch requests by setting the enable_batching parameter to True. If we do so, we also need to define the batching configurations for TensorFlow in a separate file (passed via the batching_parameters_file argument). Please refer to this resource for more information about the options we can specify in that file.\nConfiguring TensorFlow Serving\nOnce you have a custom TensorFlow Serving Docker image, you can deploy it with the k8s resource objects: Deployment and ConfigMap as shown below. This section shows how to write ConfigMap to write batching configurations and Deployment to add TensorFlow Serving specific runtime options. We also show you how to mount the ConfigMap to inject batching configurations into TensorFlow Serving\u2019s batching_parameters_file option.\n\napiVersion: apps/v1\nkind: Deployment\n...\n    spec:\n      containers:\n      - image: gcr.io/gcp-ml-172005/tfs-resnet-cpu-opt:latest\n        name: tfs-k8s\n        imagePullPolicy: Always\n        args: [\"--tensorflow_inter_op_parallelism=2\", \n               \"--tensorflow_intra_op_parallelism=8\", \n               \"--enable_batching=true\", \n               \"--batching_parameters_file=/etc/tfs-config/batching_config.txt\"]\n        ...\n        volumeMounts:\n          - mountPath: /etc/tfs-config/batching_config.txt\n            subPath: batching_config.txt\n            name:  tfs-config\n\u2026\n\nThe URI of the custom built TensorFlow Serving Docker image can be specified in spec.containers.image, and the behavior of TensorFlow Serving can be customized by providing arguments in the spec.containers.args in the Deployment. This post shows how to configure three kinds of custom behavior: tensorflow_inter_op_parallelism, tensorflow_intra_op_parallelism, and enable_batching.\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: tfs-config\ndata:\n  batching_config.txt: |\n    max_batch_size { value: 128 }\n    batch_timeout_micros { value: 0 }\n    max_enqueued_batches { value: 2 }\n    num_batch_threads { value: 2 }\n\nWhen enable_batching is set to true, we can further customize the batch inference by defining its specific batching-related configurations in a ConfigMap. Then, the ConfigMap can be mounted as a file with spec.containers.volumeMounts, and we can specify which file to look up for the batching_parameters_file argument in Deployment.\nKustomize to Manage Various Experiments\nAs you see, there are lots of parameters to determine the behavior of TensorFlow Serving, and the optimal values for them are usually found by running experiments. Indeed, we have experimented with various parameters within a number of different environmental setups: different numbers of nodes, different numbers of vCPU cores, and different RAM capacity.\n\u251c\u2500\u2500 base\n|   \u251c\u2500\u2500kustomization.yaml\n|   \u251c\u2500\u2500deployment.yaml\n|   \u2514\u2500\u2500service.yaml\n\u2514\u2500\u2500 experiments\n    \u251c\u2500\u2500 2vCPU+4GB+inter_op2\n    ...\n    \u251c\u2500\u2500 4vCPU+8GB+inter_op2\n    ...\n    \u251c\u2500\u2500 8vCPU+64GB+inter_op2_w_batch\n    |   \u251c\u2500\u2500kustomization.yaml\n    |   \u251c\u2500\u2500deployment.yaml\n    |   \u2514\u2500\u2500tfs-config.yaml\n    ... \n\nWe used kustomize to manage the YAML files of various experiments. We keep common YAML files of Deployment and Service in the base directory while having specific YAML files for certain experimental environments and configurations under the experiments directory. With this and kustomize, the contents of the base YAML files could be easily overlaid with different numbers of replicas, different values of tensorflow_inter_op_parallelism, tensorflow_intra_op_parallelism, enable_batching, and batch configurations.\nruns:\n  using: \"composite\"\n  steps:\n    - name: Setup Kustomize\n      ...\n\n    - name: Deploy to GKE\n      working-directory: .kube/\n      run: |-\n        ./kustomize build experiments/$TARGET_EXPERIMENT | kubectl apply -f -\n\nYou can simply select the experiment that you want to test or that you think is optimal by setting $TARGET_EXPERIMENT. For example, the best experiment that we found was \u201c8vCPU+16GB+inter_op4\u201d which means each VM is configured with an 8vCPU and 16GB RAM while tensorflow_inter_op_parallelism is set to 4. Then the kustomize build command will provision the YAML files for the selected experiment for the k8s clusters.\nCosts\nWe used the GCP cost estimator for this purpose. Pricing for each experiment configuration was assumed to be live for 24 hours per month (which was sufficient for our experiments).\n\nMachine Configuration (E2 series) Pricing (USD)\n2vCPUs, 4GB RAM, 8 Nodes\n11.15\n4vCPUs, 8GB RAM, 4 Nodes\n11.15\n8vCPUs, 16GB RAM, 2 Nodes\n11.15\n8vCPUs, 64GB RAM, 2 Nodes\n18.21\nConclusion\nIn this post, we discussed how to automatically deploy and experiment with an already trained model with various configurations. We leveraged TensorFlow Serving, Kubernetes, and GitHub Actions to streamline the deployment and experiments. We hope that you found this setup useful and reliable and that you will use this in your own model deployment projects.\n\n\nAcknowledgements\n\nWe are grateful to the ML Developer Programs team that provided GCP credits for supporting our experiments. We also thank Hannes Hapke and Robert Crowe for providing us with helpful feedback and guidance.",
    "link": "https://blog.tensorflow.org/2022/09/automated-deployment-of-tensorflow-models-with-tensorflow-serving-and-github-actions.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj6g9FPO3ZE-4rsM-yQ4JpMLg4NX4srfqKg8dudM4-gFz27p4eoDmpEIdv3ZVYEydArXWTx9NvdQ-VQ_h_8ZTYEcNyMMd9vc_x_BOy9AAQGTXJL--8JUcAKd4CJseNgV26AYsmnfrCWAPGs0Y5MDjSSVZ5HS5gUI_5Oxe9NzGNI9xNQGwxnICOwMPXS/s1600/Screen-Shot-2022-07-09-at-9-26-47-AM.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj6g9FPO3ZE-4rsM-yQ4JpMLg4NX4srfqKg8dudM4-gFz27p4eoDmpEIdv3ZVYEydArXWTx9NvdQ-VQ_h_8ZTYEcNyMMd9vc_x_BOy9AAQGTXJL--8JUcAKd4CJseNgV26AYsmnfrCWAPGs0Y5MDjSSVZ5HS5gUI_5Oxe9NzGNI9xNQGwxnICOwMPXS/w660-h441/Screen-Shot-2022-07-09-at-9-26-47-AM.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgGVw0F8YyI3I5sbV5OBct4kSPzmvz2HAZSMQHqb1Q8sBDy9ftazHGxzj8nhg8ICLrYMFPEK-Oce6ikRrvBxhN1X4sIIb-IY5Ar3BJv3YZR6gx7z0LOJ-ly7p7oJ6NzkoUl22415aGmA-YqGGVCIHiywKAgTdjfxrE0Lx2OVnE-WUQ-KlJ6ZPOG7E1X/s1600/image1.png"
    ],
    "time": "2023/12/02 00:58:43"
  },
  {
    "title": "Why Karrot Uses TFX, and How to Improve Productivity on ML Pipeline Development",
    "content": "Posted by Ukjae Jeong, Gyoung-yoon Yoo, and Myeonghyeon Song from Karrot\nKarrot (the global service of Danggeun Market in Korea) is a local community service app that connects neighbors based on a secondhand marketplace. Danggeun Market was launched in 2015, and over 23 million people in Korea are using Danggeun Market in their local communities. Currently, Karrot is operated in 440 local communities in four countries: the U.K., the U.S., Canada, and Japan. In our service, scrolling through feeds to find inexpensive and useful items has become a daily pleasure for users. For better user experiences, we\u2019ve been applying several machine learning models such as recommendation models.\nWe are also working on ways to effectively and efficiently apply ML models. In particular, we\u2019re putting lots of effort into building machine learning pipelines for periodic deployment, rapid experiments, and continuous model improvement.\nFor the ML pipelines, we\u2019ve been using TFX (TensorFlow Extended) for production. So in this article, we will briefly introduce why we use TFX, and how we utilize TFX to improve productivity.\nMachine Learning in Karrot\nThere are many ML projects inside Karrot. ML models are running inside the services. For example, we use automation models to detect fraud, and there are recommendation models to improve the user experience on our app feed. If you are interested in detailed descriptions of the models, please refer to our team blogs, which are written in Korean.\nAs we\u2019ve been using Kubeflow for our ML models, we were able to periodically train, experiment, and deploy models but still, we had some pain points. As we started to use TFX with Kubeflow last year, TFX pushed this line further and let the team easily use our production pipelines.\nHow TFX helps us with production ML\nTFX helps build and deploy production ML pipelines easily with open and extendable design.\nTFX, completely open-sourced in 2019, is an end-to-end platform for production ML pipelines. It supports writing ML workflows in component units, which then can be run in multiple environments - Apache Beam, Dataflow, Kubeflow, and Airflow. It also comes with well-written standard components for data ingestion/transformation, training, and deployment.\nStandard Components\nTFX provides several standard components. If you are looking for components for data ingestion, there are CsvExampleGen based on local CSV files, PrestoExampleGen, and BigQueryExampleGen which can ingest data directly from Presto, BigQuery, and many other sources with some customization. So you can easily process data from multiple sources just by connecting pre-built components to your TFX pipelines.\nIt can also handle large-scale data processing smoothly. Since the Transform component that performs feature engineering is implemented on Apache Beam, you can execute it on GCP Dataflow or another compute cluster in a distributed manner.\nOf course, many other convenient components exist and are added constantly.\nComponent Reusability\nIn order to adapt TFX to our product, there is a need for custom components. TFX has a well-structured component design that enables us to create custom components naturally and easily connect them to existing TFX pipelines. A simple Python function or container can be transformed into a TFX component, or you can write the whole component in the exact same way as standard components are written. For more details, check out the custom component guide.\nTo enhance our productivity by delivering these advantages, we share custom components that have similar use cases among our ML pipelines as an internal library of Karrot Market.\nVarious Runners are Supported\nTFX is compatible with a variety of environments. It can be run locally on your laptop or run on DataFlow, GCP\u2019s batch data processing service compatible with Apache Beam. You can visualize the output by manually running each component in a Jupyter Notebook. TFX also supports KubeFlow and Vertex AI, which have recently been released with new features as well. Therefore, the pipeline code is written once, and can then be run almost anywhere. We can simply create development, experiment, and deployment environments at once. For that reason, the burden of deploying models to production was significantly reduced by using TFX for our services.\nTechnical lessons\nAs we set up our ML pipelines with TFX, code qualities and our experiences in model development have increased.\nHowever, there were difficulties. We didn't have a uniform project structure or best practices among our team. Maybe this is because TFX itself is relatively new and we've been using it before version 1. It became harder to understand codes and start to contribute. As the pipelines are becoming larger and more complex, it\u2019s getting harder to understand the meaning of custom components, corresponding config values, and dependencies. In addition, it was difficult to introduce some of the latest features to the team.\nImproving the Development Experience\nWe decided to create and use a template for TFX pipelines to make it easier to understand each other's code, implement pipelines with the same pattern, and share know-how with each other. We merged components frequently used in Karrot and put them in a shared library so that ML pipelines can be developed very quickly.\nIt was expected that the template would accelerate the development of new projects. In addition, as mentioned above, we expected that each project would have a similar structure, making it easier to understand each other's projects.\nSo far, we have briefly introduced the template project. Here are some of our considerations to make better use of TFX in this project.\nConfiguration first\nWe prioritize our configuration first. It should be enough to understand how pipelines work by reading their configuration. If we can understand specific settings very easily, we can set up various experiments and proceed with them to AB testing.\nexample_gen_config.proto written in Protocol Buffer(Protobuf), denotes the specification of config. config.pbtxt holds the values, and pipeline.py builds up the pipeline.\n// config.pbtxt\nexample_gen_config {\n    big_query_example_gen_config {\n        query: \"# query for example gen\"\n    }\n\n\n    ...\n}\n\n...\n// example_gen_config.proto\nmessage ExampleGenConfig {\n    oneof config {\n        BigQueryExampleGenConfig big_query_example_gen_config = 1;\n        CsvExampleGenConfig csv_example_gen_config = 2;\n    }\n\n    ...\n}\n\n// When BigQueryExampleGen is used\nmessage BigQueryExampleGenConfig {\n    optional string query = 1;\n}\n\n// When CsvExampleGenConfig is used\nmessage CsvExampleGenConfig {\n    optional string input_base = 1;\n}\n# pipeline.py\ndef create_pipeline(config):\n   ...\n   example_gen = _create_example_gen(config.example_gen_config)\n   ...\n\n\n\n\ndef _create_example_gen(config: example_gen_config_pb2.ExampleGenConfig):\n    ...\n\n    if config.HasField(\"big_query_example_gen_config\"):\n        ...\n        return ...\n\n\n    if config.HasField(\"csv_example_gen_config\"):\n        ...\n        return ...\n\n\n    raise ...\nAll configurations of ExampleGen are determined by a single ExampleGenConfig message. Similarly, all pipeline components only depend on their configs and are created from them. This way, you can understand the structure of the pipeline just by looking at the configuration file. There is also the intention to make customization and code understanding easier by separating the part that defines each component.\nFor example, let's assume the following situation: In order to test the data transformation later, the Transform component needs to support various data processing methods. You might want to add a data augmentation process in the transform component. Then it should be done by adding a config related to the data augmentation function. Similarly, you can extend the predefined Protobuf specification to easily support multiple processing methods and make it easy to see which processing method to use.\nManaging Configs with Protobuf\nAbout the example code above, some people may wonder why they use Protobuf as a configuration tool. There are several reasons for this, and we will compare advantages with YAML, which is one of the common practices for configuration.\nFirst, Protobuf has a robust interface, and validation such as type checking is convenient. There is no need to check whether any field is defined, as Protobuf defines the object structure in advance. In addition, it is useful to support backward/forward compatibility in a project under active development.\nAlso, you can easily check the pipeline structure. YAML has a hierarchical structure, but in the case of hydra, which is often used in the machine learning ecosystem, the stage (e.g. production, dev, alpha) settings are divided into several files, so we thought that Protobuf has better stability and visibility.\nIf you use Protobuf as your project setup tool, many of the Protobuf definitions defined in TFX can be reused.\nTensorFlow Ecosystem with Bazel\nBazel is a language-independent build system that is easy to extend and supports a variety of languages and tools. From simple projects to large projects using multiple languages and tools, it can be used quickly and concisely in most situations. For more information, please refer to Bazel Vision on the Bazel documentation page.\nUsing Bazel in a Python project is an uncommon setting, but we used Bazel as the project build system of the TFX template project. A brief introduction to the reason is as follows.\nFirst of all, it works really well with Protobuf. Because Bazel is a language-independent build system, you can easily tie your Protobuf build artifacts as dependencies with other builds without worry. In addition, the Protocol Buffer repository itself uses Bazel, so it is easy to integrate it into the Bazel-based project.\nThe second reason is the special environment of the TensorFlow ecosystem. Many projects in the TensorFlow ecosystem use Bazel, and TFX also uses Bazel, so you can easily link builds with other projects (TensorFlow, TFX) using Bazel.\nInternal Custom TFX Modules\nAs mentioned before, we\u2019ve been building an internal library for the custom TFX modules (especially the custom components) that are frequently used across multiple projects. Anyone in Karrot can add their components and share them with the team.\nFor example, we are using ArgoCD to manage applications(e.g. TF Serving) in Kubernetes clusters, so if someone develops a component for deploying with ArgoCD, we can easily share it via an internal library. The library now contains several custom modules for our team for productivity.\nThe reason why we can share our custom features as an internal shared library is probably thanks to the modular structure of TFX. Through this, we were able to improve the overall productivity of the team easily. We can reuse most of the components that were developed from several projects, and develop new projects very easily.\nConclusion\nTFX provides lots of great features to develop production ML pipelines. We\u2019re using TFX on Kubeflow for ML pipelines to develop, experiment, and deploy in a better way, and it brings us many benefits. So we decided to introduce how we are using TFX in this blog post.\nTo learn more about Karrot, check out our website (Korea, US, and Canada). For the TFX, check out the TFX documentation page.",
    "link": "https://blog.tensorflow.org/2022/05/why-karrot-uses-tfx.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhL6IR5tPygfQ1vGwPYF8w9W5TjhAGUwNULyl8-laV4IzLH9iOYx5B8xk70btanduPLdpp8Oox5PKj5lXHAnBZUNICsPQqS04cggdRn6TUmaM6lYlaEOgRl5bX4ZSkzLZcJvPX_fYyTdiKKSWJTxHNJcc6v4UPH6SPz2_VnxhV88E3dQZ55kRAVVuIn/s1600/karrot.jpeg",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhL6IR5tPygfQ1vGwPYF8w9W5TjhAGUwNULyl8-laV4IzLH9iOYx5B8xk70btanduPLdpp8Oox5PKj5lXHAnBZUNICsPQqS04cggdRn6TUmaM6lYlaEOgRl5bX4ZSkzLZcJvPX_fYyTdiKKSWJTxHNJcc6v4UPH6SPz2_VnxhV88E3dQZ55kRAVVuIn/s1600/karrot.jpeg",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg4t-wkbET2oIO6sJtP8Ig5R59tJRn28hF9wcSuelwOIddtZYLuDEiP7kKZNHl_azrtxaNDnkAio97duVXNAhn-H0SNkGPlUHT2H739AZS1Jh3LdywZMBw2p5Y0E9I8cEpCC9g8Jh6r2OqBemoKSNp9MvjsWVZYQ4MCj5D6ACNrVR_JKAN95WZ-clD5/s1600/productionMLtfx.png"
    ],
    "time": "2023/12/02 00:58:43"
  },
  {
    "title": "Continuous Adaptation for Machine Learning System to Data Changes",
    "content": "A guest post by Chansung Park, Sayak Paul (ML-GDEs)\nContinuous integration and delivery (CI/CD) is a much sought-after topic in the DevOps domain. In the MLOps (Machine Learning + Operations) domain, we have another form of continuity -- continuous evaluation and retraining. MLOps systems evolve according to the changes of the world, and that is usually caused by data/concept drift. So, to cater to the data changes we need to continuously evaluate our deployed ML models and retrain and re-deploy them as necessary.\nIn this blog post, we present a project that implements a workflow combining batch prediction and model evaluation for continuous evaluation retraining In order to capture changes in the data. We will first discuss the general setup of the project. Then we will move on to key components (batch prediction, new data spans, retraining, etc.) that are important for continuously evaluating an ML model and then re-training it if needed. Rather than discussing the technical implementation details of the project, we will keep it high-level so that we will focus on understanding the underlying concepts.\nThe project is implemented with TensorFlow Extended (TFX), Keras, and various services offered from Google Cloud Platform. You can find the project on GitHub.\nOverview\nThis project shows how to build two separate pipelines working together to create a CI/CD workflow which responds to changes in the data. The first pipeline is for model training, and the second pipeline is for model evaluation based on the result of a batch prediction as shown in Figure 1.\nFigure 1. Overview of the project structure (original)\nThe model training pipeline is built by combining standard TFX components such as ImportExampleGen and Trainer with custom TFX components such as VertexUploader and VertexDeployer. Since the Pusher standard component had an issue when we were doing this project, we have brought custom components from our previous project, Dual Deployments.\nThere is one significant implementation detail on how ImportExampleGen handles the dataset to be fed into the model. We have designed our project to hold datasets from different distributions in separate folders with filesystem paths which indicate the span number. For instance, the initial training and test dataset can be stored in SPAN-1/train and SPAN-2/test while the drifted dataset can be stored in SPAN-2/train and SPAN-2/test respectively as shown in Figure 2.\nFor the sake of the versioning feature in Google Cloud Storage (GCS), you might think we don\u2019t need to manage datasets in this manner. However, we thought our way makes datasets much more manageable. For example, you might want to pick data from SPAN-1 and SPAN-2 or SPAN-1 and SPAN-3 to train the model depending on situations. Also, datasets belonging to the same distribution can still benefit from the versioning feature in GCS.\nFigure 2. How datasets are managed (original)\nThe batch evaluation pipeline does not leverage any standard TFX components. Rather it consists of five custom TFX components which are FileListGen, BatchPredictionGen, PerformanceEvaluator, SpanPreparator, and PipelineTrigger. These components are available as standalone modules here.\nFigure 3. Custom TFX components in batch evaluation pipeline (original)\nFileListGen generates a text file to be looked up by the currently deployed model on Vertex AI to perform batch prediction according to the format required by Vertex Prediction. Then BatchPredictionGen will simply perform Vertex Prediction based on the prepared text file from the FileListGen and output a set of files containing the batch prediction results. PerformanceEvaluator calculates the average accuracy based on the batch prediction results and outputs False if it is less than the threshold. If the output is True, the pipeline will be terminated. Or if the output is False, SpanPreparator prepares TFRecord files by compressing the list of raw data, and then puts those TFRecords into a new folder whose name contains the successive span number such as span-2. Finally, PipelineTrigger triggers the model training pipeline by passing the span numbers for the data which should be included for training the model through RuntimeParameter.\nGeneral setup\nIn this section, we walk through the key components of the project and also leave some notes on the tools we used to implement them.\nGetting the initial model ready\nWe focus on the concepts and consider implementing them in a minimal manner so that our implementations are as reproducible and as accessible as possible. Keeping that in mind, we use the CIFAR-10 training set as our training data and we fine-tune a ResNet50 model to fit the data. Our training pipeline is demonstrated in this notebook.\nSimulating data drift and labeling new data\nTo simulate a data drift scenario, we then collect a bunch of images from the internet matching CIFAR-10 classes. To make it easy to follow we implement this workflow inside a Colab Notebook which is available here. This workflow also includes uploading and deploying the trained model as a service on the Vertex AI platform.\nContinuous evaluation with batch inference\nWe then perform inference on these images with the trained model from the above step. We perform batch inference rather than online inference to get the results. We use Vertex AI\u2019s batch prediction service to realize this. In practice, usually after this step, the model test images and model predictions are sent to domain experts for audit purposes. They also provide the expected ground-truth labels on the test images. Only after that, we can validate the prediction results. But for the purpose of this project, we eliminate this step and pretend that the ground-truth labels are already available. So, as soon as the batch prediction results are available we evaluate them. This entire workflow is covered in this notebook.\nWe deploy a Cloud Function to monitor a specific location inside a Google Cloud Storage (GCS) bucket. If there is a sufficient number of new test images available inside that location, we trigger the batch prediction pipeline. We cover this workflow in this notebook. This is how we achieve the \u201ccontinuous evaluation\u201d aspect of our project.\nThere are other ways to capture drift in data, though. For example, using JS-Divergence, we can compare the distributions between the newly available data and training data. You can follow this Coursera lecture from Robert Crowe which dives deep into these techniques.\nModel retraining\nAfter the batch predictions are evaluated, the next step is to determine if we need to re-train the model based on a predefined performance threshold that generally depends on the business context and a lot of other factors. We set this threshold to 0.9 in the project. If we need to re-train then we trigger the same model training pipeline (as shown in this notebook) but with the newly available data added to the CIFAR-10 training set. We can either warm-start our model from a previous checkpoint or we can train the model from scratch using all the available training data. For this project, we do the latter.\nIn the following section, we will go over a few non-trivial components from our implementation and discuss their motivation and technicalities. As a reminder, our implementation is fully open-sourced here.\nImplementation details on managing datasets with span numbers\nIn this section, we walk through the implementation details on some key aspects of the project. Please go through the project repository and review all notebooks for further information.\nThe initial CIFAR-10 datasets are stored in {bucket-name}/span-1/train and {bucket-name}/span-1/test GCS locations respectively. This step is done through the first notebook. Then, we download more images of the same categories as in CIFAR-10 by using Bing Image Downloader. Those images are resized by 32x32 to make them compatible with CIFAR-10 datasets, and they are stored in a separate GCS bucket such as {bucket-batch-prediction}/2021-10/.\nNote we used the YYYY-MM for the name where the images are stored. This is because Cloud Function which is fired by Cloud Scheduler will look for the latest GCS location to launch the batch evaluation pipeline as shown below.\ndef get_latest_directory(storage_client, bucket):\n    blobs = storage_client.list_blobs(bucket)\n\n    folders = list(\n        set(\n            [\n                os.path.dirname(blob.name)\n                for blob in blobs\n                if bool(\n                    re.match(\n                        \"[1-9][0-9][0-9][0-9]-[0-1][0-9]\", os.path.dirname(blob.name)\n                    )\n                )\n                is True\n            ]\n        )\n    )\n\n    folders.sort(key=lambda date: datetime.strptime(date, \"%Y-%m\"))\n    return folders[0]\nAs you see, it only looks for the GCS location that exactly matches the YYYY-MM format. The Cloud Function launches the batch evaluation pipeline by passing which GCS location to look up for batch prediction via RuntimeParameter. The code snippet below shows how it is passed to the pipeline with the name data_gcs_prefix on the Cloud Function side.\nfrom kfp.v2.google.client import AIPlatformClient\n\napi_client = AIPlatformClient(project_id=project, region=region)\n\nresponse = api_client.create_run_from_job_spec(\n     ...\n     parameter_values={\"data_gcs_prefix\": latest_directory},\n)\nThe pipeline recognizes data_gcs_prefix is a type of RuntimeParameter, and it is used in the FileListGen component which prepares a text file in the required format to perform Vertex AI Batch Prediction.\ndef _create_pipeline(\n    data_gcs_prefix: data_types.RuntimeParameter,\n    ...\n) -> Pipeline:\n\n   filelist_gen = FileListGen(\n        ...\n        gcs_source_bucket=data_gcs_bucket,\n        gcs_source_prefix=data_gcs_prefix,\n    ).with_id(\"filelist_gen\")\n\n    ....\nLet\u2019s skip the batch prediction performed by the BatchPredictionGen component.\nWhen the PerformanceEvaluator component determines that retraining should be performed based on the result from the BatchPredictionGen component, the SpanPreparator prepares a TFRecord file with the newly collected images, moves it to {bucket-name}/span-1/train and {bucket-name}/span-2/test where the training pipeline is ingesting data for model training, and renames the GCS location where the newly collected images are to {bucket-batch-prediction}/YYYY-MM_old/.\nWe add the _old suffix so that Cloud Function will ignore the renamed GCS location. If the retrained model doesn\u2019t show a good enough performance metric, then you can have a chance to collect more data and merge them with the images in the _old GCS location.\nThe PipelineTrigger component at the end of the batch evaluation pipeline will trigger the training pipeline by passing which span numbers to look for in order to do model training. The data will be consumed by ImportExampleGen, based on the glob pattern matching feature. For instance, if data from span-1 and span-2 should be used for model training, then the glob pattern for the training dataset might be span-[12]/train/*.tfrecord. The code snippet below clearly shows the generalized version of the idea.\nresponse = api_client.create_run_from_job_spec(\n ...\n parameter_values={\n  \"input-config\": json.dumps(\n      {\n         \"splits\": [\n             {\n                \"name\": \"train\",\n                \"pattern\": f\"span-[{int(latest_span)-1}{latest_span}]/train/*.tfrecord\",\n             },\n             {\n                \"name\": \"val\",\n                \"pattern\": f\"span-[{int(latest_span)-1}{latest_span}]/test/*.tfrecord\",\n             },\n        ]\n      }\n  ),\n  \"output-config\": json.dumps({}),\n },\n)\nThe reason we formed the RuntimeParameter in the parameter_values in this way is that the pattern matching feature of the ImportExampleGen component should be specified in the input-config and output-config parameters. We do not need the output-config parameter for our purpose, but it is required when passing the input-config parameter as a RuntimeParameter. That\u2019s why the output-config parameter is left empty. Note that you have to form the parameter in protocol buffer format when using RuntimeParameter for standard TFX components. The code below shows how the passed input-config and output-config can be consumed by the ImportExampleGen component.\nexample_gen = tfx.components.ImportExampleGen(\n     input_base=data_root, input_config=input_config, output_config=output_config\n)\nIt is worth noting that you can leverage the rolling window feature supported by TFX with the standard components if the backend environment is Kubeflow Pipeline v1. The code snippet below shows how to achieve this with the CsvExampleGen component and a Resolver node.\nexamplegen_range_config = proto.RangeConfig(\n     static_range=proto.StaticRange(\n         start_span_number=2, end_span_number=2))\n\nexample_gen = tfx.components.CsvExampleGen(\n     input_base=data_root,\n     input_config=examplegen_input_config,\n     range_config=examplegen_range_config)\n\nresolver_range_config = proto.RangeConfig(\n     rolling_range=proto.RollingRange(num_spans=2))\n\nexamples_resolver = tfx.dsl.Resolver(\n     strategy_class=tfx.dsl.experimental.SpanRangeStrategy,\n     config={\n         'range_config': resolver_range_config\n     },\n     examples=tfx.dsl.Channel(\n         type=tfx.types.standard_artifacts.Examples,\n         producer_component_id=example_gen.id)).with_id('span_resolver')\nThis is a much better way since it reuses the artifacts generated by the previous ExampleGens, and the current pipeline run only takes care of the data in the new span. Unfortunately however this feature is not supported by Vertex AI Pipeline which is based on Kubeflow Pipeline v2. We had an extensive discussion with the TFX team about this, which is why we came up with a different approach from the standard way.\nCost\nVertex AI Training is a separate service from Pipeline. We need to pay for the Vertex AI Pipeline individually, and at the time of writing this article, it costs about $0.03 USD per pipeline run. The type of compute instance for each TFX component was e2-standard-4, and it costs about $0.134 per hour. Since the whole pipeline took less than an hour to be finished, we can estimate that the total cost was about $0.164 for a Vertex AI Pipeline run.\nThe cost of custom model training depends on the type of machine and the number of hours. Also, you have to consider that you pay for the server and the accelerator separately. For this project, we chose n1-standard-4 machine type whose price is $0.19 per hour and NVIDIA_TESLA_K80 accelerator type whose price is $0.45 per hour. The training for each model was done in less than an hour, so it cost about $1.28 in total. So, as per our estimates, the upper bound of the costs incurred should not be more than $5.\nThe cost only stems from Vertex AI because the rest of the components like Pub/Sub, Cloud Functions, etc., have very minimal usage. So even if we add a small estimate for those costs, the upper bound of the total cost for this project should not be more than $5. Please refer to the official documents on the price: Vertex AI price reference, Cloud Build price reference.\nIn any case, you should use this GCP Price Calculator to get a better understanding of how your cost for the GCP services might differ.\nSummary\nIn this blog post, we touched upon the idea of continuous evaluation and re-training for machine learning systems as well as the tooling needed to implement them. There is also a more traditional form of CI/CD for ML systems in response to code changes including changes in hyperparameters, model architecture, etc. We have a separate project demonstrating that use case. You are encouraged to check them here: Part I and Part II.\nAcknowledgements\nWe are grateful to the ML-GDE program that provided GCP credits for supporting our experiments. We sincerely thank Robert Crowe and Jiayi Zhao of Google for their help with the review.",
    "link": "https://blog.tensorflow.org/2021/12/continuous-adaptation-for-machine.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/a/AVvXsEg18fTDnl-xjY7DXSYNqo4HEKwPrAZNuy8rQiaXxcL4RSUGMsFEksJwSxIbJYcM0r75drbMeKG4BExDbCnV9AebAo7cbSGhsiRHaDHXzwynucj1MEurWmodh1-fEodtwN8R6JRRfFL4xysXKgftOPao5yGuRnJ3hFGBEAjcdq04dymCb4K4u9TV8Ubc",
      "https://blogger.googleusercontent.com/img/a/AVvXsEg18fTDnl-xjY7DXSYNqo4HEKwPrAZNuy8rQiaXxcL4RSUGMsFEksJwSxIbJYcM0r75drbMeKG4BExDbCnV9AebAo7cbSGhsiRHaDHXzwynucj1MEurWmodh1-fEodtwN8R6JRRfFL4xysXKgftOPao5yGuRnJ3hFGBEAjcdq04dymCb4K4u9TV8Ubc",
      "https://blogger.googleusercontent.com/img/a/AVvXsEh2qX7PNjIewMtZqdQhh4HTdcFVJWm5GLjMNkPIcEQk2sxRo8unzKcQRKO_Qg6FSQwQKDGbciMC5srxXVUpUtv7K0IfiLj5RvRpE3gSBiYvaenxxpQfTm8PciNSqn7Gn6SzHW7jzesr2sTXpNIEKb3rZYwQmkBkKZ82bcNyBHLmJjAwodBAQf9HnAaA",
      "https://blogger.googleusercontent.com/img/a/AVvXsEjYB8bsRfkc24FbX76vguBhkTrxXoVVozQdNDG5xg2zWAPOS6VBCHBlZwrRS6gvqqSZAL0tj79rbQ7Ti9EQ2XbMNb_O74sIZLxpFQYDFGyNl3DiWofN_CL_IDQfL9VpGgvK5-MfK8eGmiu4gGniEY8Rt2nwOxIY4KYMPIpDCUTPh1Gx1gcB5gZc6G8E"
    ],
    "time": "2023/12/02 00:58:43"
  },
  {
    "title": "How Waze Uses TFX to Scale Production-Ready ML",
    "content": "Posted by Gal Moran, Iris Shmuel, and Daniel Marcous (Data Scientists at Waze)\nWaze\nWaze is the world's largest community-based traffic and navigation app. It uses real-time data to help users circumvent literal and figurative bumps in the road. On top of mobile navigation, Waze offers a web platform, a carpool app, partnership services, an advertisement platform and more. Such a broad portfolio brings along diverse technological challenges and many different use cases.\nML @Waze\nWaze relies on many ML solutions, including:\nPredicting ETA\nMatching Riders & Drivers (Carpool)\nServing The Right Ads\nBut it\u2019s not that easy to get something like these right and \u201cproduction grade\u201d. It is very common for these kinds of projects to have requirements for complex surrounding infrastructure for getting them to production and hence require multiple engineers (data scientist, software engineer and software reliability engineers) and a lot of time. Even more so when you mix in the Waze-y requirements like large scale data, low (real-time, actually) latency inference, diverse use cases, and a whole lot of geospatial data.\nThe above is a good reason why opportunistically starting to do ML created a chaotic state at Waze. For us it manifested as:\nMultiple ML frameworks - you name it (sklearn, xgboost, TensorFlow, fbprophet, Java PMML, hand made etc.)\nML & Ops disconnect - models & feature engineering embedded in (Java) backend servers by engineers with limited monitoring and validation capabilities\nSemi-manual operations for training, validation and deployment\nA hideously long development cycle from idea to production\nOverall, data scientists ended up spending a lot of their time on ops and monitoring instead of focusing on the actual modelling and data processing. At a certain level of growth we\u2019ve decided to organize the chaos and invest in automation and processes so we can scale faster. We\u2019ve decided to heavily invest in a way to dramatically increase velocity and quality by adopting a full cycle data science philosophy. This means that in this new world we wanted to build, a single data scientist is able to close the product cycle from research to a production grade service.\nData scientists now directly contribute to production to maximize impact. They focus on modelling and data processing and get many infrastructures and ops work out-of-the-box. While we are not yet at the end of this journey fully realizing the above vision, we feel like the effort layed out here was crucial in putting us on the right track.\nWaze\u2019s ML Stack\nTranslating the above philosophy to a tech spec, we were set on creating an easy, stable, automated and uniform way of building ML pipelines at Waze.\nDeep diving into tech requirements we came up with the below criteria:\nSimple \u2014 to understand, use, operate\nManaged \u2014 no servers, no hardware, just code\nCustomizable \u2014 get the simple stuff for free, yet flexible enough to go crazy for the 5% that would require going outside the lines\nScalable \u2014 auto scalable data processing, training, inference\nPythonic \u2014 we need something production-ready, that works with most tools and code today and fits the standard data scientist. There are practically no other options than Python these days.\nFor the above reasons we\u2019ve landed on TFX and the power of its built-in components to deliver these capabilities mostly out of the box.\nIt\u2019s worth saying - Waze runs its tech stack on Google Cloud Platform (GCP).\nIt happens to be that GCP offers a suite of tools called Vertex AI. It is the ML infrastructure platform Waze is building on top of. While we use many components of Vertex AI\u2019s managed services, we will focus here on - Vertex Pipelines - a framework for ML pipelines that helps us encapsulate TFX (or any pipeline) complexity and setup.\nTogether with our data tech stack, the overall ML architecture at Waze (all managed, scaled, pythonic etc.) is as follows:\nCareful readers will notice the alleged caveat here - we go all in on TensorFlow.\nTFX means TensorFlow (even though that's not exactly true anymore, let's assume it is).\nIt might be a little scary at first when you have many different use cases.\nFortunately, the TF ecosystem is rich and Waze has the merit of having large enough data that neural nets converge.\nSince starting this we\u2019ve yet to find a use case that TF magic does not solve better or adequately as other frameworks (and not talking about micro % points, not trying to do a Kaggle competition here but get something to production).\nWaze TFX\nYou might think that landing on TFX and Vertex Pipelines solved all our problems, but that\u2019s not exactly true.\nIn order to make things truly simple we\u2019ve had to write some \u201cglue code\u201d (integrating the various products in the above architecture diagram) and abstracting enough details so the common data scientist could use this stuff effectively and fast.\nThat resulted in:\nEliminated boilerplate\nHiding all common TFX components so data scientists only focus on feature engineering and modelling and get the entire pipeline for free\nGenerating BigQuery based train / eval split\nProviding pre-implemented optional common features transform (e.g. scaling, normalization, imputations)\nProviding pre-implemented Keras models (e.g. DNN/RNN model. TF Estimator like but in Keras that speaks TFX)\nUtility functions (e.g. TF columns preparation)\nUnit testing framework for tf.transform feature engineering code\nOrchestrated and scheduled pipeline runs from Airflow using a Cloud run instance with all TFX packages installed (without installing it on the Airflow composer)\nWe\u2019ve put it all in an easy to use Python package called \u201cwaze-data-tfx\u201d\nOn top, we provided a super detailed walkthrough, usage guides and code templates, to our data scientists, so the common DS workflow is: fork, change config, tweak the code a little, deploy.\nFor reference this is how a simple waze-data-tfx pipeline looks like:\nConfiguration\n_DATASET_NAME = 'tfx_examples'\n_TABLE_NAME = 'simple_template_data'\n\n_LABEL_KEY = 'label'\n_CATEGORICAL_INT_FEATURES = {\n   \"categorical_calculated\": 2,\n}\n_DENSE_FLOAT_FEATURE_KEYS = [\"numeric_feature1\", \"numeric_feature2\"]\n_BUCKET_FEATURES = {\n   \"numeric_feature1\": 5,\n}\n_VOCAB_FEATURES = {\n   \"categorical_feature\": {\n       'top_k': 5,\n       'num_oov_buckets': 3\n   }\n}\n\n_TRAIN_BATCH_SIZE = 128\n_EVAL_BATCH_SIZE = 128\n_NUM_EPOCHS = 250\n\n_TRAINING_ARGS = {\n   'dnn_hidden_units': [6, 3],\n   'optimizer': tf.keras.optimizers.Adam,\n   'optimizer_kwargs': {\n       'learning_rate': 0.01\n   },\n   'layer_activation': None,\n   'metrics': [\"Accuracy\"]\n}\n\n_EVAL_METRIC_SPEC = create_metric_spec([\n   mse_metric(upper_bound=25, absolute_change=1),\n   accuracy_metric()\n])\nFeature Engineering\ndef preprocessing_fn(inputs):\n   \"\"\"tf.transform's callback function for preprocessing inputs.\n\n   Args:\n       inputs: map from feature keys to raw not-yet-transformedfeatures.\n\n   Returns:\n       Map from string feature key to transformed feature operations.\n   \"\"\"\n   outputs = features_transform(\n       inputs=inputs,\n       label_key=_LABEL_KEY,\n       dense_features=_DENSE_FLOAT_FEATURE_KEYS,\n       vocab_features=_VOCAB_FEATURES,\n       bucket_features=_BUCKET_FEATURES,\n   )\n   return outputs\nModelling\ndef _build_keras_model(**training_args):\n   \"\"\"Build a keras model.\n\n   Args:\n       hidden_units: [int], the layer sizes of the DNN (input layer first).\n       learning_rate: [float], learning rate of the Adam optimizer.\n\n   Returns:\n       A keras model\n   \"\"\"\n   feature_columns = \\\n       prepare_feature_columns(\n           dense_features=_DENSE_FLOAT_FEATURE_KEYS,\n           vocab_features=_VOCAB_FEATURES,\n           bucket_features=_BUCKET_FEATURES,\n       )\n\n   return _dnn_regressor(deep_columns=list(feature_columns.values()),\n                         dnn_hidden_units=training_args.get(\n                             \"dnn_hidden_units\"),\n                         dense_features=_DENSE_FLOAT_FEATURE_KEYS,\n                         vocab_features=_VOCAB_FEATURES,\n                         bucket_features=_BUCKET_FEATURES,\n                         )\nOrchestration\npipeline_run = WazeTFXPipelineOperator(\n   dag=dag,\n   task_id='pipeline_run',\n   model_name='basic_pipeline_template',\n   package=tfx_pipeline_basic,\n   pipeline_project_id=EnvConfig.get_value('gcp-project-infra'),\n   table_project_id=EnvConfig.get_value('gcp-project-infra'),\n   project_utils_filename='utils.py',\n   gcp_conn_id=gcp_conn_id,\n   enable_pusher=True,\n)\nSimple, right?\nWhen you commit a configuration file to the code base it gets deployed and sets up continuous training, and a full blown pipeline including all TFX and Vertex AI magics like data validation, transforms deployed to Dataflow, monitoring etc.\nSummary\nWe knew we were up to something good when one of our data scientists came back from a long leave and had to use this new framework for a use case. She said that she was able to spin up a full production-ready pipeline in hours, something that before her leave would have taken her weeks to do.\nGoing forward we have much planned that we want to bake into `waze-data-tfx`. A key advantage that we see in having this common infrastructure is that once a feature is added, then everyone can enjoy it \u201cfor free\u201d. For example, we plan on adding additional components to the pipeline, such as Infra Validator and Fairness Indicators. Once these are supported, every new or existing ML pipeline will add these components out-of-the-box, no extra code needed.\nAdditional improvements we are planning are around deployment. We wish to provide deployment quality assurance while automating as much as possible.\nOne way we are currently exploring doing so is using canary deployments. A data scientist will simply need to configure an evaluation metric and the framework (using Vertex Prediction traffic splitting capabilities and other continuous evaluation magic) would test the new model in production and gradually deploy or rollback according to the evaluated metrics.",
    "link": "https://blog.tensorflow.org/2021/09/how-waze-uses-tfx-to-scale-production-ready-ml.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-1PyaDt2SNqI/YTeYlDoLNBI/AAAAAAAAEdw/bYa48eL6fLkrX4BpU70L8R5wdiQMMYiWgCLcBGAsYHQ/s0/waze.gif",
      "https://1.bp.blogspot.com/-AemiiMllOuA/YTeZYpTVzjI/AAAAAAAAEd4/LuzL3NesUJE0JcChuElKOtBpEAukx6IEgCLcBGAsYHQ/s0/cloud%2Bplatform%2Bchart.png",
      "https://1.bp.blogspot.com/-I12_moVrRqU/YTeaCwdO6HI/AAAAAAAAEeA/UYZjPLyeC0oBpobtsJ74LNiKrd5_j1PngCLcBGAsYHQ/s0/waze%2Bdata%2Bfx.png"
    ],
    "time": "2023/12/02 00:58:43"
  },
  {
    "title": "How Digitec Galaxus trains and serves millions of personalized newsletters per week with TFX",
    "content": "Posted by Christian Sager (Product Owner, Digitec Galaxus) and Anant Nawalgaria (ML Specialist, Google)\nIn the retail industry it is important to be able to engage and excite users by serving personalized content on newsletters at scale. It is important to do this in a manner which leverages existing trends, while exploring and unearthing potentially new trends with an even higher user engagement. This project was done as a collaboration between Digitec Galaxus and Google, by designing a system based on Contextual Bandits to personalize newsletters for more than 2 million users every week.\nTo accomplish this, we leveraged several products in the TensorFlow ecosystem and Google Cloud including TF Agents, TensorFlow Extended (TFX) running on Vertex AI , to build a system that personalizes newsletters in a scalable, modularized and cost effective manner with low latency. In this article, we\u2019ll highlight a few of the pieces, and point you to resources you can use to learn more.\nAbout Digitec Galaxus\nDigitec Galaxus AG is the largest online retailer in Switzerland. It offers a wide range of products to its customers, from electronics to clothes. As an online retailer, we naturally make use of recommendation systems, not only on our home or product pages but also in our newsletters. We have multiple recommendation systems in place already for newsletters, and have been extensive early adopters of the Google Cloud recommendations AI. Because we have multiple recommendation systems and very large amounts of data, we are faced with the following complications.\n1. Personalization\nWe have over 12 recommenders that it uses in the newsletters, however we would like to contextualize these by choosing different recommenders (which in turn select the items) for different users. Furthermore, we would like to exploit existing trends as well as experiment with new ones.\n2. Latency\nWe would like to ensure that the ranked list of recommenders can be retrieved with sub 50 ms latency.\n3. End-to-end easy to maintain and generalizable/modular architecture\nWe wanted the solution to be architected using an easy to maintain, platform invariant, complete with all MLops capabilities required to train and use contextual bandits models. It was also important to us that it is built in a modular fashion such that it can be adapted easily to other use cases which have in mind such as recommendations on the homepage, Smartags and more.\nBefore we get to the details of how we built a machine learning infrastructure capable of dealing with all requirements, we'll dig a little deeper into how we got here and what problem we're trying to solve.\nUsing contextual bandits\nDigitec Galaxus has multiple recommendation systems in place already. Because we have multiple recommendation systems, it is sometimes difficult to choose between them in a personalized fashion. Hence we reached out to Google seeking assistance with implementing Contextual Bandit driven recommendations, which personalizes our homepage as well as our newsletter. Because we only send newsletters to registered users, we can incorporate features for every user.\nWe chose TFAgents to implement the contextual bandit model. Training and serving pipelines were orchestrated by Vertex AI pipelines running TFX, which in turn used TFAgents for the development of the contextual bandit models. Here\u2019s an overview of our approach.\nRewarding subscribes, and penalizing unsubscribes\nGiven some features (context) about the user, and each of the 12 available recommenders, we aim to suggest best recommender (action) which increases the chance (reward) of the user clicking (reward = 1) on at least one of the recommendations by the selected recommender, and minimizes the chance of incurring a click which leads to unsubscribe (reward = -1).\nBy formulating the problem and reward function in this manner, we hypothesized that the system would optimize for increasing clicks, while still showing relevant (and not click-baity) content to the user in order to sustain the potential increase in performance. This is because the reward functions penalizes an event when a user unsubscribes, which a click-baity content is likely to lead to. The problem was then tackled by using contextual bandits because of the fact that they excel at exploiting trends that work well, as well as exploring and uncovering potentially even better-performing trends.\nServing millions of users every week with low latency\nA diagram showing the high-level architecture of the recommendation training and prediction systems on GCP.\nThere\u2019s a lot of detail here, as the architecture shown in the diagram covers three phases of ML development, training, and serving. Here are some of the key pieces.\nModel development\nVertex Notebooks are used as data science environments for experimentation and prototyping, in addition to implementing model training and scoring components and pipelines. The source code is version controlled in GitHub. A continuous integration (CI) pipeline is set up to run unit tests, build pipeline components, and store the container images to Cloud Container Registry.\nTraining\nThe training pipeline is executed using TFX on Vertex Pipelines. In essence, the pipeline trains the model using new training data extracted from BigQuery, validates the produced model, and stores it in the model registry. In our system, the model registry is curated in Cloud Storage. The training pipeline uses Dataflow for large scale data extraction, validation, processing and model evaluation, as well as Vertex Training for large scale distributed training of the model. In addition, AI Platform Pipelines stores artifacts produced by the various pipeline steps to Cloud Storage, and information about these artifacts is stored in an ML metadata database in Cloud SQL.\nServing\nPredictions are produced using a batch prediction pipeline, and stored in Cloud Datastore for consumption. The batch prediction pipeline is made using TFX and runs on Vertex Pipelines. The pipeline uses the most recent model in the model registry to score the serving queries from BigQuery. A Cloud Function is provided as a REST/HTTP endpoint to retrieve predictions from Datastore.\nContinuous Training Pipeline\nA diagram of the TFX pipeline for the training workflow.\nThere are many components used in our TFX-based Continuous training workflow, training is currently done on an on-demand basis, but later on it is planned to be executed on a bi-weekly cadence. Here is a little bit of detail on the important ones.\nRaw Data\nOur data consists of multiple datasets stored in heterogeneous formats across BigQuery tables and other formats, that are then joined in denormalized fashion by the customer into a single BigQuery table for training. To help avoid bias and drift in our model we train the model on a rolling window of 4 weeks cadence with one overlapping week per training cycle. This was a simple design choice as it was very straightforward to implement, as BigQuery has good compatibility as a source with TFX, and also allows the user to do some basic data preprocessing and cleaning during fetching.\nBigQueryExampleGen\nWe first leverage BigQuery by leveraging built-in functions to preprocess the data. By embedding our own specific processes into the query calls made by the ExampleGen component, we were able to avoid building out a separate ETL that would exist outside the scope of a TFX pipeline. This ultimately proved to be a good way to get the model in production more quickly. This preprocessed data is then split into training and eval and converted to tf.Examples via the ExampleGen component.\nTransform\nThis component does the necessary feature engineering and transformations necessary to handle strings, fill in missing values, log-normalize values, setup embeddings etc. The major benefit here is that the resulting transformation is ultimately prepended to the computational graph, so that the exact same code is used for training and serving. The Transform component runs on Cloud Dataflow in production.\nTrainer\nThe Trainer component trains the model using TF-Agents. We leverage parallel training on Vertex Training to speed things up. The model is designed such that the user id passes in from the input to the output unaltered, so that it can be used as part of the downstream serving pipeline. The Trainer component runs on Vertex Training in production.\nEvaluator\nThe Evaluator compares the existing production model to the model received by the Trainer and prepares the metrics required by the validator component to bless the \"better\" one for use in production. The model gating criteria is based on the AUC scores as well as counterfactual policy evaluation and possibly other metrics in the future. It is easy to implement custom metrics which meet the business requirements owing to the extensibility of the evaluator component. The Evaluator runs on Vertex AI.\nPusher\nThe Pusher\u2019s primary function is to send the blessed model to our TFServing deployment for production. However, we added functionality to use the custom metrics produced in the Evaluator to determine decisioning criteria to be used in serving, and attach that to the computational graph. The level of abstraction available in TFX components made it easy to make this custom modification. Overall, the modification allows the pipeline to operate without a human in the loop so that we are able to make model updates frequently, while continuing to deliver consistent performance on metrics that are important to our business.\nHyperparametersGen\nThis is a custom TFX component which creates a dictionary with hyperparameters (e.g., batch size, learning rate) and stores the dictionary as an artifact. The hyperparameters are passed as input to the trainer.\nServingModelResolver\nThis custom component takes a serving policy (which includes exploration) and a corresponding eval policy (without exploration), and resolves which policy will be used for serving.\nPushing_finalizer\nThis custom component copies the pushed/blessed model from the TFX artifacts directory to a curated destination.\nThe out-of-box components from TFX provided most of the functionality we require, and it is easy to create some new custom components to make the entire pipeline satisfy our requirements. There are also other components of the pipeline such as StatisticsGen (which also runs on Dataflow).\nBatch Prediction Pipeline\nA diagram showing the TFX pipeline for the batch prediction workflow.\nHere are a few of the key pieces of our batch prediction system.\nInference Dataset\nOur inference dataset has nearly identical format to the training dataset, except that it is emptied and repopulated with new data daily.\nBigQueryExampleGen\nJust like for the Training pipeline, we use this component to read data from BigQuery and convert it into tf.Examples.\nModel Importer\nThis component imports the computation graph exported by the Pusher component of the training pipeline. As mentioned above, since it contains the whole computation graph generated by the training pipeline, including feature transformation and the tf.Agents model (including the exploration/exploitation aspect), this is very portable and prevents train/test skew.\nBulkInferrer\nAs the name implies, this component uses the imported computation graph to perform mass inference on the inference dataset. It runs on Cloud Dataflow in production and makes it very easy to scale.\nPredictionStorer\nThis is a custom Python Component which takes the inference results from Bulkinfererrer, post-processes them to format/filter the fields as required, and persists it to Cloud Datastore. This runs on Cloud Dataflow in production as well.\nServing is done via cloud functions which take the user ids as input, and returns the precomputed results for each userId stored in DataStore with sub 50 ms latency.\nExtending the work so far\nIn the few months since implementation of the first version we have been making dozens of improvements to the pipeline, everything from changing the architecture/approach of the original model, to changing the way the model's results are used in the downstream application to generate newsletters. Moreover, each of these improvements brings new value to us more quickly than we've been able to in the past.\nSince our initial implementation of this reference architecture, we have released a simple Vertex AI pipeline based github code samples to implementing recommender systems using TF Agents here. By using this template and guide, it will help them build recommender systems using contextual bandits on GCP in a scalable, modularized, low latency and cost effective manner. It's quite remarkable how many of the existing TFX components that we have in place carry over to new projects, and even more so how drastically we've reduced the time it takes to get a model in production. As a result, even the software engineers on our team without much ML expertise feel confident in being able to reuse this architecture and adapt it to more use cases. The data scientists are able to spend more of their time optimizing the parameters and architectures of the models they produce, understanding their impact on the business, and ultimately delivering more value to the users and the business.\nAcknowledgements\nNone of this would have been possible without the joint collaboration of the following Googlers: Khalid Salama, Efi Kokiopoulou, G\u00e1bor Bart\u00f3k and Digitec Galaxus\u2019s team of engineers.\nA Google Cloud blog on this project can be found here.",
    "link": "https://blog.tensorflow.org/2021/08/how-digitec-galaxus-trains-and-serves-millions-of-personalized-newsletters-per-week-with-TFX.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-vB4o8_mCLOc/YRLLlT6qyiI/AAAAAAAAEac/aVpPlNSb_r0_RCADxfT7nRfKP2dAJBU0QCLcBGAsYHQ/s0/%255BTF%2BBLOG%255D%2BDigitec%2BTFX%2BBlog%2BPost%2B1.gif",
      "https://1.bp.blogspot.com/-vB4o8_mCLOc/YRLLlT6qyiI/AAAAAAAAEac/aVpPlNSb_r0_RCADxfT7nRfKP2dAJBU0QCLcBGAsYHQ/s0/%255BTF%2BBLOG%255D%2BDigitec%2BTFX%2BBlog%2BPost%2B1.gif",
      "https://1.bp.blogspot.com/-Zvx93twC6x4/YRQOXs3ebeI/AAAAAAAAEbc/Wdvb_f9H9jY4b0987JrgPQAa-V3F2J6mwCLcBGAsYHQ/s0/PNM_FINAL_Copy_of_Digitec_TFX_Blog_Post_1.jpeg",
      "https://1.bp.blogspot.com/-3FTkOUCRiEc/YRQoWFlGeuI/AAAAAAAAEb8/Gz_2EVN1Bccl-_DrSRF4t59PFarMfPnjwCLcBGAsYHQ/s0/bandits%2Brecommendation%2B-%2BTDD%2B%25281%2529.jpg",
      "https://1.bp.blogspot.com/-mwYDOExwxnI/YRQvw2uXlMI/AAAAAAAAEcU/LuNhRRZ6wk0HiBDMkrkpfSFX9NuBt4eBwCLcBGAsYHQ/s0/bandits%2Brecommendation%2B-%2BTDD%2B%25282%2529%2B%25281%2529.jpg",
      "https://1.bp.blogspot.com/-ksvvpEjvH-E/YRQwS28r8CI/AAAAAAAAEcc/vaXSGC7aEZAb8Wo4CyOErAX0VFQoUHj8gCLcBGAsYHQ/s0/bandits%2Brecommendation%2B-%2BTDD%2B%25283%2529.jpg"
    ],
    "time": "2023/12/02 00:58:43"
  },
  {
    "title": "Using TFX inference with Dataflow for large scale ML inference patterns",
    "content": "Posted by Reza Rokni, Snr Staff Developer Advocate\nIn part I of this blog series we discussed best practices and patterns for efficiently deploying a machine learning model for inference with Google Cloud Dataflow. Amongst other techniques, it showed efficient batching of the inputs and the use of shared.py to make efficient use of a model.\nIn this post, we walk through the use of the RunInference API from tfx-bsl, a utility transform from TensorFlow Extended (TFX), which abstracts us away from manually implementing the patterns described in part I. You can use RunInference to simplify your pipelines and reduce technical debt when building production inference pipelines in batch or stream mode.\nThe following four patterns are covered:\nUsing RunInference to make ML prediction calls.\nPost-processing RunInference results. Making predictions is often the first part of a multistep flow, in the business process. Here we will process the results into a form that can be used downstream.\nAttaching a key. Along with the data that is passed to the model, there is often a need for an identifier \u2014 for example, an IOT device ID or a customer identifier \u2014 that is used later in the process even if it\u2019s not used by the model itself. We show how this can be accomplished.\nInference with multiple models in the same pipeline.Often you may need to run multiple models within the same pipeline, be it in parallel or as a sequence of predict - process - predict calls. We walk through a simple example.\nCreating a simple model\nIn order to illustrate these patterns, we\u2019ll use a simple toy model that will let us concentrate on the data engineering needed for the input and output of the pipeline. This model will be trained to approximate multiplication by the number 5.\nPlease note the following code snippets can be run as cells within a notebook environment.\nStep 1 - Set up libraries and imports\n%pip install tfx_bsl==0.29.0 --quiet\nimport argparse\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow_serving.apis import prediction_log_pb2\n\nimport apache_beam as beam\nimport tfx_bsl\nfrom tfx_bsl.public.beam import RunInference\nfrom tfx_bsl.public import tfxio\nfrom tfx_bsl.public.proto import model_spec_pb2\n\nimport numpy\n\nfrom typing import Dict, Text, Any, Tuple, List\n\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\nproject = \"<your project>\"\nbucket = \"<your bucket>\"\n\nsave_model_dir_multiply = f'gs://{bucket}/tfx-inference/model/multiply_five/v1/'\nsave_model_dir_multiply_ten = f'gs://{bucket}/tfx-inference/model/multiply_ten/v1/'\nStep 2 - Create the example data\nIn this step we create a small dataset that includes a range of values from 0 to 99 and labels that correspond to each value multiplied by 5.\n\"\"\"\nCreate our training data which represents the 5 times multiplication table for 0 to 99. x is the data and y the labels. \n\nx is a range of values from 0 to 99.\ny is a list of 5x\n\nvalue_to_predict includes a values outside of the training data\n\"\"\"\nx = numpy.arange(0, 100)\ny = x * 5\nStep 3 - Create a simple model, compile, and fit it\n\"\"\"\nBuild a simple linear regression model.\nNote the model has a shape of (1) for its input layer, it will expect a single int64 value.\n\"\"\"\ninput_layer = keras.layers.Input(shape=(1), dtype=tf.float32, name='x')\noutput_layer= keras.layers.Dense(1)(input_layer)\n\nmodel = keras.Model(input_layer, output_layer)\nmodel.compile(optimizer=tf.optimizers.Adam(), loss='mean_absolute_error')\nmodel.summary()\nLet\u2019s teach the model about multiplication by 5.\nmodel.fit(x, y, epochs=2000)\nNext, check how well the model performs using some test data.\nvalue_to_predict = numpy.array([105, 108, 1000, 1013], dtype=numpy.float32)\nmodel.predict(value_to_predict)\nFrom the results below it looks like this simple model has learned its 5 times table close enough for our needs!\nOUTPUT: \n\narray([[ 524.9939],\n       [ 539.9937],\n       [4999.935 ],\n       [5064.934 ]], dtype=float32)\nStep 4 - Convert the input to tf.example\nIn the model we just built, we made use of a simple list to generate the data and pass it to the model. In this next step we make the model more robust by using tf.example objects in the model training.\ntf.example is a serializable dictionary (or mapping) from names to tensors, which ensures the model can still function even when new features are added to the base examples. Making use of tf.example also brings with it the benefit of having the data be portable across models in an efficient, serialized format.\nTo use tf.example for this example, we first need to create a helper class, ExampleProcessor, that is used to serialize the data points.\nclass ExampleProcessor:\n  \n   def create_example_with_label(self, feature: numpy.float32,\n                            label: numpy.float32)-> tf.train.Example:\n       return tf.train.Example(\n           features=tf.train.Features(\n                 feature={'x': self.create_feature(feature),\n                          'y' : self.create_feature(label)\n                 }))\n\n   def create_example(self, feature: numpy.float32):\n       return tf.train.Example(\n           features=tf.train.Features(\n                 feature={'x' : self.create_feature(feature)})\n           )\n\n   def create_feature(self, element: numpy.float32):\n       return tf.train.Feature(float_list=tf.train.FloatList(value=[element]))\nUsing the ExampleProcess class, the in-memory list can now be moved to disk.\n# Create our labeled example file for 5 times table\n\nexample_five_times_table = 'example_five_times_table.tfrecord'\n\nwith tf.io.TFRecordWriter(example_five_times_table) as writer:\n for i in zip(x, y):\n   example = ExampleProcessor().create_example_with_label(\n       feature=i[0], label=i[1])\n   writer.write(example.SerializeToString())\n\n# Create a file containing the values to predict\n\npredict_values_five_times_table = 'predict_values_five_times_table.tfrecord'\n\nwith tf.io.TFRecordWriter(predict_values_five_times_table) as writer:\n for i in value_to_predict:\n   example = ExampleProcessor().create_example(feature=i)\n   writer.write(example.SerializeToString())\nWith the new examples stored in TFRecord files on disk, we can use the Dataset API to prepare the data so it is ready for consumption by the model.\nRAW_DATA_TRAIN_SPEC = {\n'x': tf.io.FixedLenFeature([], tf.float32),\n'y': tf.io.FixedLenFeature([], tf.float32)\n}\n\nRAW_DATA_PREDICT_SPEC = {\n'x': tf.io.FixedLenFeature([], tf.float32),\n}\nWith the feature spec in place, we can train the model as before.\ndataset = tf.data.TFRecordDataset(example_five_times_table) \ndataset = dataset.map(lambda e : tf.io.parse_example(e, RAW_DATA_TRAIN_SPEC)) \ndataset = dataset.map(lambda t : (t['x'], t['y'])) \ndataset = dataset.batch(100) \ndataset = dataset.repeat()\nmodel.fit(dataset, epochs=500, steps_per_epoch=1)\nNote that these steps would be done automatically for us if we had built the model using a TFX pipeline, rather than hand-crafting the model as we did here.\nStep 5 - Save the model\nNow that we have a model, we need to save it for use with the RunInference transform. RunInference accepts TensorFlow saved model pb files as part of its configuration. The saved model file must be stored in a location that can be accessed by the RunInference transform. In a notebook this can be the local file system; however, to run the pipeline on Dataflow, the file will need to be accessible by all the workers, so here we use a GCP bucket.\nNote that the gs:// schema is directly supported by the tf.keras.models.save_model api.\ntf.keras.models.save_model(model, save_model_dir_multiply)\nDuring development it's useful to be able to inspect the contents of the saved model file. For this, we use the saved_model_cli that comes with TensorFlow. You can run this command from a cell:\n!saved_model_cli show --dir {save_model_dir_multiply} --all\nAbbreviated output from the saved model file is shown below. Note the signature def 'serving_default', which accepts a tensor of float type. We will change this to accept another type in the next section.\nOUTPUT: \n\nsignature_def['serving_default']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['example'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: serving_default_example:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['dense_1'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: StatefulPartitionedCall:0\n  Method name is: tensorflow/serving/predict\nRunInference will pass a serialized tf.example to the model rather than a tensor of float type as seen in the current signature. To accomplish this we have one more step to prepare the model: creation of a specific signature.\nSignatures are a powerful feature as they enable us to control how calling programs interact with the model. From the TensorFlow documentation:\n\"The optional signatures argument controls which methods in obj will be available to programs which consume SavedModels, for example, serving APIs. Python functions may be decorated with @tf.function(input_signature=...) and passed as signatures directly, or lazily with a call to get_concrete_function on the method decorated with @tf.function.\"\nIn our case, the following code will create a signature that accepts a tf.string data type with a name of 'examples'. This signature is then saved with the model, which replaces the previous saved model.\n@tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string , name='examples')])\ndef serve_tf_examples_fn(serialized_tf_examples):\n \"\"\"Returns the output to be used in the serving signature.\"\"\"\n features = tf.io.parse_example(serialized_tf_examples, RAW_DATA_PREDICT_SPEC)\n return model(features, training=False)\n\nsignature = {'serving_default': serve_tf_examples_fn}\n\ntf.keras.models.save_model(model, save_model_dir_multiply, signatures=signature)\nIf you run the saved_model_cli command again, you will see that the input signature has changed to DT_STRING.\nPattern 1: RunInference for Predictions\nStep 1 - Use RunInference within the pipeline\nNow that the model is ready, the RunInference transform can be plugged into an Apache Beam pipeline. The pipeline below uses TFXIO TFExampleRecord, which it converts to a transform via RawRecordBeamSource(). The saved model location and signature are passed to the RunInference API as a SavedModelSpec configuration object.\npipeline = beam.Pipeline()\n\ntfexample_beam_record = tfx_bsl.public.tfxio.TFExampleRecord(file_pattern=predict_values_five_times_table)\n\nwith pipeline as p:\n   _ = (p | tfexample_beam_record.RawRecordBeamSource()\n          | RunInference(\n              model_spec_pb2.InferenceSpecType(\n                  saved_model_spec=model_spec_pb2.SavedModelSpec(model_path=save_model_dir_multiply)))\n          | beam.Map(print)\n       )\nNote:\nYou can perform two types of inference using RunInference:\nIn-process inference from a SavedModel instance. Used when the saved_model_spec field is set in inference_spec_type.\nRemote inference by using a service endpoint. Used when the ai_platform_prediction_model_spec field is set in inference_spec_type.\nBelow is a snippet of the output. The values here are a little difficult to interpret as they are in their raw unprocessed format. In the next section the raw results are post-processed.\nOUTPUT: \n\npredict_log {\n  request { \nmodel_spec { signature_name: \"serving_default\" }\n                inputs {\n      key: \"examples\"\n... \n       string_val: \"\\n\\022\\n\\020\\n\\007example\\022\\005\\032\\003\\n\\001i\"\n...\nresponse {\n    outputs {\n      key: \"output_0\"\n      value {\n   ...\n        float_val: 524.993896484375\nPattern 2: Post-processing RunInference results\nThe RunInference API returns a PredictionLog object, which contains the serialized input and the output from the call to the model. Having access to both the input and output enables you to create a simple tuple during post-processing for use downstream in the pipeline. Also worthy of note is that RunInference will consider the amenable-to-batching capability of the model (and does batch inference for performance purposes) transparently for you.\nThe PredictionProcessor beam.DoFn takes the output of RunInference and produces formatted text with the questions and answers as output. Of course in a production system, the output would more normally be a Tuple[input, output], or simply the output depending on the use case.\nclass PredictionProcessor(beam.DoFn):\n\n   def process(\n           self,\n           element: prediction_log_pb2.PredictionLog):\n       predict_log = element.predict_log\n       input_value = tf.train.Example.FromString(predict_log.request.inputs['examples'].string_val[0])\n       output_value = predict_log.response.outputs\n       yield (f\"input is [{input_value.features.feature['x'].float_list.value}] output is {output_value['output_0'].float_val}\");\n\npipeline = beam.Pipeline()\n\ntfexample_beam_record = tfx_bsl.public.tfxio.TFExampleRecord(file_pattern=predict_values_five_times_table)\n\nwith pipeline as p:\n   _ = (p | tfexample_beam_record.RawRecordBeamSource()\n          | RunInference(\n              model_spec_pb2.InferenceSpecType(\n                  saved_model_spec=model_spec_pb2.SavedModelSpec(model_path=save_model_dir_multiply)))\n          | beam.ParDo(PredictionProcessor())\n          | beam.Map(print)\n       )\nNow the output contains both the original input and the model's output values.\nOUTPUT: \n\ninput is [[105.]] output is [523.6328735351562]\ninput is [[108.]] output is [538.5157470703125]\ninput is [[1000.]] output is [4963.6787109375]\ninput is [[1013.]] output is [5028.1708984375]\nPattern 3: Attaching a key\nOne useful pattern is the ability to pass information, often a unique identifier, with the input to the model and have access to this identifier from the output. For example, in an IOT use case you could associate a device id with the input data being passed into the model. Often this type of key is not useful for the model itself and thus should not be passed into the first layer.\nRunInference takes care of this for us, by accepting a Tuple[key, value] and outputting Tuple[key, PredictLog]\nStep 1 - Create a source with attached key\nSince we need a key with the data that we are sending in for prediction, in this step we create a table in BigQuery, which has two columns: One holds the key and the second holds the test value.\nCREATE OR REPLACE TABLE\n  maths.maths_problems_1 ( key STRING OPTIONS(description=\"A unique key for the maths problem\"),\n    value FLOAT64 OPTIONS(description=\"Our maths problem\" ) );\nINSERT INTO\n  maths.maths_problems_1\nVALUES\n  ( \"first_question\", 105.00),\n  ( \"second_question\", 108.00),\n  ( \"third_question\", 1000.00),\n  ( \"fourth_question\", 1013.00)\nStep 2 - Modify post processor and pipeline\nIn this step we:\nModify the pipeline to read from the new BigQuery source table\nAdd a map transform, which converts a table row into a Tuple[ bytes, Example]\nModify the post inference processor to output results along with the key\nclass PredictionWithKeyProcessor(beam.DoFn):\n\n   def __init__(self):\n       beam.DoFn.__init__(self)\n\n   def process(\n           self,\n           element: Tuple[bytes, prediction_log_pb2.PredictionLog]):\n       predict_log = element[1].predict_log\n       input_value = tf.train.Example.FromString(predict_log.request.inputs['examples'].string_val[0])\n       output_value = predict_log.response.outputs\n       yield (f\"key is {element[0]} input is {input_value.features.feature['x'].float_list.value} output is { output_value['output_0'].float_val[0]}\" )\n\npipeline_options = PipelineOptions().from_dictionary({'temp_location':f'gs://{bucket}/tmp'})\npipeline = beam.Pipeline(options=pipeline_options)\n\nwith pipeline as p:\n _ = (p | beam.io.gcp.bigquery.ReadFromBigQuery(table=f'{project}:maths.maths_problems_1')\n         | beam.Map(lambda x : (bytes(x['key'], 'utf-8'), ExampleProcessor().create_example(numpy.float32(x['value']))))\n         | RunInference(\n             model_spec_pb2.InferenceSpecType(\n                 saved_model_spec=model_spec_pb2.SavedModelSpec(model_path=save_model_dir_multiply)))\n         | beam.ParDo(PredictionWithKeyProcessor())\n         | beam.Map(print)\n     )\nkey is b'first_question' input is [105.] output is 524.0875854492188\nkey is b'second_question' input is [108.] output is 539.0093383789062\nkey is b'third_question' input is [1000.] output is 4975.75830078125\nkey is b'fourth_question' input is [1013.] output is 5040.41943359375\nPattern 4: Inference with multiple models in the same pipeline\nIn part I of the series, the \"join results from multiple models\" pattern covered the various branching techniques in Apache Beam that make it possible to run data through multiple models.\nThose techniques are applicable to RunInference API, which can easily be used by multiple branches within a pipeline, with the same or different models. This is similar in function to cascade ensembling, although here the data flows through multiple models in a single Apache Beam DAG.\nInference with multiple models in parallel\nIn this example, the same data is run through two different models: the one that we\u2019ve been using to multiply by 5 and a new model, which will learn to multiply by 10.\n\"\"\"\nCreate multiply by 10 table.\n\nx is a range of values from 0 to 100.\ny is a list of x * 10\n\nvalue_to_predict includes a values outside of the training data\n\"\"\"\nx = numpy.arange( 0, 1000)\ny = x * 10\n\n# Create our labeled example file for 10 times table\n\nexample_ten_times_table = 'example_ten_times_table.tfrecord'\n\nwith tf.io.TFRecordWriter( example_ten_times_table ) as writer:\n for i in zip(x, y):\n   example = ExampleProcessor().create_example_with_label(\n       feature=i[0], label=i[1])\n   writer.write(example.SerializeToString())\n\ndataset = tf.data.TFRecordDataset(example_ten_times_table) \ndataset = dataset.map(lambda e : tf.io.parse_example(e, RAW_DATA_TRAIN_SPEC)) \ndataset = dataset.map(lambda t : (t['x'], t['y'])) \ndataset = dataset.batch(100)\ndataset = dataset.repeat() \n\nmodel.fit(dataset, epochs=500, steps_per_epoch=10, verbose=0)\n\ntf.keras.models.save_model(model,\n                           save_model_dir_multiply_ten,\n                           signatures=signature)\nNow that we have two models, we apply them to our source data.\npipeline_options = PipelineOptions().from_dictionary(\n                                     {'temp_location':f'gs://{bucket}/tmp'})\n\npipeline = beam.Pipeline(options=pipeline_options)\n\nwith pipeline as p:\n questions = p | beam.io.gcp.bigquery.ReadFromBigQuery(\n                                   table=f'{project}:maths.maths_problems_1')\n\n multiply_five = ( questions\n             | \"CreateMultiplyFiveTuple\" >>\n             beam.Map(lambda x : (bytes('{}{}'.format(x['key'],' * 5'),'utf-8'),\n                                   ExampleProcessor().create_example(x['value'])))\n            \n             | \"Multiply Five\" >> RunInference(\n                 model_spec_pb2.InferenceSpecType(\n                 saved_model_spec=model_spec_pb2.SavedModelSpec(\n                                           model_path=save_model_dir_multiply)))\n     )\n multiply_ten = ( questions\n         | \"CreateMultiplyTenTuple\" >>\n         beam.Map(lambda x : (bytes('{}{}'.format(x['key'],'* 10'), 'utf-8'),\n                              ExampleProcessor().create_example(x['value'])))\n         | \"Multiply Ten\" >> RunInference(\n             model_spec_pb2.InferenceSpecType(\n             saved_model_spec=model_spec_pb2.SavedModelSpec(\n                                         model_path=save_model_dir_multiply_ten)))\n )\n _ = ((multiply_five, multiply_ten) | beam.Flatten()\n                                    | beam.ParDo(PredictionWithKeyProcessor())\n                                    | beam.Map(print))\nOutput:\n\nkey is b'first_question * 5' input is [105.] output is 524.0875854492188\nkey is b'second_question * 5' input is [108.] output is 539.0093383789062\nkey is b'third_question * 5' input is [1000.] output is 4975.75830078125\nkey is b'fourth_question * 5' input is [1013.] output is 5040.41943359375\nkey is b'first_question* 10' input is [105.] output is 1054.333984375\nkey is b'second_question* 10' input is [108.] output is 1084.3131103515625\nkey is b'third_question* 10' input is [1000.] output is 9998.0908203125\nkey is b'fourth_question* 10' input is [1013.] output is 10128.0009765625\nInference with multiple models in sequence\nIn a sequential pattern, data is sent to one or more models in sequence, with the output from each model chaining to the next model.\nHere are the steps:\nRead the data from BigQuery\nMap the data\nRunInference with multiply by 5 model\nProcess the results\nRunInference with multiply by 10 model\nProcess the results\npipeline_options = PipelineOptions().from_dictionary(\n                                       {'temp_location':f'gs://{bucket}/tmp'})\n\npipeline = beam.Pipeline(options=pipeline_options)\n\ndef process_interim_inference(element : Tuple[\n                                        bytes, prediction_log_pb2.PredictionLog\n                                        ])-> Tuple[bytes, tf.train.Example]:\n  \n  key = '{} original input is {}'.format(\n             element[0], str(tf.train.Example.FromString(\n                 element[1].predict_log.request.inputs['examples'].string_val[0]\n                 ).features.feature['x'].float_list.value[0]))\n  \n  value = ExampleProcessor().create_example(\n              element[1].predict_log.response.outputs['output_0'].float_val[0])\n  \n  return (bytes(key,'utf-8'),value)\n\nwith pipeline as p:\n  \n questions = p | beam.io.gcp.bigquery.ReadFromBigQuery(\n                                   table=f'{project}:maths.maths_problems_1')\n\n multiply = ( questions\n             | \"CreateMultiplyTuple\" >>\n             beam.Map(lambda x : (bytes(x['key'],'utf-8'),\n                                   ExampleProcessor().create_example(x['value'])))\n             | \"MultiplyFive\" >> RunInference(\n                 model_spec_pb2.InferenceSpecType(\n                 saved_model_spec=model_spec_pb2.SavedModelSpec(\n                                   model_path=save_model_dir_multiply)))\n            \n     )\n\n _ = ( multiply\n         | \"Extract result \" >> \n         beam.Map(lambda x : process_interim_inference(x))\n         | \"MultiplyTen\" >> RunInference(\n             model_spec_pb2.InferenceSpecType(\n             saved_model_spec=model_spec_pb2.SavedModelSpec(\n                             model_path=save_model_dir_multiply_ten)))\n         | beam.ParDo(PredictionWithKeyProcessor())\n         | beam.Map(print)\n )\nOutput: \n\nkey is b\"b'first_question' original input is 105.0\" input is [524.9771118164062] output is 5249.7822265625\nkey is b\"b'second_question' original input is 108.0\" input is [539.9765014648438] output is 5399.7763671875\nkey is b\"b'third_question' original input is 1000.0\" input is [4999.7841796875] output is 49997.9453125\nkey is b\"b'forth_question' original input is 1013.0\" input is [5064.78125] output is 50647.91796875\nRunning the pipeline on Dataflow\nUntil now the pipeline has been run locally, using the direct runner, which is implicitly used when running a pipeline with the default configuration. The same examples can be run using the production Dataflow runner by passing in configuration parameters including --runner. Details and an example can be found here.\nHere is an example of the multimodel pipeline graph running on the Dataflow service:\nWith the Dataflow runner you also get access to pipeline monitoring as well as metrics that have been output from the RunInference transform. The following table shows some of these metrics from a much larger list available from the library.\nConclusion\nIn this blog, part II of our series, we explored the use of the tfx-bsl RunInference within some common scenarios, from standard inference, to post processing and the use of RunInference API in multiple locations in the pipeline.\nTo learn more, review the Dataflow and TFX documentation, you can also try out TFX with Google Cloud AI platform pipelines..\nAcknowledgements\nNone of this would be possible without the hard work of many folks across both the Dataflow TFX and TF teams. From the TFX and TF team we would especially like to thank Konstantinos Katsiapis, Zohar Yahav, Vilobh Meshram, Jiayi Zhao, Zhitao Li, and Robert Crowe. From the Dataflow team I would like to thank Ahmet Altay for his support and input throughout.",
    "link": "https://blog.tensorflow.org/2021/05/using-tfx-inference-with-dataflow-for-large-scale-ml-inference-patterns.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-g3BCkkrMeL8/YJQFWRdCPhI/AAAAAAAAEME/He6i47qwLR4psp5wjvglw0aDQbPEgKUWgCLcBGAsYHQ/s0/Screen_Shot_2021-04-28_at_1.49.24_PM.max-700x700_%25281%2529.jpeg",
      "https://1.bp.blogspot.com/-KhfKse2JA1w/YJGWk4coNNI/AAAAAAAAEJU/md7DKpUIMmEAgbED_9NiarWSawfRooKogCLcBGAsYHQ/s0/Screen_Shot_2021-04-28_at_1.49.24_PM.max-700x700.png",
      "https://1.bp.blogspot.com/-hnTxYmDsw3Q/YJGXYM1OFdI/AAAAAAAAEJc/Ru23QirY8HkuZZLacarkalyDBk_2s160wCLcBGAsYHQ/s0/Screen_Shot_2021-04-28_at_1.51.12_PM.max-300x300.png",
      "https://1.bp.blogspot.com/-ODXN7A9tVDc/YJGYEHDe5hI/AAAAAAAAEJk/ij_2s_W06CY4fvkNmMpAaK5VsXVNCZbJQCLcBGAsYHQ/s0/Screenshot_2021-04-15_at_12.53.50_PM.max-400x400.png",
      "https://1.bp.blogspot.com/-vbKAH8pACas/YJGYV8g6h0I/AAAAAAAAEJs/4hzDQi2pGq45LvvsKja8BV1Vq0zM8zusQCLcBGAsYHQ/s0/TFX%2BTable.png"
    ],
    "time": "2023/12/01 00:58:43"
  },
  {
    "title": "How OpenX Trains and Serves for a Million Queries per Second in under 15 Milliseconds",
    "content": "A guest post by Larry Price, OpenX\nEdited by Robert Crowe, Anusha Ramesh - TensorFlow\nOverview\nAdtech is an industry built on latency at scale. At OpenX this means that during peak traffic periods our exchange processes more than one million requests for ads every second, most of which require a response in under 300 milliseconds. Under such high volume and strict time budgets, it's crucial to prioritize traffic to ensure we're simultaneously helping publishers get top dollar for their inventory as well as ensuring buyers hit their campaign goals.\nTo accomplish this, we've leveraged several products in the TensorFlow ecosystem & Google Cloud including TensorFlow Extended (TFX), TF Serving, and Kubeflow Pipelines - to build a service that prioritizes traffic to our buyers (demand side platforms, or DSPs in adtech lingo) and more specifically to brands, and agencies within those DSPs.\nAbout OpenX\nOpenX operates the world's largest independent advertising exchange. At a basic level, the exchange is a marketplace connecting tens of thousands of top brands to consumers across the most-visited websites and mobile apps.\nThe fundamental means of transacting is the auction, where buyers representing brands bid on publishers' inventory, which are ad impressions on websites and mobile apps. The auctions themselves are fairly straightforward, but there are two facts that make this system incredibly complicated:\nScale: At peak traffic our systems process more than one million requests for ads every second. A typical day sees more than 1.5 trillion bid transactions, resulting in petabytes of raw data.\nLatency: Preserving user experience on both the web and mobile apps is crucial to publishers, so most of the requests we process have strict time limits of 300 milliseconds or less, most of which is spent asking for and receiving the buyers' bids. This means that any overhead introduced by machine learning models at auction time must be limited to at most about 15 milliseconds, otherwise we risk not giving buyers enough time to submit their bids.\nThis need for low latency coupled with the high throughput requirement is fairly atypical for machine learning systems. Before we get to the details of how we built a machine learning infrastructure capable of dealing with both requirements, we'll dig a little deeper into how we got here and what problem we're trying to solve.\nCloud Transformation: A rare opportunity\nIn 2019 OpenX undertook the ambitious task of moving off of on-premise computing resources to Google Cloud Platform (GCP). We completed the process over a span of seven months. As a company, we were empowered to utilize managed services and modify our stack as we transition, so it wasn't just a simple \"lift-and-shift\". We really took this to heart on the Data Science team.\nPrior to the move to GCP, our legacy machine learning infrastructure followed a pattern where models trained by scientists had to be re-implemented by engineers in the components that needed to execute the models. This scenario satisfies the scale and latency requirements but comes with a whole host of other issues:\nIt takes a long time to get models to production because the scientist's work (typically in Python) now has to be reproduced by an engineer in the native language of the component that has to call it.\nThe same is true for changes to model architecture, or even the way data transformations are performed.\nIt's essentially a recipe for training-serving skew.\nQA was challenging.\nFor these and several other reasons we decided to start from scratch. At the same time, we were working on a new problem and decided to tie the two efforts together and develop a new framework as part of the new project.\nOur problem\nThe OpenX marketplace is not completely unlike an equities market or stock exchange. And much like high volume financial markets, to ensure the buyers fulfill their campaign goals and simultaneously help publishers monetize appropriately on their inventory, there's a need to prioritize traffic. Fundamentally, this means we need a model that can accurately value and hence rank every single request that hits the exchange.\nWhy TensorFlow\nAs we looked for a solution for our next-generation platform we had a couple of goals in mind. We were looking primarily to drastically reduce the time and effort to put a model into production, and as part of getting there try to use managed services wherever possible. TensorFlow had already been in use at OpenX for a while prior to our migration to GCP, but our legacy infrastructure involved a number of custom scripts for data transformation and pipelining. At the same time as we were researching our options, both TensorFlow Extended (TFX) and Kubeflow Pipelines (KFP) were reaching a level of maturity that made them interesting for our purposes. It was a no-brainer to adopt these technologies into our stack.\nHow we solved it\nTraining Terabytes of Data Every Day\nOur pipeline looks something like this.\nIt's useful to spend some time breaking down the topology of the pipeline:\nRaw Data - Our data consists of transaction logs that are streamed directly from StackDriver into a BigQuery sink as they arrive. To help avoid bias in our model we train on a fraction of the total data that is held out from our prioritization system, resulting in roughly 50TB of new data daily. This was a simple design choice as it was very straightforward to implement, and the big benefit is that we can use BigQuery on the data directly without an additional ETL.\nBigQueryExampleGen - The first place we leverage BigQuery is using builtin functions to preprocess the data. By embedding our own specific processes into the query calls made by the ExampleGen component, we were able to avoid building out a separate ETL that would exist outside the scope of a TFX pipeline. This ultimately proved to be a good way to get the model in production more quickly. This preprocessed data is then split into training and test sets and converted to tf.Examples via the ExampleGen component.\nTransform - This component does the necessary feature engineering and transformations necessary to handle strings, normalize values, setup embeddings etc. The major benefit here is that the resulting transformation is ultimately prepended to the computational graph, so that the exact same code is used for training and serving. TFX makes use of Apache Beam for its pre-processing libraries, this allows for our developers to make use of the local Direct Runner for development and testing, while making use of Google Cloud Dataflow for the production TFX runs on the large volume workloads. (See also)\nTrainer - The Trainer component does just that. We leverage parallel training on AI Platform to speed things up.\nEvaluator - The Evaluator compares the existing production model to the model received by the Trainer and blesses the \"better\" one for use in production. The decisioning criteria is based on custom metrics aligned with business requirements (as opposed to, e.g. precision and recall). It was easy to implement the custom metrics meeting the business requirements owing to the extensibility of the evaluator component.\nPusher - The Pusher\u2019s primary function is to send the blessed model to our TFServing deployment for production. However, we added functionality to use the custom metrics produced in the Evaluator to determine decisioning criteria to be used in serving, and attach that to the computational graph. The level of abstraction available in TFX components made it easy to make this custom modification. Overall, the modification allows the pipeline to operate without a human in the loop so that we are able to make model updates frequently, while continuing to deliver consistent performance on metrics that are important to our business.\nOverall, out-of-the box TFX components provided most of the functionality we require. The biggest need we had to address is that our marketplace changes constantly, which requires frequent model updates. As mentioned previously, the design of TFX made those augmentations straightforward to implement.\nHowever this really only solves the model training part of our problem. Serving up a million queries per second, each in under 15 milliseconds, is a major challenge. For that we turned to TensorFlow Serving.\nServing Over a Million Queries Per Second (QPS)\nTensorFlow Serving enabled us to quickly take our TensorFlow models and serve them in production in a performant and scalable way. Using TensorFlow Serving provided us with a number of benefits. First, because it natively supports Google Cloud Storage as a model warehouse, we can automatically update our models used in serving simply by uploading to a GCS bucket. This allows us to quickly refresh our models with the newest data and have them instantly served in production. Next, TensorFlow Serving supports a batching mode that drastically increases throughput by queuing up several requests and processing them in a single graph run at the same time. This was an essential feature that massively helped us achieve our throughput goals just by setting a single option. Finally, TensorFlow Serving exposes metrics out of the box that allow us to monitor the throughput and latency of our requests and observe any scaling bottlenecks and inefficiencies.\nAll of these out of the box features in TensorFlow Serving were a massive win for us and helped us achieve our goals, but scaling it to millions of requests a second was not without challenges. By using large virtual machines with many CPUs we were able to hit our target goal of 15 millisecond predictions, but it did not scale very cost effectively and we knew we could do better. Luckily, TensorFlow Serving has several knobs and parameters that we used to tune our production VMs for better efficiency and scalability. By setting things like the number of batch threads, inter- and intra-op parallelism, and batch timeout, we were able to efficiently autoscale on custom sized VMs while still maintaining our throughput and latency goals.\nThe end result was a TensorFlow Serving deployment running on Google Kubernetes Engine serving 2.5 million prediction requests per second under 15 milliseconds each. This deployment spans over 25 kubernetes clusters across 10 different GCP regions and is able to scale up and down seamlessly to respond to spikes in traffic and save costs by scaling down during quiet periods. With around 500 TensorFlow Serving instances running around the world at peak times, each 8-CPU deployment is able to handle 5000 requests per second.\nBuilding on Success\nIn the few months since implementing this we've been able to make dozens of improvements to the model - everything from changing the architecture of the original model, to changing the way certain features are processed - without support from any other engineering team. Changes at this pace were all but impossible with our legacy architecture. Moreover, each of these improvements brings new value to our customers - the buyers and sellers in our marketplace - more quickly than we've been able to in the past.\nSince our initial implementation of this reference architecture, we've used it as a template for both new projects and the migration of existing models. It's quite remarkable how many of the existing TFX components that we have in place carry over to new projects, and even more so how drastically we've reduced the time it takes to get a model in production. As a result, data scientists are able to spend more of their time optimizing the parameters and architectures of the models they produce, understanding their impact on the business, and ultimately delivering more value to our customers.\nAcknowledgements\nNone of this would have been possible without the hard work of Michal Brys, Andy Gooden, Junbo Park, and Paul Selden, along with the rest of the OpenX Data Science and Engineering Teams as well as the support of Paul Ryan. We're also grateful for the support of strategic cloud engineers Will Beebe and Leonid Kuligin, as well as Dillon Do, Iman Kafarah, and Kyle Winn from the GCP account management team. Many thanks to the TensorFlow (TFX, TF Serving), and Kubeflow Teams, particularly Robert Crowe and Anusha Ramesh for helping to bring this case study to life.",
    "link": "https://blog.tensorflow.org/2021/02/how-openx-trains-and-serves-for-million-queries-per-second.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-VFie6pq5jl4/YCa-HLFdtFI/AAAAAAAAD_g/_OKWPT8LfTY9txvG8mxcnbmBA8q26KKpwCLcBGAsYHQ/s0/OpenX_Logo_WHITE_R_2013.jpeg",
      "https://1.bp.blogspot.com/-ABagN0cNbA4/YCXGa4Qp7BI/AAAAAAAAD_U/HA_rachNEtkdr6Q73bMOI90RJLqHiXynwCLcBGAsYHQ/s0/OpenX_Logo_R_2013.jpg",
      "https://1.bp.blogspot.com/-jkrLQ_qcmoo/YCW5rCeIt0I/AAAAAAAAD_I/B0H_k4lo47gxMv3rw2BvFp_Gep8hkABQgCLcBGAsYHQ/s0/TF%2BOpenX.png"
    ],
    "time": "2023/12/01 00:58:43"
  },
  {
    "title": "ML Metadata: Version Control for ML",
    "content": "Posted by Ben Mathes and Neoklis Polyzotis, on behalf of the TFX Team\n\nWhen you write code, you need version control to keep track of it. What\u2019s the ML equivalent of version control? If you\u2019re building production ML systems, you need to be able to answer questions like these:\nWhich dataset was this model trained on?\nWhat hyperparameters were used?\nWhich pipeline was used to create this model?\nWhich version of TensorFlow (and other libraries) were used to create this model?\nWhat caused this model to fail?\nWhat version of this model was last deployed?\nEngineers at Google have learned, through years of hard-won experience, that this history and lineage of ML artifacts is far more complicated than a simple, linear log. You use Git (or similar) to track your code; you need something to track your models, datasets, and more. Git, for example, may simplify your life a lot, but under the hood there\u2019s a graph of many things! The complexity of ML code and artifacts like models, datasets, and much more requires a similar approach.\nThat\u2019s why we built Machine Learning Metadata (MLMD). It\u2019s a library to track the full lineage of your entire ML workflow. Full lineage is all the steps from data ingestion, data preprocessing, validation, training, evaluation, deployment, and so on. MLMD is a standalone library, and also comes integrated in TensorFlow Extended. There\u2019s also a demo notebook to see how you can integrate MLMD into your ML infrastructure today.\nBeyond versioning your model, ML Metadata captures the full lineage of the training process, including the dataset, hyperparameters, and software dependencies.\nHere\u2019s how MLMD can help you:\nIf you\u2019re a ML Engineer: You can use MLMD to trace bad models back to their dataset, or trace from a bad dataset to the models you trained on it, and so on.\nIf you\u2019re working in ML infrastructure: You can use MLMD to record the current state of your pipeline and enable event-based orchestration. You can also enable optimizations like skipping a step if the inputs and code are the same, memoizing steps in your pipelines. You can integrate MLMD into your training system so it automatically creates logs for querying later. We\u2019ve found that this auto-logging of the full lineage as a side effect of training is the best way to use MLMD. Then you have the full history without extra effort.\nMLMD is more than a TFX research project. It\u2019s a key foundation to multiple internal MLOps solutions at Google. Furthermore, Google Cloud integrates tools like MLMD into its core MLOps platform:\nThe foundation of all these new services is our new ML Metadata Management service in AI Platform. This service lets AI teams track all the important artifacts and experiments they run, providing a curated ledger of actions and detailed model lineage. This will enable customers to determine model provenance for any model trained on AI Platform for debugging, audit, or collaboration. AI Platform Pipelines will automatically track artifacts and lineage and AI teams can also use the ML Metadata service directly for custom workloads, artifact and metadata tracking.\nWant to know where your models come from? What training data was used? Did anyone else train a model on this dataset already, and was their performance better? Are there any tainted datasets we need to clean up after?\nIf you want to answer these questions for your users, check out MLMD on github, as a part of TensorFlow Extended, or in our demo notebook.",
    "link": "https://blog.tensorflow.org/2021/01/ml-metadata-version-control-for-ml.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-3GN3lYuOfCQ/X_d6gmvxUFI/AAAAAAAAD6E/WAxc28EbE3cS-AAexegbN9pCV47f6bw-wCLcBGAsYHQ/s0/TF_Metadata.jpeg",
      "https://1.bp.blogspot.com/-NvxIubZOKQI/X_d4vuOZFWI/AAAAAAAAD54/tN1bbfWDhGAvg-kOrsCfRspDkvuwGkfSgCLcBGAsYHQ/s0/TF%2BMetadata.png"
    ],
    "time": "2023/12/01 00:58:43"
  },
  {
    "title": "Neural Structured Learning in TFX",
    "content": "Posted by Arjun Gopalan, Software Engineer, Google Research\nEdited by Robert Crowe, TensorFlow Developer Advocate, Google Research\nIntroduction\nNeural Structured Learning (NSL) is a framework in TensorFlow that can be used to train neural networks with structured signals. It handles structured input in two ways: (i) as an explicit graph, or (ii) as an implicit graph where neighbors are dynamically generated during model training. NSL with an explicit graph is typically used for Neural Graph Learning while NSL with an implicit graph is typically used for Adversarial Learning. Both of these techniques are implemented as a form of regularization in the NSL framework. As a result, they only affect the training workflow and so, the model serving workflow remains unchanged. In the rest of this post, we will mostly focus on how graph regularization can be implemented using the NSL framework in TFX.\nThe high-level workflow for building a graph-regularized model using NSL entails the following steps:\nBuild a graph, if one is not available.\nUse the graph and the input example features to augment the training data.\nUse the augmented training data to apply graph regularization to a given model.\nThese steps don\u2019t immediately map onto existing TFX pipeline components. However, TFX supports custom components which allow users to implement custom processing within their TFX pipelines. See this blog post for an introduction to custom components in TFX. So, to create a graph-regularized model in TFX incorporating the above steps, we will make use of additional custom TFX components.\nTo illustrate an example TFX pipeline with NSL, let\u2019s consider the task of sentiment classification on the IMDB dataset. A colab-based tutorial demonstrating the use of NSL for this task with native TensorFlow is available here, which we will use as the basis for our TFX pipeline example.\nGraph Regularization With Custom TFX Components\nTo build a graph-regularized NSL model in TFX for this task, we will define three custom components using the custom Python functions approach. Here is a TFX pipeline schematic for our example using these custom components. For brevity, we have skipped components that typically come after the Trainer component like the Evaluator, Pusher, etc.\nFigure 1: Example TFX pipeline for text classification using graph regularization\nIn this figure, only the custom components (in pink) and the Graph-regularized Trainer component have NSL-related logic. It\u2019s worth noting that the custom components shown here are only illustrative and it may be possible to build a functionally equivalent pipeline in other ways. We now describe each of the custom components in further detail and show code snippets for them.\nIdentifyExamples\nThis custom component assigns a unique ID to each training example that is used to associate each training example with its corresponding neighbors from the graph.\n @component\ndef IdentifyExamples(\n    orig_examples: InputArtifact[Examples],\n    identified_examples: OutputArtifact[Examples],\n    id_feature_name: Parameter[str],\n    component_name: Parameter[str]\n  ) -> None:\n\n  # Compute the input and output URIs.\n  ...\n\n  # For each input split, update the TF.Examples to include a unique ID.\n  with beam.Pipeline() as pipeline:\n    (pipeline\n     | 'ReadExamples' >> beam.io.ReadFromTFRecord(\n         os.path.join(input_dir, '*'),\n         coder=beam.coders.coders.ProtoCoder(tf.train.Example))\n     | 'AddUniqueId' >> beam.Map(make_example_with_unique_id, id_feature_name)\n     | 'WriteIdentifiedExamples' >> beam.io.WriteToTFRecord(\n         file_path_prefix=os.path.join(output_dir, 'data_tfrecord'),\n         coder=beam.coders.coders.ProtoCoder(tf.train.Example),\n         file_name_suffix='.gz'))\n\n  identified_examples.split_names = orig_examples.split_names\n  return\nThe make_example_with_unique_id() function updates a given example to include an additional feature containing a unique ID.\nSynthesizeGraph\nAs mentioned above, in the IMDB dataset, no explicit graph is given as an input. So, we will build one before we can demonstrate graph regularization. For this example, we will use a pre-trained text embedding model to convert raw text in the movie reviews to embeddings, and then use the resulting embeddings to build a graph.\nThe SynthesizeGraph custom component handles graph building for our example and notice that it defines a new Artifact called SynthesizedGraph, which will be the output of this custom component.\n \"\"\"Custom Artifact type\"\"\"\nclass SynthesizedGraph(tfx.types.artifact.Artifact):\n  \"\"\"Output artifact of the SynthesizeGraph component\"\"\"\n  TYPE_NAME = 'SynthesizedGraphPath'\n  PROPERTIES = {\n      'span': standard_artifacts.SPAN_PROPERTY,\n      'split_names': standard_artifacts.SPLIT_NAMES_PROPERTY,\n  }\n\n@component\ndef SynthesizeGraph(\n    identified_examples: InputArtifact[Examples],\n    synthesized_graph: OutputArtifact[SynthesizedGraph],\n    similarity_threshold: Parameter[float],\n    component_name: Parameter[str]\n  ) -> None:\n\n  # Compute the input and output URIs\n  ...\n\n  # We build a graph only based on the 'train' split which includes both\n  # labeled and unlabeled examples.\n  create_embeddings(train_input_examples_uri, output_graph_uri)\n  build_graph(output_graph_uri, similarity_threshold)\n  synthesized_graph.split_names = artifact_utils.encode_split_names(\n      splits=['train'])\n  return\nThe create_embeddings() function involves converting the text in movie reviews to corresponding embeddings using some pre-trained model on TensorFlow Hub. The build_graph() function involves invoking the build_graph() API in NSL.\nGraphAugmentation\nThe purpose of this custom component is to combine the example features (text in the movie reviews) with the graph built from embeddings to produce an augmented training dataset. The resulting training examples will include features from their corresponding neighbors as well.\n@component\ndef GraphAugmentation(\n    identified_examples: InputArtifact[Examples],\n    synthesized_graph: InputArtifact[SynthesizedGraph],\n    augmented_examples: OutputArtifact[Examples],\n    num_neighbors: Parameter[int],\n    component_name: Parameter[str]\n  ) -> None:\n\n  # Compute the input and output URIs\n  ...\n\n  # Separate out the labeled and unlabeled examples from the 'train' split.\n  train_path, unsup_path = split_train_and_unsup(train_input_uri) \n\n  # Augment training data with neighbor features.\n  nsl.tools.pack_nbrs(\n    train_path, unsup_path, graph_path, output_path, add_undirected_edges=True,\n    max_nbrs=num_neighbors\n  )\n\n  # Copy the 'test' examples from input to output without modification.\n  ...\n\n  augmented_examples.split_names = identified_examples.split_names\n  return\nThe split_train_and_unsup() function involves splitting the input Examples into labeled and unlabeled examples and the pack_nbrs() NSL API creates the augmented training dataset.\nGraph-regularized Trainer\nNow that all of our custom components are implemented, the remaining NSL-specific addition to the TFX pipeline is in the Trainer component. Below is a simplified view of the graph-regularized Trainer component.\n  ...\n\n  estimator = tf.estimator.Estimator(\n       model_fn=feed_forward_model_fn, config=run_config, params=HPARAMS)\n  \n  # Create a graph regularization config.\n  graph_reg_config = nsl.configs.make_graph_reg_config(\n      max_neighbors=HPARAMS.num_neighbors,\n      multiplier=HPARAMS.graph_regularization_multiplier,\n      distance_type=HPARAMS.distance_type,\n      sum_over_axis=-1)\n  \n  # Invoke the Graph Regularization Estimator wrapper to incorporate\n  # graph-based regularization for training.\n  graph_nsl_estimator = nsl.estimator.add_graph_regularization(\n      estimator,\n      embedding_fn,\n      optimizer_fn=optimizer_fn,\n      graph_reg_config=graph_reg_config)\n\n ... \nAs you can see, once a base model has been created (in this case a feed-forward neural network), it\u2019s straightforward to convert it to a graph-regularized model by invoking the NSL wrapper API.\nAnd that\u2019s it! We now have all of the missing pieces that are required to build a graph-regularized NSL model in TFX. A colab-based tutorial that demonstrates this example end-to-end in TFX is available here. Feel free to try it and customize it as you want!\nAdversarial Learning\nAs mentioned in the introduction above, another aspect of Neural Structured Learning is adversarial learning where instead of using explicit neighbors from a graph for regularization, implicit neighbors are created dynamically and adversarially to confuse the model. So, regularizing using adversarial examples is an effective way to improve a model\u2019s robustness. Adversarial learning using NSL can be easily integrated into a TFX pipeline. It does not require any custom components and only the trainer component needs to be updated to invoke the adversarial regularization wrapper API in NSL.\nSummary\nWe have demonstrated how to build a graph-regularized model with NSL in TFX using custom components. It\u2019s certainly possible to build graphs in other ways as well as structure the overall pipeline differently. We hope that this example provides a basis for your own NSL workflows.\nAdditional Links\nFor more information on NSL, check out the following resources:\nNSL in TFX colab tutorial\nNSL Website\nNSL GitHub\nMore NSL tutorials and videos\nAcknowledgements:\nWe\u2019d like to thank the Neural Structured Learning and TFX teams at Google as well as Aur\u00e9lien Geron for their support and contributions.",
    "link": "https://blog.tensorflow.org/2020/10/neural-structured-learning-in-tfx.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-fEOB4_5FLno/X33ZXSSWVsI/AAAAAAAADpM/3OiF1o0fsp0gFffsryoV0UC4fFksyWU2gCLcBGAsYHQ/s0/NSL%2Bin%2BTFX%2Bwith%2Bcustom%2Bcomponents.jpg"
    ],
    "time": "2023/12/01 00:58:43"
  },
  {
    "title": "Towards ML Engineering: A Brief History Of TensorFlow Extended (TFX)",
    "content": "Posted by Konstantinos (Gus) Katsiapis on behalf of the TFX Team\nTable of Contents\nAbstract\nWhere We Are Coming From\nSibyl (2007 - 2020)\nTFX (2017 - ?)\nLessons From Our 10+ Year Journey Of ML Platform Evolution\nWhat Remains The Same And Why\nWhat Is Different And Why\nWhere We Are Going\nDrive Interoperability And Standards\nIncrease Automation\nImprove ML Understanding\nUphold High Standards And Best Practices\nImprove Tooling\nA Joint Journey\nThe TFX Team\nThe TFX Team \u2026 Extended\nAbstract\nSoftware Engineering, as a discipline, has matured over the past 5+ decades. The modern world heavily depends on it, so the increased maturity of Software Engineering was an eventuality. Practices like testing and reliable technologies help make Software Engineering reliable enough to build industries upon. Meanwhile, Machine Learning (ML) has also grown over the past 2+ decades. ML is used more and more for research, experimentation and production workloads. ML now commonly powers widely-used products integral to our lives.\nBut ML Engineering, as a discipline, has not widely matured as much as its Software Engineering ancestor. Can we take what we have learned and help the nascent field of applied ML evolve into ML Engineering the way Programming evolved into Software Engineering?\nIn this article we will give a whirlwind tour of Sibyl and TensorFlow Extended (TFX), two successive end-to-end (E2E) ML platforms at Alphabet. We will share the lessons learned from over a decade of applied ML built on these platforms, explain both their similarities and their differences, and expand on the shifts (both mental and technical) that helped us on our journey. In addition, we will highlight some of the capabilities of TFX that help realize several aspects of ML Engineering. We argue that in order to unlock the gains ML can bring, organizations should advance the maturity of their ML teams by investing in robust ML infrastructure and promoting ML Engineering education. We also recommend that before focusing on cutting-edge ML modeling techniques, product leaders should invest more time in adopting interoperable ML platforms for their organizations. In closing, we will also share a glimpse into the future of TFX.\nWhere We Are Coming From\nApplied ML has been an integral part of Google products and services over the last decade, and is becoming more so over time. We discovered early on from our endeavors to apply ML in production that while ML algorithms are important, they are usually insufficient in realizing the successful application of ML in a product. In particular, E2E ML platforms, which help with all aspects of the ML lifecycle, are usually needed to both accelerate ML adoption and make its use durable and sustainable.\nSibyl (2007 - 2020)\nE2E ML platforms are not a new thing at Google. Sibyl, founded in 2007, was a platform that enabled massive-scale ML, catered to production use. Sibyl offered a decent amount of modeling flexibility on top of \u201cwide\u201d models (linear, logistic, poisson regression and later factorization machines) coupled with non-linear transformations and customizable loss functions and regularization. Importantly, Sibyl also offered tools for several aspects of the ML workflow including Data Ingestion, Data Analysis and Validation, Training (of course), Model Analysis, and Training-Serving Skew Detection. All these were packaged as a single integrated product that allowed for iterative experimentation. This holistic product offering, coupled with the Sibyl team\u2019s user focus, rendered Sibyl to, once upon a time, be one of the most widely used E2E ML platforms at Google. Sibyl has since been decommissioned. It was in production for ~14 years, and the vast majority of its workloads migrated to TFX.\nTFX (2017 - ?)\nWhile several of us were still working on Sibyl, a notable revolution was happening in the ML algorithms fields with the popularization of Deep Learning (DL). In 2015, Google publicly released TensorFlow (which was itself a successor to a previous system called DistBelief). Since its inception, TensorFlow supported a variety of applications with a focus on DL training and inference. Its flexible programming model allowed it to be used for a lot more than DL and its popularity in both research and production positioned it as the lingua franca for authoring ML algorithms. While TensorFlow offered flexibility, it lacked a complete end-to-end production system. On the other hand, Sibyl had robust end-to-end capabilities, but lacked flexibility. It became apparent that we needed an E2E ML platform for TensorFlow in order to accelerate ML at Google; in 2017, nearly a decade after the birth of Sibyl, we launched TFX within Google. TFX is now the most widely used, general purpose E2E ML platform at Alphabet, including Google.\nIn the 3 years since its launch, TFX has enabled Alphabet to realize what might be described as \u201cindustrial-scale\u201d ML: TFX is used by thousands of users within Alphabet, and it powers hundreds of popular Alphabet products, including Cloud AI services on Google Cloud Platform (GCP). On any given day there are thousands of TFX pipelines running, which are processing exabytes of data and producing tens of thousands of models, which in turn are performing hundreds of millions of inferences per second. TFX\u2019s widespread adoption helps Alphabet realize the flow of research into production and enables very diverse use cases for both direct and indirect TFX users. This widespread adoption also enables teams to focus on model development rather than ML platform development, allowing ML to be more easily used in novel product areas, and creating a virtuous cycle of ML platform evolution from ML applications.\nBased on our internal success, and the expectation that equivalents of ML engineering will be needed by organizations and individuals everywhere in the world, we decided to publicly describe the design and initial deployment of TFX within Google and to, step by step, make more of our learnings and our technology publicly available (including open source), while we continue building more of each. We were able to accomplish this in part because, like Sibyl, TFX built upon robust infrastructural dependencies. For example, Sibyl made heavy use of MapReduce and its successor Flume for its distributed data processing, and now TFX heavily uses their portable successor, Apache Beam, for the same.\nFollowing in TensorFlow\u2019s footsteps, the public TFX offering was released in early 2019 and widely adopted in under a year across environments including on-premises and GCP with Cloud AI Platform Pipelines. Some of our partners have also publicly shared their use cases powered by TFX, including how it radically improved their applied ML velocity.\nLessons From Our 10+ Year Journey Of ML Platform Evolution\nThough the journey of ML Platform(s) evolution at Google has been a long and exciting one, we expect that the majority of excitement is yet to come! To that end, we want to share a summary of our learnings, some of which were more painfully gained than others. The learnings fall into two categories, namely what remained the same as part of the evolution, but also what changed, and why! We present the learnings in the context of two successive platforms, Sibyl and TFX, though we believe them to be widely applicable.\nWhat Remains The Same And Why\nThe areas discussed in this section capture a few examples of things that seem enduring and pass the test of time. As such, we expect these to also remain applicable in the future, across different incarnations of ML platforms and frameworks. We look at these from both an applied ML perspective and an infrastructure perspective.\nApplied ML\nThe Rules Of Machine Learning\nSuccessfully applying ML to a product is very much a discipline. It involves a steep learning curve and necessitates some mental model shifts (or perhaps augmentations). To make this challenging task easier, we have publicly shared The Rules of Machine Learning. These are rules that represent learnings from iteratively applying ML to a lot of products at Google. Notably, the adoption of ML in Google products illustrates a common evolution:\nStart with simple rules and heuristics, and generate data to learn from; this journey usually starts from the serving side.\nMove to simple ML (i.e., simple models) and realize large gains; this is usually the entry point for introduction of ML pipelines.\nMove to ML with more features and more advanced models to realize decent gains.\nMove to state-of-the-art ML, manage refinement and complexity (for solutions to the problems that are worth it), and realize small gains.\nApply the above launch-and-iterate cycle to more aspects of products and to solve more problems, bearing in mind return on investment (and diminishing returns).\nWe have found The Rules of Machine Learning to be steadfast across platforms and time and we hope they end up being as valuable to others as they have been to us and our users. In particular, we believe that following the rules will help others be better at the discipline of ML engineering, including helping them avoid the mistakes that we and our users have made in the past. TFX is an attempt to codify these rules, quite literally, in code. We hope to benefit ourselves but also accelerate ML, done well, for the entire industry.\nThe Discipline Of ML Engineering\nIn developing The Rules of Machine Learning, we realized that the discipline for building robust systems where the core logic is produced by complex processes involving both code and data requires additional scrutiny beyond that which software engineering provides. As such, we define ML Engineering as a superset of the discipline of software engineering designed to handle the unique complexities of the practical application of ML.\nAttempting to summarize the totality of the discipline of ML engineering would be somewhat difficult, if not impossible, especially given how our understanding of it is still limited, and the discipline itself continues to evolve. We do take solace in the following though:\nThe limited understanding we do have seems to be enduring across platforms and time.\nAnalogy can be a powerful tool, so several aspects of the better understood discipline of software engineering have helped us draw parallels of how ML engineering could evolve from ML programming, much like how software engineering evolved from programming.\nAn early realization we had was the following: artifacts are first class citizens in ML, on par with the processes that produce and consume them.\nThis realization affected the implementation and evolution of Sibyl; it was entrenched in TFX by the time we publicly wrote about it and was ultimately generalized and formalized in ML Metadata, now powering TFX.\nBelow we present fundamental elements of ML engineering, some examples of ML artifacts and their first class citizenship, and make an attempt to draw analogies with software engineering where possible.\nData\nSimilarly to how code is at the heart of software, data is at the heart of ML. Data management represents serious challenges in production ML. Perhaps the simplest analogy would be to think about what constitutes a unit test for data. Unit tests verify expectations on how code should behave, by testing the contracts of the pertinent code and instilling trustworthiness in said contracts. Similarly, setting explicit expectations on the form of the data (including its schema, invariants and value distributions), and checking that the data agrees with implicit expectations embedded in the training code can, more so together, make the data trustworthy enough to train models with. Though unit tests can be exhaustive and verify strong contracts, data contracts are in general a lot weaker even if they are necessary. Though unit tests can be exhaustively consumed and verified by humans, data can usually be meaningful to humans only in summarized fashion.\nJust as code repositories and version control are pillars for managing code evolution in software engineering, systems for managing data evolution and understanding are pillars of ML engineering.\nTFX\u2019s ExampleGen, StatisticsGen, SchemaGen and ExampleValidator components help one treat data as first class citizens, by enabling data management, analysis and validation in (continuous) ML pipelines.\nModels\nSimilarly to how a software engineer produces code that is compiled into programs, an ML engineer produces data and code which is \u201ccompiled\u201d into ML programs, more commonly known as models. These two kinds of programs are however very different in nature. Though programs that come out of software usually have strong contracts, models have much weaker contracts. These weak contracts are usually statistical in nature and as such only verifiable in some summarized form (such as a model having sufficient accuracy on a subset of labeled data). This is not at all surprising since models are the product of code and data, and the latter itself doesn\u2019t have strong contracts and is also only digestible in summarized form.\nJust as code and data evolve over time, models also evolve over time. However, model evolution is more complicated than the evolution of its constituent code and data. For example, high test coverage (with fuzzing) can give good confidence in both the correctness and the correct evolution of a piece of code, but out-of-distribution and counterfactual yet realistic data for model evaluation can be notoriously difficult to produce.\nIn the same way that putting together multiple programs in a system necessitates integration testing which is a pillar of software engineering, putting together code and data necessitates end-to-end model validation and understanding which is a pillar of ML engineering.\nTFX\u2019s Evaluator and InfraValidator components provide validation and understanding of models, treating them as first class citizens of ML engineering.\nMergeable Fragments\nSimilarly to how a software engineer merges together pre-existing libraries (or systems) with their code in order to build useful programs, an ML engineer merges together code fragments, data fragments, analysis fragments and model fragments on a regular basis in order to build useful ML pipelines. A notable difference between software engineering and ML engineering is that even when the code is fixed for the latter, data is usually volatile for it (e.g. new data arrives on a regular basis) and as such the downstream artifacts need to be produced frequently and efficiently. For example, a new version of a model usually needs to be produced if any part of its input data has changed. As such, it is important for ML pipelines to produce artifacts that are mergeable. For example, a summary of statistics from one dataset should be easily mergeable with that of another dataset such that it is easy to summarize the statistics of the union of the two datasets. Similarly, it should be easy to transfer the learnings of one model to another model in general, and the learnings of a previous version of a model to the next version of the same model in particular.\nThere is however a catch, which relates to the previous discussion regarding the equivalents of test coverage for models. Merging new fragments into a model could necessitate creation of novel out-of-distribution and counterfactual evaluation data, contributing to the difficulty of (efficient) model evolution, thus rendering it a lot harder than pure code evolution.\nTFX\u2019s ExampleGen, Transform, Trainer and Tuner components, together with TensorFlow Hub, help one treat artifacts as first class citizens by enabling production and consumption of mergeable fragments in workflows that perform data caching, analyzer caching, warmstarting and transfer learning.\nArtifact Lineage\nDespite all the advanced methodology and tooling that exists for software engineering, the programs and systems that are built invariably need to be debugged. The same holds for ML programs, but debugging them is notoriously harder because non-proximal effects are a lot more prevalent for ML programs due to the plethora of artifacts involved. A model might be inaccurate due to bad artifacts from several sources of error, including flaws in the code, the learning algorithm, the training data, the serving path, or the serving data, to name a few. Much like how stack traces are invaluable for identifying root causes of defects in software programs, the lineage of all artifacts produced and consumed by an ML pipeline is invaluable for identifying root causes of defects in ML models. Additionally, by knowing which downstream artifacts were produced from a problematic artifact, we can identify all impacted systems and users and take mitigating actions.\nTFX\u2019s use of ML Metadata (MLMD) helps treat artifacts as first class citizens. MLMD enables advanced cataloging and querying of metadata and lineage associated with artifacts which can together increase the confidence of sharing artifacts even outside the boundaries of a pipeline. MLMD also helps with advanced debugging and, when coupled with the underlying data storage layer, forms the foundation of TFX\u2019s ML compliance mechanisms.\nContinuous Learning And Unlearning\nML production pipelines operate in a dynamic environment:\nNew data can arrive continuously.\nThe modeling code can change, particularly in the early stages of model development.\nThe surrounding infrastructure can change, e.g., a new version of some underlying (ML) library.\nWhen changes happen, a pipeline needs to react, often by rerunning its steps in the new environment. This dynamicity increases the importance of provenance tracking in order to facilitate debugging and root-cause analysis. As a simple example, to debug a model failure, it is necessary to know not only which data was used to train the model, but also the versions of the modeling code and any surrounding infrastructure.\nML pipelines must also support low-friction mechanisms to handle these changes. Consider for example the arrival of new data, which necessitates retraining the model. This is a natural requirement in rapidly changing environments, like recommender systems or adversarial systems. Requiring the user to manually retrain the model can be unrealistic, given that the data can arrive at a regular and frequent rate. Instead, we can employ automation by way of \u201ccontinuous training\u201d, where the pipeline detects the presence of new data and automatically schedules the generation of updated models. In turn, this functionality requires automatically: orchestrating work based on the presence of artifacts (including data), recovering from intermittent failures, and catching up to real-time when recovering. It is common for ML pipelines to run for years ingesting code and data, continuously producing models that make predictions that inform decisions.\nAnother example of a low-friction mechanism is support for \u201cbackfilling\u201d an ML pipeline. In this case, the user might need to rerun the pipeline on existing artifacts but using updated versions of the components, such as rerunning the trainer on existing data using a new version of the modeling code/library. Another use of backfilling is rerunning the pipeline with new versions of existing data, say, to fix an error in the data. These backfills are orthogonal to continuous training and can be used together. For instance, the user can manually trigger a rerun of the trainer, and the generated model artifact can then automatically trigger model evaluation and validation.\nTFX was built from the ground up in a way that enables continuous learning (and unlearning) which fundamentally shaped its design. At the same time, these advanced capabilities also allow it to be used in a \u201cone-shot\u201d, discontinuous, fashion. In fact, within Alphabet, both modes of deployment are widely used. Moreover, TFX also supports different types of backfill operations to enable fine-grained interventions during normal pipeline execution.\nEven though the public TFX offering doesn\u2019t yet offer continuous ML pipelines, we are actively working on making our existing technology portable so that it can be made publicly available (e.g RFC).\nInfrastructure\nBuilding On The Shoulders Of Giants\nRealizing ambitious goals necessitates building on top of solid foundations, collaborating with others and leveraging each other's work. TFX reuses many of Sibyl's system designs, hardened over a decade of Sibyl\u2019s production ML experience. Additionally, TFX incorporates new technologies in areas where robust standards emerged:\nSimilarly to how Sibyl built its algorithms and workflows on top of MapReduce, TFX leverages both TensorFlow and Apache Beam for its distributed training and data processing workflows.\nSimilarly to how Sibyl was columnar, TFX adopted Apache Arrow as the columnar in-memory representation for its compute-intensive libraries.\nTaking dependencies where robust standards have emerged has allowed TFX and its users to achieve seamless performance and scalability. It also enables TFX to focus its energy on building the deltas of what is needed for applied ML, as opposed to re-implementing difficult-to-get-right technology. Some of our dependencies, like Kubeflow Pipelines or Apache Airflow, are selected by TFX\u2019s users themselves when the value / features they get from them outweigh the costs that the additional dependencies entail.\nTaking dependencies unfortunately incurs costs. We have found that taking dependencies requires effort that is super-linear to the number of dependencies. Said costs are often absorbed by us and our sister teams but can (and sometimes do) leak to our users, usually in the form of conflicting (version) dependencies or incompatibilities between environments and dependencies.\nInteroperability And Positive Externalities\nML platforms do not operate in a vacuum. They instead operate within the context of a bigger system or infrastructure, connecting to data producing sources upstream and model consuming sinks downstream, which in turn frequently produce the data that feeds the ML platform, thereby closing the loop. Strong adoption of a platform usually necessitates interoperability with other important technologies in its environment.\nSimilarly to how Sibyl interoperated with Google\u2019s Ads technology stack for data ingestion and model serving, TFX offers a plethora of connectors for data ingestion and allows serving the produced model in multiple deployment environments and devices.\nSimilarly to how Sibyl interoperated with Google\u2019s compute stack, TFX leverages Apache Beam to execute on Apache Flink and Apache Spark clusters as well as serverless offerings like Google Cloud Dataflow.\nTFX built an orchestration abstraction on top of MLMD and provides orchestration options on top of Apache Airflow, Apache Beam, Kubeflow Pipelines as well as the primitives to integrate with one\u2019s custom orchestrator. MLMD itself works with several relational databases like SQLite and MySQL.\nInteroperability necessitates some amount of abstraction and standardization and usually enables sum-greater-than-its-parts effects. TFX is both a beneficiary and a benefactor of the positive externalities created by said interoperability, both within and outside of Alphabet. TFX\u2019s users are also beneficiaries of the interoperability as they can more easily deploy and use TFX on top of their existing installed base.\nInteroperability also comes with costs. The combination of multiple technology stacks can lead to an exponential number of distinct deployment configurations. While we test some of the distinct deployment configurations end-to-end and at-scale, like for example TFX on GCP, we have neither the expertise nor the resources to do so for the combinatorial explosion of all possible deployment options. We thus encourage the community to work with us on the deployment configurations that are most useful for them.\nWhat Is Different And Why\nThe areas discussed in this section capture a few examples of things that needed to change in order for our ML platform to adapt to a new reality and as such remain useful and impactful.\nEnvironment And Device Portability\nSibyl was a massive scale ML platform designed to be deployed on Google\u2019s large-scale cluster, namely Borg. This made sense as applied ML at Google was, originally, primarily used in products that were widely used. As ML expertise grew across the world, and ML could be applied to more use cases (large and small) across environments both within and outside of Google, the need for portability gradually but surely became a hard constraint.\nWhile Sibyl ran only on Google\u2019s datacenters, TFX runs on laptops, workstations, servers, datacenters, and public Clouds. In particular, when TFX runs on Google\u2019s Cloud, it leverages automation and optimizations offered by GCP Services, enabled by Google\u2019s unique infrastructure.\nWhile Sibyl ran only on CPUs, TFX leverages TensorFlow to run on different kinds of hardware including CPUs, GPUs and Google\u2019s TPUs.\nWhile Sibyl\u2019s models ran on servers, TFX leverages TensorFlow to produce models that run on laptops, workstations, and servers via TensorFlow Serving and Apache Beam, on mobile and IoT devices via TensorFlow Lite, and on browsers via TensorFlow JS.\nTFX\u2019s portability enabled it to be used in a very diverse set of environments and devices, in order to solve problems from small scale to massive scale.\nUnfortunately, portability comes with costs. We have found that maintaining a portable core with environment-specific and device-specific specialization requires effort that is super-linear to the number of environments / devices. Said costs are however largely absorbed by us and our sister teams and as such are frequently not visible to our users.\nModularity And Layering\nEven though Sibyl\u2019s offering as an integrated product was immensely valuable, its structure and interface were somewhat monolithic, limiting it to a specific set of \u201cdirect\u201d users who would have to adopt it wholesale. In contrast, TFX evolved to be a modular and layered architecture, and became more so over time as partnerships with other teams and products grew. Notable layers (with examples) in TFX include:\nLayer Examples\nML Services\nCloud AutoML\nCloud Recommendations AI\nCloud AI Platform\nCloud Dataflow\nCloud BigQuery\nPipelines\n(of composable Components)\nTensorFlow Extended (TFX)\nBinaries\nTensorFlow Serving (TFS)\nLibraries\nTensorFlow Data Validation (TFDV)\nTensorFlow Transform (TFT)\nTensorFlow Hub (TFH)\nTensorFlow Model Analysis (TFMA)\nTFX Basic Shared Libraries (TFX_BSL)\nML Metadata (MLMD)\nTFX\u2019s layered architecture enables it to be used by a very diverse set of users whether that\u2019s piecemeal via its libraries, wholesale via its pipelines (with or without the pertinent services), or in a fashion that\u2019s completely oblivious to the end users (e.g. by them using ML services which TFX powers under the hood)!\nUnfortunately, layering comes with costs. We have found that maintaining multiple publicly accessible layers of our product requires effort that is roughly linear to the number of layers. Said costs occasionally leak to our users in the form of confusion regarding what layer makes the most sense for them to use.\nMulti-faceted Flexibility\nEven though Sibyl was more flexible in terms of modeling capabilities compared to available alternatives at the time, aspects of its flexibility across several parts of the ML workflow fell short of Google\u2019s needs for accelerating ML for novel use cases, which led to the development of TFX.\nWhile Sibyl only offered specific kinds of data analysis, TFX\u2019s StatisticGen component offers more built-in capabilities and the ability to realize custom analyses, via TensorFlow Data Validation.\nWhile Sibyl only offered transformations that were pure composable mappers, TFX\u2019s Transform component offers more mappers, custom mappers, more analyzers, custom analyzers, as well as arbitrarily composed (custom) mappers and (custom) analyzers, via TensorFlow Transform.\nWhile Sibyl only offered \u201cwide\u201d models, TFX\u2019s Trainer component offers any model that can be realized on top of TensorFlow, including models that can be shared and can transfer-learn, via TensorFlow Hub.\nWhile Sibyl only offered automatic feature crossing (a.k.a. feature conjunctions) on top of \u201cwide\u201d models, TFX\u2019s Tuner component allows for arbitrary hyper parameter optimization based on state of the art algorithms.\nWhile Sibyl only offered specific kinds of model analysis, TFX\u2019s Evaluator component offers more built-in metrics, custom metrics, confidence intervals and fairness indicators, via TensorFlow Model Analysis.\nWhile Sibyl\u2019s pipeline topology was fixed (albeit somewhat customizable), TFX\u2019s SDK allows one to create custom (optionally containerized) components and use them together with standard components in a flexible and fully customizable pipeline topology.\nThe increase of flexibility in all these dimensions enabled improved experimentation, wider reach, more use cases, as well as accelerated flow from research to production.\nFlexibility does not come without costs. A more flexible system is one that is harder to get right in the first place as well as harder for us to maintain and to evolve as developers of the ML platform. Users may also have to manage increased complexity as they take advantage of this flexibility. Furthermore, we might not be able to offer as strong of a support story on top of an ML platform that is Turing complete.\nWhere We Are Going\nArmed with the knowledge of the past, we present a glimpse of what we plan for the future of TFX, as of 2020. We will continue our work on enabling ML Engineering in order to democratize applied ML, and help everyone practice responsible AI and apply it in a fashion that upholds Google\u2019s AI Principles.\nDrive Interoperability And Standards\nIn order to meet the demand for the burgeoning variety of ML solutions, we will continue to increase our technology\u2019s interoperability. Our work on interoperability and standards as well as open-sourcing more of our technology, reflects our principle to \u201cbe socially beneficial\u201d as well as to \u201cbe made available for uses that accord with these principles\u201d by making it easier for everyone to follow these practices. As part of this mission, we will empower the industry to build advanced ML systems by open-sourcing more of our technology, and by standardizing ML artifacts and metadata. Some select examples of this work include:\nTFX Standardized Inputs.\nAdvanced TFX DSL semantics, Data Model and IR.\nStandardization of ML artifacts and metadata.\nStandardization of distributed workloads on heterogeneous runtime environments.\nInference on distributed and streaming models.\nImprovements to interoperability with mobile and edge ML deployments.\nImprovements for ML framework interoperability and artifact sharing.\nIncrease Automation\nAutomation is the backbone of reliable production systems, and TFX is heavily invested in improving and expanding its use of automation. Our work in increased automation reflects our principles of helping make ML deployments \u201cbe built and tested for safety\u201d and \u201cavoid creating or reinforcing unfair bias\u201d. Some upcoming efforts include a TFX Pipeline testing framework, automated model improvement in the TFX Tuner, auto-detecting surprising model behavior on multidimensional slices, facilitating automatic production of Model Cards and improving our training-serving skew detection capabilities. TFX on GCP will also continue driving requirements for new (and will better make use of existing) advanced automation features of pertinent services.\nImprove ML Understanding\nML understanding is an important aspect of deploying production ML, and TFX is well positioned to provide significant gains in this field. Our work on improving ML understanding reflects our principles to help \u201cavoid creating or reinforcing unfair bias\u201d and help make ML deployments \u201cbe accountable to people\u201d. Critical to understanding is to be able to track the lineage of artifacts used to produce a model, an area TFX will continue to invest in. Improvements to TFX technologies like struct2tensor will further enable training, serving, and analyzing models on structured data, thus allowing reasoning about models closer to the original data semantics. We also plan to utilize TFX as a vehicle to expand support for fairness evaluation, remediation, and documentation.\nUphold High Standards And Best Practices\nAs a vehicle for amplification of ML technology, TFX must continue to \u201cuphold high standards of scientific excellence\u201d and promote best practices. The team will continue publishing scientific papers and conducting public outreach via our existing channels, as well as offer educational courses in partnership with established institutions. We will also improve trust in our model analysis tools using integrated uncertainty measures by, for example, enabling scalable computation of confidence intervals for model metrics, and we will improve our training-serving skew detection capabilities. It\u2019s also critical for research and production to be able to have reproducible ML artifacts, enabled by our work in precise provenance tracking for auditing and reproducing models. Also key is reproducibility of measurements, driven by efforts like NitroML, which will provide tooling for benchmarking AutoML pipelines.\nGiven that several of the areas where we expand our technology are new to us, we will make an effort to distinguish the battle-tested from the experimental aspects of our technology, in order to enable our users to confidently choose the set of capabilities that meet their desires and needs.\nImprove Tooling\nDespite TFX providing tools for aspects of ML engineering and several phases of the ML lifecycle, we believe this is still a nascent area. While improving tooling is a natural fit for TFX, it also reflects our principle of helping ML deployments \u201cbe made available for uses that accord with these principles\u201d, \u201csupporting scientific excellence,\u201d and being \u201cbuilt and tested for safety\u201d .\nOne area of improvement is applying ML to the data itself, be it through sensing anomalies or finding patterns in data or enriching data with predictions from ML models. Making it easy to enrich large volumes of data (especially critical streaming data used for low-latency, high volume actions) has always been a challenge. Bringing TFX capabilities into data processing frameworks is our first step here. We have already made it possible to enrich streaming events with labels or make predictions in Apache Beam and, by extension, Cloud Dataflow. We plan to follow this work by leveraging pre-built models (served out of Cloud AI Pipelines and TensorFlow Serving) to make adding a new field in a distributed dataset representing predictions from streams of models trivially easy.\nFurthermore, while there are many tools for detecting, discovering, and auditing ML workflows, there is still a need for automated (or assisted) mitigation of discovered issues, and we will invest in this area. For example, proactively predicting which pipeline runs won\u2019t result in better models based on the currently-executing pipeline, perhaps even before training, can significantly reduce time and resources spent on creating poor models.\nA Joint Journey\nBuilding TFX and exploring the fundamentals of ML engineering was the cumulative effort of many people over many years. As we continue to make strides and further develop this field, it\u2019s important we recognize the collaborative effort of those who got us here.\nOf course, it will take many more collaborations to drive the future of this field, and as such, we invite you to join us on this journey \u201cTowards ML Engineering\u201d!\nFor further reference, read here.\nThe TFX Team\nThe TFX project is realized via collaboration of multiple organizations within Google. Different organizations usually focus on different technology and product layers, though there is a lot of overlap on the portable parts of our technology. Overall we consider ourselves a single team and below we present an alphabetically sorted list of current TFX team members who are contributors to the ideation, research, design, implementation, execution, deployment, management, and advocacy (to name a few) aspects of TFX; they continue to inspire, help, teach, and challenge each other to advance our field:\nAbhijit Karmarkar, Adam Wood, Aleksandr Zaks, Alina Shinkarsky, Neoklis Polyzotis, Amy Jang, Amy McDonald Sandjideh, Amy Skerry-Ryan, Andrew Audibert, Andrew Brown, Andy Lou, Anh Tuan Nguyen, Anirudh Sriram, Anna Ukhanova, Anusha Ramesh, Archana Jain, Arun Venkatesan, Ashley Oldacre, Baishun Wu, Ben Mathes, Billy Lamberta, Chandni Shah, Chansoo Lee, Chao Xie, Charles Chen, Chi Chen, Chloe Chao, Christer Leusner, Christina Greer, Christina Sorokin, Chuan Yu Foo, CK Luk, Connie Huang, Daisy Wong, David Smalling, David Zats, Dayeong Lee, Dhruvesh Talati, Doojin Park, Elias Moradi, Emily Caveness, Eric Johnson, Evan Rosen, Florian Feldhaus, Gal Oshri, Gautam Vasudevan, Gene Huang, Goutham Bhat, Guanxin Qiao, Gus Katsiapis, Gus Martins, Haiming Bao, Huanming Fang, Hui Miao, Hyeonji Lee, Ian Nappier, Ihor Indyk, Irene Giannoumis, Jae Chung, Jan Pfeifer, Jarek Wilkiewicz, Jason Mayes, Jay Shi, Jiayi Zhao, Jingyu Shao, Jiri Simsa, Jiyong Jung, Joana Carrasqueira, Jocelyn Becker, Joe Liedtke, Jongbin Park, Jordan Grimstad, Josh Gordon, Josh Yellin, Jungshik Jang, Juram Park, Justin Hong, Karmel Allison, Kemal El Moujahid, Kenneth Yang, Khanh LeViet, Kostik Shtoyk, Lance Strait, Laurence Moroney, Li Lao, Liam Crawford, Magnus Hyttsten, Makoto Uchida, Manasi Joshi, Mani Varadarajan, Marcus Chang, Mark Daoust, Martin Wicke, Megha Malpani, Mehadi Hassen, Melissa Tang, Mia Roh, Mig Gerard, Mike Dreves, Mike Liang, Mingming Liu, Mingsheng Hong, Mitch Trott, Muyang Yu, Naveen Kumar, Ning Niu, Noah Hadfield-Menell, No\u00e9 Lutz, Nomi Felidae, Olga Wichrowska, Paige Bailey, Paul Suganthan, Pavel Dournov, Pedram Pejman, Peter Brandt, Priya Gupta, Quentin de Laroussilhe, Rachel Lim, Rajagopal Ananthanarayanan, Rene van de Veerdonk, Robert Crowe, Romina Datta, Ron Yang, Rose Liu, Ruoyu Liu, Sagi Perel, Sai Ganesh Bandiatmakuri, Sandeep Gupta, Sanjana Woonna, Sanjay Kumar Chotakur, Sarah Sirajuddin, Sheryl Luo, Shivam Jindal, Shohini Ghosh, Sina Chavoshi, Sydney Lin, Tanya Grunina, Thea Lamkin, Tianhao Qiu, Tim Davis, Tris Warkentin, Varshaa Naganathan, Vilobh Meshram, Volodya Shtenovych, Wei Wei, Wolff Dobson, Woohyun Han, Xiaodan Song, Yash Katariya, Yifan Mai, Yiming Zhang, Yuewei Na, Zhitao Li, Zhuo Peng, Zhuoshu Li, Ziqi Huang, Zoey Sun, Zohar Yahav\nThank you, all!\nThe TFX Team \u2026 Extended\nBeyond the current TFX team members, there have been many collaborators both within and outside of Alphabet whose discussions, technology, as well as direct and indirect contributions, have materially influenced our journey. Below we present an alphabetically sorted list of these collaborators:\nAbdulrahman Salem, Ahmet Altay, Ajay Gopinathan, Alexandre Passos, Alexey Volkov, Anand Iyer, Andrew Bernard, Andrew Pritchard, Chary Aasuri, Chenkai Kuang, Chenyu Zhao, Chiu Yuen Koo, Chris Harris, Chris Olston, Christine Robson, Clemens Mewald, Corinna Cortes, Craig Chambers, Cyril Bortolato, D. Sculley, Daniel Duckworth, Daniel Golovin, David Soergel, Denis Baylor, Derek Murray, Devi Krishna, Ed Chi, Fangwei Li, Farhana Bandukwala, Gal Elidan, Gary Holt, George Roumpos, Glen Anderson, Greg Steuck, Grzegorz Czajkowski, Haakan Younes, Heng-Tze Cheng, Hossein Attar, Hubert Pham, Hussein Mehanna, Irene Cai, James L. Pine, James Pine, James Wu, Jeffrey Hetherly, Jelena Pjesivac-Grbovic, Jeremiah Harmsen, Jessie Zhu, Jiaxiao Zheng, Joe Lee, Jordan Soyke, Josh Cai, Judah Jacobson, Kaan Ege Ozgun, Kenny Song, Kester Tong, Kevin Haas, Kevin Serafini, Kiril Gorovoy, Kostik Steuck, Kristen LeFevre, Kyle Weaver, Kym Hines, Lana Webb, Lichan Hong, Lukasz Lew, Mark Omernick, Martin Zinkevich, Matthieu Monsch, Michel Adar, Michelle Tsai, Mike Gunter, Ming Zhong, Mohamed Hammad, Mona Attariyan, Mustafa Ispir, Neda Mirian, Nicholas Edelman, Noah Fiedel, Panagiotis Voulgaris, Paul Yang, Peter Dolan, Pushkar Joshi, Rajat Monga, Raz Mathias, Reiner Pope, Rezsa Farahani, Robert Bradshaw, Roberto Bayardo, Rohan Khot, Salem Haykal, Sam McVeety, Sammy Leong, Samuel Ieong, Shahar Jamshy, Slaven Bilac, Sol Ma, Stan Jedrus, Steffen Rendle, Steven Hemingray, Steven Ross, Steven Whang, Sudip Roy, Sukriti Ramesh, Susan Shannon, Tal Shaked, Tushar Chandra, Tyler Akidau, Venkat Basker, Vic Liu, Vinu Rajashekhar, Xin Zhang, Yan Zhu, Yaxin Liu, Younghee Kwon, Yury Bychenkov, Zhenyu Tan\nThank you, all!",
    "link": "https://blog.tensorflow.org/2020/09/brief-history-of-tensorflow-extended-tfx.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-Nq1A6ygcOsQ/X4XSyqt-ZpI/AAAAAAAAJ84/tZs_9dRqSRgmkEmvM1KoKm7ODT8avQElACNcBGAsYHQ/s320/tfxlogo3.png"
    ],
    "time": "2023/12/01 00:58:43"
  },
  {
    "title": "Part 2: Fast, scalable and accurate NLP: Why TFX is a perfect match for deploying BERT",
    "content": "Guest author Hannes Hapke, Senior Data Scientist, SAP Concur Labs. Edited by Robert Crowe on behalf of the TFX team\n\nTransformer models and the concepts of transfer learning in Natural Language Processing have opened up new opportunities around tasks like sentiment analysis, entity extractions, and question-answer problems.\n\nBERT models allow data scientists to stand on the shoulders of giants. Pre-trained on large corpora, data scientists can then apply transfer learning using these multi-purpose trained transformer models and achieve state-of-the-art results for their domain-specific problems.\n\nIn part one of our blog post, we discussed why current deployments of BERT models felt too complex and cumbersome and how the deployment can be simplified through libraries and extensions of the TensorFlow ecosystem. If you haven\u2019t checked out the post, we recommend it as a primer for the implementation discussion in this blog post.\n\nAt SAP Concur Labs, we looked at simplifying our BERT deployments and we discovered that the TensorFlow ecosystem provides the perfect tools to achieve simple and concise Transformer deployments. In this blog post, we want to take you on a deep dive of our implementation and how we use components of the TensorFlow ecosystem to achieve scalable, efficient and fast BERT deployments.\nWant to jump ahead to the code?\nIf you would like to jump to the complete example, check out the Colab notebook. It showcases the entire TensorFlow Extended (TFX) pipeline we used to produce a deployable BERT model with the preprocessing steps as part of the model graph. If you want to try out our demo deployment, check out our demo page at SAP ConcurLabs showcasing our sentiment classification project.\nWhy use Tensorflow Transform for Preprocessing?\nBefore we answer this question, let\u2019s take a quick look at how a BERT transformer works and how BERT is currently deployed.\nWhat preprocessing does BERT require?\nTransformers like BERT are initially trained with two main tasks in mind: masked language models and next sentence predictions (NSP). These tasks require an input data structure beyond the raw input text. Therefore, the BERT model requires, besides the tokenized input text, a tensor input_type_ids to distinguish between different sentences. A second tensor input_mask is used to note the relevant tokens within the input_word_ids tensor. This is required because we will expand our input_word_ids tensors with pad tokens to reach the maximum sequence length. That way all input_word_ids tensors will have the same lengths but the transformer can distinguish between relevant tokens (tokens from our input sentence) and irrelevant pads (filler tokens).\nFigure 1: BERT tokenization\nCurrently, with most transformer model deployments, the tokenization and the conversion of the input text is either handled on the client side or on the server side as part of a pre-processing step outside of the actual model prediction.\n\nThis brings a few complexities with it: if the preprocessing happens on the client side then all clients need to be updated if the mapping between tokens and ids changes (e.g., when we want to add a new token). Most deployments with server-side preprocessing use a Flask-based web application to accept the client requests for model predictions, tokenize and convert the input sentence, and then submit the data structures to the deep learning model. Having to maintain two \u201csystems\u201d (one for the preprocessing and one for the actual model inference) is not just cumbersome and error prone, but also makes it difficult to scale.\nFigure 2: Current BERT deployments\nIt would be great if we could get the best of both solutions: easy scalability and simple upgradeability. With TensorFlow Transform (TFT), we can achieve both requirements by building the preprocessing steps as a graph, exporting them together with the deep learning model, and ultimately only deploying one \u201csystem\u201d (our combined deep learning model with the integrated preprocessing functionality). It\u2019s worth pointing out that moving all of BERT into preprocessing is not an option when we want to fine-tune the tf.hub module of BERT for our domain-specific task.\nFigure 3: BERT with TFX\nProcessing Natural Language with tf.text\nIn 2019, the TensorFlow team released a new tensor type: RaggedTensors which allow storing arrays of different lengths in a tensor. The implementation of RaggedTensors became very useful specifically in NLP applications, e.g., when we want to tokenize a 1-D array of sentences into a 2-D RaggedTensor with different array lengths.\n\nBefore tokenization:\n[\n \u201cClara is playing the piano.\u201d\n \u201cMaria likes to play soccer.\u2019\u201d\n \u201cHi Tom!\u201d\n]\nAfter the tokenization:\n[\n   [[b'clara'], [b'is'], [b'playing'], [b'the'], [b'piano'], [b'.']],\n   [[b'maria'], [b'likes'], [b'to'], [b'play'], [b'soccer'], [b'.']],\n   [[b'hi'], [b'tom'], [b'!']]\n]\nAs we will see in a bit, we use RaggedTensors for our preprocessing pipelines. In late October 2019, the TensorFlow team then released an update to the tf.text module which allows wordpiece tokenization required for the preprocessing of BERT model inputs.\nimport tensorflow_text as text\n \nvocab_file_path = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n \nbert_tokenizer = text.BertTokenizer(\nvocab_lookup_table=vocab_file_path, \ntoken_out_type=tf.int64, \nlower_case=do_lower_case) \nTFText provides a comprehensive tokenizer specific for the wordpiece tokenization (BertTokenizer) required by the BERT model. The tokenizer provides the tokenization results as strings (tf.string) or already converted to word_ids (tf.int32).\n\nNOTE: The tf.text version needs to match the imported TensorFlow version. If you use TensorFlow 2.2.x, you will need to install TensorFlow Text version 2.2.x, not 2.1.x or 2.0.x.\nHow can we preprocess text with TensorFlow Transform?\nEarlier, we discussed that we need to convert any input text to our Transformer model into the required data structure of input_word_ids, input_mask, and input_type_ids. We can perform the conversion with TensorFlow Transform. Let\u2019s have a closer look.\n\nFor our example model, we want to classify the sentiment of IMDB reviews using the BERT model.\n    \u2018This is the best movie I have ever seen ...\u2019       -> 1\n \u2018Probably the worst movie produced in 2019 ...\u2019     -> 0\n \u2018Tom Hank\\\u2019s performance turns this movie into ...\u2019 -> ?\nThat means that we\u2019ll input only one sentence with every prediction. In practice, that means that all submitted tokens are relevant for the prediction (noted by a vector of ones) and all tokens are part of sentence A (noted by a vector of zeros). We won\u2019t submit any sentence B in our classification case.\n\nIf you want to use a BERT model for other tasks, e.g., predicting the similarity of two sentences, entity extraction or question-answer tasks, you would have to adjust the preprocessing step.\n\nSince we want to export the preprocessing steps as a graph, we need to use TensorFlow ops for all preprocessing steps exclusively. Due to this requirement, we can\u2019t reuse functions of Python\u2019s standard library which are implemented in CPython.\n\nThe BertTokenizer, provided by TFText, handles the preprocessing of the incoming raw text data. There is no need for lower casing your strings (if you use the uncased BERT model) or removing unsupported characters. The tokenizer from the TFText library requires a table of the support tokens as input. The tokens can be provided as TensorFlow LookupTable, or simply as a file path to a vocabulary file. The BERT model from TFHub provides such a file and we can determine the file path with\nimport tensorflow_hub as hub\n \nBERT_TFHUB_URL = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\"\nbert_layer = hub.KerasLayer(handle=BERT_TFHUB_URL, trainable=True)\nvocab_file_path = \n    bert_layer.resolved_object.vocab_file.asset_path.numpy()\nSimilarly, we can determine if the loaded BERT model is case-sensitive or not.\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\nWe can now pass the two arguments to our TFText BertTokenizer and specify the data type of our tokens. Since we are passing the tokenized string to the BERT model, we need to provide the tokens as token indices (provided as int64 integers)\nbert_tokenizer = text.BertTokenizer(\nvocab_lookup_table=vocab_file_path, \ntoken_out_type=tf.int64, \nlower_case=do_lower_case\n)\nAfter instantiating the BertTokenizer, we can perform the tokenizations with the tokenize method.\n tokens = bert_tokenizer.tokenize(text)\nOnce the sentence is tokenized into token ids, we will need to prepend the start and append a separation token.\n CLS_ID = tf.constant(101, dtype=tf.int64)\n    SEP_ID = tf.constant(102, dtype=tf.int64)\n    start_tokens = tf.fill([tf.shape(text)[0], 1], CLS_ID)\n    end_tokens = tf.fill([tf.shape(text)[0], 1], SEP_ID)\n    tokens = tokens[:, :sequence_length - 2]\n    tokens = tf.concat([start_tokens, tokens, end_tokens], axis=1)\nAt this point, our token tensors are still ragged tensors with different lengths. TensorFlow Transform expects all tensors to have the same length, therefore we will be padding the truncating the tensors to a maximum length (MAX_SEQ_LEN) and pad shorter tensors with a defined pad token.\nPAD_ID = tf.constant(0, dtype=tf.int64)\ntokens = tokens.to_tensor(default_value=PAD_ID)\n      padding = sequence_length - tf.shape(tokens)[1]\n      tokens = tf.pad(tokens, \n     [[0, 0], [0, padding]], \n     constant_values=PAD_ID)\nThe last step provides us with constant length token vectors which are the final step of the major preprocessing steps. Based on the token vectors, we can create the two required, additional data structures, input_mask, and input_type_ids.\n\nIn the case of the input_mask, we want to note all relevant tokens, basically all tokens besides the pad token. Since the pad token has the value zero and all ids are greater or equal zero, we can define the input_mask with the following ops.\ninput_word_ids = tokenize_text(text)\n        input_mask = tf.cast(input_word_ids > 0, tf.int64)\n        input_mask = tf.reshape(input_mask, [-1, MAX_SEQ_LEN])\nTo determine the input_type_ids is even simpler in our case. Since we are only submitting one sentence, the type ids are all zero in our classification example.\ninput_type_ids = tf.zeros_like(input_mask)\nTo complete the preprocessing setup, we will wrap all steps in the preprocessing_fn function which is required by TensorFlow Transform.\ndef preprocessing_fn(inputs):\n    \n    def tokenize_text(text, sequence_length=MAX_SEQ_LEN):\n   ...\n        return tf.reshape(tokens, [-1, sequence_length])\n \n    def preprocess_bert_input(text, segment_id=0):\n        input_word_ids = tokenize_text(text)\n        ...        \n        return (\n            input_word_ids, \n            input_mask,\n            input_type_ids\n        )\n    ...\n \n    input_word_ids, input_mask, input_type_ids = \\\n        preprocess_bert_input(_fill_in_missing(inputs['text']))\n \n    return {\n        'input_word_ids': input_word_ids,\n        'input_mask': input_mask,\n        'input_type_ids': input_type_ids,\n        'label': inputs['label']\n    }\nTrain the Classification Model\nThe latest updates of TFX allow the use of native Keras models. In the example code below, we define our classification model. The model takes advantage of the pretrained BERT model and KerasLayer provided by TFHub. To avoid any misalignment between the transform step and the model training, we are creating the input layers dynamically based on the feature specification provided by the transformation step.\n feature_spec = tf_transform_output.transformed_feature_spec() \n    feature_spec.pop(_LABEL_KEY)\n \n    inputs = {\nkey: tf.keras.layers.Input(\nshape=(max_seq_length), \nname=key, \ndtype=tf.int32)\n            for key in feature_spec.keys()}\nWe need to cast the variables since TensorFlow Transform can only output variables as one of the types: tf.string, tf.int64 or tf.float32 (tf.int64 in our case). However, the BERT model from TensorFlow Hub used in our Keras model above expects tf.int32 inputs. So, in order to align the two TensorFlow components, we need to cast the inputs in the input functions or in the model graph before passing them to the instantiated BERT layer.\n input_word_ids = tf.cast(inputs[\"input_word_ids\"], dtype=tf.int32)\n    input_mask = tf.cast(inputs[\"input_mask\"], dtype=tf.int32)\n    input_type_ids = tf.cast(inputs[\"input_type_ids\"], dtype=tf.int32)\nOnce our inputs are converted to tf.int32 data types, we can pass them to our BERT layer. The layer returns two data structures: a pooled output, which represents the context vector for the entire text and list of vectors providing context specific vector representation for each submitted token. Since we are only interested in the classification of the entire text, we can ignore the second data structure.\nbert_layer = load_bert_layer()\n    pooled_output, _ = bert_layer(\n        [input_word_ids, \n         input_mask, \n         input_type_ids\n        ]\n    )\nAfterwards, we can assemble our classification model with tf.keras. In our example, we used the functional Keras API.\n x = tf.keras.layers.Dense(256, activation='relu')(pooled_output)\n    dense = tf.keras.layers.Dense(64, activation='relu')(x)\n    pred = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n \n    model = tf.keras.Model(\n        inputs=[inputs['input_word_ids'], \n                inputs['input_mask'], \n                inputs['input_type_ids']], \n        outputs=pred\n    )\n    model.compile(loss='binary_crossentropy', \n                  optimizer='adam', \n                  metrics=['accuracy'])\nThe Keras model can then be consumed by our run_fn function which is called by the TFX Trainer component. With the recent updates to TFX, the integration of Keras models was simplified. No \u201cdetour\u201d with TensorFlow\u2019s model_to_estimator function is required anymore. We can now define a generic run_fn function which executes the model training and exports the model after the completion of the training.\n\nHere is an example of the setup of a run_fn function to work with the latest TFX version:\ndef run_fn(fn_args: TrainerFnArgs):\n    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n    train_dataset = _input_fn(\n        fn_args.train_files, tf_transform_output, 32)\n    eval_dataset = _input_fn(\n        fn_args.eval_files, tf_transform_output, 32)\n \n    mirrored_strategy = tf.distribute.MirroredStrategy()\n    with mirrored_strategy.scope():\n        model = get_model(tf_transform_output=tf_transform_output)\n \n    model.fit(\n        train_dataset,\n        steps_per_epoch=fn_args.train_steps,\n        validation_data=eval_dataset,\n        validation_steps=fn_args.eval_steps)\n \n    signatures = {\n        'serving_default':\n            _get_serve_tf_examples_fn(model, tf_transform_output\n            ).get_concrete_function(\n                                 tf.TensorSpec(\n                                 shape=[None],\n                                 dtype=tf.string,\n                                 name='examples')),\n    }\n    model.save(\n        fn_args.serving_model_dir, \n        save_format='tf', \n        signatures=signatures)\nIt is worth taking special note of a few lines from the example Trainer function. With the latest release of TFX, we can now take advantage of the distribution strategies introduced in Keras last year in our TFX trainer components.\nmirrored_strategy = tf.distribute.MirroredStrategy()\n    with mirrored_strategy.scope():\n        model = get_model(tf_transform_output=tf_transform_output)\nIt is most efficient to preprocess the data sets ahead of the model training, which allows for faster training, especially when the trainer passes multiple times over the same data set.\nTherefore, TensorFlow Transform will perform the preprocessing prior to the training and evaluation, and store the preprocessed data as TFRecords.\n{'input_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n 'input_type_ids': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n 'input_word_ids': array([  101,  2023,  3319,  3397, 27594,  2545,  2005,  2216,  2040, ..., 2014,   102]),\n 'label': array([0], dtype=float32)}\nThis allows us to generate a preprocessing graph which then can be applied during our post-training prediction mode. Because we reuse the preprocessing graph, we can avoid skew between the training and the prediction preprocessing.\n\nIn our run_fn function we can then \u201cwire up\u201d the preprocessed training and evaluation data sets instead of the raw data sets to be used during the training:\n tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n    train_dataset = _input_fn(fn_args.train_files, tf_transform_output, 32)\n    eval_dataset = _input_fn(fn_args.eval_files, tf_transform_output, 32) \n         ...\n    model.fit(\n        train_dataset,\n        validation_data=eval_dataset,\n        ...)\nOnce the training is completed, we can export our trained model together with the processing steps.\nExport the Model with its Preprocessing Graph\nAfter the model.fit() completes the model training, we are calling model.save()to export the model in the SavedModel format. In our model signature definition, we are calling the function _get_serve_tf_examples_fn() which parses serialized tf.Example records submitted to our TensorFlow Serving endpoint (e.g. in our case the raw text strings to be classified) and then applies the transformations preserved in the TensorFlow Transform graph. The model prediction is then performed with the transformed features which are the output of the model.tft_layer(parsed_features)call. In our case, this would be the BERT token ids, masks ids and type ids.\ndef _get_serve_tf_examples_fn(model, tf_transform_output):\n   model.tft_layer = tf_transform_output.transform_features_layer()\n \n   @tf.function\n   def serve_tf_examples_fn(serialized_tf_examples):\n       feature_spec = tf_transform_output.raw_feature_spec()\n       feature_spec.pop(_LABEL_KEY)\n       parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n \n       transformed_features = model.tft_layer(parsed_features)\n       return model(transformed_features)\n \n   return serve_tf_examples_fn\nThe _get_serve_tf_examples_fn() function is the important connection between the transformation graph generated by TensorFlow Transform, and the trained tf.Keras model. Since the prediction input is passed through the model.tft_layer(), it guarantees that the exported SavedModel will include the same preprocessing that was performed during training. The SavedModel is one graph, consisting of both the preprocessing and the model graphs.\n\nWith the deployment of the BERT classification model through TensorFlow Serving, we can now submit raw strings to our model server (submitted as tf.Example records) and receive a prediction result without any preprocessing on the client side or a complicated model deployment with a preprocessing step.\nFuture work\nThe presented work allows a simplified deployment of BERT models. The preprocessing steps shown in our demo project can easily be extended to handle more complicated preprocessing, e.g., for tasks like entity extractions or question-answer tasks. We are also investigating if the prediction latency can be further reduced if we reuse a quantized or distilled version of the pre-trained BERT model (e.g., Albert).\n\nThank you for reading our two-part blog post. Feel free to get in touch if you have questions or recommendations by email.\nFurther Reading\nIf you are interested in an overview of the TensorFlow libraries we used in this project, we recommend the part one of this blog post.\n\nIn case you want to try out our demo deployment, check out our demo page at SAP ConcurLabs showcasing our sentiment classification project.\n\nIf you are interested in the inner workings of TensorFlow Extended (TFX) and TensorFlow Transform, check out this upcoming O\u2019Reilly publication \u201cBuilding Machine Learning Pipelines with TensorFlow\u201d (pre-release available online).\nFor more information\nTo learn more about TFX check out the TFX website, join the TFX discussion group, dive into other posts in the TFX blog, watch our TFX playlist on YouTube, and subscribe to the TensorFlow channel.\nAcknowledgments\nThis project wouldn\u2019t have been possible without the tremendous support from Catherine Nelson, Richard Puckett, Jessica Park, Robert Reed, and the SAP\u2019s Concur Labs team. Thanks goes also out to Robert Crowe, Irene Giannoumis, Robby Neale, Konstantinos Katsiapis, Arno Eigenwillig, and the rest of the TensorFlow team for discussing implementation details and for the detailed review of this post. A special thanks to Varshaa Naganathan, Zohar Yahav, and Terry Huang from Google\u2019s TensorFlow team for providing updates to the TensorFlow libraries to make this pipeline implementation possible. Big thanks also to Cole Howard from Talenpair for always enlightening discussions about Natural Language Processing.",
    "link": "https://blog.tensorflow.org/2020/06/part-2-fast-scalable-and-accurate-nlp.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-K-RmM98YbkQ/XtE2yC8VGeI/AAAAAAAADLI/MY5D_AsCXj4Q9yiRvrNhPsGD8EceIds-ACLcBGAsYHQ/s1600/figure1.gif",
      "https://4.bp.blogspot.com/-Z3-UfzYGDUU/XtE25YLGZ-I/AAAAAAAADLM/CelA7xfP1vMF9LL_m5vJoNvKIwg_g6k9QCLcBGAsYHQ/s1600/figure2.png",
      "https://3.bp.blogspot.com/-PbFxlXwgj-w/XtE3B7iQvpI/AAAAAAAADLU/b65fs9D7dZ4WWBw2a1u30N2iKtRCsfwnACLcBGAsYHQ/s1600/figure3.png"
    ],
    "time": "2023/12/01 00:58:43"
  },
  {
    "title": "How Airbus Detects Anomalies in ISS Telemetry Data Using TFX",
    "content": "A guest post by Philipp Grashorn, Jonas Hansen and Marcel Rummens from Airbus\n\nThe International Space Station and it\u2019s different modules. Airbus designed and built the Columbus module in 2008.\nAirbus provides several services for the operation of the Columbus module and its payloads on the International Space Station (ISS). Columbus was launched in 2008 and is one of the main laboratories onboard the ISS. To ensure the health of the crew as well as hundreds of systems onboard the Columbus module, engineers have to keep track of many telemetry datastreams, which are constantly beamed to earth.\n\nThe operations team at the Columbus Control Center, in collaboration with Airbus, keeps track of thousands of parameters, monitored in 24/7 shifts. If an operator detects an anomaly, he or she creates an anomaly report which is resolved by Airbus system experts. The team at Airbus created the ISS Analytics project to automate part of the workflow of detecting anomalies.\nPrevious, manual workflow\nDetecting Anomalies\nThe Columbus module consists of several subsystems, each of which is composed of multiple components, resulting in about 17,000 unique telemetry parameters. As each subsystem is highly specialized, it made sense to train a separate model for each subsystem.\nLambda Architecture\nIn order to detect anomalies within the real time telemetry data stream, the models are trained on about 10 years worth of historical data, which is constantly streamed to earth and stored in a specialized database. On average, the data is streamed in a frequency of one hertz. Simply looking at the data of the last 10 years results in over 5 trillion data points, (10y * 365d * 24h * 60min * 60s * 17K params).\n\nA problem of this magnitude requires big data technologies and a level of computational power which is typically only found in the cloud. As of now a public cloud was adopted, however as more sensitive systems are integrated in the future, the project has to be migrated to the Airbus Private Cloud for security purposes.\n\nTo tackle this anomaly detection problem, a lambda architecture was designed which is composed of two parts: the speed and the batch layer.\nHigh Level architecture of ISS Analytics\nThe batch layer consists only of the learning pipeline, fed with historical time series data which is queried from an on-premise database. Using an on-premise Spark cluster, the data is sanitized and prepared for the upload to GCP. TFX on Kubeflow is used to train an LSTM Autoencoder (details in the next section) and deploy it using TF-Serving.\n\nThe speed layer is responsible for monitoring the real-time telemetry stream, which is received using multiple ground stations on earth. The monitoring process uses the deployed TensorFlow model to detect anomalies and compare them against a database of previously detected anomalies, simplifying the root cause analysis and decreasing the time to resolution. In case the neural network detects an anomaly, a reporting service is triggered which consolidates all important information related to the potential anomaly. A notification service then creates an abstract and informs the responsible experts.\nTraining an Autoencoder to Detect Anomalies\nAs mentioned above, each model is trained on a subset of telemetry parameters. The objective of the model is to represent the nominal state of the subsystem. If the model is able to reconstruct observations of nominal states with a high accuracy, it will have difficulties reconstructing observations of states which deviate from the nominal state. Thus, the reconstruction error of the model is used as an indicator for anomalies during inference, as well as part of the cost function in training. Details of this practice can be found here and here.\n\nThe anomaly detection approach outlined above was implemented using a special type of artificial neural network called an Autoencoder. An Autoencoder can be divided into two parts: the encoder and the decoder. The encoder is a mapping from the input space into a lower dimensional latent space. The decoder is a mapping from the latent space into the reconstruction space with a dimensionality equal to the input space.\n\nWhile the encoder generates a compressed representation of the input, the decoder generates a representation as close as possible to the original input, using the latent vector from the encoder. Dimensionality reduction acts as a funnel which enables the autoencoder to ignore signal noise.\n\nThe difference between the input and the reconstruction is called reconstruction error and is calculated as the root-mean-square error. The reconstruction error, as mentioned above, is minimized in the training step and acts as an indicator for anomalies during inference (e.g., an anomaly would have high reconstruction error).\nExample Architecture of an Autoencoder\nLSTM for sequences\nThe Autoencoder uses LSTMs to process sequences and capture temporal information. Each observation is represented as a tensor with shape [number_of_features,number_of_timesteps_per_sequence]. The data is prepared using TFT\u2019s scale_to_0_1 and vocabulary functions. Each LSTM layer of the encoder is followed by an instance of tf.keras.layers.Dropout to increase the robustness against noise.\nModel Architecture of ISS Analytics (Red circles represent dropout)\nUsing TFX\nThe developed solution contains many but not all of the TensorFlow Extended (TFX) components. However it is planned to research and integrate additional components included with the TFX suite in the future.\n\nThe library that is most used in this solution is tf.Transform, which processes the raw telemetry data and converts it into a format compatible with the Autoencoder model. The preprocessing steps are defined in the preprocessing_fn() function and executed on Apache Beam. The resulting transformation graph is stored hermetically within the graph of the trained model. This ensures that the raw data is always processed using the same function, independent of the environment it is deployed in. This way the data fed into the model is consistent.\n\nThe sequence-based approach which was outlined in an earlier section posed some challenges. The input_fn() of model training reads the data, preprocessed in the preceding tf.Transform step and applies a windowing function to create sequences. This step is necessary because the data is stored as time steps without any sequence information. Afterwards, it creates batches of size sequence_length * batch_size and converts the whole dataset into a sparse tensor for the input layer of the Autoencoder (tf.contrib.feature_column.sequence_input_layer()expects sparse tensors).\n\nThe serving_input_fn() on the other hand receives already sequenced data from upstream systems (data-stream from the ISS). But this data is not yet preprocessed and therefore the tf.Transform step has to be applied. This step is preceded and followed by reshaping calls, in order to temporarily remove the sequence-dimension of the tensor for the preprocessing_fn().\n\nOrchestration for all parts of the machine learning pipeline (transform, train, evaluate) was done with Kubeflow Pipelines. This toolkit simplifies and accelerates the process of training models, experimenting with different architectures and optimizing hyperparameters. By leveraging the benefits of Kubernetes on GCP, it is very convenient to run multiple experiments in parallel. In combination with the Kubeflow UI, one can analyze the hyperparameters and results of these runs in a well-structured form. For a more detailed analysis of specific models and runs, TensorBoard was used to examine learning curves and neural network topologies.\n\nThe last step in this TFX use case is to connect the batch and the speed layer by deploying the trained model with TensorFlow Serving. This turned out to be the most important component of TFX, actually bringing the whole machine learning system into production. Its support for features like basic monitoring, a standardized API, effortless rollover and A/B testing, have been crucial for this project.\n\nWith the modular design of TFX pipelines, it was possible to train separate models for many subsystems of the Columbus module, without any major modifications. Serving these models as independent services on Kubernetes allows scaling the solution, in order to apply anomaly detection to multiple subsystems in parallel.\n\nUtilizing TFX on Kubeflow brought many benefits to the project. Its flexible nature allows a seamless transition between different environments and will help the upcoming migration to the Airbus Private Cloud. In addition, the work done by this project can be repurposed to other products without any major rework, utilizing the development of generic and reusable TFX components.\n\nCombining all these features the system is now capable of analysing large amounts of telemetry parameters, detecting anomalies and triggering the required steps for a faster and smarter resolution.\nThe partially automated workflow after the ISS Analytics project\nTo learn more about Airbus checkout out the Airbus website or dive deeper into the Airbus Space Infrastructure. To learn more about TFX check out the TFX website, join the TFX discussion group, dive into other posts in the TFX blog, or watch the TFX playlist on YouTube.",
    "link": "https://blog.tensorflow.org/2020/04/how-airbus-detects-anomalies-iss-telemetry-data-tfx.html",
    "imgSource": [
      "https://4.bp.blogspot.com/-ypmzV0szw5I/Xo42cwUl3NI/AAAAAAAAC7c/CgZCWomrNEwoxxr2Z2NwAwzv6nyrcIeDACLcBGAsYHQ/s1600/internationalspace.png",
      "https://1.bp.blogspot.com/-ognX5Ght7VY/Xo4236cyqZI/AAAAAAAAC7k/AC3gUUBgQeo9OHc08fRcDaRFTDz9qn7IwCLcBGAsYHQ/s1600/manualworkflowprev.png",
      "https://4.bp.blogspot.com/-q_MVUKBE8Jc/Xo43Q2WDFoI/AAAAAAAAC7s/6i2YM8DSPPIy2V7xc-tbT6yTJqw86BCZACLcBGAsYHQ/s1600/high%2Blevel%2Barchitecture%2Biss%2B.png",
      "https://2.bp.blogspot.com/-G1DqTL6hJv8/Xo44JOu21JI/AAAAAAAAC70/fNUJaynGS3cgJRSxq01rWWtnubLn32dQQCLcBGAsYHQ/s1600/autoencoder.png",
      "https://2.bp.blogspot.com/-zN7l7hhkyPU/Xo46ZzPptYI/AAAAAAAAC8A/BZFUCXV1R74T-nWHZ9VcdkFBUB0u7hwnwCLcBGAsYHQ/s1600/lstm.png",
      "https://4.bp.blogspot.com/-x0vPb0-9jcM/Xo467WMUBDI/AAAAAAAAC8I/dLFiuvCibXY0kDLpQRdrfGycNdZYqkC3wCLcBGAsYHQ/s1600/partiallyautomated.png"
    ],
    "time": "2023/12/01 00:58:43"
  },
  {
    "title": "Part 1: Fast, scalable and accurate NLP: Why TFX is a perfect match for deploying BERT",
    "content": "Posted by Guest author Hannes Hapke, Senior Machine Learning Engineer at SAP\u2019s Concur Labs. Edited by Robert Crowe on behalf of the TFX team.\n\nTransformer models, especially the BERT model, have revolutionized NLP and broken new ground on tasks such as sentiment analysis, entity extractions, or question-answer problems. BERT models allow data scientists to stand on the shoulders of giants. When the models have been pre-trained on large corpora by corporations, data scientists can apply transfer learning to these multi-purpose trained transformer models and achieve groundbreaking results for their domain-specific problems.\n\nAt SAP\u2019s Concur Labs, we wanted to join the party and use BERT for new problems in the travel and expense domain. We wanted to simplify our BERT inferences. Unfortunately, the solutions we tried never felt perfect.\n\nBy collaborating with the Google/TensorFlow team, and using their latest developments, we were finally able to achieve our goal of consistent, simple, and especially fast BERT model inferences. With the proposed implementation, we achieve predictions from raw text to a classification in a few milliseconds. Let\u2019s take a look at how the various TensorFlow libraries and components have helped us reach that milestone.\n\nThis blog post will provide you an overview of how the TensorFlow ecosystem can be used to achieve a scalable, fast, and efficient BERT deployment. If you are interested in a deep dive into the implementation, check out Part Two of this blog post on the details of the implementation steps. If you want to try out our demo deployment, check out our demo page at Concur Labs showcasing our sentiment classification project.\nA note about Serving\nThe approach discussed in this blog post will allow developers to train TensorFlow models using TensorFlow Extended (TFX) v0.21 or later. However, support for the tf.text ops which are included in the resulting trained model is not yet included in the current release of TensorFlow Serving (v2.1), but is included in the nightly docker release and will be included in the v2.2 release.\nWant to jump ahead to the code?\nIf you would like to jump to the complete example, check out the Colab notebook. It showcases the entire TensorFlow Extended (TFX) pipeline to produce a deployable BERT model with the preprocessing steps as part of the model graph.\nThe current state of BERT deployments\nThe recent developments of transformer models have been astonishing. But unfortunately, taking the models to production never felt simple or perfect. Ideally, we\u2019d like to send the raw text to a server, but the BERT model requires preprocessing of the input text before we can get predictions from the actual model. Some existing solutions have solved this problem by preprocessing the text on the client-side, while other solutions have implemented an intermediate step on the server-side to manipulate the input data. Both options never felt quite right, since they require additional deployment coordination (e.g. during the client-server handoff) or make the inferences less efficient (e.g. complicated prediction batching capabilities due to the intermediate transformation step).\nFigure 1: Current BERT deployments\nWhat does the optimal deployment look like?\nWhen it comes to deploying models, simpler is better. We wanted to deploy transformer models with the preprocessing as part of the model graph. Because the preprocessing is integrated into the model graph, we are able to simply deploy a single model to our model server, remove any other deployment dependencies (client or intermediate preprocessing), and take advantage of the full benefits of model servers (e.g. batching prediction requests for the optimal usage of our inference hardware).\nDeploying BERT with the TensorFlow Ecosystem\nTensorFlow has been a very productive framework for us because it isn\u2019t just a machine learning framework, but it also provides an extensive ecosystem of supporting packages and tools. A tool that has been useful to us is TensorFlow Serving. It provides simple, consistent, and scalable model deployments.\n\nAnother ecosystem project we follow very closely is TensorFlow Transform. It provides us the opportunity to build our model preprocessing steps as graphs which we can then export together with actual deep learning models. TensorFlow Transform requires that all preprocessing steps are expressed as TensorFlow ops. This is why the most recent developments of TensorFlow Text were extremely helpful. Not only did the implementation of RaggedTensors open up new implementations, but the library also provided the needed functionality to implement natural language preprocessing steps.\n\nOne of the new capabilities of TensorFlow Text, presented at TensorFlowWorld 2019, is the complete implementation of a BERT Tokenizer. Because of this, we were able to express our preprocessing steps with a few lines of TensorFlow code. We also achieved our goal of consistent model pipelines and deployments by utilizing one more TensorFlow tool: TensorFlow Extended (TFX). TFX allows us to express our entire ML pipelines in a reproducible way and therefore helps us deploy consistent machine learning models.\nFigure 2: TFX pipeline with tf.Text\nWriting the preprocessing steps with TensorFlow ops\nThe ideal model deployment accepts raw text as an input to the model and provides the model predictions in return. The key to the simplification of our BERT deployments is the expression of the preprocessing steps as TensorFlow ops. The BERT model requires that the raw input text to be tokenized into token IDs, an accompanying data structure of an input mask, and generated input type IDs. With the help of TensorFlow Text, we can now achieve this with far fewer lines of code. In the second part of this blog post, we are discussing the details of the conversion from raw text to the BERT specific data structures, including the adding of the BERT specific tokens.\nvocab_file_path = load_bert_layer().resolved_object.vocab_file.asset_path\nbert_tokenizer = text.BertTokenizer(vocab_lookup_table=vocab_file_path, \n                                    token_out_type=tf.int64, \n                                    lower_case=do_lower_case)\n...\ninput_word_ids = tokenize_text(text)\ninput_mask = tf.cast(input_word_ids > 0, tf.int64)\ninput_mask = tf.reshape(input_mask, [-1, MAX_SEQ_LEN])\n \nzeros_dims = tf.stack(tf.shape(input_mask))\ninput_type_ids = tf.fill(zeros_dims, 0)\ninput_type_ids = tf.cast(input_type_ids, tf.int64)\nFigure 3: BERT tokenizer\nUsing TensorFlow Transform and the code above, the preprocessing graph can then be exported together with the trained TensorFlow model. With the latest updates to TensorFlow Serving, our deployed BERT model can now accept the raw text as inputs. Voila! No additional deployment dependencies.\n\nUsing TensorFlow Transform provided us a few practical benefits. On the one hand, we can organizationally split the responsibilities between the data preprocessing and the model architecture work. On the other hand, we can easily debug, test and generate statistics of the preprocessing output. The transform component outputs the transformed training sets (as TFRecords), which can be easily inspected. During the \u201cdebugging\u201d of the Transform output, we discovered small bugs that wouldn\u2019t have failed the training of the model, but probably influenced its performance (e.g. an offset in the [SEP] token). TensorFlow Transform technically isn\u2019t required here. Since each example preprocessing happens independently from the entire corpus, we could have easily built it directly into the model graph. But we found it easier to construct and debug the pipeline this way.\nFigure 4: BERT layer\nIf you are interested in a deep dive into the implementation, we recommend part two of this blog post with an in-depth look at the implementation.\nThe ideal deployment?\nSimplified development\nBy utilizing a variety of TensorFlow tools, we were able to deploy BERT models in a simple and concise way. Integrating the preprocessing steps into the model graph reduces the risk of skew between the training and inference data. The deployed model requires no additional client or server dependencies, which further reduces the risk of model errors. We can deploy our BERT models consistently with TensorFlow Serving while also taking advantage of the model optimizations like batch inferences.\nInference performance\nOur initial performance tests are looking very promising. Inferences on our demo BERT model graph containing the preprocessing steps and the model average to around 15.5 ms per prediction (measured on a single V100 GPU, max 128 tokens, gRPC requests, non-optimized TensorFlow Serving build for GPUs, uncased Base BERT model). Previous deployments with the BERT tokenization on the client-side and the classification model being hosted with TensorFlow Serving averages to about the same inference time. Of course depending on your machine (and model), you may see different results.\nFor more information\nIf you are interested in a deep dive into the implementation, we recommend part two of this blog post. If you would like to dive into the code, check out the Colab notebook with an example implementation of a sentiment classification model using a pre-trained BERT model. If you want to try out our demo deployment, check out our demo page at Concur Labs showcasing our sentiment classification project.\n\nIf you are interested in the inner workings of TensorFlow Extended (TFX) and TensorFlow Transform, dive into the TFX User Guide and check out this upcoming O\u2019Reilly publication \u201cBuilding Machine Learning Pipelines, Automating Model Life Cycles With TensorFlow\u201d (pre-release available online).\n\nTo learn more about TFX, check out the TFX website, join the TFX discussion group, dive into other posts in the TFX blog, watch our TFX playlist on YouTube, and subscribe to the TensorFlow channel.\nAcknowledgments\nThis project wouldn\u2019t have been possible without the tremendous support from Catherine Nelson, Richard Puckett, Jessica Park, Robert Reed, and the Concur Labs team. Thanks also goes out to Robby Neale, Robert Crowe, Irene Giannoumis, Terry Huang, Zohar Yahav, Konstantinos Katsiapis, Arno Eigenwillig, and the rest of the TensorFlow team for discussing implementation details and for providing updates to the TensorFlow libraries. Big thanks also to Cole Howard from Talenpair for always enlightening discussions about Natural Language Processing.",
    "link": "https://blog.tensorflow.org/2020/03/part-1-fast-scalable-and-accurate-nlp-tensorflow-deploying-bert.html",
    "imgSource": [
      "https://3.bp.blogspot.com/-JT9WD4QAXBc/XmfsoCT0CMI/AAAAAAAAC0s/M64XXUH_3IYRQhijx8deIYHf0Ocr2OHKACLcBGAsYHQ/s1600/figure1.png",
      "https://4.bp.blogspot.com/-tO84Er3IFZE/XmfxzPCS6XI/AAAAAAAAC04/4ZrCH1Xt7P8q9VlEg98Q-ad6Yv_QZonRQCLcBGAsYHQ/s1600/figure2.png",
      "https://3.bp.blogspot.com/-9mqSeRr4dcM/XmfyPtCq0PI/AAAAAAAAC1A/JAb337ebMk85lBmfISQ8KbeNJR0aUOsBQCLcBGAsYHQ/s1600/figure3.png",
      "https://4.bp.blogspot.com/-0gnTZ-ef8Q4/XmfyYvxAK4I/AAAAAAAAC1E/bxSTD8Mr7bc1pLvabajyeZGQ72iM4jqyACLcBGAsYHQ/s1600/figure4.png"
    ],
    "time": "2023/12/01 00:58:43"
  },
  {
    "title": "TensorFlow Extended (TFX): Using Apache Beam for large scale data processing",
    "content": "Posted by By Reza Rokni, Developer Advocate Google Cloud, on behalf of the TFX and Dataflow teams\n\n\nTFX core mission is to allow models to be moved from research to production, creating and managing production pipelines. Many models will be built using large volumes of data, requiring multiple hosts working in parallel to serve both the processing and serving needs of your production pipelines.\n\nUsing capabilities inherited from Apache Beam, TFX's data processing framework, we will look at how a TFX pipeline, developed against a small dataset, can be scaled out for your production dataset.\nApache Beam\nThe origins of Apache Beam can be traced back to FlumeJava, which is the data processing framework used at Google (discussed in the FlumeJava paper (2010)). Google Flume is heavily in use today across Google internally, including the data processing framework for Google's internal TFX usage.\n\nGoogle Flume was the basis for the development of Google Cloud Dataflow (released in 2015). The SDK for Dataflow was open sourced in 2016 as Apache Beam. Similar to Google's internal TFX implementation (discussed in the TFX paper (2017)) , the external version of TFX makes use of the external version of Google Flume, Apache Beam.\n\nThe Apache Beam portable API layer powers TFX libraries (for example TensorFlow Data Validation, TensorFlow Transform, and TensorFlow Model Analysis), within the context of a Directed Acyclic Graph (DAG) of execution. Apache Beam pipelines can be executed across a diverse set of execution engines, or \u201crunners\u201d. A comprehensive list of runners and their capabilities can be found at:\n\nhttps://beam.apache.org/documentation/runners/capability-matrix/.\n\nThe runner, used in this blog, is Dataflow which shares a large percentage of its code with Google Flume, with further unification in progress.\n\nBelow we can see the graph created by the TFX component ExampleGen when it is run on the Dataflow runner.\nApache Beam Benefits\nThis freedom to choose different execution engines was an important factor in deciding to make use of Apache Beam for TFX. Development can be done on a local DirectRunner, with production workloads run on production runners. For example, the production Apache Flink runner can run in a local data center, or you can use a fully managed cloud runner like Dataflow.\nBy using production runners, we can make use of tens of thousands of cores, all working in parallel to carry out the computation done in TFX libraries, without changing the core code created during the development of the pipeline.\n\nWe will show this ability using two examples. First, using a core TFX library AnalyzeAndTransformDataset and finally via two TFX components ExampleGen and StatisticsGen.\n\nNOTE:\nBigQuery and Dataflow are chargeable services, please ensure you understand the cost implications before running any of the samples in this blog.\nhttps://cloud.google.com/bigquery/pricing\nhttps://cloud.google.com/dataflow/pricing\nhttps://cloud.google.com/storage/pricing/\nTFX Libraries\nTFX pipeline components are built upon TFX libraries. For example, TensorFlow Transform, which uses Apache Beam. We will explore this library using two different Apache Beam runners. Initially, the local development runner DirectRunner will be used. This will be followed by some minor code modifications to run the sample with the production Dataflow runner. The DirectRunner is a lightweight runner for development purposes. It runs locally and does not require a distributed processing framework.\n\nThe example pipeline below is borrowed from the tutorial (Preprocess data (beginner)) which provides an example of how TensorFlow Transform (tf.Transform) can be used to preprocess data.\n\nFor details of the preprocessing_fn, please refer back to the tutorial. For now, we just need to know that it is transforming the data points passed into the function.\n\nNOTE:\nEnvironment used for this blog post:\nvirtualenv tfx-beam --python=python3\nsource tfx-beam/bin/activate\npip install tfx\ndef main():\n  with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n    transformed_dataset, transform_fn = (\n        (raw_data, raw_data_metadata) | tft_beam.AnalyzeAndTransformDataset(\n            preprocessing_fn))\n  transformed_data, transformed_metadata = transformed_dataset\n  print('\\nRaw data:\\n{}\\n'.format(pprint.pformat(raw_data)))\n  print('Transformed data:\\n{}'.format(pprint.pformat(transformed_data)))\nif __name__ == '__main__':\n  main()\nNOTE:\nApache Beam uses a special syntax to define and invoke transforms. For example, in this line:\nresult = pass_this | 'name this step' >> to_this_call\nThe method to_this_call is being invoked and passed the object called pass_this, and this operation will be referred to as name this step in a stack trace.\n\nThe example above, implicitly will make use of the local development / test runner DirectRunner. To switch from the local DirectRunner to Dataflow, first we need to wrap the beam_impl.Context within a beam.Pipeline. This gives the ability to pass in arguments, for example \"--runner\". For a quick local test you can run the sample below with --runner set to DirectRunner.\nimport apache_beam as beam\n\nargv=['--runner=DirectRunner']\n\ndef main():\n     with beam.Pipeline(argv=argv) as p:\n       # Ignore the warnings\n       with beam_impl.Context(temp_dir=tempfile.mkdtemp()):  \n         input = p | beam.Create(raw_data)  \n         transformed_dataset, transform_fn = (  \n             (input, raw_data_metadata)\n            | beam_impl.AnalyzeAndTransformDataset(preprocessing_fn))\n         transformed_dataset[0] |\"Print Transformed Dataset\" >>  beam.Map(print)\n     \nif __name__ == '__main__':\n  main()\nNext, we will switch to using the Dataflow Runner. Since Dataflow is a fully managed runner working on Google Cloud, we will need to provide the pipeline with some environmental information. This includes the Google Cloud project and locations for the temporary files used by the pipeline.\n\nNOTE:\nYou must set the correct permissions to submit a pipeline job to the Dataflow service.\nRead more information on authentication at: https://cloud.google.com/dataflow/docs/concepts/security-and-permissions\n\n# Setup our Environment\n\n## The location of Input / Output between various stages ( TFX Components )\n## This will also be the location for the Metadata \n\n### Can be used when running the pipeline locally\n#LOCAL_PIPELINE_ROOT =\n\n### In production you want the input and output to be stored on non-local location\n#GOOGLE_CLOUD_STORAGE_PIPELINE_ROOT=\n\n#GOOGLE_CLOUD_PROJECT = \n\n#GOOGLE_CLOUD_TEMP_LOCATION = \n\n# Will need setup.py to make this work with Dataflow\n#\n# import setuptools\n#\n# setuptools.setup(\n#   name='demo',\n#   version='0.0',\n#   install_requires=['tfx==0.21.1'],\n#   packages=setuptools.find_packages(),)\n\nSETUP_FILE = \"./setup.py\"\n\nargv=['--project={}'.format(GOOGLE_CLOUD_PROJECT),\n      '--temp_location={}'.format(GOOGLE_CLOUD_TEMP_LOCATION),\n      '--setup_file={}'.format(SETUP_FILE),\n      '--runner=DataflowRunner']\ndef main():\n    with beam.Pipeline(argv=argv) as p:\n        with beam_impl.Context(temp_dir=GOOGLE_CLOUD_TEMP_LOCATION):\n            input = p | beam.Create(raw_data) \n            transformed_data, transformed_metadata = (\n                (input, raw_data_metadata)\n                | beam_impl.AnalyzeAndTransformDataset(preprocessing_fn))\n\nif __name__ == '__main__':\n  main()\nTo get a feel for how much work TFX has abstracted away, below is a visual representation of the graph that the pipeline processed. We had to shrink the image to fit it all in as there are a lot of transforms!\n\nUsing TFX Components with Beam\nNext, let's make use of some TFX components , which are composed from the TFX libraries discussed above. We will use ExampleGen to ingest the data and StatisticsGen which generates descriptive statistics on the data.\nExampleGen\nThe ExampleGen TFX Pipeline component ingests data into TFX pipelines. It consumes external files/services to generate Examples which will be read by other TFX components. It also splits the data into training and evaluation splits, or additional splits if required, and optionally shuffles the dataset. The process is listed below:\nSplit data into training and evaluation sets (by default, 2/3 training + 1/3 eval)\nConvert data into the tf.Example format\nCopy data into the _tfx_root directory for other components to access, for other components to access\nBigQueryExampleGen allows us to directly query data in BigQuery.\ndef createExampleGen(query: Text):\n    # Output 2 splits: train:eval=3:1.\n    output = example_gen_pb2.Output(\n             split_config=example_gen_pb2.SplitConfig(splits=[\n                 example_gen_pb2.SplitConfig.Split(\n                                 name='train', hash_buckets=3),\n                 example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=1)\n             ]))\n    return BigQueryExampleGen(query=query, output_config=output)\nAs well as the SQL query to be run, BigQueryExampleGen code is also being passed configuration information via the SplitConfig object.\n\nThe data for this example comes from the public Chicago taxi trips dataset located on BigQuery's public datasets (Google Cloud's Data warehouse).\n\nbigquery-public-data.chicago_taxi_trips.taxi_trips.\n\nNOTE: You can find more details about BigQuery public datasets at: https://cloud.google.com/bigquery/public-data/\n\nThe query below, will extract the data in the correct format for processing by ExampleGen.\nquery=\"\"\"\nSELECT\npickup_community_area,\n  fare,\n  EXTRACT(MONTH FROM trip_start_timestamp)  trip_start_month,\n  EXTRACT(HOUR FROM trip_start_timestamp)  trip_start_hour,\n  EXTRACT(DAYOFWEEK FROM trip_start_timestamp)  trip_start_day,\n  UNIX_Millis(trip_start_timestamp) trip_start_ms_timestamp,\n  pickup_latitude,\n  pickup_longitude,\n  dropoff_latitude,\n  dropoff_longitude,\n  trip_miles,\n  pickup_census_tract,\n  dropoff_census_tract,\n  payment_type,\n  company,\n  trip_seconds,\n  dropoff_community_area,\n  tips\nFROM\n  `bigquery-public-data.chicago_taxi_trips.taxi_trips`\nLIMIT 100\n\"\"\"\nNote the use of LIMIT 100, which will limit the output to 100 records, allowing us to quickly test out our code for correctness.\nStatisticsGen\nThe StatisticsGen TFX pipeline component generates descriptive statistics over both training and evaluation data, which can be used by other pipeline components. It works on the results of the previous step ExampleGen.\ndef createStatisticsGen(bigQueryExampleGen: BigQueryExampleGen):\n    # Computes statistics over data for visualization and example validation.\n    return StatisticsGen(examples=bigQueryExampleGen.outputs['examples'])\nAs the output of ExampleGen is required by StatisticsGen, we now have a dependency between the two steps. This producer-consumer pattern is seen throughout most production ML pipelines. To automate this pipeline, we will need something that coordinates these dependencies.\nPipeline Orchestration\nOne solution would be to write a simple, lightweight python script. However, what about debugging, failure modes, retries, logging, etc.?\n\nLuckily for us, this is taken care of by TFX integrations with two pipeline orchestration engines - Kubeflow and Apache Airflow.\n\nAs well as these two orchestration engines, we can also again make use of Apache Beam as an orchestrator since the dependencies can be modeled as a DAG. So, we can use a DAG with transforms that themselves are DAG's. ... \"we have to go deeper\"... :-) .\n\nThe choice of which engine to use is dependent on your production needs and requirements, which is beyond the scope of this blog. For now, we will use Apache Beam for the orchestration, via TFX's BeamDagRunner. This means we are using Beam in two different roles - as an execution engine for processing data, and as an orchestrator for sequencing the TFX tasks.\n# Used for setting up the orchestration \nfrom tfx.orchestration import pipeline\nfrom tfx.orchestration import metadata\nfrom tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\nThe following code creates our pipeline object ready to be executed by the BeamDagRunner.\nfrom typing import Text\nfrom typing import Type\n\ndef createTfxPipeline(pipeline_name: Text, pipeline_root: Text, query: Text,\n                      beam_pipeline_args) -> pipeline.Pipeline:\n    output = example_gen_pb2.Output(\n        # Output 2 splits: train:eval=3:1.\n        split_config=example_gen_pb2.SplitConfig(splits=[\n            example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=3),\n            example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=1)\n        ]))\n\n    # Brings data into the pipeline or otherwise joins/converts training data.\n    example_gen = BigQueryExampleGen(query=query, output_config=output)\n    \n    # Computes statistics over data for visualization and example validation.\n    statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])\n\n    return pipeline.Pipeline(\n      pipeline_name=pipeline_name,\n      pipeline_root=pipeline_root,\n      components=[\n          example_gen, statistics_gen\n      ],\n      metadata_connection_config=metadata.sqlite_metadata_connection_config(\n          os.path.join(\".\", 'metadata', pipeline_name,'metadata.db')),\n      enable_cache=False,\n      additional_pipeline_args=beam_pipeline_args)\nTo test the code make use of the query using \"LIMIT 100\" via the local DirectRunner.\ntfx_pipeline = createTfxPipeline(\n    pipeline_name=\"my_first_directRunner_pipeline\",\n    pipeline_root=LOCAL_PIPELINE_ROOT,\n    query=query,\n    beam_pipeline_args=                               {\n        'beam_pipeline_args':[\n            '--project={}'.format(GOOGLE_CLOUD_PROJECT),\n            '--runner=DirectRunner']})\nBeamDagRunner().run(tfx_pipeline)\nYou can see the results produced by using tfdv with the output to LOCAL_PIPELINE_ROOT;\nimport os\nimport tensorflow_data_validation as tfdv\n\nstats = tfdv.load_statistics(os.path.join(LOCAL_PIPELINE_ROOT,\"StatisticsGen\",\"statistics\",\"\",\"train\",\"stats_tfrecord\"))\ntfdv.visualize_statistics(stats)\nThat works fine for one hundred records, but what if the goal was to process all 187,002,0025 rows in the dataset? For this, the pipeline is switched from the DirectRunner to the production Dataflow runner. A few extra environment parameters are also set, for example the Google Cloud project to run the pipeline in.\ntfx_pipeline = createTfxPipeline(\n    pipeline_name=\"my_first_dataflowRunner_pipeline\",\n    pipeline_root=GOOGLE_CLOUD_STORAGE_PIPELINE_ROOT,\n    query=query,\n    beam_pipeline_args={\n    'beam_pipeline_args':[\n        '--project={}'.format(GOOGLE_CLOUD_PROJECT)\n,\n    '--temp_location={}'.format(GOOGLE_CLOUD_TEMP_LOCATION),\n    '--setup_file=./setup.py',\n    '--runner=DataflowRunner']})\nBeamDagRunner().run(tfx_pipeline)\nThe BeamDagRunner takes care of submitting ExampleGen and StatisticsGen as separate pipelines, ensuring ExampleGen completed successfully first before starting StatisticsGen. The Dataflow service automatically takes care of spinning up workers, autoscaling, retries in the event of worker failure, centralized logging, and monitoring. Autoscaling is based on various signals including throughput rate, illustrated below;\nThe Dataflow monitoring console shows us various metrics about the pipeline, for example, the CPU utilization of the workers. Below we see the utilization of machines as they come on-line, consistently high with most workers over 90%:\nApache Beam supports custom counters, which allows developers to create metrics from within their pipelines. The TFX team has used this to create useful information counters for the various components. Below we can see some of the counters recorded during the StatisticsGen run. Filtering for the key word \"num_*_feature\", there were roughly a billion integers and float features values.\nSummary\nIn this blog, we showed how TFX's use of Apache Beam lets you switch from a development environment to production infrastructure without having to change the core code. We started with the TFX libraries and moved to a pipeline with two core TFX pipeline components ExampleGen and StatisticsGen.\nFor more information\nTo learn more about TFX, check out the TFX website, join the TFX discussion group, read the TFX blog, watch our TFX playlist on YouTube, and subscribe to the TensorFlow channel.",
    "link": "https://blog.tensorflow.org/2020/03/tensorflow-extended-tfx-using-apache-beam-large-scale-data-processing.html",
    "imgSource": [
      "https://4.bp.blogspot.com/-q0yJ7TRXzAo/XmajPFevFAI/AAAAAAAACyE/5rRb42zAZecTPeEC5Uri_F50Iojp34EZACLcBGAsYHQ/s1600/beam-reza.png",
      "https://2.bp.blogspot.com/-yx-ZMC4-t6E/XmakRyPLv3I/AAAAAAAACyY/X2_gdKddMkwM5wvPBE9d4p0753Bghu-xACLcBGAsYHQ/s1600/Screenshot%2B2020-01-08%2Bat%2B12.47.15%2BPM.png",
      "https://2.bp.blogspot.com/--DROesIZWPw/XmanB5oC1kI/AAAAAAAACyk/05tjgUBY5pkHOV4XHz9YLgdcsFWee9h2gCLcBGAsYHQ/s1600/Screenshot%2B2020-02-04%2Bat%2B1.33.46%2BPM.png",
      "https://4.bp.blogspot.com/-eHlGkAMnVhc/XmapaSD1vvI/AAAAAAAACyw/w2_P3S_4HdAjbVoqWjkbU49sYOZ6AXhpQCLcBGAsYHQ/s1600/records.png",
      "https://1.bp.blogspot.com/-KN0KQTfJ3yc/XmapxT5nECI/AAAAAAAACy4/qySvaM1K7K0O8adMQkYrQ-6MrqAeBBLGACLcBGAsYHQ/s1600/autoscaling.png",
      "https://1.bp.blogspot.com/-au1U5wSD5P8/Xmap5se2duI/AAAAAAAACy8/m0mvSn9m9VcV-8F6m1DBIsHRSSKvWO5HgCLcBGAsYHQ/s1600/cputilization.png",
      "https://3.bp.blogspot.com/-uifLu-b-UyQ/XmaqECvPfbI/AAAAAAAACzE/-1nGo4Jv8TsIjj1VT_nakpuXhJ28Ksz_QCLcBGAsYHQ/s1600/customcounters.png"
    ],
    "time": "2023/12/01 00:58:43"
  },
  {
    "title": "Distributed PCA using TFX",
    "content": "Guest post by Hamza Tahir of maiot, along with Robert Crowe and Tris Warkentin on behalf of the TFX team\nIntroduction\nPrincipal Component Analysis (PCA) is a dimensionality reduction technique, useful in many different machine learning scenarios. In essence, PCA reduces the dimension of input vectors in a way that retains the maximal variance in your dataset. Reducing the dimensionality of the model input can increase the performance of the model, reduce the size and resources required for training, and decrease non-random noise.\n\nTensorFlow Extended (TFX) is a free and open-source platform for creating production-ready, end-to-end machine learning pipelines. At maiot, TFX is an important building block of our Core Engine. Initially built as the foundation of our asset optimization platform, developers can now independently use the Core Engine to manage their own deep learning workloads.\n\nInside the Engine, we offer many mechanisms for pre-processing data. This includes applying PCA to huge input data for visualization and learning purposes. In light of this, we prepared this post to showcase how to use TFX to apply distributed PCA over a dataset.\nTensorFlow Transform\nA TFX pipeline consists of components, that in turn leverage a variety of TensorFlow libraries. One of these is TensorFlow Transform: A powerful library used for preprocessing input data for TensorFlow. The output of TensorFlow Transform is exported as a TensorFlow graph, used at both training and serving time. This prevents skew since the same transformations are applied in both stages.\n\nLike many of the libraries and components of TFX, TensorFlow Transform performs processing using Apache Beam to distribute workloads on compute clusters. This enables Transform to process very large datasets and to make efficient use of available resources. Apache Beam runs as an abstraction layer on top of widely available distributed computing frameworks, including Apache Spark, Apache Flink, and Google Cloud Dataflow. At maiot, we run Apache Beam on the managed and serverless Cloud Dataflow service, part of the Google Cloud.\n\nWith TensorFlow Transform, it is possible to apply PCA as part of your TFX pipeline. PCA is often implemented to run on a single compute node. Thanks to the distributed nature of TFX, it\u2019s now easier than ever to implement a distributed PCA algorithm for scalable processing of large datasets.\nShowcase - PCA with TFX\nThis example colab notebook contains a complete example of running a TFX pipeline with PCA. It utilizes the TFX Interactive Notebook context to create a TFX pipeline that outputs the principal component projection of the widely used Iris dataset.\n\nAll the magic happens inside the preprocessing_fn function that gets fed into the Transform component in the TFX pipeline. This function accepts a dictionary of feature tensors and outputs a dictionary of features with applied relevant transformations. While you can use normal TensorFlow code here, many fundamental transformations are already built-in out of the box with TensorFlow Transform (e.g., normalize, bucketize, compute vocabularies, etc.). Find the full list of out-of-the-box transforms here.\n\nOne of these built-in transforms is the tft.pca transform, which we will use to compute the PCA of our dataset. Here is how you can utilize this transform in a preprocessing_fn function.\ndef preprocessing_fn(inputs):\n    features = []\n    outputs = {}\n    for feature_tensor in inputs.values():\n        # standard scaler pre-req for PCA\n        features.append(tft.scale_to_z_score(feature_tensor))\n          \n    # concat to make feature matrix for PCA to run over\n    feature_matrix = tf.concat(features, axis=1)  \n    \n    # get orthonormal vector matrix\n    orthonormal_vectors = tft.pca(feature_matrix, output_dim=2, dtype=tf.float32)\n    \n    # multiply matrix by feature matrix to get projected transformation\n    pca_examples = tf.linalg.matmul(feature_matrix, orthonormal_vectors)\n    \n    # unstack and add to output dict\n    pca_examples = tf.unstack(pca_examples, axis=1)\n    outputs['Principal Component 1'] = pca_examples[0]\n    outputs['Principal Component 2'] = pca_examples[1]\n\n\n    return outputs\nNote: In this example, we have assumed that all input features are numerical, and are all fed into the PCA transform. If needed, only a subset of the input features may be used.\n\nThere are a lot of things going on in the above snippet, so let\u2019s take a closer look.\n\nFirstly, we apply a normalization transform to all input tensors. This is important as the PCA algorithm expects that input vector components have been converted to similar units of measurement.\n\nSecond, we concatenate our input tensors together to create a feature matrix. Here is where we apply the tft.pca function. This calculates the orthonormal vector matrix of our data. As explained in the tft.pca documentation, the matrix can be used to calculate the final projection of our data. We do this by multiplying this matrix with the feature matrix. The final step is to \u2018unstack\u2019 the projection matrix, separating the individual principal components. We then return these in the output dictionary.\n\nWhen you actually execute a Transform component with the above preprocessing_fn, a lot goes on under the hood that is abstracted away. To perform distributed processing on a compute cluster TFX creates a distributed Apache Beam pipeline which computes the relevant co-variances and orthonormal vector matrix. It also creates a normal TensorFlow graph with this transformation embedded, which will become part of your trained model, so that you can use the PCA transformation at serving time. The result of PCA is a new vector space with fewer dimensions. At serving time, new data will be projected into that lower dimensional space from the original higher dimensional space.\n\nAfter running a successful TFX pipeline, you can easily use the output of the Transform component to extract the transformed data for visualization. In the accompanying colab, this is exactly what is shown:\n\nAs you can see, the separation between the three classes is clearly visible in the reduced dimension space.\nConclusion\nPCA is just one of the data transformations that can improve the performance of your machine learning models through feature engineering. Like PCA, many transformations require substantial processing horsepower, especially with large datasets. We\u2019ve shown in this post how TensorFlow Transform enables developers to apply sophisticated transforms like PCA in a scalable way, taking advantage of the resources available in compute clusters. We\u2019ve also shown how to include transform processing in a TFX pipeline, and include those feature engineering transformations with your trained models so that exactly the same transformations are performed when the model makes predictions.\nFor more information\nTo learn more about TFX check out the TFX website, join the TFX discussion group, dive into other posts in the TFX blog, watch our TFX playlist on YouTube, and subscribe to the TensorFlow channel.",
    "link": "https://blog.tensorflow.org/2020/02/distributed-pca-using-tfx.html",
    "imgSource": [
      "https://2.bp.blogspot.com/-boj42d-ZO9U/XlBot5yjgtI/AAAAAAAACu4/q4t8qlT_6tMar9o4XvipCs2aTSoWQLUIwCLcBGAsYHQ/s1600/hamza1.png"
    ],
    "time": "2023/12/01 00:58:43"
  },
  {
    "title": "Creating a Custom TFX Component",
    "content": "Posted by Ruoyu Liu and Robert Crowe on behalf of the TFX team\n\nTensorFlow Extended (TFX) is a platform for creating production-ready machine learning (ML) pipelines. TFX was created by Google and provides the backbone of Google\u2019s ML services and applications, and now Google has open sourced TFX for anyone who wants to create production ML pipelines.\n\nTFX can be extended and customized in several ways. We previously covered how to change the behavior of a TFX component by using a custom executor. In this post, we will demonstrate how to customize TFX by creating a completely new TFX component and using it a TFX pipeline.\nIntroduction\nTFX provides a collection of standard components that can be linked together to form a standard ML workflow. While this meets the needs of many use cases, there are still some scenarios that standard components do not support. To support those scenarios, TFX can be extended with custom components.\n\nIn cases such as in the previous blog post where the upstream and downstream semantics - the inputs and outputs of the component - are the same as an existing component, you can create a new \u201csemi-custom\u201d component by reusing the existing component and replacing the behavior of the executor. The existing component may be one of the standard components, or may be a custom component that you or someone else has created.\n\nIf however the upstream and downstream semantics of your new component are not the same as an existing component, then you need to create a new \u201cfully-custom\u201d custom component, which is the topic of this blog post.\n\nThe remainder of this post will illustrate how to create a custom component from scratch with a simple HelloWorld component. For simplicity, the HelloWorld component will only replicate all its inputs as its own outputs and make them available to downstream components, to demonstrate consuming and emitting data artifacts.\nUpdated Pipeline Workflow\nBefore we start coding, let\u2019s take a look at the updated workflow with the new custom component. As illustrated in Figures 1 and 2 below, we\u2019ll insert the new HelloWorld component between ExampleGen and all downstream components which depends on example data. This implies two facts about the new component:\nIt needs to take the output of ExampleGen as one of its inputs\nIt needs to produce the output of the same type as ExampleGen so that the components that originally depend on ExampleGen will have the same type of input\nFigure 1. Before inserting the new custom component\nFigure 2. After inserting the new custom component\nBuilding your own custom component\nNext we will build the new component step by step.\nChannels\nTFX Channel is an abstract concept that connects data producers and data consumers. Conceptually a component reads input artifacts from channels and writes output artifacts to channels which will be used by downstream components as inputs. Channels are typed with artifact type (as discussed in the next section), which means all artifacts written to or read from a channel share the same artifact type.\nComponentSpec\nThe first step is to define the inputs and outputs of the new component as well as other parameters that will be used in component execution. ComponentSpec is the class where we will define this contract with detailed type info. There are three parameters expected:\nINPUTS: A dictionary of typed parameters for the input artifacts that will be passed into the component executor. Normally input artifacts are the outputs from upstream components and thus share the same type.\nOUTPUTS: A dictionary of typed parameters for the output artifacts which the component will produce.\nPARAMETERS: A dictionary of additional ExecutionParameter items that will be passed into the component executor. These are non-artifact parameters that we want to define flexibly in the pipeline DSL and pass into execution.\nAs discussed in previous section, we need to guarantee that:\nOne of the inputs of the HelloWorld component is the same type as the ExampleGen output, since it is directly passed down by it. As shown in Figure 3, 'input_data' is the spec for it.\nOne of the outputs of the HelloWorld component is the same type as the ExampleGen output, since it will be passed down to downstream components, which originally expected ExampleGen output. As shown in Figure 3, 'output_data' is the spec for it.\nIn the parameters spec section, only 'name'is declared for demonstration purpose.\nclass HelloComponentSpec(types.ComponentSpec):\n  \"\"\"ComponentSpec for Custom TFX Hello World Component.\"\"\"\n  # The following declares inputs to the component.\n  INPUTS = {\n    'input_data': ChannelParameter(type=standard_artifacts.Examples),\n  }\n  # The following declares outputs from the component.\n  OUTPUTS = {\n    'output_data': ChannelParameter(type=standard_artifacts.Examples),\n  }\n  # The following declares extra parameters used to create an instance of\n  # this component\n  PARAMETERS = {\n    'name': ExecutionParameter(type=Text),\n  }\nFigure 3. ComponentSpec for HelloWorld component.\nExecutor\nNext, let\u2019s write the code for the executor of our new component. As we discussed in the previous post, we will need to create a new subclass of base_executor.BaseExecutor and override its Do function.\nclass Executor(base_executor.BaseExecutor):\n  \"\"\"Executor for HelloWorld component.\"\"\"\n  ...  \n  def Do(self, input_dict: Dict[Text, List[types.Artifact]],\n         output_dict: Dict[Text, List[types.Artifact]],\n         exec_properties: Dict[Text, Any]) -> None:\n    ...\n    split_to_instance = {}\n    for artifact in input_dict['input_data']:\n      for split in json.loads(artifact.split_names):\n        uri = os.path.join(artifact.uri, split)\n        split_to_instance[split] = uri\n    for split, instance in split_to_instance.items():\n      input_dir = instance\n      output_dir = artifact_utils.get_split_uri(\n          output_dict['output_data'], split)\n      for filename in tf.io.gfile.listdir(input_dir):\n        input_uri = os.path.join(input_dir, filename)\n        output_uri = os.path.join(output_dir, filename)\n        io_utils.copy_file(src=input_uri, dst=output_uri, overwrite=True)\nFigure 4. Executor for HelloWorld component.\nAs shown in Figure 4, we can get input and output artifacts and execution properties using the same keys we defined previously in the ComponentSpec. After we have all the needed values, we can go ahead and add more logic using them and write the output into the URI pointed to by the output artifact ('output_data').\n\nDon\u2019t forget to test it before moving on to the next step! We have created a convenience script for you to try out your executor before putting it into production. You should write similar code to exercise unit tests for your code. As with any production software deployment, when developing for TFX you should make sure to have good test coverage and a strong CI/CD framework.\nComponent interface\nNow that we have finished the most complex part, we need to assemble these pieces into a component interface, to enable the component to be used in a pipeline. The process (shown in Figure 5) requires the following steps:\nMake the component interface a subclass of base_component.BaseComponent\nAssign a class variable SPEC_CLASS with the HelloComponentSpec class we defined earlier\nAssign a class variable EXECUTOR_SPEC with the Executor class we defined earlier\nDefine the __init__() function by using the args to the function to construct an instance of HelloComponentSpec and invoke the super function with the value, along with an optional name\nWhen an instance of the component is created, type checking logic in the base_component.BaseComponent class will be invoked to ensure that the arguments passed in are compatible with the parameter types defined in the HelloComponentSpec class.\nfrom hello_component import executor\nclass HelloComponent(base_component.BaseComponent):\n  \"\"\"Custom TFX HelloWorld Component.\"\"\"\n  SPEC_CLASS = HelloComponentSpec\n  EXECUTOR_SPEC = executor_spec.ExecutorClassSpec(executor.Executor)\n\n  def __init__(self,\n               input_data: channel.Channel,\n               output_data: channel.Channel,\n               name: Text):\n    if not output_data:\n      examples_artifact = standard_artifacts.Examples()\n      examples_artifact.split_names = input_data.get()[0].split_names\n      output_data = channel_utils.as_channel([examples_artifact])\n    spec = HelloComponentSpec(input_data=input_data,\n                              output_data=output_data, name=name)\n    super(HelloComponent, self).__init__(spec=spec)\nFigure 5. Component interface.\nPlugging into the TFX pipeline\nGood news! Our brand new component is ready to use after our hard work in the previous sections. Let\u2019s plug it into our Chicago taxi example pipeline. Beside adding an instance of the new component, we also need to:\nAdjust the parameters when we instantiate components that originally expected the output of ExampleGen to now take the output of our new component\nAdd the new component instance to the components list when constructing the pipeline\nFigure 6 highlights these changes. Full example can be found in our GitHub repo.\ndef _create_pipeline():\n  ...\n  example_gen = CsvExampleGen(input_base=examples)\n  hello = component.HelloComponent(\n      input_data=example_gen.outputs['examples'], name=u'HelloWorld')\n  statistics_gen = StatisticsGen(examples=hello.outputs['output_data'])\n  return pipeline.Pipeline(\n      ...\n      components=[example_gen, hello, statistics_gen],\n      ...\n  )\nFigure 6. Using the new component.\nFor more information\nTo learn more about TFX check out the TFX website, join the TFX discussion group, read the TFX blog, and watch our TFX playlist on YouTube, and subscribe to the TensorFlow channel.",
    "link": "https://blog.tensorflow.org/2020/01/creating-custom-tfx-component.html",
    "imgSource": [
      "https://4.bp.blogspot.com/-ZA3N31ngUj4/XidCFK4y4HI/AAAAAAAACmw/wT_Ze6qJelQqkp7wmMfrIdS50RwU0nN7gCLcBGAsYHQ/s1600/custom-comp-figure1.png",
      "https://2.bp.blogspot.com/-m_6arSVUlb8/XidCO_zprjI/AAAAAAAACm0/zzg4f2PUdHQfQ5RhPrasnKwLqoitsxiAwCLcBGAsYHQ/s1600/custom-comp-figure2.png"
    ],
    "time": "2023/12/01 00:58:43"
  },
  {
    "title": "Fairness Indicators: Scalable Infrastructure for Fair ML Systems",
    "content": "Posted by Catherina Xu and Tulsee Doshi, Product Managers, Google Research\n\nWhile industry and academia continue to explore the benefits of using machine learning (ML) to make better products and tackle important problems, algorithms and the datasets on which they are trained also have the ability to reflect or reinforce unfair biases. For example, consistently flagging non-toxic text comments from certain groups as \u201cspam\u201d or \u201chigh toxicity\u201d in a moderation system leads to exclusion of those groups from conversation.\n\nIn 2018, we shared how Google uses AI to make products more useful, highlighting AI principles that will guide our work moving forward. The second principle \u2014 \u201cAvoid creating or reinforcing unfair bias\u201d \u2014 outlines our commitment to avoid creating or reinforcing unjust biases and impacts on people.\n\nAs part of this commitment, recently at TensorFlow World, we released a beta version of Fairness Indicators, a suite of tools that enable regular computation and visualization of fairness metrics for binary and multi-class classification, helping teams take a first step towards identifying unjust impacts. Fairness Indicators can be used to generate metrics for transparency reporting, such as those used for model cards, to help developers make better decisions about how to deploy models responsibly. Because fairness concerns and evaluations differ case by case, we also include in this release an interactive case study with Jigsaw\u2019s Unintended Bias in Toxicity dataset to illustrate how Fairness Indicators can be used to detect and remediate bias in a production machine learning (ML) model, depending on the context in which it is deployed. Fairness Indicators is now available in beta for you to try for your own use cases.\nWhat is ML Fairness?\n\nBias can manifest in any part of a typical machine learning pipeline, from an unrepresentative dataset, to learned model representations, to the way in which the results are presented to the user. Errors that result from this bias can disproportionately impact some users more than others.\n\nTo detect this unequal impact, evaluation over individual slices, or groups of users, is crucial as overall metrics can obscure poor performance for certain groups. These groups may include, but are not limited to, those defined by sensitive characteristics such as race, ethnicity, gender, nationality, income, sexual orientation, ability, and religious belief. However, it is also important to keep in mind that fairness cannot be achieved solely through metrics and measurement; high performance, even across slices, does not necessarily prove that a system is fair. Rather, evaluation should be viewed as one of the first ways, especially for classification models, to identify gaps in performance.\nThe Fairness Indicators Suite of Tools\nThe Fairness Indicators tool suite enables computation and visualization of commonly-identified fairness metrics for classification models, such as false positive rate and false negative rate, making it easy to compare performance across slices or to a baseline slice. The tool computes confidence intervals, which can surface statistically significant disparities, and performs evaluation over multiple thresholds. In the UI, it is possible to toggle the baseline slice and investigate the performance of various other metrics. The user can also add their own metrics for visualization, specific to their use case.\n\nFurthermore, Fairness Indicators is integrated with the What-If Tool (WIT) -- clicking on a bar in the Fairness Indicators graph will load those specific data points into the the WIT widget for further inspection, comparison, and counterfactual analysis. This is particularly useful for large datasets, where Fairness Indicators can be used to identify problematic slices before WIT is used for a deeper analysis.\nUsing Fairness Indicators to visualize metrics for fairness evaluation.\nClicking on a slice in Fairness Indicators will load all the data points in that slice inside the What-If Tool widget. In this case, all data points with the \u201cfemale\u201d label are shown.\nThe Fairness Indicators beta launch includes the following:\npip package: Includes Tensorflow Model Analysis (TFMA), Fairness Indicators, Tensorflow Data Validation (TFDV), What-If Tool, and example Colabs:\nFairness Indicators Example Colab \u2014 an introduction to Fairness Indicators usage\nFairness Indicators for TensorBoard \u2014 a TensorBoard plug-in usage example\nFairness Indicators with TFHub Embeddings \u2014 a Colab that investigates the effects of different embeddings on downstream fairness metrics\nFairness Indicators with Cloud Vision API's Face Detection Model -- a Colab showing how Fairness Indicators can be used to generate evaluation results for model cards\nGitHub repository: Source code\nGuidance for usage: Fairness is highly contextual, and it\u2019s important to carefully think through each use case and potential implications for users. This document provides guidance for selecting groups and metrics, and highlights evaluation best practices.\nCase Study: Interactive case study on using Fairness Indicators, showing how Jigsaw's Conversation AI team detects bias in a classification model using the Toxic Comment Classification dataset.\nHow To Use Fairness Indicators in Models Today\nFairness Indicators is built on top of TensorFlow Model Analysis, a component of TensorFlow Extended (TFX) that can be used to investigate and visualize model performance. Based on the specific ML workflow, Fairness Indicators can be incorporated into a system in one of the following ways:\n\nIf using TensorFlow models and tools such as TFX:\nAccess Fairness Indicators as part of the Evaluator component in TFX\nAccess Fairness Indicators in TensorBoard when evaluating other real-time metrics\nIf not using existing TensorFlow tools:\nDownload the Fairness Indicators pip package, and use Tensorflow Model Analysis as a standalone tool\nFor non-TensorFlow models:\nUse Model Agnostic TFMA to compute Fairness Indicators based on the output of any model\nFairness Indicators Case Study\nWe created a case study and introductory video that illustrates how Fairness Indicators can be used with a combination of tools to detect and mitigate bias in a model trained on Jigsaw\u2019s Unintended Bias in Toxicity dataset. The dataset was developed by Conversation AI, a team within Jigsaw that works to train ML models to protect voices in conversation. Models are trained to predict whether text comments are likely to be abusive along a variety of dimensions including toxicity, insult, and sexual explicitness.\n\nThe primary use case for models such as these is content moderation. If a model penalizes certain types of messages in a systematic way (e.g., often marks comments as toxic when they are not, leading to a high false positive rate), those voices will be silenced. In the case study, we investigated false positive rate on subgroups sliced by gender identity keywords that are present in the dataset, using a combination of tools (Fairness Indicators, TFDV, and WIT) to detect, diagnose, and take steps toward remediating the underlying problem.\nWhat\u2019s next?\nFairness Indicators is only the first step. We plan to expand vertically by enabling more supported metrics, such as metrics that enable you to evaluate classifiers without thresholds, and horizontally by creating remediation libraries that utilize methods, such as active learning and min-diff. Because we believe it is important to learn through real examples, we hope to ground our work in and release more case studies over the next few months, as more features become available.\n\nTo get started, see the Fairness Indicators GitHub repo. For more information on how to think about fairness evaluation in the context of your use case, see this link.\n\nWe would love to partner with you to understand where Fairness Indicators is most useful, and where added functionality would be valuable. Please reach out at tfx@tensorflow.org to provide any feedback on your experience!\nAcknowledgements\nThe core team behind this work includes Christina Greer, Manasi Joshi, Huanming Fang, Shivam Jindal, Karan Shukla, Osman Aka, Sanders Kleinfeld, Alicia Chang, Alex Hanna, and Dan Nanas. We would also like to thank James Wexler, Mahima Pushkarna, Meg Mitchell and Ben Hutchinson for their contributions to the project.",
    "link": "https://blog.tensorflow.org/2019/12/fairness-indicators-fair-ML-systems.html",
    "imgSource": [
      "https://2.bp.blogspot.com/-8QvtLgzaEdc/XfAu-A-LAFI/AAAAAAAABhA/o83cd8unZJcEa0E0lYnWzPLEZIpBT445ACLcBGAsYHQ/s1600/MLpipeline.png",
      "https://1.bp.blogspot.com/-BHcKzUfIRYI/XfBOJv8GuTI/AAAAAAAABks/8rfNLL6tkcQlZkkOotHq5783OnSpALP1wCLcBGAsYHQ/s1600/Screen%2BShot%2B2019-11-26%2Bat%2B4.20.42%2BPM.png",
      "https://3.bp.blogspot.com/-XflYgMF1UOU/XfBHHFgOBiI/AAAAAAAABjU/5bPjj5cna-c8g_0bBk4mzYGBBSwTBn31gCLcBGAsYHQ/s1600/graph2.png"
    ],
    "time": "2023/12/01 00:58:43"
  },
  {
    "title": "Introducing the TFX interactive notebook",
    "content": "Posted by Charles Chen, Joe Lee, and Kenny Song on behalf of the TFX team\n\n\nRun TFX in Google Colab\nTensorFlow Extended (TFX) is a platform for creating end-to-end machine learning pipelines. TFX was created by Google to provide the backbone of our own ML applications and services, and we\u2019re steadily open-sourcing TFX to enable other companies and teams to easily build production-grade ML systems (learn more in this blog post).\n\nIn TFX 0.15, we\u2019re excited to release a faster way to start using TFX. Now you can build, debug, and run your TFX pipeline inside an interactive Google Colab or Jupyter notebook! Within this notebook environment, you can run TFX component-by-component, which makes it easier to iterate and experiment on your ML pipeline.\n\nTo get started, this new Colab-based TFX tutorial contains all TFX components, requires no setup, and runs all in your browser! It\u2019s free to use, so try out TFX in a Colab and send us your feedback!\nRun TFX in Google Colab\nWhen you\u2019re done developing your pipeline in-notebook, you can convert the notebook code to a pipeline file that can be orchestrated with Apache Airflow or Apache Beam (export to Kubeflow Pipelines coming soon). We recommend this export path for productionizing your TFX pipeline: notebooks are for experimentation, while pipelines are for production.\n\nA key difference between experimentation and production is how you run components. In a production setting, an orchestration engine such as Apache Airflow will execute components for you. During experimentation, the human (you!) running the notebook cells is the orchestrator. The magic that enables this is the InteractiveContext, which manages component execution and state in the notebook.\ncontext = InteractiveContext()\nFor example, here\u2019s how we can run a StatisticsGen component in a notebook. First, we instantiate a StatisticsGen component and pass in our training data (usually ingested by another TFX component, such as ExampleGen).\nstatistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])\nNext, to run the component, we simply call context.run() and run that cell.\ncontext.run(statistics_gen)\nYou\u2019re done! As you might expect from the name, StatisticsGen will generate statistics, at the feature-level, over your dataset. After the cell finishes running, you can review these statistics with a built-in TFX visualization by calling context.show().\ncontext.show(statistics_gen.outputs['statistics'])\nThe output of this function is an interactive visualization that you can explore to analyze the shapes and properties of your data.\n\nYou can run all TFX components in this way, including training a TensorFlow model in a Trainer component, and performing deep analysis of your model\u2019s performance with Tensorflow Model Analysis in an Evaluator component.\n\nThis enables fast, easy experimentation. For production, everything you write in the notebook can be converted into an orchestrate-able pipeline file by calling context.export_to_pipeline():\ncontext.export_to_pipeline(notebook_filepath=_notebook_filepath,\n                           export_filepath=_pipeline_export_filepath,\n                           runner_type=_runner_type)\nTFX provides many more components that you can use in your production ML pipelines. To learn more and try out all TFX components in a Colab notebook, check out the tutorial.\n\nWe'd also love your feedback \u2013 let us know what you think on the TFX mailing list.",
    "link": "https://blog.tensorflow.org/2019/11/introducing-tfx-interactive-notebook.html",
    "imgSource": [
      "https://3.bp.blogspot.com/-Yj75eZPLlWA/XdhGHLnRVzI/AAAAAAAABHw/9Z_pn_7oHdQx904-K5eAb1gTGqSmaYjPQCLcBGAsYHQ/s1600/tfx%2Bconfig.png",
      "https://3.bp.blogspot.com/-lB-nK8bc6T8/XdhH0_E3q7I/AAAAAAAABH8/iMUl2NmDcg8BxYl6J4XuUIcvLQ87Y51HgCLcBGAsYHQ/s1600/data.gif"
    ],
    "time": "2023/12/01 00:58:43"
  },
  {
    "title": "Creating a Custom TFX Executor",
    "content": "Posted by Kevin Haas, Zhitao Li, and Robert Crowe on behalf of the TFX team\n\nTensorFlow Extended (TFX) is a platform for creating production-ready ML pipelines. TFX was created by Google and provides the backbone of Google\u2019s ML services and applications, and we\u2019ve been open sourcing TFX for everyone who needs to create production ML pipelines.\n\nTFX can be extended and customized in several ways, including developing new components and including them in your pipeline. A TFX pipeline is a series of TFX components, each of which performs a separate task, which are sequenced as a directed acyclic graph (DAG). In this post we\u2019ll present an example to illustrate the process of developing a new TFX component. Watch for more posts that will discuss additional ways to extend and customize TFX.\nScenario\nWe want to replace the behavior of the existing TFX Trainer component with a new component that includes an executor which submits a job to run the same training on the Google Cloud Platform (GCP). Since the upstream and downstream semantics will remain unchanged, we will reuse the existing Trainer component and replace the behavior of the executor.\nAnatomy of a Component\nTFX components consist of three main pieces:\nDriver\nExecutor\nPublisher\nDriver and Publisher\nThe driver supplies metadata to the executor by querying the ML Metadata (MLMD) store and the publisher takes the results of the executor and updates the metadata store. As a developer, you will typically not need to interact with the driver and publisher directly, but messages logged by the driver and publisher may be useful during debugging.\nExecutor\nThe executor is where a component performs its processing. As a developer you write code which runs in the executor, based on the requirements of the classes which implement the type of component that you\u2019re working with. For example, when you\u2019re working on a Transform component you will need to develop a preprocessing_fn. Executors consume and create artifacts, which are kept in the metadata store.\nAdding a custom executor\nCreating a custom executor\nTo create the custom executor, we start with a copy of the current Trainer executor and then make the modifications to our custom executor to initiate a training job on Google Cloud AI Platform. Much of the basic executor structure will remain the same as the inputs, outputs, and execution parameters will be the same. The changes will be in how the inputs are processed and the outputs are generated. This is achieved by creating a new Executor class that extends tfx.components.base.base_executor.BaseExecutor and implements Do().\nclass Executor(base_executor.BaseExecutor):\n  \"\"\"Start a trainer job on Google Cloud AI Platform.\"\"\"\n\n  def Do(self, input_dict,\n         output_dict,\n         exec_properties):\n    \"\"\"Starts a trainer job on Google Cloud AI Platform.\n\nDon\u2019t forget to test it before moving on to the next step! We have created a convenience script for you to try out your executor before putting it into production. You should write similar code to exercise unit tests for your code. As with any production software deployment, when developing for TFX you should make sure to have good test coverage and a strong CI/CD framework.\nOverride the executor used by the Trainer component\nIn order to do this, we will replace the default trainer executor used by TFX with the new custom executor which will create the training job on Google Cloud AI Platform. This is achieved with the optional executor_class component parameter.\nfrom tfx.extensions.google_cloud_ai_platform.trainer\nimport executor as ai_platform_trainer_executor\n...\ntrainer = Trainer(\n    ...,\n   custom_executor_spec = executor_spec.ExecutorClassSpec(\n         ai_platform_trainer_executor.Executor)\n)\nThat\u2019s it! Now when the Trainer component is called by the workflow engine, it will run the custom executor instead of the default executor, while creating and consuming the same ML Metadata artifacts as the default executor.\nPass the component arguments to your trainer\nTFX executors are self-contained binaries focused on running a single step of the ML pipeline. Custom executors require the same three parameters as all other TFX executors: input_dict, output_dict, exec_properties. More details on the semantics of these parameters can be found in the BaseExecutor class.\nWhen processing data flowing through your TFX pipeline you will often typically want to read input data from the artifact URIs in your input_dict, and often you might want to write your output to artifact URIs from your output_dict. This may include reading and writing to more than one split, as in the case of processing with train and eval splits.\nfrom tfx.types import artifact_utils\n\ntrain_input_examples_uri = artifact_utils.get_split_uri(\n  input_dict['my_input_data'], 'train')\neval_input_examples_uri = artifact_utils.get_split_uri(\n  input_dict['my_input_data'], 'eval')\n\ntrain_output_examples_uri = artifact_utils.get_split_uri(\n  output_dict['my_output_data'], 'train')\neval_output_examples_uri = artifact_utils.get_split_uri(\n  output_dict[\u2018my_output_data'], 'eval')\nIn the example above the dictionary keys my_input_data and my_output_data are defined in the ComponentSpec for the component that you\u2019re overriding the executor for.\nclass MyComponentSpec(tfx.types.ComponentSpec):\n  PARAMETERS = {\n      <...>\n  }\n  INPUTS = {\n      'my_input_data':   \n      ChannelParameter(type=standard_artifacts.Examples),\n  }\n  OUTPUTS = {\n      'my_output_data':\n      ChannelParameter(type=standard_artifacts.Examples),                                                            \n  }\nThe splits are defined in the output Channel for the component that you\u2019re overriding the executor for, typically in the constructor:\noutput_data = tfx.types.Channel(\n  type=standard_artifacts.Examples,\n    artifacts=[\n      standard_artifacts.Examples(split=split)\n      for split in artifact.DEFAULT_EXAMPLE_SPLITS\n    ])\nspec = LabelingComponentSpec(\n  input_data=input_data,\n  my_output_data=output_data)\nAdditional parameters are passed to your custom trainer executor using a custom_config dict. These can be retrieved by the custom executor using exec_properties.get(\u2018custom_config\u2019).get(\u2018your_config_key\u2019). In the example below, all of the additional arguments needed to submit a Google Cloud AI Platform training job can be found in _ai_platform_training_args.\n_ai_platform_training_args = {\n   'pythonModule': None,  # Will be populated by TFX\n   'args': None,  # Will be populated by TFX\n   'region': _gcp_region,\n   'jobDir': os.path.join(_output_bucket, 'tmp'),\n   'project': \u2018your GCP project id\u2019\n}\n\n...\ntrainer = Trainer(\n    ...,\n    custom_config={'ai_platform_training_args':\n    _ai_platform_training_args})   \n}\nLinking the custom trainer\u2019s output to the expected output artifact\nHooking up the custom trainer to emit the expected outputs is essential to the success of downstream components. For the Google Cloud AI Platform custom trainer, we serialize the executor input parameters so they can be transmitted as part of the GCP training job. Because the Google Cloud AI Platform (CAIP) executor is redirecting the default TFX executor to run on Google Cloud AI Platform, both take the same {transformed examples, a transform_fn, and a schema} input parameters to create a TF model. The custom executor used in this example submits a CAIP training job that will invoke (via run_executor.py) the default TFX trainer as the CAIP python module, effectively opening a conduit from the local workstation to run the TFX trainer on CAIP.\n# Configure Google Cloud AI Platform job\ntraining_inputs = exec_properties.get('custom_config',\n    {}).pop('ai_platform_training_args')\nexecutor_class_path = '%s.%s' % \n    (tfx_trainer_executor.Executor.__module__,\n    tfx_trainer_executor.Executor.__name__)\n\n# Start Google Cloud AI Platform job\nreturn runner.start_cmle_training(input_dict, output_dict,\n    exec_properties, executor_class_path, training_inputs)\n\nRunning the pipeline remotely with your custom executor\nSo far we\u2019ve been assuming that your pipeline is running locally, using code available in your $PYTHONPATH. An upcoming blog post will explain how to execute custom executors packaged in containers, or as PyPI packages.\nRelated topics\nIn addition to Trainer, the TFX ExampleGen component also supports executor-level customization. ExampleGen provides a generic component and a base executor which apply ML best practices, e.g. data shuffling and consistent/configurable partitions.\nIf existing ExampleGen components don\u2019t meet your needs, create a new Apache Beam PTransform for handling the conversion from an input split to TF examples and TFX will do the rest. The ExampleGen doc has more details.\nFor more information\nTo learn more about TFX check out the TFX website, join the TFX discussion group, and watch our TFX playlist on YouTube, and subscribe to the TensorFlow channel.",
    "link": "https://blog.tensorflow.org/2019/09/creating-custom-tfx-executor_19.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-L7X4qAifxWc/XbNd0XMXP1I/AAAAAAAAALI/8wsbxWmI43sU71r4bdv92AzDML5S0MzdwCNcBGAsYHQ/s1600/tfx.png",
      "https://1.bp.blogspot.com/-butZzBkB6QI/XbNeEjIUciI/AAAAAAAAALQ/aiorMFK1q6ULwFruXShYBBVWHLrP8e1pACNcBGAsYHQ/s1600/tfx1.png"
    ],
    "time": "2023/11/30 00:58:43"
  },
  {
    "title": "TensorFlow Extended (TFX): Real World Machine Learning in Production",
    "content": "Posted by Robert Crowe, Konstantinos (Gus) Katsiapis, and Kevin Haas, on behalf of the TFX team\n\nWhen you think about machine learning, you usually only think about the great models that you can now create. After all, that\u2019s what many of the research papers are focused on. But when you want to take those amazing models and make them available to the world, you need to think about all the things that a production solution requires\u200a\u2014\u200amonitoring, reliability, validation, etc. That\u2019s why Google created TensorFlow Extended (TFX)\u200a\u2014\u200ato provide production-grade support for our machine learning (ML) pipelines. We are sharing this with the open source community so that developers everywhere can create and deploy their models on production-grade TFX pipelines.\n\nGoogle created TFX because we needed it, and there was nothing already available that could meet our needs. Google, and more generally, Alphabet, makes extensive use of ML in most of our products. In fact, TFX wasn\u2019t the first ML pipeline framework that Google created. It evolved out of earlier attempts and is now the default framework for the majority of Google\u2019s ML production solutions. Beyond Google, TFX has also had a deep impact on our partners, including Twitter, Airbnb, and PayPal.\nIt\u2019s not just about ML\nWhen you start planning for incorporating ML into an application, you have all the normal ML things to think about. This includes getting labeled data if you\u2019re doing supervised learning, and making sure that your dataset covers well the space of possible inputs. You also want to minimize the dimensionality of your feature set while maximizing the predictive information it contains. And you need to think about fairness, and make sure that your application won\u2019t be unfairly biased. You also need to consider rare conditions, especially in applications like healthcare where you might be making predictions for conditions that only occur in rare but important situations. And finally you need to consider that this will be a living solution that will evolve over time as new data flows in and conditions change, and plan for lifecycle management of your data.\n\nBut in addition to all that, you need to remember that you\u2019re putting a software application into production. That means that you still have all the requirements that any production software application has, including scalability, consistency, modularity, and testability, as well as safety and security. You\u2019re way beyond just training a model now! By themselves these are challenges for any production software deployment, and you can\u2019t forget about them just because you\u2019re doing ML. How are you going to meet all these needs, and get your amazing new model into production?\n\nThat\u2019s what TFX is all about. TFX allows you to create production ML pipelines that include many of the requirements for production software deployments and best practices. It starts with ingesting your data, and flows through data validation, feature engineering, training, evaluating, and serving. In addition to TensorFlow itself Google has created libraries for each of the major phases of an ML pipeline\u200a\u2014\u200aTensorFlow Data Validation, TensorFlow Transform, and TensorFlow Model Analysis. Google has also created frameworks for a wide range of deployment targets, including server farms (TensorFlow Serving), native mobile applications (TensorFlow Lite), and JavaScript applications (TensorFlow JS). TFX implements a series of pipeline components which leverage the libraries, and allows you to create your own components too.\n\nTo tie this all together Google has created some horizontal layers for things like pipeline storage, configuration, and orchestration. These layers are really important for managing and optimizing your pipelines and the applications that you run on them.\n\nML in production presents many challenges, and Google doesn\u2019t pretend to have all the answers. This is an evolving field in the ML community, and we welcome contributions. This paper provides a great overview of the challenges of machine learning in production environments.\nWhat are \u201cpipelines\u201d and \u201ccomponents\u201d?\nTFX pipelines are created as a sequence of components, each of which performs a different task. Components are organized into Directed Acyclic Graphs, or \u201cDAGs\u201d. But what exactly is a component?\n\nA TFX component has three main parts: a driver, an executor, and a publisher. Two of these parts\u200a\u2014\u200athe driver and publisher\u200a\u2014\u200aare mostly boilerplate code that you could change, but probably will never need to. The executor is really where you insert your code and do your customization.\n\nThe driver inspects the state of the world and decides what work needs to be done, coordinating job execution and feeding metadata to the executor. The publisher takes the results of your executor and updates the metadata store. But the executor is really where the work is done for each of the components.\n\nSo first, you need a configuration for your component, and with TFX that configuration is done using Python. Next, you need some input for your component, and a place to send our results. That\u2019s where the metadata store comes in. We\u2019ll talk more about the metadata store in a bit, but for now just be aware that for most components the input metadata will come from the metadata store, and the resulting metadata will be written back to the metadata store.\n\nSo as your data moves through your pipeline, components will read metadata that was produced by an earlier component, and write metadata that will probably be used by a later component farther down the pipeline. There are some exceptions, like at the beginning and end of the pipeline, but for the most part that\u2019s how data flows through a TFX pipeline.\nOrchestrating a TFX pipeline\nTo organize all these components and manage these pipelines, you need orchestration. But what is orchestration exactly, and how does it help you?\n\nTo put an ML pipeline together, define the sequence of components that make up the pipeline, and manage their execution, you need an orchestrator. An orchestrator provides a management interface that you can use to trigger tasks and monitor our components.\n\nIf all that you need to do is kick off the next stage of the pipeline, task-aware architectures are enough. You can simply start the next component as soon as the previous component finishes. But a task- and data-aware architecture is much more powerful, and really almost a requirement for any production system, because it stores all the artifacts from every component over many executions. Having that metadata creates a much more powerful pipeline and enables a lot of things which would be very difficult otherwise, so TFX implements a task- and data-aware pipeline architecture.\n\nOne of the ways that TFX is open and extendable is with orchestration. Google provides support for Apache Airflow and Kubeflow out of the box, but you can write code to use a different orchestrator if you need to. If you\u2019ve already got a workflow engine that you like, you can build a runner to use it with TFX.\nWhy should you store metadata?\nTFX implements a metadata store using ML-Metadata (MLMD), which is an open source library to define, store, and query metadata for ML pipelines. MLMD stores the metadata in a relational backend. The current implementation supports SQLite and MySQL out of the box, but you can write code to extend ML-Metadata for basically any SQL compatible database. But what exactly do you store in your metadata store?\n\nFirst, we store information about the models that you\u2019ve trained, the data that you trained them on, and their evaluation results. We refer to this type of metadata as \u201cartifacts\u201d, and artifacts have properties. The data itself is stored outside the database, but the properties and location of the data are kept in the metadata store.\n\nNext, we keep execution records for every component, each time it was run. Remember that an ML pipeline is often run frequently over a long lifetime as new data comes in or conditions change, so keeping that history becomes important for debugging, reproducibility, and auditing.\n\nFinally, we also include the lineage or provenance of the data objects as they flow through the pipeline. That allows you to track forward and backward through the pipeline to understand the origins and results of running our components as your data and code changes. This is really important when you need to optimize or debug our pipeline, which would be quite hard without it.\nMetadata-powered functionality\nNow that you have some idea what\u2019s in your metadata store, let\u2019s look at some of the functionality that you get from it.\n\nFirst, having the lineage or provenance of all of your data artifacts allows you to trace forward and backward in your pipeline\u200a\u2014\u200afor example, to see what data our model was trained with, or what impact some new feature engineering had on your evaluation metrics. In some use cases, this ability to trace the origins and results of your data may even be a regulatory or legal requirement.\n\nRemember that it\u2019s not just for today\u2019s model or today\u2019s results. You\u2019re likely also interested in understanding how your data and results change over time as you take in new data and retrain your model. You often want to compare to model runs that you ran yesterday, or last week, to understand why your results got better or worse. Production solutions aren\u2019t one-time things, they live for as long as you need them, and that can be months or years.\n\nYou can also make your pipeline much more efficient by only rerunning components when necessary, and using a warm start to continue training. Remember that you\u2019re often dealing with large datasets that can take hours or days to run. If you\u2019ve already trained your model for a day and you want to train it some more, you can start from where you left off instead of starting over from the beginning. That\u2019s much easier if you\u2019ve saved information about our model in metadata.\n\nYou can also make the other components of your pipeline much more efficient by only rerunning them when the input or code has changed. Instead of rerunning the component again, you can just pull the previous results from cache. For example, if a new run of the pipeline only changes parameters of the trainer, then the pipeline can reuse any data-preprocessing artifacts such as vocabularies\u200a\u2014\u200aand this can save a lot of time given that large data volumes make data preprocessing expensive. With TFX and MLMD this reuse comes out of the box, while you see a simpler \u201crun pipeline\u201d interface and don\u2019t have to worry about manually selecting which components to run. Again, that can save you hours of processing. That\u2019s much easier if you\u2019ve saved our component\u2019s input and results in metadata.\nComponents? What kind of components?\nSo now that you have an orchestrator, let\u2019s talk about the components that come standard with TFX.\nBut first, let\u2019s talk about Apache Beam\nBut before we talk about the standard components, let\u2019s talk about Apache Beam. To handle distributed processing of large amounts of data, especially compute intensive data like ML workloads, you really need a distributed processing pipeline framework like Apache Spark, or Apache Flink, or Google Cloud Dataflow. Most of the TFX components run on top of Apache Beam, which is a unified programming model that can run on several execution engines. Beam allows you to use the distributed processing framework you already have, or choose one that you like, rather than forcing you to use the one that we chose. Currently Beam Python can run on Flink, Spark, and Dataflow runners, but new runners are being added. It also includes a direct runner, which enables you to run a TFX pipeline in development on your local system, like your laptop.\nStandard components\n\nTFX includes a fairly complete set of standard components when you first install it, each designed for a different part of a production ML pipeline. The Transform component for example uses Apache Beam to perform feature engineering transformations, like creating a vocabulary or doing primary component analysis (PCA). Those transformations could be running on your Flink or Spark cluster, or on the Google Cloud using Dataflow. Thanks to the portability of Apache Beam you could migrate between them without changing your code.\n\nThe Trainer component really just uses TensorFlow. Remember when all you were thinking about was training your amazing model? That\u2019s the code you\u2019re using here. Note that currently TFX only supports tf.estimators. Other information on compatibility is listed here. Some components are very simple. The Pusher component for example only needs Python to do its job.\n\nWhen you put all these together, and manage it all with an orchestrator, you have a TFX pipeline. On one end you\u2019re ingesting your data, and on the other you\u2019re pushing your SavedModels to one or more of your deployment targets. That includes model repositories like TensorFlow Hub, or JavaScript environments using TensorFlow JS, or native mobile applications using TensorFlow Lite, or server farms using TensorFlow Serving, or all of the above.\n\nNow let\u2019s look at each of these components in a little more detail.\nRead in data\nFirst, you ingest your input data using ExampleGen. ExampleGen is one of the components that runs on Beam. It reads in data from various supported sources and types, splits it into training and eval, and formats it as tf.examples. The configuration for ExampleGen is very simple, just two lines of Python.\n\nNext, StatisticsGen makes a full pass over the data using Beam, one full epoch, and calculates descriptive statistics for each of your features. To do that it leverages the TensorFlow Data Validation (TFDV) library, which includes support for some visualization tools that you can run in a Jupyter notebook. That lets you explore and understand your data, and find any issues that you may have. This is typical data wrangling stuff, the same thing we all do when we\u2019re preparing our data to train our model.\n\nThe next component, SchemaGen, also uses the TensorFlow Data Validation library. It looks at the statistics which were generated by StatisticsGen and tries to infer the basic properties of your features, including data types of feature values, value ranges, and categories. You should examine and adjust the schema as needed, for example adding new categories that you expect to see.\n\nThe next component, ExampleValidator, takes the statistics from StatisticsGen and the schema (which may be the output of SchemaGen or the result of user curation) and looks for problems. It looks for different classes of anomalies, including missing values or values that don\u2019t match your schema, training-serving skew, and data drift, and produces a report of what it finds. Remember that you\u2019re taking in new data all the time, so you need to be aware of problems when they pop up.\nFeature engineering\nTransform is one of the more complex components, and requires a bit more configuration as well as additional code. Transform uses Beam to do feature engineering, applying transformations to your features to improve the performance of your model. For example, Transform can create vocabularies, or bucketize values, or run PCA over your input. The code that you write depends on what feature engineering you need to do for your model and dataset.\n\nTransform will make a full pass over your data, one full epoch, and create two different kinds of results. For things like calculating the median or standard deviation of a feature, numbers which are the same for all examples, Transform will output a constant. For things like normalizing a value, values which will be different for different examples, Transform will output TensorFlow Ops.\n\nTransform will then output a TensorFlow graph with those constants and ops. That graph is hermetic, so it contains all of the information you need to apply those transformations, and will form the input stage for your model. That means that the same transformations are applied consistently between training and serving, which eliminates training/serving skew. If instead you\u2019re moving your model from a training environment into a serving environment or application, and trying to apply the same feature engineering in both places, you hope that the transformations are the same but sometimes you find that they\u2019re not. We call that training/serving skew, and Transform eliminates it by using exactly the same code anywhere you run your model.\nTraining your model\nNow you\u2019re finally ready to train your model, the part of the process that you often think about when you think about machine learning. Trainer takes in the transform graph and data from Transform, and the schema from SchemaGen, and trains a model using your modeling code. This is normal model training, but when training is complete Trainer will save two different SavedModels. One is a SavedModel that will be deployed to production, and the other is an EvalSavedModel that will be used for analyzing the performance of your model.\n\nThe configuration for Trainer is what you\u2019d expect, things like the number of steps and whether or not to use warm starting. The code that you create for Trainer is your modeling code, so it can be as simple or complex as you need it to be.\n\nTo monitor and analyze the training process you can use TensorBoard, just like you would normally. In this case you can look at the current model training run or compare the results from multiple model training runs. This is only possible because of the ML-Metadata store, which was discussed above. TFX makes it fairly easy to do this kind of comparison, which is often revealing.\nGood enough?\nNow that you\u2019ve trained your model, how do the results look? The Evaluator component will take the EvalSavedModel that Trainer created, and the original input data, and do deep analysis using Beam and the TensorFlow Model Analysis library. It\u2019s not just looking at the top level results across your whole dataset. It\u2019s looking deeper than that, at individual slices of your dataset. That\u2019s important, because the experience that each user of your model has will depend on their individual data point.\nYour model may do well over your entire dataset, but if it does poorly on the datapoint that a user gives it, that user\u2019s experience is poor. We\u2019ll talk about this more in a future post.\n\nSo now that you\u2019ve looked at our model\u2019s performance, should you push it to production? Is it better or worse than what you already have in production? You probably don\u2019t want to push a worse model just because it\u2019s new. So the ModelValidator component uses Beam to do that comparison, using criteria that you define, to decide whether or not to push the new model to production.\n\nIf ModelValidator decides that your new model is ready for production, then Pusher does the work of actually pushing it to your deployment targets. Those targets could be TensorFlow Lite if you\u2019re doing a mobile application, or TensorFlow JS if you\u2019re deploying to a JavaScript environment, or TensorFlow Serving if you\u2019re deploying to a server farm, or all of the above.\nWhere do you go from here?\nThe goal of this post was to give you a basic overview of TFX and ML pipelines in general, and to introduce the main concepts. In the posts to follow we\u2019ll dig deeper into TFX, including discussing ways that you can extend TFX to make it fit your needs. TFX is open source, so Google is also encouraging the software and ML community to help us make it better. A good place to start would be to try the TFX developer tutorial!\n\nRelated topics\n\nTensorFlow Extended: Machine Learning Pipelines and Model Understanding (Google I/O\u201919)\nTFX: A TensorFlow-Based Production-Scale Machine Learning Platform (KDD 2017)\nTFX on YouTube",
    "link": "https://blog.tensorflow.org/2019/06/tensorflow-extended-tfx-real-world_26.html",
    "imgSource": [
      "https://3.bp.blogspot.com/-bQBziQmr03s/XSPOWqFwDmI/AAAAAAAAAJk/a9McZk1bidEdIbdrzo7AE5ThoM4QLcPbQCLcBGAs/s1600/1.png",
      "https://2.bp.blogspot.com/-_ErebmqxVuM/XSPPkaAaksI/AAAAAAAAAJw/R6c0CumjnYoX5mDllnNuQymEgqVzMhGsQCLcBGAs/s1600/2.png",
      "https://3.bp.blogspot.com/-2ziNBOuPIBQ/XSPP6SDoyDI/AAAAAAAAAJ4/zMgWxfUdpz0BbzGixVH6xHS-EvEEJppEwCLcBGAs/s1600/3.png"
    ],
    "time": "2023/11/30 00:58:43"
  },
  {
    "title": "From Research to Production with TFX Pipelines and ML Metadata",
    "content": "Posted by Jarek Wilkiewicz on behalf of the TFX team\n\nIf your code runs in production, you probably are already familiar with version control / software configuration management (SCM), continuous integration and continuous deployment (CI/CD) as well as many other software engineering best practices. These took years to develop and now we often take them for granted. Much like how writing an efficient algorithm implementation is just the beginning a software engineer\u2019s journey, machine learning (ML) model code typically represents only 5% of the overall system\u00b9 required to deploy it to production. At Google, we\u2019ve also been working on improving the remaining 95% over many years\u00b2. A fruit of our labour, TensorFlow Extended (TFX\u00b3), aims to introduce the benefits of software engineering discipline to the fast growing space of ML. In an upcoming series of blog posts, we\u2019ll highlight what\u2019s new in TFX and show you how TFX can help you build and deploy your ML models to production environments.\n\nUntil recently only the underlying TFX libraries (TensorFlow Data Validation, TensorFlow Transform, TensorFlow Model Analysis, TensorFlow Serving) were available in open source which meant that developers still had to build their own ML pipeline components using the libraries. You can now create a complete TFX ML pipeline using several ready-made components, configure them for many typical ML use cases with a high level Python API, and execute them with an orchestration system of your choice such as Apache Airflow or Kubeflow as shown in the figure below.\n\nWhen the TFX pipeline executes, ML Metadata (MLMD, another Google open source project) keeps track of artifacts pipeline components depend upon (e.g. training data) and produce (e.g. vocabularies and models). ML Metadata is available as a standalone library and has also been integrated with TFX components for your convenience. MLMD allows you to discover the lineage of an artifact (for example what data a model was trained on), find all artifacts created from an artifact (for example all models trained on a specific dataset), and enables many other use cases.\n\nTo better understand how this all fits together check out the Google I/O \u201919 presentation: \u201cTensorFlow Extended (TFX): Machine Learning Pipelines and Model Understanding\u201d.\n\n\n\nIn our next TFX blog post, we will describe the TFX pipeline components in more detail. Until then, please try the TFX developer tutorial. You\u2019ll follow a typical ML development process, starting by examining the dataset, and ending up with a complete working ML pipeline. If you have TFX questions please reach us on Stack Overflow, bug reports and pull requests are always welcome on GitHub, and we invite general discussion at tfx@tensorflow.org.\n\n- - - - - - - -\n\n[1] Sculley, D., Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young, Jean-Fran\u00e7ois Crespo and Dan Dennison. \u201cHidden Technical Debt in Machine Learning Systems.\u201d NIPS (2015).\n\n[2] Tushar Chandra, \u201cSibyl: A System for Large Scale Machine Learning at Google\u201d, IEEE DSN (Dependable Systems and Networks) conference keynote, Atlanta, GA, June 25th 2014.\n\n[3] Denis Baylor, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo, Zakaria Haque, Salem Haykal, Mustafa Ispir, Vihan Jain, Levent Koc, Chiu Yuen Koo, Lukasz Lew, Clemens Mewald, Akshay Naresh Modi, Neoklis Polyzotis, Sukriti Ramesh, Sudip Roy, Steven Euijong Whang, Martin Wicke, Jarek Wilkiewicz, Xin Zhang, and Martin Zinkevich. 2017. TFX: A TensorFlow-Based Production-Scale Machine Learning Platform. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD \u201817). ACM, New York, NY, USA, 1387\u20131395. DOI: https://doi.org/10.1145/3097983.3098021.",
    "link": "https://blog.tensorflow.org/2019/05/research-to-production-with-tfx-ml.html",
    "imgSource": [
      "https://2.bp.blogspot.com/-bAi9PTHhS_A/XdZJ89UhUII/AAAAAAAABFE/NbSy0gqGRW4AVCEmC6QhSu---NbVmUWcQCLcBGAsYHQ/s1600/0_4wQdCndV1ame3Bpm.png"
    ],
    "time": "2023/11/30 00:58:43"
  },
  {
    "title": "Introducing TensorFlow Model Analysis: Scaleable, Sliced, and Full-Pass Metrics",
    "content": "Posted by Clemens Mewald, Product Manager for TFX\n\nToday we\u2019ve launched TensorFlow Model Analysis (TFMA), an open-source library that combines the power of TensorFlow and Apache Beam to compute and visualize evaluation metrics. Before deploying any machine learning (ML) model, ML developers need to evaluate it to ensure that it meets specific quality thresholds and behaves as expected for all relevant slices of data. Additionally, this computation should seamlessly scale from a small dataset that fits into memory to large, distributed computation. In this post we will give an overview of TFMA and how developers can use it to address all of the aforementioned challenges.\nNot all evaluation metrics are created equal\u2026\nBefore we jump into how TFMA works, we will first look into how it differs from the evaluation metrics that can already be computed and observed in TensorBoard.\nDuring training vs after training\nTensorFlow metrics that are visualized in TensorBoard are computed during a training run and show how a metric changes against global training steps (across all training workers). This can answer questions such as \u201cIs my model converging?\u201d.\n\nTFMA exports a SavedModel containing the eval graph and additional metadata to compute metrics, which means it computes metrics once using the exported model. It is important to evaluate performance on the final model because that\u2019s the model that will be deployed.\nFigure 1: TensorBoard visualizes streaming metrics that are computed from checkpoints. TFMA computes and visualizes metrics using an exported SavedModel.\nOne model vs multiple models over time\nTensorBoard is commonly used to inspect the training progress of a single model. It can also be used to visualize metrics for more than one model, with performance for each plotted against their global training steps as they are training.\n\nTFMA also allows developers to visualize model metrics over time in a time series graph. The difference between TensorBoard and TFMA lies within the horizontal axis. TensorBoard visualizes streaming metrics of multiple models over global training steps, whereas TFMA visualizes metrics computed for a single model over multiple versions of the exported SavedModel.\nFigure 2: TensorBoard can overlay streaming metrics of multiple models over global training steps. TFMA only shows metrics computed based on the exported SavedModel, but can visualize those as a time series across multiple model versions.\nAggregate vs sliced metrics\nMost model evaluation results look at aggregate metrics. A model may have an acceptable AUC over the entire eval dataset, but underperform on specific slices. In general, a model with good performance \u201con average\u201d may exhibit failure modes that are not apparent by looking at an aggregate metric.\n\nSlicing metrics allows us to analyze the performance of a model on a more granular level. This functionality enables developers to identify slices where examples may be mislabeled, or where the model over- or under-predicts. For example, TFMA could be used to analyze whether a model that predicts the generosity of a taxi tip works equally well for riders that take the taxi during day hours vs night hours (if sliced by the feature hour).\nFigure 3: TFMA allows us to slice a metric by different segments of our eval dataset, enabling more fine grained analysis of a model and how it performs on different slices.\nStreaming vs full-pass metrics\nTensorFlow metrics that are visualized in TensorBoard are commonly computed on a mini-batch basis during training. They are called \u201cstreaming metrics\u201d and are approximations based on those observed mini-batches.\n\nTFMA uses Apache Beam to do a full pass over the specified evaluation dataset. This not only allows more accurate calculation of metrics, but also scales up to massive evaluation datasets, since Beam pipelines can be run using distributed processing back-ends.\n\nNote that TFMA computes the same TensorFlow metrics that are computed by the TensorFlow eval worker, just more accurately by doing a full pass over the specified dataset. TFMA can also be configured to compute additional metrics that were not defined in the model.\n\nFurthermore, if evaluation datasets are sliced to compute metrics for specific segments, each of those segments may only contain a small number of examples. To compute accurate metrics, a deterministic full pass over those examples is important.\nHow does TensorFlow Model Analysis work?\nWhen designing the API for TFMA, we paid particular attention to the developer workflow and to minimize the amount of information needed to enable this additional functionality. As a result, TFMA simply requires developers to export a separate evaluation graph from the trainer into its own SavedModel. TFMA uses the graph in this SavedModel to compute (sliced) metrics and provides visualization tools to analyze those metrics.\nExporting the eval graph\nIf developers use TensorFlow Estimators, they are already familiar with exporting a SavedModel for TensorFlow Serving, using the export_savedmodel method. TFMA provides an analogous export_eval_savedmodel method that exports a SavedModel containing the eval metrics and all additional information needed to compute them. The eval_input_fn may also contain analysis-only features, i.e. features that were not trained on but that are used for slicing the eval metrics.\n# use TFMA to export an eval graph from the TensorFlow Estimator\ntfma.export.export_eval_savedmodel(estimator=estimator,\n                        eval_input_receiver_fn=eval_input_fn, \u2026)\nComputing (sliced) metrics\nOnce the evaluation SavedModel has been exported, developers can either immediately run TFMA, which will compute the metrics that have been configured in the Estimator, or additionally configure slices they want to have computed. The code example below shows how to configure a feature for slicing.\n\nTFMA uses Apache Beam to evaluate the model using the entire dataset. We use the Beam SDK to define a data-parallel processing pipeline which can be run using several distributed processing back-ends (or runners). On a local machine (or in a notebook), a DirectRunner can be used to run this pipeline locally. One of the major benefits of using the Beam SDK is that the same data processing pipeline can be scaled up on different backends, e.g. in the cloud with the DataflowRunner.\n# Specify the path to the eval graph and to where the result should be written\neval_model_dir = \u2026\nresult_path = \u2026\n# specify slicing spec\nslice_spec = [slicer.SingleSliceSpec(columns=[\u2018column_name\u2019])]\n# configure Beam pipeline\nwith beam.Pipeline(\u2026) as pipeline:\n  raw_data = pipeline | \u2018ReadData\u2019 >> beam.io.Read(\u2026)\n  # run TFMA Beam job\n  _ = raw_data | \u2018EvaluateAndWriteResults\u2019 >> tfma.EvaluateAndWriteResults(eval_model_dir,\n                                slice_spec,\n                                result_path)\nVisualize metrics in Jupyter\n# load evaluation results\nresult = tfma.load_eval_result(result_path)\n# render results\ntfma.viewer.render_slicing_metrics(result)\nThe render_slicing_metrics call loads the slicing browser component in the Jupyter notebook (see Figure 4) and allows developers to inspect the results of the TFMA eval run.\nFigure 4: Slicing browser component loaded in a Jupyter notebook. It allows developers to display a histogram of metrics sliced by the feature value and to identify underperforming slices.\nThe animation in Figure 4 demonstrates a workflow of how to identify a slice with the lowest AUC by first selecting \u201cMetrics Histogram\u201d for visualization, filtering to show only slices with 100 or more examples, selecting the metric \u201cauc\u201d, sorting the table by \u201cauc\u201d and selecting the row with the lowest AUC. In this example it is the slice with the feature value 2 for trip_start_hour (using the model and data from our end-to-end example). Once an interesting slice has been identified, developers can investigate whether this is expected or needs to be mitigated.\nTry out TensorFlow Model Analysis today!\nWe\u2019ve open-sourced TFMA and published it on GitHub at github.com/tensorflow/model-analysis under the Apache 2.0 License. This release includes everything needed for exporting an evaluation SavedModel, computing sliced metrics using Apache Beam, and visualizing them in a Jupyter notebook.\n\nWe\u2019ve also published an extensive end-to-end example, showcasing how developers can use TensorFlow Transform, TensorFlow Estimators, TensorFlow Model Analysis, and TensorFlow Serving together.\n\nTo help developers get started, we suggest reading and trying out the end-to-end example on GitHub.\nWhat\u2019s next for TensorFlow Model Analysis\nAs mentioned in the introduction, developers can use TFMA for evaluating model quality against thresholds, analyzing model behavior on different slices of data, and identifying failure modes that need to be addressed either via data cleaning or improved modeling.\n\nWith this release we make TFMA available to the TensorFlow community to analyze TensorFlow models and improve the quality of models that are deployed. We will soon add the capability of computing metrics that cannot be computed in a TensorFlow graph, e.g. ranking metrics.\n\nIn addition to being able to run TFMA locally (with the DirectRunner) and on Google Cloud (with the DataflowRunner), the Apache Flink and Apache Beam communities are nearing completion of a Flink Runner. Follow the corresponding JIRA ticket, Apache Beam blog, or mailing lists to get notifications about availability of the Flink Runner. The community has also started work on the Spark Runner which will be available in a few months.\n\nAt Google, we use Tensorflow Model Analysis as part of a larger ML platform called TensorFlow Extended (TFX). In continuously running TFX pipelines we use it for automated validation of TensorFlow models and interactive model analysis workflows (e.g. using the slicing browser component shown in Figure 4.) As announced at the TensorFlow Dev Summit, we will be open sourcing more of TFX in the future. Stay tuned to the TensorFlow Blog for an upcoming TFX post, and keep in touch by following TensorFlow on Twitter.",
    "link": "https://blog.tensorflow.org/2018/03/introducing-tensorflow-model-analysis.html",
    "imgSource": [
      "https://4.bp.blogspot.com/-0LitVILcq-4/XhJ_GyT-vVI/AAAAAAAACMQ/dm4QcOMH5qsI5msjXK39CR83kv-3dZ_nwCLcBGAsYHQ/s1600/fig1.png",
      "https://1.bp.blogspot.com/-r_W-3aEDBk0/XhJ_aZw7V5I/AAAAAAAACMY/bFI5GnvpbDQZvZQPxcBov2KjRAXLbNGRQCLcBGAsYHQ/s1600/fig2.png",
      "https://2.bp.blogspot.com/-bOnJWk8HDFk/XhJ_nb2wbiI/AAAAAAAACMc/ti4u2sTXvigb3PMZMo7oRxv68mhdth4QgCLcBGAsYHQ/s1600/fig3.png",
      "https://2.bp.blogspot.com/-sR1iiCvsiQU/XhKAi_q24-I/AAAAAAAACMs/eFdeisrXEL8d-XR33z3X2hZlKBjfKStIQCLcBGAsYHQ/s1600/fig4.gif"
    ],
    "time": "2023/11/30 00:58:43"
  },
  {
    "title": "Join us at the third Women in ML Symposium!",
    "content": "Posted by Sharbani Roy \u2013 Senior Director, Product Management, Google\n\nWe're back with the third annual Women in Machine Learning Symposium on December 7, 2023! Join us virtually from 9:30 am to 1:00 pm PT for an immersive and insightful set of deep dives for every level of Machine Learning experience.\n\nThe Women in ML Symposium is an inclusive event for anyone passionate about the transformative fields of Machine Learning (ML) and Artificial Intelligence (AI). Dive into the latest advancements in generative AI, explore the intricacies of privacy-preserving AI, dig into the underlying accelerators and ML frameworks that power models, and uncover practical applications of ML across multiple industries.\nOur event offers sessions for all expertise levels, from beginners to advanced practitioners. Hear about what\u2019s new in ML and building with Google AI from our keynote speakers, gain insights from seasoned industry leaders across Google Health, Nvidia, Adobe, and more \u2013 and discover a wealth of knowledge on topics ranging from foundational AI concepts to open source tools, techniques, and beyond.\nRSVP today to secure your spot and explore our exciting agenda. We can't wait to see you there!",
    "link": "https://blog.tensorflow.org/2023/11/join-us-at-third-women-in-ml-symposium.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtd2GtibClrQS_fEHuU5Y8j25qKKsKNxCet5OOhia8zI09w7WmspXpsXQIV7I7HTNPtEqkP7wnlmE-gSj05yrIWaFKxYiocXH-pOsVen-Aq2dPb7AgtMz4lmGzHnXiLerzl8IHcajiwrkJkP-wIV5IPppznk_3BCi2AWQzcv4cW9B0AaXGIgzDCRFmiQE/s1600/WiML-2023-Social.png"
    ],
    "time": "2023/11/30 00:58:43"
  },
  {
    "title": "Get ready for Google I/O",
    "content": "Posted by Timothy Jordan, Director, Developer Relations & Open Source\nI/O is just a few days away and we couldn\u2019t be more excited to share the latest updates across Google\u2019s developer products, solutions, and technologies. From keynotes to technical sessions and hands-on workshops, these announcements aim to help you build smarter and ship faster.\nHere are some helpful tips to maximize your experience online.\nStart building your personal I/O agenda\nStarting now, you can save the Google and developer keynotes to your calendar and explore the program to preview content. Here are just a few noteworthy examples of what you\u2019ll find this year:\nWhat's new in Android\nGet the latest news in Android development: Android 14, form factors, Jetpack + Compose libraries, Android Studio, and performance.\nWhat\u2019s new in Web\nExplore new features and APIs that became stable across browsers on the Web Platform this year.\nWhat\u2019s new in Generative AI\nDiscover a new suite of tools that make it easy for developers to leverage and build on top of Google's large language models.\nWhat\u2019s new in Google Cloud\nLearn how Google Cloud and generative AI will help you develop faster and more efficiently.\nFor the best experience, create or connect a developer profile and start saving content to My I/O to build your personal agenda. With over 200 sessions and other learning material, there\u2019s a lot to cover, so we hope this will help you get organized.\nThis year we\u2019ve introduced development focus filters to help you navigate content faster across mobile, web, AI, and cloud technologies. You can also peruse content by topic, type, or experience level so you can find what you\u2019re interested in, faster.\nConnect with the community\nAfter the keynotes, you can talk to Google experts and other developers online in I/O Adventure chat. Here you can ask questions about new releases and learn best practices from the global developer community.\nIf you\u2019re craving community now, visit the Community page to meet people with similar interests in your area or find a watch party to attend.\nWe hope these updates are useful, and we can\u2019t wait to connect online in May!",
    "link": "https://blog.tensorflow.org/2023/04/get-ready-for-google-io.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEizjsHSPpRYoieEo3e820PnUGyL1A4AAAFlLNQDFhgPMoyjOP76apEK60exJ4Qu1IXqLfMvMmiEMVzuND8FnoE-8rVpi2_0K4AAERvnMgK2TRcbFYiyVosE2U6QtkiBcMLg3_wIHLvzbW8kCiicgwWkVQjhSb5ZB8gxlcfLa06ann-HKJ9rghUGZxz3/s1600/278944847__42532861__715727.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgFVMIlCP4As_DCgFpzemOv1Zs19s40uVLFgOV1n7k3UD7yxlit1lTDPaZLaTyJzf0Wrd9T4lx3KE09cM8bnVGkn0xvvR80KlyMe0Ri2ymrNYOWzmQmYnvegpiZIrPJ9vXH4ZeRvR-zz8-SIDQwz3D5DoVmbkv5pkzKzIr5aJgrRpsKmwF8Ohyi4745/s1600/278944847__42532860__715727.png"
    ],
    "time": "2023/11/30 00:58:43"
  },
  {
    "title": "Enabling Optimal Inference Performance on AMD EPYC\u2122 Processors with the ZenDNN Library",
    "content": "Posted by Sarina Sit, AMD\nAMD launched the 4th Generation of AMD EPYC\u2122 processors in November of 2022. 4th Gen AMD EPYC processors include numerous hardware improvements over the prior generation, such as AVX-512 and VNNI instruction set extensions, that are well-suited for improving inference performance. However, hardware is only one piece of the puzzle; software is a crucial component for effectively taking advantage of the underlying hardware.\nWe are happy to announce the new availability of the TensorFlow-ZenDNN plug-in for TensorFlow v2.12 and above, which represents the ongoing and focused effort by AMD to improve the accessibility of ZenDNN optimizations for the community via framework upstreaming. This plug-in enables neural network inferencing on AMD EPYC CPUs with the AMD ZenDNN library. \nZenDNN \nZenDNN, which is available open-source from GitHub, is a low-level AMD deep neural network library that includes basic neural network building blocks optimized for AMD EPYC CPUs. ZenDNN is purpose-built to help deep learning application and framework developers improve inference performance on AMD EPYC CPUs across an array of workloads, including computer vision, natural language processing, and recommender systems.\nTF-ZenDNN \nWe have integrated ZenDNN into high-level AI frameworks for ease of use. Our prototype integration with TensorFlow, called TF-ZenDNN, is done by forking the TensorFlow repository at a specific version and directly modifying TensorFlow code. TF-ZenDNN is available as a binary package for direct integration from AMD's ZenDNN developer resources page (diagram 1 below), with installation instructions available in our TensorFlow + ZenDNN User Guide.\nDiagram 1. The ZenDNN v4.0 binary package available on our ZenDNN developer resources page is referred to in this blog as our TF-ZenDNN direct integration version.\nTF-ZenDNN optimizes graphs at the network level and provides tuned primitive implementations at a library level, including Convolution, MatMul, Elementwise, and Pooling (Max and Average). We have seen performance benefits across a variety of neural network models, including the breadth of convolutional neural networks depicted by the orange line below in Graph 1. Optimizing Tencent's AI Applications with the ZenDNN AI Inference Library and TF-ZenDNN impact on TinyDefectNet demonstrates the high performance of ZenDNN and its integration with TensorFlow, respectively. \nGraph 1. Performance uplift of the TensorFlow-ZenDNN plug-in v0.1 and TF-ZenDNN direct integration v4.0 compared to TF-vanilla (without ZenDNN). As optimizations continue to be added to the TensorFlow-ZenDNN plug-in, the extent of performance uplift is expected to compare to that of TF-ZenDNN direct integration. Please see endnotes ZD-045 through ZD-051 at the end of this blog.\nTensorFlow-ZenDNN Plug-in \nTF-ZenDNN direct integration, as in the binary form described in the section above, requires significant changes in the TensorFlow code. Upstreaming such changes to the TensorFlow repository would be cumbersome and unsustainable. TensorFlow v2.5 provides a PluggableDevice mechanism that enables modular, plug-and-play integration of device-specific code. AMD adopted PluggableDevice when implementing the TensorFlow-ZenDNN plug-in for AMD EPYC CPUs. TensorFlow-ZenDNN plug-in adds custom kernel implementations and operations specific to AMD EPYC CPUs to TensorFlow through its kernel and op registration C API (diagram 2 below).\nDiagram 2. The TensorFlow-ZenDNN plug-in upstreamed into TFv2.12 enables the addition of custom kernels and operations developed by AMD for performance improvement on AMD EPYC processors.\nThe main difference between the TensorFlow-ZenDNN plug-in and TF-ZenDNN direct integration is compatibility with standard TensorFlow packages. TF-ZenDNN direct integration is a standalone package which replaces standard TensorFlow packages. TensorFlow-ZenDNN plug-in is a supplemental package to be installed alongside standard TensorFlow packages starting from TF version 2.12 onwards.\nFrom a TensorFlow developer\u2019s perspective, the TensorFlow-ZenDNN plug-in approach simplifies the process of leveraging ZenDNN optimizations compared to the TF-ZenDNN direct integration approach. With TF-ZenDNN direct integration, the developer needs to download the foundational TensorFlow build and navigate separately to the AMD ZenDNN developer resources page to download the specific TF-ZenDNN binary for integration. In contrast, with the TensorFlow-ZenDNN plug-in approach, everything that a user needs to take advantage of ZenDNN resides on TensorFlow pages, as described further in the next section, \u201cStep-by-Step Guide to using ZenDNN on AMD EPYC Processors''.\nThe TensorFlow-ZenDNN plug-in, in its first iteration (v0.1), currently offers 16 common ZenDNN ops, including Conv2D, MatMul, BatchMatMul, FusedBatchNorm, AvgPool, and MaxPool. Other ops that are not covered will fall back to TensorFlow\u2019s native kernels. TensorFlow-ZenDNN plug-in provides competitive performance with TF-ZenDNN direct integration package for models such as ResNet, Inception, and VGG variants, as represented in Graph 1 above, with the blue bars representing TensorFlow-ZenDNN plug-in performance and the orange line representing TF-ZenDNN direct integration performance. However, TF-ZenDNN direct integration still outperforms the plug-in for other models, such as MobileNet and EfficientNet, because the plug-in does not yet support graph optimizations that are currently featured in TF-ZenDNN direct integration packages. We expect the performance to be closer once the TensorFlow-ZenDNN plug-in reaches feature parity with TF-ZenDNN direct integration.\nStep-by-Step Guide to using ZenDNN\non AMD EPYC Processors \nTaking advantage of ZenDNN optimizations in TensorFlow is straightforward:  \n1. Download ZenDNN Plug-in CPU wheel file from the TensorFlow Community Supported Builds webpage.\n2. Pip install the ZenDNN plug-in using the following commands:\npip install tensorflow-cpu==2.12.0 \npip install tensorflow_zendnn_plugin-0.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n 3. Enable ZenDNN optimizations in your inference flow by setting the following environment variables:\nexport TF_ENABLE_ZENDNN_OPTS=1\nexport TF_ENABLE_ONEDNN_OPTS=0\nTo disable ZenDNN optimizations in your inference execution, you can set the corresponding ZenDNN environment variable to 0:\nexport TF_ENABLE_ZENDNN_OPTS=0.  \nTensorFlow-ZenDNN plug-in is supported with ZenDNN v3.3. Please see Chapter 5 of the TensorFlow-ZenDNN Plug-in User Guide for performance tuning guidelines. \nFor optimal inference performance, AMD recommends using the TF-ZenDNN direct integration binaries available on the AMD ZenDNN developer resources page. \nWhat\u2019s Coming Next with ZenDNN \nTensorFlow v2.12 marks the first release of our TensorFlow-ZenDNN plug-in. AMD intends to continue improving the performance of the TensorFlow-ZenDNN plug-in on current- and future-generation AMD EPYC processors by supporting more ZenDNN ops, graph optimizations, and quantization in subsequent TensorFlow-ZenDNN plug-in releases. Such enhancements include a planned plug-in version transition from ZenDNN v3.3 to ZenDNN v4.0 to enable optimizations that take advantage of the AVX-512 and VNNI capability in 4th Gen EPYC processors.\nWith our aim of continuously improving the TensorFlow-ZenDNN plug-in for the community, we encourage TensorFlow developers to test this new TensorFlow-ZenDNN plug-in and share comments and concerns on our ZenDNN GitHub page. Technical support resources can also be reached via the following email address: zendnnsupport@amd.com.\nWe are excited to continue collaborating with TensorFlow to improve the ZenDNN experience for the wider TensorFlow developer community!\nAcknowledgements\nThe development and upstreaming of the TensorFlow-ZenDNN plug-in is the work of many people from AMD and the TensorFlow team at Google.\nFrom AMD: Chandra Kumar Ramasamy, Aakar Dwivedi, Savan Anadani, Arun Ramachandran, Avinash-Chandra Pandey, Ratan Prasad, Aditya Chatterjee, Alok Ranjan Srivastava, Prakash Raghavendra, Pradeep Kumar Sinha, Vincent Dee.\nFrom Google: Penporn Koanantakool, Eugene Zhulenev, Douglas Yarrington.\nLegal Endnotes\nZD-045 through ZD-051:\nTesting conducted by AMD Performance Labs as of Tuesday, February 7, 2023 on test systems comprising of:\nAMD System: AMD Eng Sample of the AMD EPYC\u2122 9004 96-core processor, dual socket, with hyperthreading on, 2150 MHz CPU frequency (Max 3700 MHz), 768GB RAM, 768MB L3 Cache, NPS1 mode, Ubuntu\u00ae 20.04.5 LTS version, kernel version 5.4.0-131-generic, BIOS TQZ1000F, GCC/G++ version 11.1.0, GNU ID 2.31, Python 3.8.15. For no ZenDNN, Tensorflow 2.12. For the ZenDNN plug-in, AOCL BLIS 3.0.6, Tensorflow 2.12, ZenDNN version 3.3; for Direct Integration AOCL BLIS 4.0, Tensorflow Version 2.10, ZenDNN 4.0.\nTests run all from Unified Inference Frontend 1.1 (UIF1.1) model zoo:\nFP32 EfficientNet\nFP32 Inception v3\nFP32 MobileNet v1\nFP32 VGG16\nFP32 RefineDet\nFP32 BERT Base\nFP32 ResNet50\nResults may vary based on factors such as software versions and BIOS settings. ZD-045 thru ZD-051",
    "link": "https://blog.tensorflow.org/2023/03/enabling-optimal-inference-performance-on-amd-epyc-processors-with-the-zendnn-library.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhhrrJ9LR5XOxRindKFRR9DFNZ40-_ZMFFqn-rebdd35-TN_jF0lKJMOVjucjAPgeT5aJMX9JENst_X1uvLdfRezoVr222-wWxJJuEZWHQLZ0yCZUGKcqj_PT0Oka1RtIil_iWxT_e4OQ5tzv3wfBJ7KTUav5IObRsYeBrur0V0cGLzVjKR13_ICxj4/s1600/image5.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgko68ZDY7iY0oNYpqgzhAOi9msygbMDVKpcPle6LuXTWFk50FRr27dLaRX-N5HKLUWXKZYQQropTH1OSMffBoWbp7oufFce7lvrgf9hdrDCcNyrThW2FWlwj2kZuMi3zVbC6JxSQgJzDW63Cyi5Z-mvzYCwI2yvrzsfMXmryHDEgZLOFWz2KAHfFkX/s1600/image1.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxZ_hAcXp2GL5lD9pa5twUVYGmy75-MRPd0x5uJh-wntzt-XdDyOaG4_0ZTU1ZEsvo2rib95FuaQMYMSAUDZheHQm1gpH0zsRkBMOARVlvC3saKdK1scbOWnfU4YfXfwWLdN3QGLX7wPe918Ke5sBvgiy7VhSrR_bXGm1GnNZBxyHfkrxdJeFzTpI7/s1600/image6.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHANWG-nXnE_WxHdTtKiyWNq2InAvr9cy1dblq01dcccQ7HzVG4t0dr8hhXcasLbuxnKUQt6yWgA80Iv_NzAIO-tuXknE7OsTx5UoT8Euegrf5iFCKtb3s9H4gYgvIKpEmMXd_aJ9Ke-rTSa0Bk26qZ0B69ruVCHftcR8qlzWkG0G-KT1R85oUXpR0/s1600/image2.png"
    ],
    "time": "2023/11/30 00:58:43"
  },
  {
    "title": "TensorFlow with MATLAB",
    "content": "Posted by Sivylla Paraskevopoulou, Product Marketing Manager at MathWorks\nIn this blog post I will show you how to use TensorFlow\u2122 with MATLAB\u00ae for deep learning applications. More specifically, I will show you how to convert pretrained TensorFlow models to MATLAB models, convert models from MATLAB to TensorFlow, and use MATLAB and TensorFlow together.\nThese interoperability features, offered by MATLAB, enable collaboration between colleagues, teams, and communities that work on different platforms. Today\u2019s post will show you how to use these features, and give you examples of when you might want to use them and how they connect the work of AI developers and engineers to enable domain-specific AI system design.\nIntroduction\nWhat is MATLAB?\nMATLAB is a computing platform tailored for engineering and scientific applications like data analysis, signal and image processing, control systems, wireless communications, and robotics. MATLAB includes a programming language, interactive apps, and tools for automatically generating embedded code. MATLAB is also the foundation for Simulink\u00ae, a block diagram environment for simulating complex multi-domain systems.\nSimilarly to Python\u00ae libraries, MATLAB provides toolboxes for achieving different goals. More specifically, MATLAB provides the Deep Learning Toolbox\u2122 for deep learning workflows. Deep Learning Toolbox provides a framework for designing and implementing deep neural networks with algorithms, pretrained models, and apps. It can be combined with domain-specific toolboxes in areas such as computer vision, signal processing, and audio applications.\nFigure:Python and MATLAB are programming languages; Python can leverage the TensorFlow library for deep learning workflows, while MATLAB provides the Deep Learning Toolbox.\nWhy TensorFlow and MATLAB?\nBoth TensorFlow and MATLAB are widely used for deep learning. Many MATLAB customers are interested in integrating TensorFlow models into their AI design, for creating customized tools, simulating complex systems, or optimizing data modeling. TensorFlow users can also leverage MATLAB to generate, analyze, and visualize training data, post-process model output, and deploy trained neural networks to desktop, web apps, or embedded hardware.\nFor example, engineers have integrated TensorFlow models into Simulink (MATLAB simulation environment) to develop a battery state-of charge estimator for an electric vehicle and scientists have used MATLAB with TensorFlow to build a custom toolbox for reading climate data. For more details on these examples, see Integrate TensorFlow Model into Simulink for Simulation and Code Generation and Climate Data Store Toolbox for MATLAB.\nWhat\u2019s Next?\nNow you have started to see the benefits of using TensorFlow with MATLAB. Let\u2019s get into more of the technical details on how to use TensorFlow with MATLAB in the following three sections.\nConvert Model from TensorFlow to MATLAB\nConvert Model from MATLAB to TensorFlow\nRun TensorFlow and MATLAB Together\nYou will see how straightforward it is to use TensorFlow with MATLAB and why I (and other engineers) like having the option to combine them for deep learning applications. Why choose when you don\u2019t have to?\nConvert Model from TensorFlow to MATLAB\nYou can convert a pretrained model from TensorFlow to MATLAB by using the MATLAB function importTensorFlowNetwork. A scenario when this function might be useful; a data scientist creates a model in TensorFlow and then an engineer integrates this model into an AI system created in MATLAB.\nWe will show you here how to import an image classification TensorFlow model into MATLAB and (1) use it for prediction and (2) integrate it into an AI system.\nConvert model from TensorFlow to MATLAB\nBefore importing a pretrained TensorFlow model into MATLAB network, you must save the TensorFlow model in the SavedModel format.\nPython code:\nimport tensorflow as tf\n  tf.saved_model.save(model.modelFolder)\n\nThen, you can import the TensorFlow model into MATLAB by using the MATLAB function importTensorFlowNetwork. You only need one line of code!\n\nMATLAB code:\nmodelFolder = \u201cEfficientNetV2L\u201d;\nnet = importTensorFlowNetwork(modelFolder,OutputLayerType=\u201dclassification\u201d)\n\nClassify Image\nRead the image you want to classify. Resize the image to the input size of the network.\nMATLAB code:\nIm = imread(\u201cmydoc.jpg\u201d);\nInputSize = net.Layers(1).InputSize;\nIm = imresize(Im,InputSize(1:2));\nBefore you classify the image, the image might require further preprocessing or changing the dimension ordering from TensorFlow to MATLAB. To learn more and get answers to common questions about importing models, see Tips on Importing Models from TensorFlow.\nPredict and plot image with classified label. MATLAB code:\nlabel = classify(net,Im);\nimshow(Im)\ntitle(\"Predicted label: \" + string(label));\n\nTo see the full example on how to import an image classification TensorFlow model into MATLAB and use the model for prediction, see Image Classification in MATLAB Using Converted TensorFlow Model. To learn more on importing TensorFlow models into MATLAB, check out the blog post Importing Models from TensorFlow, PyTorch, and ONNX.\nTransfer Learning\nA common reason to import a pretrained TensorFlow model into MATLAB is to perform transfer learning. Transfer learning is the process of taking a pretrained deep learning model and fine-tuning to fit the model to a new problem. For example, you are doing object detection in MATLAB, and you find a TensorFlow model that can improve the detection accuracy, but you need to retrain the model with your data. Using transfer learning is usually faster and easier than training a network from scratch.\nIn MATLAB, you can perform transfer learning programmatically or interactively by using the Deep Network Designer (DND) app. It\u2019s easy to do model surgery (prepare a network to train on new data) with a few lines of MATLAB code by using built-in functions that replace, remove, or add layers at any part of the network architecture. For an example, see Train Deep Learning Network to Classify New Images. With DND, you can interactively prepare the network for training, train the network, export the retrained network, and then use it for the new task. For an example, see Transfer Learning with Deep Network Designer.\nFigure:Edit pretrained model with a low-code app for transfer learning.\n\nAI System Design in Simulink\nSimulink is a block diagram environment used to design systems with multi-domain models, simulate systems before moving to hardware, and deploy without writing code. Simulink users have expressed interest in the ability to bring in AI models and simulate entire systems. In fact, this is very easy to do with Simulink blocks.\nIn the following figure, you can see a very simple AI system that reads and classifies an image using an imported TensorFlow model. Essentially, the Simulink system executes the same workflow shown above. To learn more about how to design and simulate such a system, see Classify Images in Simulink with Imported TensorFlow Network.\nFigure:Simple Simulink system for predicting image label\nOf course, Simulink capabilities extend far beyond classifying an image of my dog after I gave him a bad haircut and trying to predict his breed. For example, you can use deep neural networks inside a Simulink model to perform lane and vehicle detection. To learn more, see Machine Learning with Simulink and NVIDIA Jetson.\nLane and vehicle detection in Simulink using deep learning\nConvert Model from MATLAB to TensorFlow\nYou can convert a trained or untrained model from MATLAB to TensorFlow by using the MATLAB function exportNetworkToTensorFlow. In MATLAB, we refer to trained models as networks and to untrained models as layer graphs. The Pretrained Deep Neural Networks documentation page shows you all the options of how to get a pretrained network. You can alternatively create your own network.\nCreate Untrained Model\nCreate a bidirectional long short-term memory (BiLSTM) network to classify sequence data. An LSTM network takes sequence data as input and makes predictions based on the individual time steps of the sequence data.\nFigure:Architecture of LSTM model\nMATLAB code:\ninputSize = 12;\nnumHiddenUnits = 100;\nnumClasses = 9;\n\nlayers = [\n       sequenceInputLayer(inputSize)\n       bilstmLayer(numHiddenUnits,OutputMode=\"last\")\n       fullyConnectedLayer(numClasses)\n       softmaxLayer];\n\nlgraph = layerGraph(layers);\nTo learn how to create the training data set for this model, see Export Untrained Layer Graph to TensorFlow. An important step is to permute the sequence data from the Deep Learning Toolbox ordering (CSN) to the TensorFlow ordering (NSC), where C is the number of features of the sequence, S is the sequence length, and N is the number of sequence observations. To learn more about the dimension ordering of the input data for different deep learning platforms, see Input Dimension Ordering.\nExport Model to TensorFlow\nExport the layer graph to TensorFlow. The exportNetworkToTensorFlow function saves the TensorFlow model in the Python package myModel.\nMATLAB code:\nexportNetworkToTensorFlow(lgraph,\u201dmyModel\u201d)\nTrain TensorFlow Model\nRun the following code in Python to load the exported model from the Python package myModel. You can also compile and train the exported model in Python. To train the model, use the training data in training_data.mat that you previously created.\nPython code:\nimport myModel\nmodel = myModel.load_model()\nLoad training data.\nPython code:\nimport scipy.io as sio\ndata = sio.loadmat(\"training_data.mat\")\nXTrain = data[\"XTrain\"]\nYTrain = data[\"TTrain\"]\nCompile and train model.\nPython code:\nmodel.compile(optimizer = \"adam\", loss = \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\nr = model.fit(XTrain, YTrain, epochs=100, batch_size=27)\nTo learn more on how to export MATLAB models to TensorFlow, check out our blog post.\nExport untrained model from MATLAB to TensorFlow and train on Google Colab\nRun TensorFlow and MATLAB Together\nYou \u2018ve seen so far how to convert models between TensorFlow and MATLAB. You also have the option to use TensorFlow and MATLAB together (run from the same environment) by either calling Python from MATLAB or calling MATLAB from Python. This way you can take advantage of the best capabilities from each environment by creating an integrated workflow.\nFor example, TensorFlow might offer newer models but you like MATLAB apps for labeling data, or you might want to train your TensorFlow model under multiple initial conditions using the Experiment Manager app (see example).\nCall Python from MATLAB\nInstead of importing a TensorFlow model into MATLAB you have the option to directly use the TensorFlow model in your MATLAB workflow by calling Python from MATLAB. You can access Python libraries by adding the py. prefix and execute any Python statement from MATLAB by using the pyrun function. For an example that shows how to call a TensorFlow model in MATLAB, see Image Classification in MATLAB Using TensorFlow.\nA use case that this option might be useful is the following. You have created an object detection workflow in MATLAB. You want to quickly compare TensorFlow models to find the best suited model for your task before importing the best suited model into MATLAB. Call TensorFlow from MATLAB to run an inference test quickly.\nCall MATLAB from Python\nYou can use MATLAB Engine API to call MATLAB from a Python environment and thus, integrate MATLAB tools and apps into your existing Python workflow. MATLAB is convenient for labeling and exploring data for domain-specific (e.g., radar, wireless, audio, and biomedical) signal processing using low-code apps. For an example, see our GitHub repo Co-Execution for Training a Speech Command Recognition System.\nConclusion\nThe bottom line is that both TensorFlow and MATLAB offer excellent tools that enable applying deep learning to your application. MATLAB integrates with TensorFlow to take full advantage of these tools and enable access to hundreds of deep learning models. Choose between the interoperability features (convert models between TensorFlow and MATLAB, or use TensorFlow and MATLAB together) to create a deep learning workflow that bridges platforms and teams.\nIf you have questions about how, when, and why to use the described interoperability, email me at sparaske@mathworks.com. I would love to hear more about your workflow and discuss how working across deep learning platforms accelerates the application of deep learning to your domain.",
    "link": "https://blog.tensorflow.org/2023/03/tensorflow-with-matlab.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMhfV0Bx5dLH4GT3A1oCtBRM61H7ToS-sl2vZGIKUilGnvS_pwB7nilmLOZW2pt8e_ZXFdq7WzkAXI7USit0TkTbtuHoMpPApYBdEwUqbBctI2heT5HO359YOs9D8GMa6RbV0yzw-ZEnysknGjfLD9OyOMxGk6906xytUtLZPsAAcFq6HaRdJos7Jn/s1600/TF%20+%20MATLAB%20Social.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVlpntCm_kGfbCyDxfoh04Y_Gt0LkSMqe5X5mgzk4iJHZpf0XKYy6wBiu_20K8QaYSsWDkgWBpckZfAXV4vrRyZqtRUAdFZOAGuNUtkJd_ZCC4fZ2deGLHXVvvkz9zWmXZGr_oCXfD-RXpr9sGgCAfW6AmPcYbmkwtObMl6c8hM94bjTmYwzrwfYCV/s1600/image1.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiwM9zeewEMfZhCJssh3b33Xg1e3G1mL76HsE2UZPSE8GD5GGQcyCB-5OXVYVBjgB8S3d5ZTirIueszVJSDmrjeTDpMaA071aP_nmfUy1DVimjW3SuKKL9P4bW-q5Crccoajut9NPKrvu9sqczXkLYXZLDYB-OPg4srpXrVIGQfX1Del4DDSguuhn9S/s1600/image3.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgC50c0Xp_zuVi1LexgqZgtjzoWaoQED3jN9_tnObThNHlxnnOYiP4N6-1-e-yjLLSHZgbwL8Oc72a77DMSixz8z6hppuhkYAehdHne8m67vitjz7NnvUvGXiFeTibIYJoQLjJChxmED3peA4hZz_upBjOeck3WUmBJw_07x3pW31rMk_KBOtnC6AeQ/s1600/image2.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBkjA2RNoLOO-lk5PPFvtimDSaVCMMGHnX3aCJiMgMcDzP0cKtq4w5R4RSo-E2d-TCmlz-P8KtqtjHQtoVyZt6E_ZcD31IDTbSXCdmcfXBDfJVUBLvLMulv0EJULOvYN9LRdrhY85tgpSAsMgrIG1zcRGg2hjR4qU7itJxmXmj7HIiPsesLdq5lvZz/s1600/image5.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgqjivsY4dIxNRwXG3r12X6rxdoX74RNhoi0frVSP5UHuLtUBKeQ3zIN7VT3_PoesafFQCDqks4fYz5Knx0YPYHeSHLdG3ZfAmy1bCKV5MmQ99UI5q7DeZJfag1YFdf6nhiQvkPLalG62LK_PcuN87l1w6TzK7f_45wqQCnhRKjco3qjndNVoS0ZYDX/s1600/image4.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuo2kIQHFTnTvekWCd20Dje8sAFbp-W439xgyampfsJEwV6Le9JtU3g6xeRoWvlASygcBe9mn4eyvhmcal_tovGkIlr5wLGLNgdhGAQxv-p8xsPmVIoQ1_XqJhZwHtIOenTDdRKnCePGrx1U3qVMEn24LkVqTxzKCGh76oICtWzu_NJQQr3VYfI8Th/s1600/LaneDetection.gif",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEieDG5utkM2vtR97zaxG1iHY-TrBEVNEGld1ojmyfjxfyobf8mUj9PiEKcMXTSnU4lpWCoxXXVoyu-yhmPBYp317DGu5nZIVQ5WQSdH29m40L6O52fPTqd_tamgLmno6ymxg0Ak__Rb-Y7cBPanMHYah5Rh8rJaJYrE7tFJ1LH_Nw3G8XxuTsiFZk4V/s1600/image8.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIK0amyM3ZzNMtdklskvyyy9xbB_40BtnXg8MVba4GxuiaTDHt-Q6uiFimtJJoMbQGTRZc9b2fPaXyzRK7kts-2V5A2M9wtIGQsGqnzfjd0EQgtlK8oz83yX0cBK3Zn0cxAzLOh-gllJBWCpSVBgVF8jEEZAPfb924iL6_N_R0JwI70uJtMOjLquik/s1600/image7.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgzHIdRl2c8li2o0UwUGD0maz7xmBwaKrMrnPiPsG2N_V3maRJ-kvCIG2EyIDuz6gch8tWsXDRCw_zz-qQxXF14vBBzfwba6lxzUa8tO5lCWCyT4HJTUvkIPGr2J9XN2ZR1xURbdiNnWv-anMZSWNrVWAdhbUhWn7nmQEx5G9NISkURRI5AaPQsHCe6/s1600/MATLAB%20gif.gif",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEieDG5utkM2vtR97zaxG1iHY-TrBEVNEGld1ojmyfjxfyobf8mUj9PiEKcMXTSnU4lpWCoxXXVoyu-yhmPBYp317DGu5nZIVQ5WQSdH29m40L6O52fPTqd_tamgLmno6ymxg0Ak__Rb-Y7cBPanMHYah5Rh8rJaJYrE7tFJ1LH_Nw3G8XxuTsiFZk4V/s1600/image8.png"
    ],
    "time": "2023/11/30 00:58:43"
  },
  {
    "title": "TensorFlow Hub \u2764\ufe0f Kaggle",
    "content": "Posted by Luiz GUStavo Martins, Google AI Developer Advocate\nWe're excited to announce our new integration with Kaggle Models, a recently launched pre-trained model platform. All 2,300+ TensorFlow models published on TFHub by Google, DeepMind, and more are now discoverable and usable on Kaggle Models, with documentation and sample code.\nWhy Kaggle?\nKaggle is a global community of over 12 million machine learners who test their knowledge in competitions and share machine learning resources, including over 200,000 public datasets. Over the past 10+ years, Kaggle\u2019s competitions have become a proving ground for what works well and what doesn\u2019t across a multitude of ML use cases. This is why Kaggle recently launched its open model hub, Kaggle Models, to better enable the ML community to stress test and validate models publicly and at scale.\nHosting TensorFlow models on Kaggle makes them more easily accessible to the broader ML community, democratizing model building and advancement. We can't wait to see what solutions come from this partnership.\nHow to Get Started\nA great place to check out the new integration is with the live Kaggle competition called BirdCLEF 2023 using the recently published Bird Vocalization Classifier model. Participants are challenged to build a model that identifies bird species by sound. Bird populations around the world are falling alarmingly, with approximately 48% of existing species experiencing population declines. The results of this competition contribute to scaling the critical work of bird species monitoring that allows researchers to better evaluate whether interventions are working.\nThe Bird Vocalization Classifier model was just open-sourced by the Google Research team on TFHub (and subsequently Kaggle Models \ud83d\ude4c). It's a global bird embedding and classification model that can identify more than 10k bird species\u2019 vocalizations, and also creates embedding vectors that can be used for other tasks.\nTo try the model on Kaggle:\nNavigate to the model here.\nClick the \u201cNew Notebook\u201d button, which will open a Kaggle Notebooks editor.\nClick the \u201cCopy Code\u201d button on the right-hand side of the editor, which will copy sample code that loads the model using the TensorFlow Hub library.\nPaste the code into the notebook\u2019s cell, and you\u2019re ready to go!\nClick the \u201cAdd Model\u201d button at the bottom. This will attach the model to your notebook.\nThe snippet imports TFHub library and loads the newly published Bird Vocalization Classifier model. To find more information about this model, you can check its documentation and even play with a full example that demonstrates how to use the model in the competition here.\nimport tensorflow_hub as hub\n    \nkeras_layer = \nhub.KerasLayer('https://kaggle.com/models/google/bird-vocalization-classifier/frameworks/TensorFlow2/variations/bird-vocalization-classifier/versions/1')\nFor more information on Kaggle Models including its current feature set and future roadmap, check the official announcement here. We look forward to seeing what you build as a result of this integration!",
    "link": "https://blog.tensorflow.org/2023/03/tensorflow-hub-kaggle.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbNR1Mwx8hIOtExrIfFEPesy_0wkOlTXmrbZNyJL49242ZkBeVweaqerBFtuU9Mlag9WgY7OBuTdYtn_T0c2Fs8zup0XxcOGL7ax3Vtp7N4lFg5czMB96IkIztY5YmqQxNQHdqWwiBaq7kA4DTNWMt5x7pOjjLubYgTSu2SFOIDAd93Im-6jlocE5x/s1600/Social%20-%20TensorFlow%20-%20TFHub%20+%20Kaggle%20Collaboration.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi2CXrFWsSD9xtmG1RvckuduETf77lTbqaZ1MVYe9Wlo58itjstCaap7R6LWI2G_KoF8FgydHt4683a4R1eI9oaX3a9QRoqq1eEUCxcOglac8YFG6hKReuw5LRYrE5ZsYNyrBpfAnse9usCFNd_8w9IlqdiGzpKzdD7zlA_1kDVbyl1PliGsWLshN0p/s1600/image1.gif"
    ],
    "time": "2023/11/30 00:58:43"
  },
  {
    "title": "Introducing Simple ML for Sheets: A No-code Machine Learning Add-on for Google Sheets",
    "content": "Posted by Mathieu Guillame-Bert, Richard Stotz, Luiz GUStavo Martins, Ashley Oldacre, Jocelyn Becker, Glenn Cameron, and Jan Pfeifer\nToday at the Women in ML Symposium thousands of ML developers and enthusiasts gathered to learn about the latest ML products from Google. Advances in machine learning (ML) technology continue to power breakthroughs in many fields. From helping to protect the Great Barrier Reef to helping amputees reclaim mobility. However, such work often requires deep ML expertise, programming experience, and time.\nTo make ML accessible beyond ML experts, we\u2019ve been working on Simple ML for Sheets. Simple ML is an add-on, in beta, for Google Sheets from the TensorFlow team that helps make machine learning accessible to all. Anyone, even people without programming or ML expertise, can experiment and apply some of the power of machine learning to their data in Google Sheets with just a few clicks. From small business owners, scientists, and students to business analysts at large corporations, anyone familiar with Google Sheets can make valuable predictions automatically.\nFor example, if you're a car repair shop owner who keeps records of past repairs with data points like car make, repair type, and mileage, you can use Simple ML to predict the number of hours necessary to fix a car. Scientists can also benefit from ML in countless domains. For example, if you are studying molecular aging, you can predict a person's age based on DNA methylation data. In either use case, these ML-powered predictions are at your fingertips in just a few clicks, all via the familiar Sheets interface you use every day.\nSimple ML works in three overall steps:\nOpen your data in Google Sheets.\nSelect and run the task that best describes what you want to do, like predicting missing values or spotting abnormal ones. Tasks are organized so you can use them even if you don't know anything about Machine Learning.\nAfter a few seconds, once the model has made a prediction, you can explore using the result to improve your business decisions, automate tasks, or do any of the seemingly endless applications that ML enables. If you are new to ML, remember these are just statistical predictions, of course, and may be inaccurate.\nPredicting missing penguin species with Simple ML for Sheets\nEven if you already know how to train and use machine learning models, Simple ML in Sheets can help make your life even easier. For instance, training, evaluating, interpreting, and exporting a model to Notebook takes only 5 clicks and as little as 10 seconds. Since Simple ML in Sheets is based on state-of-the-art ML technology that also powers TensorFlow Decision Forests , and pre-optimized, you might even get better models.\nOf course, succeeding with ML involves far more than training a model and making a prediction. If you are new to ML, you should begin with Google\u2019s free machine learning courses, including problem framing.\nBecause Simple ML runs on your browser your data stays right where you\u2019re working \u2013 secure in your spreadsheet in Google Sheets. The models get automatically saved to Google Drive so you can easily share them with the rest of your team. And because Simple ML uses TensorFlow Decision Forests underneath, you can export models trained in SimpleML to the TensorFlow ecosystem!\nWant to try it? Follow the introduction tutorial to get started. Then, try the add-on on your own data! Feedback is welcomed. And as always, use AI responsibly.",
    "link": "https://blog.tensorflow.org/2022/12/introducing-simple-ml-for-sheets.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjOmx6cfv_5rz75T4UCftEFXYMfK_z3lq6N2x3l3q5zZSfZICa2xt6vOzPFmf40oD1H4Fi83ddp3EjQcTDs4GCT103Ok4hy2tLoA6R9P291CwBXApgT2RWA_zlMpacqHluecraP1dgMl0_NtpW9Bxx3NFNC9l3LMiv2KMBiHU3B3_dAaywNf82WmfH4/s1600/Tensorflow-ML-Sheets-02.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhjGtVcRjnDoYnsBzuDAskQDxJM0FqgDDFo4KSejJKly8l1F8tGNddmimw_JZo19-AIY2QDBYyE7O-VHEZbLGFyF0TQvzOe8r_cGmLDDNHOqEEo57v2gKjnw5TfY2tUhksUYF9nU8VtnhM87y1mDz33D3G4x8FTI61GKdrvSnb4ln4uoWcomgVzA0K/s1600/simple_ml_owl.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcwDWW-WFeYIyF1tkiasWz2l2KKc8A6WRz6tMhi4aJ6ywxmt_Xqk7QzZBJPu8ujljl16FgR3UoQ72cKU_ezQQ9IxnmQ8CzZGgC_CevXM7RT2PsUELVZPdlpC9w_Haz0GAF7VKjzPV2Ns9d4tRuWMQw5lzfYJXqMELAiD6QotMbyVLsfNH48diSjMwf/s1600/optimizedsimpleML.gif"
    ],
    "time": "2023/11/30 00:58:43"
  },
  {
    "title": "Unfolding the Universe using TensorFlow",
    "content": "A guest post by Roberta Duarte, IAG/USP\nAstronomy is the science of trying to answer the Universe\u2019s biggest mysteries. How did the Universe begin? How will it end? What is a black hole? What are galaxies and how did they form? Is life a common piece in the Universe\u2019s puzzle? There are so many questions without answers. Machine learning can be a vital tool to answer those questions and help us unfold the Universe.\nAstronomy is one of the oldest sciences. The reason is simple: we just have to look at the sky and start questioning what we are seeing. It is what astronomers have been doing for centuries. Galileo discovered a series of celestial objects after he observed the sky through the lenses of his new invention: the telescope. A few years later, Isaac Newton used Galileo's contributions to find the Law of Universal Gravitation. With Newton\u2019s results, we could not only understand better how the Sun affects Earth and other planets but also why we are trapped on Earth\u2019s surface. Centuries later, Edwin Hubble found that galaxies are moving away from us and that further galaxies are moving faster than closer ones. Hubble\u2019s findings showed that the Universe is expanding and is accelerated. These are a few examples of how studying the sky can give us some answers about the universe.\nWhat all of them have in common is that they record data obtained from observations. The data can be a star\u2019s luminosity, planets\u2019 positions, or even galaxies\u2019 distances. With technology improving the observations, more data is available to help us understand the Universe around us. Recently, the most advanced telescope, James Webb Space Telescope (JWST), was launched to study the early Universe in infrared. JWST is expected to transmit 57.2 gigabytes per day of data containing information about early galaxies, exoplanets, and the Universe\u2019s structure.\nWhile this is excellent news for astronomers, it also comes with a high cost. A high computational cost. In 2020, Nature published an article about big data and how Astronomy is now in an era of big data. JWST is one of the examples of how those powerful telescopes are producing huge amounts of data every day. Vera Rubin Observatory is expected to collect 20 terabytes per night. Large Arrays collect petabytes of data every year, and next-generation Large Arrays will collect hundreds of petabytes per year. In 2019, several Astro White Papers were published with the goals and obstacles in the Astronomy field predicted for the 2020s. They outlined how Astronomy needs to change in order to be prepared for the huge volume of data expected during the 2020s. New methods are required since the traditional cannot deal with the expressive number. We see problems showing up when talking about storage, software, and processing.\nThe storage problem may have a solution in cloud computing, eg. GCP, as noted by Nature. However, processing does not have a simple solution. The methods used to process and analyze the data need to change. It is important to note that Astronomy is a science based on finding patterns. Stars with the same redshift - an estimation of the distance of stars in space relative to us by measuring the shift of the star's light waves towards higher frequencies - and similar composition can be considered candidates for the same population. Galaxies with the same morphology and activity or spectrum originating in the nucleus usually show the presence of black holes with similar behavior. We can even calculate the Universe\u2019s expansion rate by studying the pattern in the spectra of different Type I Supernovae. And, what is the best tool we have to learn patterns in a lot of data? Machine Learning.\nMachine learning is a tool that Astronomy can use to deal with the computational problems cited above. A data-driven approach offered by machine learning techniques may help to get analysis and results faster than traditional methods such as numerical simulations or MCMC - a statistical method of sampling from a probability distribution. In the past few years, we are seeing an interesting increase in the interaction between Astronomy and machine learning. To quantify, the keyword machine learning presented in Astronomy\u2019s papers increased four times from 2015 to 2020 while deep learning increased 3 times each year. More specifically, machine learning was widely used to classify celestial objects and to predict spectra from given properties. Today, we see a large range of applications since discovering exoplanets, simulations of the Universe\u2019s cosmic web, and searching for gravitational waves.\nSince machine learning offers a data-driven approach, it can accelerate scientific research in the field. An interesting example is the research around black holes. Black holes have been a hot topic for the past few years with amazing results and pictures from the Event Horizon Telescope (EHT). To understand a black hole, we need the help of computational tools. A black hole is a region of spacetime extremely curved that nothing, not even light, can escape. When matter gets trapped around its gravitational field, the matter will create a disk called accretion disk. The accretion disk dynamics are chaotic and turbulent. To understand the accretion disk physics, we need to simulate complex fluid equations. \n\nA common method to solve this and gain insight into black hole physics is to use numerical simulations. The environment around a black hole can be described using a set of conservative equations - usually, mass conservation, energy conservation, and angular momentum conservation. The set of equations can be solved using numerical and mathematical methods that iteratively solve each parameter for each time. The result is a set of dumps - or frames - with information about density, pressure, velocity field, and magnetic field for each (x, y, t) in the 2D case or (x, y, z, t) in the 3D case. However, numerical simulations are very time-consuming. A simple hydrodynamical treatment around a black hole can go up to 7 days running on 400 CPU cores.\n\nIf you start adding complexity, such as electromagnetism equations to understand the magnetic fields around a black hole and general relativity equations to realistically explain the space-time there, the time can increase significantly. We are slowly reaching a barrier in black hole physics due to computational limitations where it is becoming harder and harder to realistically simulate a black hole.\nBlack hole research\nThat is where my advisor, Rodrigo Nemmen, and I started to think about a new method to accelerate black hole physics. In other words, a new method that could accelerate the numerical simulations we needed to study these extreme objects. From the beginning machine learning seems like the method with the best perspective for us. We had the data to feed into a machine learning algorithm and there were successful cases in the literature simulating fluids using machine learning. But never around a black hole. It was worth giving it a shot. We began a collaboration with Jo\u00e3o Navarro from Nvidia Brazil and then we started solving the problem. Carefully, we chose an architecture that we would be based on while building our own scheme. Since we wanted a data-driven approach, we decided to go with supervised learning, more specifically, we decided to use deep learning linked with the great performance of convolutional neural networks.\nHow we built it\nEverything was built using TensorFlow and Keras. We started using TensorFlow 1 since it was the version available at the time. Back then, Keras was not added to TensorFlow yet but funny enough, during that time I attended the TensorFlow Roadshow 2019 in S\u00e3o Paulo, Brazil. It was during that event that I found out about TensorFlow and Keras joining forces in TensorFlow version 2 to create the powerful framework. I even took a picture of the announcement. Also, it was the first time I heard about the strategy scope implemented in TensorFlow 2, I did not know back then that I would be using the same function today.\nIt needed weeks to deal with the data and to know the best way to prepare before we could feed them to ConvNets. The data described the density of a fluid around a black hole. In our case, we got the data from sub-fed black holes, in other words, black holes with low accretion rates. Back in 2019, the simulations we used were the longest simulations of this kind - 2D profiles using a hydrodynamical treatment. The process that we went through is described in Duarte et al. 2022. We trained our ConvNet with 2D spatial + 1D temporal dimensions. A cluster with two GPUs (NVIDIA G100 and NVIDIA P6000) was our main hardware to train our neural network.\nAfter a few hours of training, our model was ready to simulate black holes. First, we tested the capacity by testing how much the model can learn the rest of the learned simulation. The video shows the target and prediction for a case that we called a direct case: we feed a simulation frame to the model as input and we analyze how well the model can predict the next step.\nBut we also want to see how much of the Physics the model could learn by only looking at some simulations. We test the model capacity to simulate a never-seen system. During the training process, we hid a simulation from the model. After the training, we input the initial conditions and a single frame so we could test how the model would perform while simulating by itself. The results are great news: the model can simulate a system by only learning Physics from other ones. And the news gets better: you have a 32000x speed-up compared to traditional methods.\nJust out of curiosity, we tested a direct prediction from a system where the accretion flow around the black hole has high variability. It\u2019s a really beautiful result to see how the model could follow the turbulent behavior of the accretion flow.\nIf you are interested in more details and results, they are available at Duarte et al. 2022.\nThis work demonstrates the power of using deep learning techniques in Astronomy to speed up scientific research. All the work was done using only TensorFlow tools to preprocess, train and predict. How great is that?\nConclusion\nAs we discussed in this post, AI is already an essential part of Astronomy and we can expect that it will only continue to grow. We have already seen that Astronomy can achieve big wins with the help of AI. It is a field with a lot of data and patterns that are perfect to build and test AI tools using real-world data. There will come a day that AI will be discovering and unfolding the Universe and hopefully, this day is soon!",
    "link": "https://blog.tensorflow.org/2022/12/unfolding-universe-using-tensorflow.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjCG0TqgTw_q12uUY4BCHXPTIBAqKzPVa5WIUKZXLv3asUlyrN52QDaZua4uljsFLWGCNDmcZL6nffct9MSNtE1lsGcD7w6VKLGBT6wqpr4EndM2duswuAZyU8oUFoiGHtBbx8hjTeAb4nD1HJPf2WCefdGflbZJbeIuKw6i9cFKTyGWArGh_VG3B3f/s1600/image2.gif",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgAbLLt9a4AZd18JLeVqLjEMjQ93HFAeTMhRBNC9--sZdkShdnJWWJMCOrnA9cvd-U685UOg7umKrMby6ajiQwMxgG4O_cNz66XSXZCQ2QVjIiKm1prCgPIdIZZ1ACVbXbTyM2fqVK_a2RT94D4XUq1inHa6iETP_IFAn8DRXvsubJzP98qsaNsG9Kt/s1600/image1.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgq7DWSI-5VsrdO5o8RO0on7WNgSXUrBM0_BrnLaNwxYYcLmL2N7cDZPT-WYq9CsRg8dZjdm5EIXiibpJizgOPLfJ7cSQNv9uV4W5miUZcyqxkg6ihZD76r2U1f4X2zrUIEtZcyUKScGktVTh_k_Eq1T7iX2tV3qqyVt7or5N26VPWfNh_96xy5EYj9/s1600/image5.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjCG0TqgTw_q12uUY4BCHXPTIBAqKzPVa5WIUKZXLv3asUlyrN52QDaZua4uljsFLWGCNDmcZL6nffct9MSNtE1lsGcD7w6VKLGBT6wqpr4EndM2duswuAZyU8oUFoiGHtBbx8hjTeAb4nD1HJPf2WCefdGflbZJbeIuKw6i9cFKTyGWArGh_VG3B3f/s1600/image2.gif",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYQe5Kb7X8_-REBUteR2XTq_wtOult0jHB8awTQFQ1Ot_hlHy7JkMgWTg6M6NlvbAfr9SA9eAFNfjZuX-KKn2S0yYLFaXCnTyrEK2QuIpkE1Lu6Y7lMShiCFGktxNHHZwiiAYRzWL8LT4zT0HfZIfYoMDUobzPdwqeX0vC7v5MFUMrwKdMfpbyw8ad/s1600/image4.gif",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgA84fbsxSlEOyh9e_ynKe1yw71VgKXJzdUM4NYeZdxZakTfnoc-gx-mp02Pi_SyntNdE1BkGO1edQTEyXbYaezB6VHh1SrH8ZpZ1sMtWdY2_GTEsQF9kppbgQfiZjKOhZ3uuZcxF5LR-8siTjiTixmaSAoyGZOTl2X61hb3a4DehmD1eWI9d4fmgvU/s1600/image3.gif"
    ],
    "time": "2023/11/30 00:58:43"
  },
  {
    "title": "How Hugging Face improved Text Generation performance with XLA",
    "content": "Posted by The Hugging Face Team \ud83e\udd17\nLanguage models have bloomed in the past few years thanks to the advent of the Transformer architecture. Although Transformers can be used in many NLP applications, one is particularly alluring: text generation. It caters to the practical goals of automating verbal tasks and to our dreams of future interactions with chatbots.\nText generation can significantly impact user experiences. So, optimizing the generation process for throughput and latency is crucial. On that end, XLA is a great choice for accelerating TensorFlow models. The caveat is that some tasks, like text generation, are not natively XLA-friendly.\nThe Hugging Face team recently added support for XLA-powered text generation in \ud83e\udd17 transformers for the TensorFlow models. This post dives deeper into the design choices that had to be made in order to make the text generation models TensorFlow XLA-compatible. Through these changes to incorporate XLA compatibility, we were able to significantly improve the speed of the text generation models ~ 100x faster than before.\nA Deeper Dive into Text Generation\nTo understand why XLA is non-trivial to implement for text generation, we need to understand text generation in more detail and identify the areas that would benefit the most from XLA.\nPopular models based on the Transformer architecture (such as GPT2) rely on autoregressive text generation to produce their outputs. Autoregressive text generation (also known as language modeling) is when a model is iteratively called to predict the next token, given the tokens generated so far, until some stopping criterion is reached. Below is a schematic of a typical text generation loop:\nAny autoregressive text generation pipeline usually contains two main stages in addition to the model forward pass: logits processing and next token selection.\nNext token selection\nNext token selection is, as the name suggests, the process of selecting the token for the current iteration of text generation. There are a couple of strategies to perform next token selection:\nGreedy decoding. The simplest strategy, known as greedy decoding, simply picks the token with the highest probability as predicted by the underlying text generation model.\nBeam search. The quality of greedy decoding can be improved with beam search, where a predetermined number of best partial solutions are kept as candidates at the cost of additional resources. Beam search is particularly promising to obtain factual information from the language model, but it struggles with creative outputs.\nSampling. For tasks that require creativity, a third strategy known as sampling is the most effective, where each subsequent input token is sampled from the probability distribution of the predicted tokens.\nYou can read more about these strategies in this blog post.\nLogit preprocessing\nPerhaps the least discussed step of text generation is what happens between the model forward pass and the next token selection. When performing a forward pass with a text generation model, you will obtain the unnormalized log probabilities for each token (also known as logits). At this stage, you can freely manipulate the logits to impart the desired behavior to text generation. Here are some examples:\nYou can prevent certain tokens from being generated if you set their logits to a very large negative value;\nToken repetition can be reduced if you add a penalty to all tokens that have been previously generated;\nYou can nudge sampling towards the most likely tokens if you multiply all logits by a constant smaller than one, also known as temperature scaling.\nBefore you move on to the XLA section of this blog post, there is one more technical aspect of autoregressive text generation that you should know about. The input to a language model is the sequence of tokens generated so far. So, if the input has N tokens, the current forward pass will repeat some attention-related computations from the previous N-1 tokens. The actual details behind these repeated computations deserve (and have) a blog post of their own, the illustrated GPT-2. In summary, you can (and should) cache the keys and the values from the masked self-attention layers where the size of the cache equals the number of input tokens obtained in the previous generation iteration.\nHere we identified three keys areas that could benefit from XLA:\nControl flow\nData structures\nUtilities accepting dynamically shaped inputs\nAdjusting Text Generation for XLA\nAs a TensorFlow user, the first thing you must do if you want to compile your function with XLA is to ensure that it can be wrapped with a tf.function and handled with AutoGraph. There are many different paths you can follow to get it done for autoregressive text generation \u2013 this section will cover the design decisions made at Hugging Face \ud83e\udd17, and is by no means prescriptive.\nSwitching between eager execution and XLA-enabled graph mode should come with as few surprises as possible. This design decision is paramount to the transformers library team. Eager execution provides an easy interface to the TensorFlow users for better interaction, greatly improving the user experience. To maintain a similar level of user experience, it is important for us to reduce the friction of XLA conversion.\nControl flow\nAs mentioned earlier, text generation is an iterative process. You condition the inputs based on what has been generated, where the first generation is usually \u201cseeded\u201d with a start token. But, this continuity is not infinite \u2013 the generation process terminates with a stopping criterion.\nFor dealing with such a continuous process, we resort to while statements. AutoGraph can automatically handle most while statements with no changes, but if the while condition is a tensor, then it will be converted to a tf.while_loop in the function created by tf.function. With tf.while_loop, you can specify which variables will be used across iterations and if they are shape-invariant or not (which you can\u2019t do with regular Python while statements, more on this later).\n# This will have an implicit conversion to a `tf.while_loop` in a `tf.function`\nx = tf.constant([10.0, 20.0])\nwhile tf.reduce_sum(x) > 1.0:\n   x = x / 2\n \n# This will give you no surprises and a finer control over the loop.\nx = tf.constant([10.0, 20.0])\nx = tf.while_loop(\n   cond=lambda x: tf.reduce_sum(x) > 1.0,\n   body=lambda x: [x / 2],\n   loop_vars=[x]\n)[0]\nAn advantage of using tf.while_loop for the text generation autoregressive loop is that the stopping conditions become clearly identifiable \u2013 they are the termination condition of the loop, corresponding to its cond argument. Here are two examples we resorted to tf.while_loop with explicit conditioning:\nGreedy search\nBeam search\nSometimes a for loop repeats the same operation for an array of inputs, such as in the processing of candidates for beam search. AutoGraph\u2019s strategy will greatly depend on the type of the condition variable, but there are further alternatives that do not rely on AutoGraph. For instance, vectorization can be a powerful strategy \u2013 instead of applying a set of operations for each data point/slice, you apply the same operations across a dimension of your data. However, it has some drawbacks. Skipping operations is not desirable with vectorized operations, so it is a trade-off you should consider.\n# Certain `for` loops might skip some unneeded computations ...\nx = tf.range(10) - 2\nx_2 = []\nfor value in x:\n   if value > 0:\n       value = value / 2\n   x_2.append(tf.cast(value, tf.float64))\ny = tf.maximum(tf.stack(x_2), 0)\n# ... but the benefit might be small for loss in readability compared to a \n# vectorized operation, especially if the performance gains from a simpler \n# control flow are factored in.\nx = tf.range(10) - 2\nx_2 = x / 2\ny = tf.maximum(x_2, 0)\nIn the beam search candidate loop, some of the iterations can be skipped because you can tell in advance that the result will not be used. The ratio of skipped iterations was low and the readability benefits of vectorization were considerable, so we adopted a vectorization strategy to execute the candidate processing in beam search. Here is one example of logit processing, benefitting from this type of vectorization.\n\nThe last type of control flow that must be addressed for text generation is the if/else branches. Similarly to while statements, AutoGraph will convert if statements into tf.cond if the condition is a tensor.\n# If statements can look trivial like this one.\nx = tf.constant(1.0)\nif x > 0.0:\n   x = x - 1.0\n \n# However, they should be treated with care inside a `tf.function`\nx = tf.constant(1.0)\nx = tf.cond(\n   tf.greater(x, 0.0),\n   lambda: x - 1.0,\n   lambda: x\n)\nThis conversion places some constraints on your design: the branches of your if statement must now be converted to function calls, and both branches must return the same number and type of outputs. This change impacts complex logit processors, such as the one that prevents specific tokens from being generated. Here is one example that shows our XLA port to filter undesirable tokens as a part of logit processing.\nData structures\nIn text generation, many data structures don\u2019t have a static dimension that depends on how many tokens were generated up to that point. This includes:\ngenerated tokens themselves,\nattention masks for the tokens,\nand cached attention data as mentioned in the previous section,\namong others. Although tf.while_loop allows you to use variables with varying shapes across iterations, this process will trigger re-tracing, which should be avoided whenever possible since it\u2019s computationally expensive. You can refer to the official commentary on tracing in case you want to delve deeper.\nThe summary here is that if you constantly call your tf.function wrapped function with the same input tensor shape and type (even if they have different data), and do not use new non-tensor inputs, you will not incur tracing-related penalties.\nAt this point, you might have anticipated why loops with dynamic shapes are not desirable for text generation. In particular, the model forward pass would have to be retraced as more and more generated tokens are used as part of its input, which would be undesirable. As an alternative, our implementation of autoregressive text generation uses static shapes obtained from the maximum possible generation length. Those structures can be padded and easily ignored thanks to the attention masking mechanisms in the Transformer architecture. Similarly, tracing is also a problem when your function itself has different possible input shapes. For text generation, this problem is handled the same way: you can (and should) pad your input prompt to reduce the possible input lengths.\n# You have to run each section separately, commenting out the other.\nimport time\nimport tensorflow as tf\n \n# Same function being called with different input shapes. Notice how the\n# compilation times change -- most of the weight lifting is done on the\n# first call.\n\n@tf.function(jit_compile=True)\ndef reduce_fn_1(vector):\n   return tf.reduce_sum(vector)\n \nfor i in range(10, 13):\n   start = time.time_ns()\n   reduce_fn_1(tf.range(i))\n   end = time.time_ns()\n   print(f\"Execution time -- {(end - start) / 1e6:.1f} ms\")\n# > Execution time -- 520.4 ms\n# > Execution time -- 26.1 ms\n# > Execution time -- 25.9 ms\n \n# Now with a padded structure. Despite padding being much larger than the\n# actual data, the execution time is much lower because there is no retracing.\n\n@tf.function(jit_compile=True)\ndef reduce_fn_2(vector):\n   return tf.reduce_sum(vector)\n \npadded_length = 512\nfor i in range(10, 13):\n   start = time.time_ns()\n   reduce_fn_2(tf.pad(tf.range(i), [[0, padded_length - i]]))\n   end = time.time_ns()\n   print(f\"Execution time -- {(end - start) / 1e6:.1f} ms\")\n# > Execution time -- 511.8 ms\n# > Execution time -- 0.7 ms\n# > Execution time -- 0.4 ms\n\nPositional embeddings\nTransformer-based language models rely on positional embeddings for the input tokens since the Transformer architecture is permutation invariant. These positional embeddings are often derived from the size of the structures. With padded structures, that is no longer possible, as the length of the input sequences no longer matches the number of generated tokens. In fact, because different models have different ways of retrieving these positional embeddings given the position index, the most straightforward solution was to use explicit positional indexes for the tokens while generating and to perform some ad-hoc model surgery to handle them.\nHere are a couple of example model surgeries that we made to make the underlying models XLA-compatible:\nT5 (pull request)\nGPT-J (pull request)\nFinally, to make our users aware of the potential failure cases and limitations of XLA, we ensured adding informative in-code exceptions (an example).\nTo summarize, our journey from a naive TensorFlow text generation implementation to an XLA-powered one consisted of:\nReplacing for/while Python loops conditional on tensors with tf.while_loop or vectorization;\nReplacing if/else operations conditioned on tensors with tf.cond;\nCreating fixed-size tensors for all tensors that had dynamic size;\nStopping relying on tensor shapes to obtain the positional embedding;\nDocumenting proper use of the XLA-enabled text generation.\nWhat\u2019s next?\nThe journey to XLA-accelerated TensorFlow text generation by Hugging Face \ud83e\udd17 was full of learning opportunities. But more importantly, the results speak for themselves: with these changes, TensorFlow text generation can execute 100x faster than before! You can try it yourself in this Colab and can check out some benchmarks here.\nBringing XLA into your mission-critical application can greatly impact driving down costs and latency. The key to accessing these benefits lies in understanding how AutoGraph and tracing work to bring the most out of them. Have a look at the resources shared in this blog post and give it a go!\n\nAcknowledgements\nThanks to the TensorFlow team for bringing support for XLA. Thanks to Joao Gante (Hugging Face) for spearheading the development of XLA-enabled text generation models for TensorFlow in \ud83e\udd17 Transformers.",
    "link": "https://blog.tensorflow.org/2022/11/how-hugging-face-improved-text-generation-performance-with-xla.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgN_U3n575Nyh7Nh-tFIa415mgpe7-I5wsDKDhXprOnIYD6AQ0PJU2sTTPIbEDk5xydxOozKrRndn3UKh-Nfqq7zHrULg3dCWBpjkNNbG0oEXnqzPem1sTk3xdV6VXOsEUAeJ0G9RmvxDW8f_ojsHLNKI_RzgUJ8iht8WR1oejqhQdFv3MaIM40gkCm/s1600/Hugging%20Face%20Social.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4yBkYOTTPuDL8CHBQTZ99x5yh7AMJIQoNU1fxkOcFPXBHJldBqoTkBWK9J4roWI_fdlkHucZ8XzQytYc_O_ysrvkhZxcMTe8QLOfR1hFWuvQP9XsCLjCFoSjU-ITVuQNYZIt15SwFYf58BzvMN3RcDs4XI774QG0rb15ezUnEYIN9MM2bALyO7OTf/s1600/image1.png"
    ],
    "time": "2023/11/30 00:58:43"
  },
  {
    "title": "Accelerating TensorFlow on Intel Data Center GPU Flex Series",
    "content": "Posted by Jianhui Li, Zhoulong Jiang, Yiqiang Li from Intel, Penporn Koanantakool from Google\nThe ubiquity of deep learning motivates development and deployment of many new AI accelerators. However, enabling users to run existing AI applications efficiently on these hardware types is a significant challenge. To reach wide adoption, hardware vendors need to seamlessly integrate their low-level software stack with high-level AI frameworks. On the other hand, frameworks can only afford to add device-specific code for initial devices already prevalent in the market \u2013 a chicken-and-egg problem for new accelerators. Inability to upstream the integration means hardware vendors need to maintain their customized forks of the frameworks and re-integrate with the main repositories for every new version release, which is cumbersome and unsustainable.\nRecognizing the need for a modular device integration interface in TensorFlow, Intel and Google co-architected PluggableDevice, a mechanism that lets hardware vendors independently release plug-in packages for new device support that can be installed alongside TensorFlow, without modifying the TensorFlow code base. PluggableDevice has been the only way to add a new device to TensorFlow since its release in TensorFlow 2.5. To bring feature-parity with native devices, Intel and Google also added a profiling C interface to TensorFlow 2.7. The TensorFlow community quickly adopted PluggableDevice and has been regularly submitting contributions to improve the mechanism together. Currently, there are 3 PluggableDevices. Today, we are excited to announce the latest PluggableDevice - Intel\u00ae Extension for TensorFlow*.\nFigure 1. Intel Data Center GPU Flex Series\nIntel\u00ae Extension for TensorFlow* accelerates TensorFlow-based applications on Intel platforms, focusing on Intel\u2019s discrete graphics cards, including Intel\u00ae Data Center GPU Flex Series (Figure 1) and Intel\u00ae Arc\u2122 graphics. It runs on Linux and Windows Subsystem for Linux (WSL2). Figure 2 illustrates how the plug-in implements PluggableDevice interfaces with oneAPI, an open, standard-based, unified programming model that delivers a common developer experience across accelerator architectures:\nDevice management: We implemented TensorFlow\u2019s StreamExecutor C API utilizing C++ with SYCL and some special support provided by the oneAPI SYCL runtime (DPC++ LLVM SYCL project). StreamExecutor C API defines stream, device, context, memory structure, and related functions, all of which have trivial mappings to corresponding implementations in the SYCL runtime.\nOp and kernel registration: TensorFlow\u2019s kernel and op registration C API allows adding device-specific kernel implementations and custom operations. To ensure sufficient model coverage, we match TensorFlow native GPU device\u2019s op coverage, implementing most performance critical ops by calling highly-optimized deep learning primitives from the oneAPI Deep Neural Network Library (oneDNN). Other ops are implemented with SYCL kernels or the Eigen math library. Our plug-in ports Eigen to C++ with SYCL so that it can generate programs to implement device ops.\nGraph optimization: The Flex Series GPU plug-in optimizes TensorFlow graphs in Grappler through Graph C API and offloads performance-critical graph partitions to the oneDNN library through oneDNN Graph API. It receives a protobuf-serialized graph from TensorFlow, deserializes the graph, identifies and replaces appropriate subgraphs with a custom op, and sends the graph back to TensorFlow. When TensorFlow executes the processed graph, the custom ops are mapped to oneDNN\u2019s optimized implementation for their associated oneDNN Graph partitions.\nProfiler: The Profiler C API lets PluggableDevices communicate profiling data in TensorFlow\u2019s native profiling format. The Flex Series GPU plug-in takes a serialized XSpace object from TensorFlow, fills the object with runtime data obtained through the oneAPI Level Zero low-level device interface, and returns the object back to TensorFlow. Users can display the execution profile of specific ops on The Flex Series GPU with TensorFlow\u2019s profiling tools like TensorBoard.\nFigure 2. How Intel\u00ae Extension for TensorFlow* implements PluggableDevice interfaces with oneAPI software components\nTo install the plug-in, run the following commands:\n$ pip install tensorflow==2.10.0\n$ pip install intel-extension-for-tensorflow[gpu]\nSee the Intel blog for more detailed information. For issues and feedback specific to Intel\u00ae Extension for TensorFlow, please provide feedback here.\n\nWe are committed to continue improving PluggableDevice with the community so that device plug-ins can run TensorFlow applications as transparently as possible. Please refer to our PluggableDevice tutorial and sample code if you would like to integrate a new device with TensorFlow. We look forward to enabling more AI accelerators in TensorFlow through PluggableDevice.\n\nContributors: Anna Revinskaya (Google), Yi Situ (Google), Eric Lin (Intel), AG Ramesh (Intel), Sophie Chen (Intel), Yang Sheng (Intel), Teng Lu (Intel), Guizi Li (Intel), River Liu (Intel), Cherry Zhang (Intel), Rasmus Larsen (Google), Eugene Zhulenev (Google), Jose Baiocchi Paredes (Google), Saurabh Saxena (Google), Gunhan Gulsoy (Google), Russell Power (Google)",
    "link": "https://blog.tensorflow.org/2022/10/accelerating-tensorflow-on-intel-data-center-gpu-flex-series.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiggAhVjGLVpN-zQt2-nd7tSvZRLJW0qVeCvD7QMNuyTrH3IgCTZO0IWm-hq4wZtwZVdpiAHwi_NW3jjX1nBRwr16bVzfuRUyophmVIquDNkkvQ2bl3UMOG5viSeMG9HHl-xBO86xKg8agcGvKM5uqe9T9DGm8d2EYvuddIyUc6XlG-T-M6qVS-pIAj/s1600/Figure-2_PluggableDevice-diagram.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhrD9ZP8p0h4eP5eWo1Nn9q7KkpSu0zkXC7DATu6rHUEHyN42j4DupIHlW3ZKK56bCa2jMPF4K2P-C8m8HMYXb0mzD-RpABb5t7Hz58G8LPxyG7pBub8VZSK8T68AKJd7V0S7Rsk4_0roEePPuBu-jW2aCGS2odo6rnMh9fssE1zWtxjzYMqH-nEBNA/s1600/Figure-1_Intel-Data-Center-GPU-Flex-Series-170.jpg",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiggAhVjGLVpN-zQt2-nd7tSvZRLJW0qVeCvD7QMNuyTrH3IgCTZO0IWm-hq4wZtwZVdpiAHwi_NW3jjX1nBRwr16bVzfuRUyophmVIquDNkkvQ2bl3UMOG5viSeMG9HHl-xBO86xKg8agcGvKM5uqe9T9DGm8d2EYvuddIyUc6XlG-T-M6qVS-pIAj/s1600/Figure-2_PluggableDevice-diagram.png"
    ],
    "time": "2023/11/29 00:58:43"
  },
  {
    "title": "Training tree-based models with TensorFlow in just a few lines of code",
    "content": "A guest post by Dinko Franceschi, Broad Institute of MIT and Harvard\nKaggle has become the go-to place to practice data science skills and participate in machine learning model-building competitions. This tutorial will provide an easy-to-follow walkthrough of how to get started with a Kaggle notebook using TensorFlow Decision Forests. It\u2019s a library that allows you to train tree-based models (like random forests and gradient-boosted trees) in TensorFlow.\nWhy should you be interested in decision forests? There are roughly two types of Kaggle competitions - and the winning solution (neural networks or decision forests) depends on the kind of data you\u2019re working with.\nIf you\u2019re working with a tabular data problem (these involve training a model to classify data in a spreadsheet which is an extremely common scenario) - the winning solution is often a decision forest. However, if you\u2019re working with a perception problem that involves teaching a computer to see or hear (for example, image classification), the winning model is usually a neural network.\nHere\u2019s where the good news starts. You can implement a decision forest in TensorFlow with just a few lines of code. This relatively simple model often outperforms a neural network on many Kaggle problems.\nWe will explore the decision forests library with a simple dataset from Kaggle, and we will build our model with Kaggle Kernels which allow you to completely build and train your models online using free cloud compute power - similar to Colab. The dataset contains vehicle information such as cost, number of doors, occupancy, and maintenance costs which we will use to assign an evaluation on the car.\nKaggle Kernels can be accessed through your Kaggle account. If you do not have an account, please begin by signing up. On the home page, select the \u201cCode\u201d option on the left menu and select \u201cNew Notebook,\u201d which will open a new Kaggle Kernel.\n\n\nOnce we have opened a new notebook from Kaggle Kernels, we download the car evaluation dataset to our environment. Click \u201cAdd data\u201d near the top right corner of your notebook, search for \u201ccar evaluation,\u201d and add the dataset.\n\nNow we are ready to start writing code. Install the TensorFlow Decision Forests library and the necessary imports, as shown below. The code in this blog post has been obtained from the Build, train and evaluate models with the TensorFlow Decision Forests tutorial which contains additional examples to look at.\n!pip install tensorflow_decision_forestsimport numpy as np\nimport pandas \nimport tensorflow_decision_forests as tfdf\nWe will now import the dataset. We should note that the dataset we downloaded did not contain headers, so we will add those first based on the information provided on the Kaggle page for the dataset. It is good practice to inspect your dataset before you start working with it by opening it up in your favorite text or spreadsheet editor.\ndf = pandas.read_csv(\"../input/car-evaluation-data-set/car_evaluation.csv\")col_names =['buying price', 'maintenance price', 'doors', 'persons', 'lug_boot', 'safety', 'class']\ndf.columns = col_names\ndf.head()\nWe must then split the dataset into train and test:\ndef split_dataset(dataset, test_ratio=0.30):  test_indices = np.random.rand(len(dataset)) < test_ratio\n  return dataset[~test_indices], dataset[test_indices]\ntrain_ds_pd, test_ds_pd = split_dataset(df)print(\"{} examples in training, {} examples for testing.\".format(\n    len(train_ds_pd), len(test_ds_pd)))\nAnd finally we will convert the dataset into tf.data format. This is a high-performance format that is used by TensorFlow to train models more efficiently, and with TensorFlow Decision Forests, you can convert your dataset to this format with one line of code:\ntrain_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=\"class\")\ntest_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_ds_pd, label=\"class\")\nNow you can go ahead and train your model right away by executing the following:\nmodel = tfdf.keras.RandomForestModel()model.fit(train_ds)\nThe library has good defaults which are a fine place to start for most problems. For advanced users, there are lots of options to choose from in the API doc as random forests are configurable.\nOnce you have trained the model, you can see how it will perform on the test data.\nmodel.compile(metrics=[\"accuracy\"])print(model.evaluate(test_ds))\nIn just a few lines of code, you reached an accuracy of >95% on this small dataset! This is a simple dataset, and one might argue that neural networks could also yield impressive results. And they absolutely can (and do), especially when you have very large datasets (think: hundreds of thousands of examples, or more). However, neural networks require more code and are resource intensive as they require significantly more compute power.\nEasy preprocessing\nDecision forests have another important advantage: there are fewer steps to preprocess the data. Notice in the code above that you were able to pass a dataset with both categorical and numeric values directly to the decision forests. You did not have to do any preprocessing like normalizing numeric values, converting strings to integers, and one-hot encoding them. This has major benefits. It makes decision forests simpler to work with (so you can train a model quickly), and there is less code that can go wrong.\nBelow, you will see some important differences between the two techniques.\nEasy to interpret\nA significant advantage of decision forests is that they are easy to interpret. While the pipeline for decision trees differs significantly from that of training neural networks, there are major advantages for selecting these models for a given task. This is because feature importance is particularly straightforward to determine with decision forests (ensemble of decision trees). Notably, the TensorFlow Decision Forests library makes it possible to visualize feature importance with its model plotter function. Let\u2019s see below how this works!\ntfdf.model_plotter.plot_model_in_colab(model, tree_idx=0)\nWe see in the root of the tree on the left the number of examples (1728) and the corresponding distribution indicated by the different colors. Here our model is looking at the number of persons that the car can fit. The largest section indicated by green stands for 2 persons and the red for 4 persons. Furthermore, as we go down the tree we continue to see how the tree splits and the corresponding number of examples. Based on the condition, examples are branched to one of two paths. Interestingly, from here we can also determine the importance of a feature by examining all of the splits of a given feature and then computing how much this feature lowered the variance.\nDecision Trees vs. Neural Networks\nNeural networks undoubtedly have incredible representation learning capabilities. While they are very powerful in this regard, it is important to consider whether they are the right tool for the problem at hand. When working with neural networks, one must think a lot about how they will construct the layers. In contrast, decision forests are ready to go out of the box (of course, advanced users can tune a variety of parameters).\nPrior to even building a neural network layer by layer, in most cases one must perform feature pre-processing. For example, this could include normalizing the features to have mean around 0 and standard deviation of 1 and converting strings to numbers. This initial step can be skipped right away with Tree-based models which natively handle mixed data.\nAs seen in the code above, we were able to obtain results in just a few steps. Once we have our desired metrics, we have to interpret them within the context of our problem. Perhaps one of the most significant strengths of Decision Trees is their interpretability. We see in the code above the diagrams that were outputted. Starting at the root, we can follow the branches and quickly get a good idea of how the model made its decisions. In contrast, neural networks are a \u201cblack box\u201d that can be difficult to interpret and to explain to a non-technical audience.\nLearning more\nIf you\u2019d like to learn more about TensorFlow Decision Forests, the best place to start is with the project homepage. You can also check out this previous article for more background. And if you have any questions or feedback, the best place to ask them is on https://discuss.tensorflow.org/ using the tag \u201ctfdf\u201d. Thanks for reading!",
    "link": "https://blog.tensorflow.org/2022/08/training-tree-based-models-with-TensorFlow.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEit5NPpiHRl9CeAKbAxAc7-BPlZIdAGyWm4xQOICiq7r-JCaURdQn7x6pzjbz4VyM_PSLBnIVkvOvxf8NUkIE8FgkzvEA8-ALh_DCnGcdjlxjG7V-ko3NFbh8qFM-V_9jcNJqIjQrSf49ydz240yGZyLWEVQ0Oj15HLWexHxeROYjpagMISjvh9FQgr/s1600/TF%20Blog%20Social%20Asset.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjJKKsbTV_o5m76CtyNsZE51uUXZp8Tu-FvDTk568ZwyWqLO6vXRAWt4bBQvCD30ZEfX7z7VGhB_Nmg2529gnspRA-AHNtgN-BHNUEqru8cE75g0GOgIaUCsb9oZzaqu7jDq_s2wMktzP8U_SOuAJOZNNgqVFey1Dm0fWRPws0lfq7b_5JRfQUTK2Xr/s1600/TF%20Blog%20Header.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3sG3tN2tLCgvVQLoeztWlFAMBGfo7xJP84JCBVp-uKBQOxxTYZtdk6OfEluP0i1HTqKHr_xGk9fEJMaflHtGnRrqZI3KF5YR3-9CCgcv5xPMfBFTQj5JwxY0VAW2spwSJoIw0xgTlDpLaEAc9_ir_sRX_cN_Q--dHl3p8oIbVZ9ZdspaC26oG6SpA/s1600/TF%20Kaggle%201.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQcVF5WVpn2cc70uxeEeBlTRwvJvIFMls1k9SkdnUUSB7atP9YKHAfB3_iv7W4wXHR26-ioVECvGtJCnENpalCPwFx7ZTaurZD1S6dD2gbki9I0_6iXPbmqqbUC0EIeO5CEue3SdqydtWWPbN4JkCGkj8d_MhL7ypW-soZUe4UHJOtox-BcxbPerrE/s1600/TF%20Kaggle%202.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhURgHiyrnk91zqNm68kOLoLN8DKTyJK3b_99D_8b2GK73y7z9F4p9yXr4bYVY-EJ9k2tZBjgr7spniFAUghUgrQZJ9fSIOvsnXLCkFdxyfVE79qx4vZWjISyNUOXlArdEWtba7GM2ZiUhk4amwNY9WWP1MQAmeYk958q6zql81TofdNqllf2NlwqoU/s1600/TF%20Kaggle%205.png"
    ],
    "time": "2023/11/29 00:58:43"
  },
  {
    "title": "Load-testing TensorFlow Serving\u2019s REST Interface",
    "content": "Posted by Chansung Park and Sayak Paul (ML-GDEs)\nIn this post, we\u2019ll share the lessons and findings learned from conducting load tests for an image classification model across numerous deployment configurations. These configurations involve REST-based deployments with TensorFlow Serving. In this way, we aim to equip the readers with a holistic understanding of the differences between the configurations.\nThis post is less about code and more about the architectural decisions we had to make for performing the deployments. We\u2019ll first provide an overview of our setup including the technical specifications. We\u2019ll also share our commentaries on the design choices we made and their impact.\nTechnical Setup\nTensorFlow Serving is feature-rich and has targeted specifications embedded in its designs (more on this later). For online prediction scenarios, the model is usually exposed as some kind of service.\nTo perform our testing we use a pre-trained ResNet50 model which can classify a variety of images into different categories. We then serve this model in the following way:\nDocker to containerize the environment.\nKubernetes to orchestrate a cluster of container nodes for scalability. We use Kubernetes Engine (GKE) to manage this.\nGitHub Actions to automatically roll out deployments on GKE.\nOur deployment platform (nodes on the Kubernetes Cluster) is CPU-based. We don\u2019t employ GPUs at any stage of our processes. For this purpose, we can build a CPU-optimized TensorFlow Serving image and take advantage of a few other options which can reduce the latency and boost the overall throughput of the system. We will discuss these later in the post.\nYou can find all the code and learn how the deployments were performed in this repository. Here, you\u2019ll find example notebooks and detailed setup instructions for playing around with the code. As such, we won\u2019t be discussing the code line by line but rather shed light on the most important parts when necessary.\nThroughout the rest of this post, we\u2019ll discuss the key considerations for the deployment experiments respective to TensorFlow Serving including its motivation, limitations, and our experimental results.\nWith the emergence of serverless offerings like Vertex AI, it has never been easier to deploy models and scale them securely and reliably. These services help reduce the time-to-market tremendously and increase overall developer productivity. That said, there might still be instances where you\u2019d like more granular control over things. This is one of the reasons why we wanted to do these experiments in the first place.\nConsiderations\nTensorFlow Serving has its own sets of constraints and design choices that can impact a deployment. In this section, we provide a concise overview of these considerations.\nDeployment infrastructure: We chose GKE because Kubernetes is a standard deployment platform when using GCP, and GKE lets us focus on the ML parts without worrying about the infrastructure since it is a fully managed Google Cloud Platform service. Our main interest is in how to deploy models for CPU-based environments, so we have prepared a CPU-optimized TensorFlow Serving image.\nTrade-off between more or fewer servers: We started experiments for TensorFlow Serving setups with the simplest possible VMs equipped with 2vCPU and 4GB RAM, then we gradually upgraded the specification up to 8vCPU and 64GB RAM. On the other hand, we decreased the number of nodes in the Kubernetes cluster from 8 to 2 because it is a trade-off between costs to deploy cheaper servers versus fewer expensive servers.\nOptions to benefit multi-core environments: We wanted to see if high-end VMs can outperform simple VMs with options to take advantage of the multi-core environment even though there are fewer nodes. To this end, we experimented with a different number inter_op_parallelism and intra_op_parallelism threads for TensorFlow Serving deployment set according to the number of CPU cores.\nDynamic batching and other considerations: Modern ML frameworks such as TensorFlow Serving usually support dynamic batching, initial model warm-up, multiple deployments of multiple versions of different models, and more out of the box. For our purpose of online prediction, we have not tested these features carefully. However, dynamic batching capability is also worth exploring to enhance the performance according to the official document. We have seen that the default batching configuration could reduce the latency a little even though the results of that are not included in this blog post.\nExperiments\nWe have prepared the following environments. In TensorFlow Serving, the number of intra_op_parallelism_threads is set equal to the number of CPU cores while the number of inter_op_parallelism_threads is set from 2 to 8 for experimental purposes as it controls the number of threads to parallelize the execution of independent operations. Below we provide the details on the adjustments we performed on the number of vCPUs, RAM size, and the number of nodes for each Kubernetes cluster. Note that the number of vCPUs and the RAM size are applicable for the cluster nodes individually.\nThe load tests are conducted using Locust. We have run each load test for 5 minutes. The number of requests are controlled by the number of users, and it depends on the circumstances on the client side. We increased the number of users by one every second up to 150 which we found the handled number of requests reaches the plateau, and the requests are spawned every second to understand how TensorFlow Serving behaves. So you can assume that requests/second doesn't reflect the real-world situation where clients try to send requests at any time.\nWe experimented with the following node configurations on a Kubernetes cluster. The configurations are read like so: {num_vcpus_per_node}-{ram}_{num_nodes}:\n2vCPUs, 4GB RAM, 8 Nodes\n4vCPUs, 8GB RAM, 4 Nodes\n8vCPUs, 16GB RAM, 2 Nodes\n8vCPUs, 64GB RAM, 2 Nodes\nYou can find code for experimenting with these different configurations in the above-mentioned repositories. The deployment for each experiment is provisioned through Kustomize to overlay the base configurations, and file-based configurations are injected through ConfigMap.\nResults\nThis section presents the results for each of the above configurations and suggests which configuration is the best based on the environments we considered. As per Figure 1, the best configuration and the environmental setup is observed as 2 nodes, 8 intra_op_parallelism_threads, 8 inter_op_parallelism_threads, 8vCPUs, 16GB RAM based on the result.\nFigure 1: Comparison between different configurations of TensorFlow Serving (original).\nWe have observed the following aspects by picking the best options.\nTensorFlow Serving is more efficient when deployed on fewer, larger (more CPU and RAM) machines, but the RAM capacity doesn\u2019t have much impact on handling more requests. It is important to find the right number of inter_op_parallelism_threads with experimentation. With a higher number the better performance is not always guaranteed even when the nodes are equipped with high-capacity hardware.\nTensorFlow Serving focuses more on reliability than throughput performance. We believe it sacrifices some throughput performance to achieve reliability, but this is the expected behavior of TensorFlow Serving, as stated in the official document. Even though handling as many requests as possible is important, keeping the server as reliable as possible is also substantially important when dealing with a production system.\nThere is a trade-off between performance and reliability, so you must be careful to choose the right one. However, it seems like the throughput performance of TensorFlow Serving is close enough to results from other frameworks such as FastAPI, and if you want to factor in richer features such as dynamic batching and sharing GPU resources efficiently between models, we believe TensorFlow Serving is the right one to choose.\nNote on gRPC and TensorFlow Serving\nWe are dealing with an image classification model for the deployments, and the input to the model will include images. Hence the size of the request payload can spiral up depending on the image resolution and fidelity. Therefore it\u2019s particularly important to ensure the message transmission is as lightweight as possible. Generally, message transmission is quite a bit faster in gRPC than REST. This post provides a good discussion on the main differences between REST and gRPC APIs.\nTensorFlow Serving can serve a model with gRPC seamlessly, but comparing the performance of a gRPC API and REST API is non-trivial. This is why we did not include that in this post. The interested readers can check out this repository that follows a similar setup but uses a gRPC server instead.\nCosts\nWe used the GCP cost estimator for this purpose. Pricing for each experiment configuration was assumed to be live for 24 hours per month (which was sufficient for our experiments).\nMachine Configuration (E2 series)\nPricing (USD)\n2vCPUs, 4GB RAM, 8 Nodes\n11.15\n4vCPUs, 8GB RAM, 4 Nodes\n11.15\n8vCPUs, 16GB RAM, 2 Nodes\n11.15\n8vCPUs, 64GB RAM, 2 Nodes\n18.21\nConclusion\nIn this post, we discussed some critical lessons we learned from our experience of load-testing a standard image classification model. We considered the industry-grade framework for exposing the model to the end-users \u2013 TensorFlow Serving. While our setup for performing the load tests may not fully resemble what happens in the wild, we hope that our findings will at least act as a good starting point for the community. Even though the post demonstrated our approaches with an image classification model, the approaches should be fairly task-agnostic.\nIn the interest of brevity, we didn\u2019t do much to push further the efficiency aspects of the model in both the APIs. With modern CPUs, software stack, and OS-level optimizations, it\u2019s possible to improve the latency and throughput of the model. We redirect the interested reader to the following resources that might be relevant:\nScaling up BERT-like model Inference on modern CPU - Part 1\nScaling up BERT-like model Inference on modern CPU - Part 2\nLoad testing and monitoring AI Platform models\nBest practices for performance and cost optimization for machine learning\nAcknowledgements\nWe are grateful to the ML Ecosystem team that provided GCP credits for supporting our experiments. We also thank Hannes Hapke and Robert Crowe for providing us with helpful feedback and guidance.",
    "link": "https://blog.tensorflow.org/2022/07/load-testing-TensorFlow-Servings-REST-interface.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3AfD6AO48MESlPbfK1z8bQQhkLoaYOP03SeyJwPSrcJb575FYz822YrwZd4x7fMA8YDTUuWZ1ESnQYLNWNR9dVW2F6Mp9p2m_5uIHbvoNiyPFQjGj81nWdb4SvWva0XVCMPG-aVvji5GHJnS61c_SBCRzMg1bZ6TCS8y4TOu2Rv3veCubUUj1HUsc/s1600/TF%20Blog%202.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3AfD6AO48MESlPbfK1z8bQQhkLoaYOP03SeyJwPSrcJb575FYz822YrwZd4x7fMA8YDTUuWZ1ESnQYLNWNR9dVW2F6Mp9p2m_5uIHbvoNiyPFQjGj81nWdb4SvWva0XVCMPG-aVvji5GHJnS61c_SBCRzMg1bZ6TCS8y4TOu2Rv3veCubUUj1HUsc/s1600/TF%20Blog%202.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgN73u2NKv-JzaPZ-NaGtIlNOTYrx_JiQeob2CB1lHbfOoLbjyYsi9GRiV76sxZEeX4M5_0HPjZlawduh1hWdbE5vYkXmmABQkteqKR26EoqJVh26A1K31RNXEV2fAjGUC6flIBDRWb7cZr8mkW4Wiqa3f1Y7cwi3Fb_QpUssA4wp2wGjdmY7kW-aRC/s1600/TF%20Blog%201.png"
    ],
    "time": "2023/11/29 00:58:43"
  },
  {
    "title": "5 steps to go from a notebook to a deployed model",
    "content": "Posted by Nikita Namjoshi, Google Cloud Developer Advocate\nWhen you start working on a new machine learning problem, I\u2019m guessing the first environment you use is a notebook. Maybe you like running Jupyter in a local environment, using a Kaggle Kernel, or my personal favorite, Colab. With tools like these, creating and experimenting with machine learning is becoming increasingly accessible. But while experimentation in notebooks is great, it\u2019s easy to hit a wall when it comes time to elevate your experiments up to production scale. Suddenly, your concerns are more than just getting the highest accuracy score.\nWhat if you have a long running job, want to do distributed training, or host a model for online predictions? Or maybe your use case requires more granular permissions around security and data privacy. What is your data going to look like at serving time, how will you handle code changes, or monitor the performance of your model overtime?\nMaking production applications or training large models requires additional tooling to help you scale beyond just code in a notebook, and using a cloud service provider can help. But that process can feel a bit daunting. Take a look at the full list of Google Cloud products, and you might be completely unsure where to start.\nSo to make your journey a little easier, I\u2019ll show you a fast path from experimental notebook code to a deployed model in the cloud.\nThe code used in this sample can be found here. This notebook trains an image classification model on the TF Flowers dataset. You\u2019ll see how to deploy this model in the cloud and get predictions on a new flower image via a REST endpoint.\nNote that you\u2019ll need a Google Cloud project with billing enabled to follow this tutorial. If you\u2019ve never used Google Cloud before, you can follow these instructions to set up a project and get $300 in free credits to experiment with.\nHere are the five steps you\u2019ll take:\nCreate a Vertex AI Workbench managed notebook\nUpload .ipynb file\nLaunch notebook execution\nDeploy model\nGet predictions\nCreate a Vertex AI Workbench managed notebook\nTo train and deploy the model, you\u2019ll use Vertex AI, which is Google Cloud\u2019s managed machine learning platform. Vertex AI contains lots of different products that help you across the entire lifecycle of an ML workflow. You\u2019ll use a few of these products today, starting with Workbench, which is the managed notebook offering.\nUnder the Vertex AI section of the cloud console, select \u201cWorkbench\u201d. Note that if this is the first time you\u2019re using Vertex AI in a project, you\u2019ll be prompted to enable the Vertex API and the Notebooks API. So be sure to click the button in the UI to do so.\nNext, select MANAGED NOTEBOOKS, and then NEW NOTEBOOK.\nUnder Advanced Settings you can customize your notebook by specifying the machine type and location, adding GPUs, providing custom containers, and enabling terminal access. For now, keep the default settings and just provide a name for your notebook. Then click CREATE.\nYou\u2019ll know your notebook is ready when you see the OPEN JUPYTERLAB text turn blue. The first time you open the notebook, you\u2019ll be prompted to authenticate and you can follow the steps in the UI to do so.\nWhen you open the JupyterLab instance, you\u2019ll see a few different notebook options. Vertex AI Workbench provides different kernels (TensorFlow, R, XGBoost, etc), which are managed environments preinstalled with common libraries for data science. If you need to add additional libraries to a kernel, you can use pip install from a notebook cell, just like you would in Colab.\nStep one is complete! You\u2019ve created your managed JupyterLab environment.\nUpload .ipynb file\nNow it\u2019s time to get our TensorFlow code into Google Cloud. If you\u2019ve been working in a different environment (Colab, local, etc), you can upload any code artifacts you need to your Vertex AI Workbench managed notebook, and you can even integrate with GitHub. In the future, you can do all of your development right in Workbench, but for now let\u2019s assume you\u2019ve been using Colab.\nColab notebooks can be exported as .ipynb files.\nYou can upload the file to Workbench by clicking the \u201cupload files\u201d icon.\nWhen you open the notebook in Workbench, you\u2019ll be prompted to select the kernel, which is the environment where your notebook is run. There are a few different kernels you can choose from, but since this code sample uses TensorFlow, you\u2019ll want to select the TensorFlow 2 kernel.\nAfter you select the kernel, any cells you execute in your notebook will run in this managed TensorFlow environment. For example, if you execute the import cell, you\u2019ll see that you can import TensorFlow, TensorFlow Datasets, and NumPy. This is because all of these libraries are included in the Vertex AI Workbench TensorFlow 2 kernel. Unsurprisingly, if you try to execute that same notebook cell in the XGBoost kernel, you\u2019ll see an error message since TensorFlow is not installed there.\nLaunch a notebook execution\nWhile we could run the rest of the notebook cells manually, for models that take a long time to train, a notebook isn\u2019t always the most convenient option. And if you\u2019re building an application with ML, it\u2019s unlikely that you\u2019ll only need to train your model once. Over time, you\u2019ll want to retrain your model to make sure it stays fresh and keeps producing valuable results.\nManually executing the cells of your notebook might be the right option when you\u2019re getting started with a new machine learning problem. But when you want to automate experimentation at a large scale, or retrain models for a production application, a managed ML training option will make things much easier.\nThe quickest way to launch a training job is through the notebook execution feature, which will run the notebook cell by cell on the Vertex AI managed training service.\nWhen you launch the training job, it\u2019s going to run on a machine you won\u2019t have access to after the job completes. So you don\u2019t want to save the TensorFlow model artifacts to a local path. Instead, you\u2019ll want to save to Cloud Storage, which is Google Cloud\u2019s object storage, meaning you can store images, csv files, txt files, saved model artifacts. Just about anything.\nCloud storage has the concept of a \u201cbucket\u201d which is what holds your data. You can create them via the UI. Everything you store in Cloud Storage must be contained in a bucket. And within a bucket, you can create folders to organize your data.\nEach file in Cloud Storage has a path, just like a file on your local filesystem. Except that Cloud Storage paths always start with gs://\nYou\u2019ll want to update your training code so that you\u2019re saving to a Cloud Storage bucket instead of a local path.\nFor example, here I\u2019ve updated the last cell of the notebook from model.save('model_ouput\").Instead of saving locally, I\u2019m now saving the artifacts to a bucket called nikita-flower-demo-bucket that I\u2019ve created in my project.\nNow we\u2019re ready to launch the execution.\nSelect the Execute button, give your execution a name, then add a GPU. Under Environment, select the TensorFlow 2.7 GPU image. This container comes preinstalled with TensorFlow and many other data science libraries.\nThen click SUBMIT.\nYou can track the status of your training job in the EXECUTIONS tab. The notebook and the output of each cell will be visible under VIEW RESULT when the job finishes and is stored in a GCS bucket. This means you can always tie a model run back to the code that was executed.\nWhen the training completes you\u2019ll be able to see the TensorFlow saved model artifacts in your bucket.\nDeploy to endpoint\nNow you know how to quickly launch serverless training jobs on Google Cloud. But ML is not just about training. What\u2019s the point of all this effort if we don\u2019t actually use the model to do something, right?\nJust like with training, we could execute predictions directly from our notebook by calling model.predict. But when we want to get predictions for lots of data, or get low latency predictions on the fly, we\u2019re going to need something more powerful than a notebook.\nBack in your Vertex AI Workbench managed notebook, you can paste the code below in a cell, which will use the Vertex AI Python SDK to deploy the model you just trained to the Vertex AI Prediction service. Deploying the model to an endpoint associates the saved model artifacts with physical resources for low latency predictions.\nFirst, import the Vertex AI Python SDK.\nfrom google.cloud import aiplatform\nThen, upload your model to the Vertex AI Model Registry. You\u2019ll need to give your model a name, and provide a serving container image, which is the environment where your predictions will run. Vertex AI provides pre-built containers for serving, and in this example we\u2019re using the TensorFlow 2.8 image.\nYou\u2019ll also need to replace artifact_uri with the path to the bucket where you stored your saved model artifacts. For me, that was \u201cnikita-flower-demo-bucket\u201d. You\u2019ll also need to replace project with your project ID.\nmy_model = aiplatform.Model.upload(display_name='flower-model',\n                                  artifact_uri='gs://{YOUR_BUCKET}',\n                                  serving_container_image_uri='us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest',\n                                  project={YOUR_PROJECT})\nThen deploy the model to an endpoint. I\u2019m using default values for now, but if you\u2019d like to learn more about traffic splitting, and autoscaling, be sure to check out the docs. Note that if your use case does not require low latency predictions, you don\u2019t need to deploy the model to an endpoint and can use the batch prediction feature instead.\nendpoint = my_model.deploy(\n     deployed_model_display_name='my-endpoint',\n     traffic_split={\"0\": 100},\n     machine_type=\"n1-standard-4\",\n     accelerator_count=0,\n     min_replica_count=1,\n     max_replica_count=1,\n   )\nOnce the deployment has completed, you can see your model and endpoint in the console\nGet predictions\nNow that this model is deployed to an endpoint, you can hit it like any other REST endpoint. This means you can integrate your model and get predictions into a downstream application.\nFor now, let\u2019s just test it out directly within Workbench.\nFirst, open a new TensorFlow notebook.\nIn the notebook, import the Vertex AI Python SDK.\nfrom google.cloud import aiplatform\nThen, create your endpoint, replacing project_number and endpoint_id.\nendpoint = aiplatform.Endpoint(\n    endpoint_name=\"projects/{project_number}/locations/us-central1/endpoints/{endpoint_id}\")\nYou can find your endpoint_id in the Endpoints section of the cloud Console.\nYou can find your Project Number on the home page of the console. Note that this is different from the Project ID.\nWhen you send a request to an online prediction server, the request is received by an HTTP server. The HTTP server extracts the prediction request from the HTTP request content body. The extracted prediction request is forwarded to the serving function. The basic format for online prediction is a list of data instances. These can be either plain lists of values or members of a JSON object, depending on how you configured your inputs in your training application.\nTo test the endpoint, I first uploaded an image of a flower to my workbench instance.\nThe code below opens and resizes the image with PIL, and converts it into a numpy array.\nimport numpy as np\nfrom PIL import Image\n\nIMAGE_PATH = 'test_image.jpg'\n\nim = Image.open(IMAGE_PATH)\nim = im.resize((150, 150))\nThen, we convert our numpy data to type float32 and to a list. We convert to a list because numpy data is not JSON serializable so we can\u2019t send it in the body of our request. Note that we don\u2019t need to scale the data by 255 because that step was included as part of our model architecture using tf.keras.layers.Rescaling(1./255). To avoid having to resizing our image, we could have added tf.keras.layers.Resizing to our model, instead of making it part of the tf.data pipeline.\n# convert to float32 list\nx_test = [np.asarray(im).astype(np.float32).tolist()]\nThen, we call call predict\nendpoint.predict(instances=x_test).predictions\nThe result you get is the output of the model, which is a softmax layer with 5 units. Looks like class at index 2 (tulips) scored the highest.\n[[0.0, 0.0, 1.0, 0.0, 0.0]]\nTip: to save costs, be sure to undeploy your endpoint if you\u2019re not planning to use it! You can undeploy by going to the Endpoints section of the console, selecting the endpoint and then the Undeploy model form endpoint option. You can always redeploy in the future if needed.\nFor more realistic examples, you\u2019ll probably want to directly send the image itself to the endpoint, instead of loading it in NumPy first. If you\u2019d like to see an example, check out this notebook.\nWhat\u2019s Next\nYou now know how to get from notebook experimentation to deployment in the cloud. With this framework in mind, I hope you start thinking about how you can build new ML applications with notebooks and Vertex AI.\n\nIf you\u2019re interested in learning even more about how to use Google Cloud to get your TensorFlow models into production, be sure to register for the upcoming Google Cloud Applied ML Summit. This virtual event is scheduled for 9th June and brings together the world\u2019s leading professional machine learning engineers and data scientists. Connect with other ML engineers and data scientists and discover new ways to speed up experimentation, quickly get into production, scale and manage models, and automate pipelines to deliver impact. Reserve your seat today!",
    "link": "https://blog.tensorflow.org/2022/05/5-steps-to-go-from-notebook-to-deployed.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiaEGCozhmfcLBv-hG8Uath2-SdRoNrWtVDfNvE_7RrmqB-fGoHWgkwQTU-Rxep9Z32WX3edWj0SgYf8fWsRrky04ezcAaqFfuJaW64E1uWcKMDiETB5HzGBuC9XZEkr4eaabS863ez4v1C_TdK37TJH-pF31Bjd4ofwLsUhcd1aB2267iJ265hQGoS/s1600/image9.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiaEGCozhmfcLBv-hG8Uath2-SdRoNrWtVDfNvE_7RrmqB-fGoHWgkwQTU-Rxep9Z32WX3edWj0SgYf8fWsRrky04ezcAaqFfuJaW64E1uWcKMDiETB5HzGBuC9XZEkr4eaabS863ez4v1C_TdK37TJH-pF31Bjd4ofwLsUhcd1aB2267iJ265hQGoS/s1600/image9.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghh2FVADiuk_W08IMOyrpwm4TvUq8zCnUjGuw_-pd0U02NLUhHUk6E7BXstgdsVNPJNuW5I_QCzdx7PDDEUUMc2um3N76-1qSgBgvD1Q7vZE1iEb0Hov64MtlL1ZD4wKZ6BVZ7lyPSW-GwmovnlXxYZd8_1hBltrx5LQE-GeZn5LCZ1QIG3_vhZvas/s1600/image13.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEihGUgeiCGozEfcrQ6fyHxY1To62hgwD6j0q_2EYQ9NJFz8sblOLVOLzqE0pjNt0kGIW1nxpAi5F00pVNh-w-CD-gYstBUHc1_pQh7IahDOJfs7Lm5FedjrmXvcQg_R2fneHIykWx3rUXRaKUGHa1yQI6fMan1HDaQuHTzq3aDF2oMq1_1JaYHhdlIg/s1600/image16.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhKANmUL-SL7BdnReszium05E8nZx_-16hHGaosCxKW1xs_H8cIW7arNAz03Ic9BOOfhzVFrp4PUIjG7Kg8K5C7CwBf47OA98xs2jSydxzceqNRZwrrKwbpjm4QPvNT63b01nHuDtfDeq8aYboOCYYulgl4r61Rp1THibDhkffUYp-3-oXUu8Y1ZXrx/s1600/image3.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHGQfxOqMSyvo29b2cBzZnuxUdID42F1HzMruez6tjId58A48C4XHSxOY0gQkFJLrTCaOOYzH3t_5D2XNFp9Ltxv5q4LYW1uKg1ENbTH4lQBZuF69y_HfTWqDmUeAwVmQWdKtQrN3qn0ZK20KcOHifAqQASzKbSuCtJD7cYXPskB9F5Jxdks_X6YmQ/s1600/image8.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiqRGpn0jG-3ZAawJVolo4v0AnRlvfUn9UP9e4iHI6vJtaAUubWdT3Wpha-NuyRB2Z89YFUccZnb1XIA7ZFcQDRf-XWfe4uKuOJMCOWwwfAyjFv9lPROyJqUHBRIFsFiGN6L4teuOGiDxAXFcs30haIuIO1ax56ZU3CKJQpMfoPBt82aKuSwPx4Ug9X/s1600/image17.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh5bW86zwVwwlMwJ0vYV_ECnnUj-TWgo70WdxTlob_ozUfYHOind4gZE6kaE6Etz3Phn3Pdk6yGrspi3etzdIXSQLUVK8suEZfqU3BFJCV73peLUSqWKNGDoIP0jaMGW6eTUssRQ12k8KGEhb1cOb3jIv_IJdLoJDLr-eit-A_c_0cSm4hGAK9bk6aa/s1600/image10.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheg17QjgbIMjvXL62woZRNxGDqQ0IB_yPxiTYL0sOxM73No7rOGspTqg8K5k4SbRAp1Vx1pg7zExC-g1D3sU1dZ9yvvoaBA2XDsJqj2CdQkcv3LfCMZWHbKhYY6niKRdw6q2-e0P7DJun6WTEylNFVRqdXe0qoEtyPcKu1hSCIDQ5BrZYYXRYU3kek/s1600/image6.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbrVCgkT_e1X-tBsCB9MwegaFuxqZx8E91MYC8GRU596yMNkDSG48FjmpPp1W3sZMeeX2NBvUw-0qMO9wfSlnV9gGDkbaqOW1F8tgEzhHLgnxNbvug_0ii1_CC4RnlmgctX9dbpuB_9Y9Do5l9TNsdYnwaBdnnw-ddkRdVc_tdcBk6X5Pdc6BFwLyc/s1600/image14.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj016o6cNcn4wgdZCSRHNk-NXMco53VVl1znMI_KLd8vk7cx9dKB_qXVdKNWn1Vl0xFvXkKaRFkr7r1epgWKshfr_distsHx3kpDXEmhaORIWp7kIhtbek8JbZCcr0ifq9R1xLtoTmU58q8SjJMgCsg5BdRktraQz-lMSAiEB5NUSGAjVD0oCbslQkJ/s1600/image5.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5I4758rM0cTwHQaF3rvHyvLxTNrmFFmiDBEho2mnHINBanIyVGSkle1ByQaPH6oil_sI-bdAbMNRr_BtaQK3AUXOLFJK9D1Lj5b7GgL9qRpEZSKFybARXLhBzYT6UQZaT56_3r46ocYuobhbNwmXiQ45V2capSE2U3TVaeMwO5H4PtPanBaBSBG_L/s1600/image18.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgF8l2FGcd8AORAsZgsVBh4OYTAbLWNK_69Pv2xGrpkUEbvo5no0-uN0X2bVCauEKL2mQNzWw3PyLbf-gN_k--DkGp29cCzRJBEdvuVKK7zdQH2xpBNBN8pm4z6Quo6kh96EztPtUX0BFdHPEvAgFx_jY13PbBBovtU6_Lv9YNi4kgHkx1Rl2Zk0-xw/s1600/image7.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBaxFQRaan-EMhe42E7f5vpmhOjW5OesLHf1DfEs9MWf4UMhW_xA7voHpwgTLxditmg8OWty183UzuS7T1cmVRKEiWsVxy_-n8iXRbmRrvzHX2YZAQbSURSuF5-Tg0ytFNBJgs19ltLr5HLEya_eHS9nSU6RxGua6U_6mdmrF5BZ0RMToR4oIKLJ77/s1600/image2.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_Ui0U3ZY1f9JrgGtsLgpdyJUZF9hk-ZEPsoZcHI5tkEyNEzx5dAA-Kupme8hMu5cWClhY4C3IJE3lrrWQNPtdySFoUXt2_tJUHC6dl6prVDfTFRnIHm1D3YR_l3WQv6iut8N17Pf8tgPI3YMVy1R7IMhD88_yKGjXu-JKCjfDaHl1j8TmLqrG9x0R/s1600/image4.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqGeHRbU8l4f8ebG6ditY8RYgIJz7CmVb5JFySpyps8ErdwDwze3SXwy7XL5FaINc6cvpe3Qr4EnlpxjcWaCfD1Y64n-vN7khhGnNd2zd3_j3Eqa_qb9csHEHElRjaq-5JN6_7sfvHF4tQzHcC-RwF7TrSNTxMvK4xrikJaP9sN7sbDK8dn3Y5kv-b/s1600/image15.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgwmebEfdn-7ylHSL4m3u5L6uf2jG3J8cBxGkv1bvKVXkkJIKOovR17otfuXD40mKbYnWoo2pDuZRyvJVnONhsWNUPYdA_a1WmTEjT_mmP3R5g7eVFNFLPJOrVBJaWtDlma7l-nlVhOl8g3Ikhp-CpzWHHVaJ0o5vm_VPXCfXeeOy5ztXrNMHVs9_u4/s1600/image1.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhCzHklYkMfdNGMXUqAdyd4S4vA3MVj6fnDO5TqTtDboT54__tHTUXQ0mPZoby54ml34CMDDH6qD5FJLLAczlZuKm9DP17L4EB-lDgUOBRw1al0yf5p0JbV4jmtZ9pSDNmy5_i0J7nR2N-Sp3lUJ8ZeCGo4TY73ZrN8R5agNLQkGLZAgzjsnusiMxls/s1600/image19.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgO_nfBF4Wa-7o472Juo5vpaJn9bA-kEVXSndD4toP4xDdCOudnsOlQ9_VCYpO0PA3zcaHXDvz_zHa4A2H-ObvKboyuCB6OxcMp-HAyD9S5eiovzzAWwv3xfP_f6nkCEUOV1Ilv-tfJdjnCcm1N9SmgiGaFk_bUXkF7DmSOAljgXc4WwrUBL1-3O0a4/s1600/image11.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg6NzTUpvH0NXX8GnSuN0fmgnOSKkBUhCVDhZ02_JHwvBbzVz0gKdwVjHS_MT6-UlMpt8tpj0DwSmUOt2IUxbAEgIuCNa24_DISvI274gMiDM6oCIvBI9t1joLh7whUyqHzTFO3Mbzf8nrUMrHcKTx2M7i5D6lfys2HuBfJNTWyqtCScPiHO6qEt9mo/s1600/image12.png"
    ],
    "time": "2023/11/29 00:58:43"
  },
  {
    "title": "Using Machine Learning to Help Protect the Great Barrier Reef in Partnership with Australia\u2019s CSIRO",
    "content": "Posted by Megha Malpani, Google Product Manager and Ard Oerlemans, Google Software Engineer\nCoral reefs are some of the most diverse and important ecosystems in the world, both for marine life and society more broadly. Not only are healthy reefs critical to fisheries and food security, they also protect coastlines from storm surge, support tourism-based economies, and advance drug discovery research, among other countless benefits.\nReefs face a number of rising threats, most notably climate change, pollution, and overfishing. In the past 30 years alone, there have been dramatic losses in coral cover and habitat in the Great Barrier Reef (GBR), with other reefs experiencing similar declines. In Australia, outbreaks of the coral-eating crown of thorns starfish (COTS) have been shown to cause major coral loss. While COTS naturally exist in the Indo-Pacific, reductions in the abundance of natural predators and excess run-off nutrients have led to massive outbreaks that are devastating already vulnerable coral communities. Controlling COTS populations is critical to promoting coral growth and resilience.\nThe Great Barrier Reef Foundation established an innovation program to develop new survey and intervention methods that radically improve COTS control. Google teamed up with CSIRO, Australia\u2019s national science agency, to develop innovative machine learning technology that can analyze video sequences accurately, efficiently, and in near real-time. The goal is to transform the underwater survey, monitoring and mapping reefs at scale to help rapidly identify and prioritize COTS outbreaks. This project is part of a broader partnership with CSIRO under Google\u2019s Digital Future Initiative in Australia.\nCSIRO developed an edge ML platform (built on top of the NVIDIA Jetson AGX Xavier) that can analyze underwater image sequences and map out detections in near real-time. Our goal was to use the annotated dataset CSIRO had built over multiple field trips to develop the most accurate object detection model (across a variety of environments, weather conditions, and COTS populations) within a set of performance constraints, most notably, processing more than 10 frames per second (FPS) on a <30 watt device.\nWe hosted a Kaggle competition, leveraging insights from the open source community to drive our experimentation plan. With over 2,000 teams and 61,000 submissions, we were able to learn from the successes and failures of far more experiments than we could hope to execute on our own. We used these insights to define our experimentation roadmap and ended up running hundreds of experiments on Google TPUs.\nWe used TensorFlow 2\u2019s Model Garden library as our foundation, making use of its scaled YOLOv4 model and corresponding training pipeline implementations. Our team of modeling experts then got to work, modifying the pipeline, experimenting with different image resolutions and model sizes, and applying various data augmentation and quantization techniques to create the most accurate model within our performance constraints.\nDue to the limited amount of annotated data, a key part of this problem was figuring out the most effective data augmentation techniques. We ran hundreds of experiments based on what we learned from the Kaggle submissions to determine which techniques in combination were most effective in increasing our model\u2019s accuracy.\nIn parallel with our modeling workstream, we experimented with batching, XLA, and auto mixed precision (which converts parts of the model to fp16) to try and improve our performance, all of which resulted in increasing our FPS by 3x. We found however, that on the Jetson module, using TensorFlow-TensorRT (converting the entire model to fp16) by itself actually resulted in a 4x total speed up, so we used TF-TRT exclusively moving forward.\nAfter the starfish are detected in specific frames, a tracker is applied that links detections over time. This means that every detected starfish will be assigned a unique ID that it keeps as long as it stays visible in the video. We link detections in subsequent frames to each other by first using optical flow to predict where the starfish will be in the next frame, and then matching detections to predictions based on their Intersection over Union (IoU) score.\nIn a task like this where recall is more important than precision (i.e. we care more about not missing COTS than false positives), it is useful to consider the F2 metric to assess model accuracy. This metric can be used to evaluate a model's performance on individual frames. However, our ultimate goal was to determine the total number of COTS present in the video stream. Thus, we cared more about evaluating the entire pipeline\u2019s accuracy (model + tracker) than frame-by-frame performance (i.e. it\u2019s okay if the model has inaccurate predictions on a frame or two as long as the pipeline correctly identifies the starfish\u2019s overall existence and location). We ended up using a sequence-based F2 metric that determines how many \u201ctracks\u201d are found at a certain average IoU threshold.\nOur current 1080p model using TensorFlow TensorRT runs at 11 FPS on the Jetson AGX Xavier, reaching a sequence-based F2 score of 0.80! We additionally trained a 720p model that runs at 22 FPS on the Jetson module, with a sequence-based F2 score of 0.78.\nGoogle & CSIRO are thrilled to announce that we are open-sourcing both COTS Object Detection models and have created a Colab notebook to demonstrate the server-side inference workflow. Our Colab tutorial allows students, marine researchers, or data scientists to evaluate our COTS ML models on image sequences with zero configuration/ML knowledge. Additionally, it provides a blueprint for implementing an optimized inference pipeline for edge ML platforms, such as the Jetson module. Please stay tuned as we plan to continue updating our models & trackers, ultimately open-sourcing a full TFX pipeline and dataset so that conservation organizations and other governments around the world can retrain and modify our model with their own datasets. Please reach out to us if you have a specific use case you\u2019d like to collaborate on!\nAcknowledgements\nA huge thank you to everyone who\u2019s hard work made this project possible!\nWe couldn\u2019t have done this without our partners at CSIRO \u2013 Brano Kusy, Jiajun Liu, Yang Li, Lachlan Tychsen-Smith, David Ahmedt-Aristizabal, Ross Marchant, Russ Babcock, Mick Haywood, Brendan Do, Jeremy Oorloff, Lars Andersson, and Joey Crosswell, the amazing Kaggle community, and last but not least, the team at Google \u2013 Glenn Cameron, Scott Riddle, Di Zhu, Abdullah Rashwan, Rebecca Borgo, Evan Rosen, Wolff Dobson, Tei Jeong, Addison Howard, Will Cukierski, Sohier Dane, Mark McDonald, Phil Culliton, Ryan Holbrook, Khanh LeViet, Mark Daoust, George Karpenkov, and Swati Singh.",
    "link": "https://blog.tensorflow.org/2022/05/Kaggle-Great-Barrier-Reef-ML.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEht3pf-aqNMrbwqRsqQ98eSfwAi-cpyPUTHKoYJgoeGMhB1uhw8Rt42TcbTkjhDF6v6f0fwiL6YfnMNp9ppOEBU6uw-Tt_PpP0akjV9vkZbNczEKo9XTxKQb0TLdtCX8UvMR-eVVKMdJKMvojlXv1Bv2nQsPohAl_-DpCfIv3FAJM_hWLLx1k9-RrhV/s1600/Screen%20Shot%202022-05-09%20at%201.06.30%20PM.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEht3pf-aqNMrbwqRsqQ98eSfwAi-cpyPUTHKoYJgoeGMhB1uhw8Rt42TcbTkjhDF6v6f0fwiL6YfnMNp9ppOEBU6uw-Tt_PpP0akjV9vkZbNczEKo9XTxKQb0TLdtCX8UvMR-eVVKMdJKMvojlXv1Bv2nQsPohAl_-DpCfIv3FAJM_hWLLx1k9-RrhV/s1600/Screen%20Shot%202022-05-09%20at%201.06.30%20PM.png"
    ],
    "time": "2023/11/29 00:58:43"
  },
  {
    "title": "Intro mPOD DxTrack: A low-cost healthcare device using TensorFlow Lite Micro",
    "content": "A guest post by Jeffrey Ly, CEO & Joanna Ashby, CMO of mPOD, Inc.\nmPOD is a NIH-funded pre-seed startup headquartered out of Johnson & Johnson\u2019s Innovation (JLABS) in New York City. In this article, we\u2019d like to share with you a hardware device we have developed independently at mPOD leveraging TensorFlow Lite Micro (TFLM) as a core technology, called DxTrack.\nmPOD DxTrack leverages TFLM and low cost hardware to enable accurate, rapid and objective interpretation of currently available lateral flow assays (LFAs) in less than 10 seconds. LFAs serve as diagnostic tools because they are low-cost and simple to use without specialized skills or equipment. Most recently popularized by COVID-19 rapid antigen tests, LFAs are also used extensively testing for pregnancy, disease tracking, STDs, food intolerances, and therapeutic drugs along with an extensive array of biomarkers totaling billions of tests sold each year. The mPOD Dxtrack is applicable to use with any type of visually read lateral flow assay, demonstrating a healthcare use case for TFLM that can directly impact our everyday lives.\nThe LFA begins with a sample (nasal swab, saliva, urine, blood, etc) loaded at (1) in the figure below. Once the sample has flowed to the green conjugate zone (2), it is labeled with a signaling moiety. Through capillary action, the sample will continue flowing until it is immobilized at (3), with these LFA tests, two lines indicate a positive result, one line indicates a negative result.\nFigure 1. Side (A) & Top (B) view of a lateral flow assay (LFA) sample where at (1) the sample (nasal swab, saliva, urine, blood, etc) is loaded before flowing to the green zone (2), where the target is labeled with a signaling moiety. Through capillary action, the sample will continue flowing until it is immobilized at (3) to form the test line. Excess material is absorbed at (4).\nFigure 2. These are the 3 possible classes results for a lateral flow assay (LFA) test.\nFigure 3. This is a diagram NOWDiagnostics ADEXUSDx lateral flow assay (LFA) designed to collect and run saliva sample in point-of-care (POC) and over-the-counter (OTC) settings.\nWhen used correctly, these tests are very effective; however self-testing presents challenges for the lay user to interpret. Significant variability is present between devices, making it difficult to tell if the test line you see is negative \u2026or a faint positive?\nFigure 4. A visualization of how the TinyML model on the mPOD DxTrack break interprets and classifies different lateral flow assay (LFA) results.\nTo address this challenge, we developed mPOD DxTrack, an over-the-counter (OTC) LFA reader that improves the utility of lateral flow assays by enabling rapid and objective readings with a simple, under $5 (Cost-of-Goods) globally-deployable device. The mPOD DxTrack aims to read lateral flow assay tests using ML to accomplish two goals: 1) enable rapid and objective readings of LFAs and 2) streamline digital reporting. Critically, TinyML allows for the software on the mPOD DxTrack to be deployed on low-cost (less-than $5) hardware that can be widely distributed - which is difficult with existing LFA readers which rely on high-cost/high complexity hardware that cost hundreds to thousands of dollars per unit. Ultimately, we believe that TinyML will enable the mPOD DxTrack to catch missed positive test results by removing human bias and increasing confidence in lateral flow device testing, reducing user error, and increasing overall result accuracy.\nFigure 5. Assembly view of the mPOD DxTrack with lateral flow assay (LFA) cassette.\nTechnical Dive\nKey Considerations\nAchieving high accuracy 99% overall accuracy, (99% sensitivity, 99% specificity) for model performance when interpreting live-run LFA strips.\nEnsuring the model can maintain that level of performance while fitting the hardware constraints.\nModel size constraints for TinyML\nDeployment of the DxTrack TinyML model on the Pico4ML Dev kit is constrained by 2 pieces of hardware: Flash memory and SRAM. The Pico4ML Dev kit has 2MB of flash memory to host the .uf2 file and 264kb of SRAM that accommodate the intermediate arrays (among other things) of the model. Ensuring the model size stays within these bounds is critical because while the code can successfully compile, run on the host machine and even successfully flash on the Pico4Ml Dev Kit, it will hang during set-up and not execute the main loop.\nRather than guess and check the size of intermediate arrays (a process we initially took with little reproducible success), we ended up developing a workflow that enabled us to quantify the model\u2019s arena size by first using the interpreter function. See below, where this function was called during setup:\nTfLiteStatus setup_status = ScreenInit(error_reporter);\nif (setup_status != kTfLiteOk){\nwhile(1){TF_LITE_REPORT_ERROR(error_reporter, \"Set up failed\\n\");};\n }\narena_size = interpreter->arena_used_bytes();\nprintf(\"Arena_Size Used: %zu \\n\", arena_size);\nWhen printed out, this is what the value from the interpreter function should look during Pico4ML Dev kit boot-up:\nDEV_Module_Init OK                                                              \nArena_Size Used: 93500                                                         \n sd_spi_go_low_frequency: Actual frequency: 122070                               \nV2-Version Card                                                                 \nR3/R7: 0x1aa                                                                    \nR3/R7: 0xff8000                                                                 \nR3/R7: 0xc0ff8000                                                               \nCard Initialized: High Capacity Card                                           \nSD card initialized\nSDHC/SDXC Card: hc_c_size: 15237                                                \nSectors: 15603712                                                               \nCapacity: 7619 MB                                                           \nsd_spi_go_high_frequency: Actual frequency: 12500000\nWith this value available to us, we are then able to set the appropriate TensorArenaSize. As you can see from above, the model uses 93500 bytes of SRAM. By setting the TensorArenaSize to just above that amount 99x1024 = 101376 bytes, we are able to allocate enough memory to host the model without going over the hardware limits (which also causes the Pico4ML Dev Kit to freeze).\n// An area of memory to use for input, output, and intermediate arrays.\nconstexpr int  kTensorArenaSize =  99* 1024; // 136 * 1024; //81 * 1024;\nstatic uint8_t tensor_arena[kTensorArenaSize];\nTransforming from Unquantized to Quantized Models\nNow that we have a reproducible methodology to quantify and deploy the model onto the Pico4ML Dev Kit, our next challenge is ensuring that the model can achieve the accuracy we require while still fitting with the size constrained by the hardware. For reference, the mPOD DxTrack platform is designed to interpret a 96x96 image. In the original model design, we were able to achieve > 99.999% accuracy with our model, but the intermediate layer is 96x96x32 at fp32 which requires over 1 MB of memory - it would never fit on the Pico4ML Dev Kit\u2019s 264KB of SRAM. In order to achieve the size requirement for the model, we needed to take the model from unquantized to quantized; our best option was to utilize full int8 quantization. In essence, instead of treating the tensor values as floating points (float32), we correlate those values to integers (int8). Unfortunately, this decreased the model size 4-fold, allowing it to fit onto the Pico4ML Dev Kit's rounding error from fp32 to int8 compounded, resulting in dramatically reduced model performance.\nTo combat this drop in model performance, we examined the effect of two different quantization strategies to improve performance: Post-training quantization (PTQ) and Quantization-aware training (QAT).\nBelow, we compare 3 different models to understand which quantization strategy is best. For reference:\nModel 1: 2-layer convolutional network\nModel 2: 3-layer convolutional network\nModel 3: 4-layer convolutional network\nAs we can see, Quantization-aware training (QAT) uniformly beats the post-training quantization (PTQ) method and it became part of our workflow moving forward.\nWhat performance can we achieve now?\nTested across over 800 real-world test runs, the mPOD DxTrack can preliminary achieve an overall accuracy of 98.7%. This version of the model is currently being evaluated by our network of manufacturing partners who we work closely with. Currently we are assembling a unique dataset of images as part of a patient-focused data pipeline to learn from each manufacturing partnership and building bespoke models.\nOur preliminary work has also helped us correlate model performance with appropriately large dataset size to achieve the performance high enough accuracy for our healthcare application. Per the figure attached, the model needs to be trained on a quality dataset of at least 15,000 images. Our commercial-ready target is likely to require datasets that are greater than 100,000 images.\nTo learn more about mPOD Inc, please visit our website at www.mpod.io. If you\u2019re interested in learning more about TinyML, we recommend checking out this book and this course.",
    "link": "https://blog.tensorflow.org/2022/03/intro-mpod-dxtrack-low-cost-healthcare.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhOzl4x5OR9H0hf81VYobIMLILeu81Y_BEP5KWbveG3Evh5ieqO2NioYqa4hRTgFGp-IymRrZ4ioBKDCj_vaz0j_oWdS_34OteYZSXbGro0WBB7mQDqKsdqkw9ELVL7HdApsm06Lf-JKRnLNNYOcebEH0XbRauiZWhYLago3T1zwaJeubDmkj1Fp7k3/s1600/image2.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEim0Z2oe1rhtzURFByHCs2DPyzak4ogcVGGqw7EugzlBzgQl5TuUVM-mOEdYf2ifS4g8zdN0iUSO4VSRpRFeUBnUAI-dqmieMEQHoiAUhDgIHsN1Nl6Y5AMgaogDOqp_TW3pZevgSOg3ZaNPYDPtoMFk-u1gU98wFpOXAs9Nr87BLERUPD0yDbLWf8h/s1600/image8.gif",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhOzl4x5OR9H0hf81VYobIMLILeu81Y_BEP5KWbveG3Evh5ieqO2NioYqa4hRTgFGp-IymRrZ4ioBKDCj_vaz0j_oWdS_34OteYZSXbGro0WBB7mQDqKsdqkw9ELVL7HdApsm06Lf-JKRnLNNYOcebEH0XbRauiZWhYLago3T1zwaJeubDmkj1Fp7k3/s1600/image2.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXj0FI77LgORp7QhzSI07oqN26vCBLQDPnJDd88IEY0Wp92rhluonloN5KkaSHT9EJmwbQdA_7_Or6FeAB9Z4wciBa0VxtBrdaSuIMZTJ08IOow85id8m-bRQI_zfNIMKDCRk5S40FmWVmkYnoCaanKmexvEbTsxWkHM6ChW5cHmoneYgEESvKyAok/s1600/image9.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh5BdMrQvgGHkPNqTtBaw4a3CUzPr23uReiYl0mUoV0Og4TSmRy7jhv8xmkZoknkvbRwjuxwZ5RrlbqoVBe8xAgXDLejTO6yczSIjDdTwuRqenlUCFUGi4I_mZV3zrPSulDmACckWn4sb30PyLyEinBO9Zrb1X3ZdlHmlJFq1C_CqM3x55_KV7CLx1K/s1600/image5.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiW41dgw5CN-QxeG6F4a4KEeXPDEjCOt5dFo7Sr_LMvofWnmgbcH7h-HLu_fLyuOjZZoJqnISvKkLKm6eW31vzK_rk_oXx9dtHiuQCweRw8zHFTvPhrTuozm7jtV-grpx5aYSqBRn80HotRUBYCfoYCl-IkBwamFkCEMotPgcbupr589V3lufpRpEin/s1600/image1.gif",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiwdPvQ1s9GmXPqK0chIpdJ4DjRqTYM4jRf_dS8G19aDvRajapFKwt0oIREC-IWXM9mJhNZOtV5yxafLrRQjZFPrmhkDpWVHGY3cv0fh-8xuz5Dyq6YHA2LuXohoQeglfzbiVj9_u_kKhyBq11NSu4aO9vd-UvF4Ap0HaInsWa1DLayUCSTVLLm7-nO/s1600/image6.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgUFYKQMeyius4mZLk8qm7rVEYDGnle51G1rOtdwFtmNaVOS2HdFwFl8jTt40bCzBAYHjYsXNPASjnYnZ3E_QHYWuh-mVcvzhpm5gQjfMMz40guMzPcH2AGpYu8R_RQEyEyHF0HZaH1-4kLVXvClukEID5xyKvVx1b04LewRCn1uwVQy8p0wWvhope3/s1600/image7.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgnlX3AOb4V8qMOt9UfNU-izH4eIh_XOtEbYOkk9YRh-TlZdlKBaJEwb36-zV9tM93UMJOUlWN6ib3QdtO1ndAvMiQTpIWp1PgrbKLsydmFeJ92bFVULkwXRpksuyJRQEoV2-NIm63dmCgug2MinW5Dx_4GjpOdrWa9MEOKSxLD-QI_JAJHOh2laoY-/s1600/image4.png",
      "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHP5hy5z6OchM1Xs8QoXV_KWccZoff2z6HYraEQcoZ3OWxtOIjtm112_qIeH0uf4w7jw5QK0MiPaRh21Cfetp5OpO4TG2QXnZglPiXz6tn_5HZQHz23Z_AHrMRISvIw5bsRQUtDkVdDWK2oox8utNIKktzQ304hXNrN5pp9Faa-N8q9FiBXtm6Q4W6/s1600/image3.png"
    ],
    "time": "2023/11/29 00:58:43"
  },
  {
    "title": "Highlights from TensorFlow\u2019s 2021 exploreCSR awards",
    "content": "Posted by Josh Gordon, Jocelyn Becker, and Sloan Davis for the TensorFlow team\nIncreasing the number of students pursuing computer science research is a priority at Google, especially for students from historically marginalized groups in the field. Since 2018, Google\u2019s exploreCSR awards have aided higher education efforts that support students interested in pursuing graduate studies and research careers in computing.\nThe TensorFlow team is proud to provide additional funding to support this important program. To date, we have awarded more than 20 professors with funding to support their education and outreach work in machine learning.\nWe\u2019d like to highlight examples of the many (and often, unique) outreach programs the 2021 award recipients have created so far. These range from research experiences with robotics, aquatic vehicles, federated learning, and offline digital libraries to mentored small group workshops on data science and programming skills. They\u2019re sorted alphabetically by university below.\nIf you\u2019re interested in creating your own programs like these with support from Google, keep an eye on the exploreCSR website for the next round of applications opening in June 2022.\nLaura Hosman and Courtney Finkbeiner, Arizona State University\nThe SolarSPELL initiative at Arizona State University will host a workshop series thanks to support from exploreCSR to encourage students underrepresented in computer science research in their academic journey. The SolarSPELL initiative produces an offline, solar-powered digital library designed to bring educational content to resource-constrained locations that may lack electricity, internet connectivity, and/or traditional libraries.\nThe exploreCSR workshop series, titled \u201cSolarSPELL exploreCSR: Computing for Good\u201d, involves 6 weeks of sessions using SolarSPELL as a case study for how students can apply machine learning to tackle real-world problems and develop solutions for social good. Students will meet SolarSPELL\u2019s co-director and learn about the history of the SolarSPELL initiative; learn about graduate programs available at ASU; and hear from guest panelists from industry.\nA solar-powered, offline digital library.\nAside from the information sessions, students will also gain hands-on experience working in teams and problem solving for real-world topics. The SolarSPELL team will present the students with three different challenges for student teams to develop a proposed solution using machine learning. Students will then be eligible to apply for paid summer fellowship positions with SolarSPELL to develop and integrate one of the proposed machine learning models into SolarSPELL\u2019s technology.\nSolarSPELL is a student-driven initiative, so the solutions that the exploreCSR students develop will be implemented in our digital libraries to improve hundreds of library users\u2019 experiences around the world. With libraries in 10 countries in the Pacific Islands and East Africa, and plans to expand to Latin America and the Middle East, these students will have a far-reaching impact.\nDaehan Kwak, Kean University\nMy colleague Xudong Zhang and I created an undergraduate research study group centered on computer vision, with projects underway on student attention detection, mask and social distancing detection, and pill recognition for healthcare scenarios. As one example, a student created a pill detection application using data from the National Library of Medicine pillbox. This can be used, for example, by high-volume distribution pharmacies to be more efficient and accurate, or by retirement homes to verify the pills a resident is taking. We\u2019re pleased to share that the pill recognition project won third place in the Kean Business Plan Competition and was accepted to be presented at Posters on the Hill 2022.\nMatthew Roberts, Macquarie University\nThe School of Computing at Macquarie University is working to lower the barrier to entry for students who are new to experimenting with ML by employing real-world examples. This month, around fifty students will spend the week testing their ideas for solving autonomous aquatic vehicles challenges (for example, navigation) under guidance from Macquarie University researchers. They will be developing their ideas with a sophisticated simulation environment, and the best solutions will be ready for deployment to real hardware testing in the water later in the year.\nA MacSim simulation of the Sydney Regatta Center (created by VRX), a placeholder for a machine learning model, is making random predictions, ready for improvements the students come up with.\nAccurately simulated sensors like cameras and LIDAR can be subjected to various models, allowing people to experiment with even more sophisticated ideas to solve complex problems. After our first year in exploreCSR, the adjustments we made to our simulator and the workshop will generate new ideas and light a spark for machine learning research early in students' careers.\nPooyan Fazli, San Francisco State University\n60+ students from 10 universities and colleges attended our 2-day virtual exploreCSR workshop. Participants were from San Francisco State University, CSU East Bay, CSU San Marcos, CSU Stanislaus, Foothill College, Northwestern University, San Diego State University, Sonoma State University, UC San Diego, and the University of San Francisco.\nWe had two invited speakers and two panels on mentorship and career pathways with 10 panelists from Google Research, Stanford, Emory University, Virginia Tech, and the University of Copenhagen.\nAs part of this workshop, we organized hands-on activities to introduce students to different aspects of AI and its applications for social good, such as with climate change. We also had mini-presentations and discussions on AI fairness, accountability, transparency and ethics in different areas, such as robotics, educational data mining, and impacts on underserved communities.\nFollowing the workshop, selected students will participate in a research project under the guidance of graduate students and faculty during the spring semester. Through the research projects, we have a two-fold aim: to help students develop a sense of belonging in the AI and machine learning research community, and to illuminate a pathway for them to pursue graduate studies in AI/ML that explores the potential of developing responsible AI toward social good.\nThe research projects will begin with eight weekly meetups and hands-on training on Python programming with open-source publicly available materials. Then, students will engage in applied research projects that focus on AI applications for social good, such as health, environment, safety, education, climate change, and accessibility.\nFarzana Rahman, Syracuse University\nEarlier this year, the Electrical Engineering and Computer Science department of Syracuse University hosted RESORC (REsearch Exposure in Socially Relevant Computing), an exploreCSR program, for the second time. This program provided research exposure to 78 undergraduate students from SU and nearby institutions targeting populations historically underrepresented in computing. The goal of these two workshops was to give students an opportunity to learn machine learning using open-source tools, and to gain experience with data science workflows including collecting and labeling data, training a model, and carefully evaluating it. The ML workshops were the mostly highly rated sessions of the RESORC program.\nErin Hestir and Leigh Bernacchi, University of California, Merced\nSince 2019, University of California, Merced has partnered with Merced College and California State University Stanislaus on the Google exploreCSR program \u00a1Valle! Get Your Start in Tech!, serving 32 Central Valley of California undergraduates in STEM annually to build a sense of belonging, practice professional networking, and develop technical skills. Participants convene on Zoom and in-person this semester. Valle students typically come from historically underrepresented groups, and the program is designed to support their pursuits of computational research, graduate school and computer science related careers. Many have gone on to achieve just that!\nThis year we added additional training thanks to Google Research to support machine learning applications for social good. This program is open to all Valle participants as well as partner schools, inclusive of graduate and undergraduate students in all STEM fields, and will be taught by creative graduate students in computer science from UC Merced. Each workshop will be taught by a near-peer mentor\u2014a practice that supports mutual success in academics\u2014and the mentor will coach teams to develop ML projects for social good.\nThe goal of the program is to overcome some of the trepidation scientists and students may have about computational science and machine learning through teamwork, fun and a higher purpose. Students will be able to develop their skills and interest, focusing on ML applications to climate, sustainability, agriculture and food, and diversity in tech and aviation.\nBasak Guler, University of California, Riverside\nAt the University of California, Riverside, we created an undergraduate research study group focused on federated and distributed machine learning. Federated learning has become widely popular in recent years due to its communication efficiency and on-device learning architecture. Our study group meets on a weekly basis, and students learn about the principles of federated and distributed learning, state-of-the-art federated learning algorithms, recent applications from financial services to healthcare, as well as recent challenges and advances in privacy, security, and fairness. Student projects provide opportunities for undergraduate students to be involved in machine learning research, and learn from the experiences of both faculty and graduate students. This program can facilitate their transition from undergraduate to graduate degrees, and prepare them for positions of leadership in industry, government, public service, and academia.\nGonzalo A. Bello, University of Illinois at Chicago\nThe computer science department is hosting a series of exploreCSR workshops, including exploreCSR: Exploring Data Science Research, to introduce students to data science and machine learning research. These workshops aim to encourage students from historically underrepresented groups to pursue graduate studies and careers in research in the field of computer science. UIC students from all majors were encouraged to apply, including those who haven\u2019t taken any computer science courses. Each semester, 60 students were selected out of more than 120 who applied, and 10 teaching assistants and a professor mentored students. In addition to lectures, students work on hands-on projects together where they explore, visualize, and build models using real-world data from the city of Chicago.\nMelanie Moses and Humayra Tasnim, The University of New Mexico\nThe UNM Google exploreCSR activity for 2021-2022 is a semester-long course called Swarmathon: The Next Generation. The students will learn technical skills like developing machine learning models for object recognition in robots, and soft skills including team building, research skills, and discussions with faculty and external speakers. The UNM exploreCSR program builds on 5 years of training students in a NASA-sponsored robotics competition called the Swarmathon (2014-2019). In 2019/2020 we developed a series of exploreCSR Swarmathon: TNG workshops which included a faculty panel, an industry mentor, an open-source tutorial, and a day-long workshop to enable \u201cSwarmie\u201d robots to classify and automatically retrieve objects.\nA glimpse of our robots in action.\nThis year, in our exploreCSR Swarmathon: TNG course, students will have additional time to actively engage in developing and tuning their own machine learning models to test in the Swarmie robots. They will develop object detection models using convolutional neural networks (CNNs). They will be provided with a dataset of images of objects (shown below) taken from the robot camera and a simple model. The students will further develop the model and training data and then test their models on actual robots in real-time to see how much they can improve object recognition models to classify and retrieve the proper specified objects.\nDifferent shaped cubes for detection.\nStudents will learn first-hand the reality gap between simulations and real-world experiments. This will encourage them to develop their own mini-research projects to enhance their model performance to resolve that gap. The exploreCSR-funded Swarmathon: TNG course will provide students with the opportunity to actively engage in hands-on robotics research. We hope the experience of defining a research objective, conducting a set of experiments, testing a model, and seeing results play out in our robotics arena will motivate students to attend graduate school and consider research careers.\nSwarmie with a cube in its gripper.\nDaniel Mej\u00eda, The University of Texas at El Paso\nWe\u2019re building a series of workshops open to undergraduate students of all majors to introduce them to applied machine learning and research topics, starting with foundational concepts in math and a newfound way of approaching a problem through the eyes of a data scientist. These workshops are open to all students, including those who do not have any prior experience. We hope to encourage students to consider pursuing graduate studies, especially those who may have not previously considered it. I believe that the earlier students are exposed, the more likely that they will pursue a graduate degree.\nHenry Griffith, The University of Texas at San Antonio\nAt the University of Texas at San Antonio, we\u2019re creating a portfolio of programs to enhance the persistence of first year Electrical and Computer Engineering students into research computing pathways. By integrating our programming with our Introduction to Electrical and Computer Engineering course, which has a total annual enrollment of approximately 200 students, we have the opportunity to achieve tremendous scale with our efforts. Our programs include an undergraduate research experience, a near-peer mentoring program, and group study projects - all designed to develop students' professional and technical skills and to accelerate their progression into research opportunities.\nJohn Akers, University of Washington\nOur exploreCSR workshop, CSNext, is scheduled to begin this April. It\u2019s a 4-week online program of workshops, seminars, and project work designed to encourage undergraduate students - particularly those from historically underrepresented groups - to consider and successfully apply to graduate schools in computer science. Participants will hear presentations from several University of Washington labs, such as Computer Vision/Graphics (GRAIL), Security and Privacy, and Human-Computer Interaction. There will be presentations on deep learning and on current graduate-level research, a panel discussion from current UW CSE grad students from varying backgrounds, opportunities to meet current graduate students from several UW CSE labs, and participants will be led through small-group exercises learning about active research from graduate student mentors. Participants will also learn about graduate school application processes and resources, led by staff from UW CSE Graduate Student Services.\nLearning more\nIf you\u2019re interested in creating your own programs like these with support from Google, keep an eye on the exploreCSR website for the next round of applications opening in June 2022.",
    "link": "https://blog.tensorflow.org/2022/02/exploreCSR-awards-highlights.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/a/AVvXsEjDPu7hWKseCInuSNKBkdkHEmHMRHIv2WoQjqs8fVMrvtglukxO4hKtDOAnMKFpLn1VslC37Y4qf-mDzO13fQHkwZoYf_fT7cL4cb0E_6XZHtm_VkuLesY83xdyli9zNoiPg79DbPZi97zCMVb4J9Logkm5O7UxG3IY2mYpKXm9Z5lWskPAAnyu86Kb",
      "https://blogger.googleusercontent.com/img/a/AVvXsEgUvY3hc5BkaB57WmKy-mVerTCQ9rHSD03STkw8TfUKR81xxJDAgyj90w7QC6FxH80J5-BvWdb979RL91fjHQuNihJ_2lhrXpClFCnAFRQJIhVE2PGA7DEebEyLv6pCQRRpCXyciF-xJsES6_5UsQq9DUZCOcnIn57qtTFPcNatd4GwY0xmoCa-_liY",
      "https://blogger.googleusercontent.com/img/a/AVvXsEiAeBoCCXY-rZSeoQLS3OcukXLsZzTz2-yjz3YwvFFTUJiFpcTPl4WKHIAWQB7I_bJwaT2R6-j6VJ3JxSF-adjRYGchmd352jmUqPKbqFJbqyhNMG4oYTLVxD1oFobbn30Qjn5nfd2vDpJbAANAeoO6EoVnuQ46yM32O0CaK-A-KQO84vzMQ3d5RzxC",
      "https://blogger.googleusercontent.com/img/a/AVvXsEhve9mJivIXHkAKKR1QTY9fYuYNlaLlkDNxMCDZ26NHr-ARueVqF7NvA_JmcRdSfEMZxcnzkkzL5bGT5cbpiyVB-ZReaOa966J_XZxF8Hn6fj7sQjX9WX_rA8KKr3dyjWcAJTYVbpWULd_VqikMgLOk-CGnapkOphlzNvpt94928-2NHoo7xVctJ67r",
      "https://blogger.googleusercontent.com/img/a/AVvXsEgn4do72xEklUYQADYsXZM277ifC9VmM83kaQmSpIM7C4cKJlFdeW9t2XPMWSooDLJkElGH5qg13B82CvNqVjiO2jxsp1HNXffOwu0ZCocGR0kXi8KiETUwnRlaqTNJZZdQQeQPuxuO06LHXzhTnq27ViMzJZg4PBaFOayFN4G4DRot0q0X5VNljbJx",
      "https://blogger.googleusercontent.com/img/a/AVvXsEhoBYEZIk106bijUo1_u5hWOQyW9VM8YKy6kznxXK1UX3jpwJr5k1vi12kncOEj5XSL3E1WgWcOFDVvZr5-wcuYbhToBZ9ZoeVxruzQ0IMzhMop0toDFfqWxcvhvW0UrELxm5tY1F7kWc7VOO7nGsejmJuOW0ZuEfpk_I6q0B9Oz5fZtHlDegZcTOBK",
      "https://blogger.googleusercontent.com/img/a/AVvXsEiM_aKpMKE40fa0ALpPQ-_oWDZByNX_noVuneLKmYQtyjQYsNMzAtOxEox6PKgzVpxy7hxbpiZEj0v6Kq4qQtB585VasDWVc-7_P2rZT7Q9qf0a7b9zfBHofVFfcUvpSPcC90lEDlbEnVYWmknGmkm_fC-BTurl-RwmhWoCjz6K7lz4Sv04gqEQ2RP_"
    ],
    "time": "2023/11/29 00:58:43"
  },
  {
    "title": "Improved TensorFlow 2.7 Operations for Faster Recommenders with NVIDIA",
    "content": "A guest post by Valerie Sarge, Shashank Verma, Ben Barsdell, James Sohn, Hao Wu, and Vartika Singh from NVIDIA\nRecommenders personalize our experiences just about everywhere you can think of. They help you choose a movie for Saturday night, or discover a new artist when you've looped over your go-to playlist one too many times. They are one of the most important applications of deep learning, yet as it stands today, recommenders remain some of the most challenging models to accelerate due to their data requirements. This doesn\u2019t just mean speeding up inference, but also training workflows so developers can iterate quickly. In this article, we\u2019ll discuss what bottlenecks are typically observed with recommender workloads in practice, and how they can be identified and alleviated.\nNVIDIA GPUs are great at handling parallelized computation, and have been successful in deep learning domains like Computer Vision (CV) or Natural Language Processing (NLP) where computation itself is usually the dominant factor in throughput as compared to the time it takes to bring the data itself to the model. However, modern recommenders tend to be memory and I/O bound as opposed to compute bound.\nRecommenders are memory intensive\nModern recommenders can have hundreds of features, with many categorical features and cardinalities to the order of hundreds of millions! Take a \u201cuserID\u201d feature for example. It isn\u2019t too hard to imagine a hundred million distinct users. On occasion, the cumulative embedding tables may become so large that they would be hard to fit on a single GPU\u2019s memory. Additionally, these large embedding tables involve pure memory lookups, whereas the deep neural networks themselves may be much smaller in terms of their memory footprint.\nThat being said, the latest advancements in NVIDIA GPU technology, especially increasingly large GPU memories and higher memory bandwidths, are progressively making GPUs even better candidates for accelerating recommenders. For instance, an NVIDIA A100 GPU 80GB has 80GB HBM2 memory with 2.0TB/s bandwidth compared to tens of GB/s bandwidth of CPU memory. This is in addition to a 40MB L2 cache that provides a whopping 6TB/s read bandwidth!\nRecommenders are I/O bound\nIn practice, you may find that recommenders tend to underutilize GPUs as they are often bound by host-to-device memory transfer bottlenecks. Reading from CPU memory into GPUs (and vice versa) is expensive! It follows that avoiding frequent data transfers between the CPU and GPU should help improve performance. Yet, many TensorFlow ops relevant to recommenders don\u2019t have a GPU implementation which leads to unavoidable back and forth data transfers between the CPU and GPU. Additionally, in typical recommender models the compute load itself is usually quite small as compared to NLP or CV models, and training tends to get held up by data loading.\nIdentifying bottlenecks\nDeep learning application performance can be limited by one or more portions of the training work, such as the input data pipeline (e.g. data loading and preprocessing), computationally-intensive layers, and/or memory reads and writes. The TensorFlow profiler, with its Trace Viewer illustrating a timeline of events for CPU and GPU, can help you identify performance bottlenecks.\nThe figure below shows a capture of the Trace Viewer from training a Wide & Deep (W&D) model on synthetic data in TensorFlow 2.4.3.\nFigure 1: Traces from training a W&D model on synthetic data in TensorFlow 2.4.3.\nIn this capture, we can see that a few types of ops are responsible for much of the training time on the CPU. Some names are cut off, but these include:\nAsString and StringToHashBucketFast, used by tf.feature_column.categorical_column_with_hash_bucket.\nSparseSegmentMean and Unique, used by tf.keras.layers.DenseFeatures, tf.feature_column.embedding_column, and tf.nn.embedding_lookup_sparse.\nThe corresponding gradient and weight update operations of the above two ops.\nYou may also notice that there are many small memory copies in this profile, see Figure 1 Stream #14(MemcpyH2D) and Stream #15(MemcpyD2H). At the core of DenseFeatures and embedding_lookup_sparse, ops like ResourceGather fetch the needed weights from embedding tables. Here ResourceGather is performed on the GPU, but ops before and after it only have CPU implementations so data is copied back and forth between the CPU and GPU. This transfer is bound by the PCIe bandwidth, which is typically an order of magnitude slower than the GPU memory bandwidth. Additionally, though most individual copies are small, each takes time to launch, so they can be time-consuming in aggregate.\nAccelerating recommenders by implementing GPU sparse operations\nTo accelerate ops like the SparseSegmentMean and Unique executed on the CPU in Figure 1 and reduce the time spent in resulting copies, TensorFlow 2.7 includes GPU implementations for a number of ops used by embedding functions, such as:\nSparseReshape\nSparseFillEmptyRows\nSparseFillEmptyRowsGrad\nUnique\nSparseSegmentMean\nSparseSegmentMeanGrad\nSeveral of the new GPU kernels leverage the CUDA CUB library to accelerate GPU primitives like scan and sort that are needed for sparse indexing calculations. The most intensive ops, SparseSegmentMean and SparseSegmentMeanGrad, use a custom GPU kernel that performs vectorized loads and stores to maximize memory throughput.\nNow, let's take a look at what these improvements mean in practice.\nBenchmarks\nLet\u2019s compare training runs of a model based on the Wide & Deep architecture with TensorFlow version 2.4.3-GPU, the latest version before the above GPU sparse ops were implemented, and version 2.7.0-GPU, the first version to include all these GPU ops. The model includes 1 binary label, 10 numerical features, and 40 categorical features (3 of which are 10-hot, others are 1-hot).\nIn the following suite of benchmarks, some categorical features can take several values for each data point (i.e. they are \u201cmulti-hot\u201d). As an example, a \u201chistory\u201d feature in a movie recommendation use case could be a list of movies a user has previously watched. In comparison, a single-hot feature can take exactly one value. For the rest of this post, the term \u201cn-hot\u201d represents a multi-hot categorical feature that can take up to n values. Collectively, the embedding tables for all features in the model are 9.1 GB. The identity categorical column was used for these features except where the benchmark states otherwise.\nThe wide portions of the model use keras.layers.Embedding and the deep portions use keras.layers.DenseFeatures. These training runs use synthetic data read from a TFRecord file (described below in \u201cAccelerating dataloading\u201d), batch size 131,072, and the SGD optimizer. Performance data was recorded on a system with a single NVIDIA A100-80GB GPU and 2x AMD EPYC 7742 64-Core CPU @ 2.25GHz.\nFigure 2: Training throughput (in samples/second)\nFrom the figure above, going from TF 2.4.3 to TF 2.7.0, we observe a ~73.5% reduction in the training step. This equates to roughly a 3.77x training speedup on an NVIDIA A100-80GB from simply upgrading to TF 2.7.0! Let\u2019s take a closer look at the changes that enabled this improvement.\nFigure 3: Training step time speedup between versions when using exclusively identity categorical columns (3.77x) vs exclusively hashed categorical columns (5.55x) in the test model. Hashed categorical columns show additional speedup thanks to a new GPU integer hashing op.\nBoth identity and hashed categorical columns benefit from the new GPU kernels. Because many of these ops were previously performed on the CPU in parallel to other parts of training, it is difficult to quantify the speedup from each, but these new kernels are collectively responsible for the majority of performance improvement.\nHashed categorical columns also benefit from a new GPU op (TensorToHashBucket) that replaces the previous AsString + StringToHashBucketFast hashing method in the Grappler pass. These ops were previously very time-consuming, so the test model using hashed categorical columns shows a larger improvement in the training step time.\nFigure 4: Comparison of time spent in device-to-host and host-to-device memory copies. Availability of GPU kernels for ops in TensorFlow 2.7.0 saves time by avoiding extra copies.\nIn addition to speedups from the GPU kernels themselves, some time is saved by performing fewer data copies. We previously mentioned that extra host-to-device and device-to-host copies are required when an op placed on the GPU is followed by one on the CPU or vice versa. Figure 4 shows the substantial reduction in time spent on copies from enabling more ops to be placed on the GPU.\nAccelerating dataloading\nRecommender training is frequently limited by the speed of loading data from disk. Below are three common ways to identity the data loading bottleneck:\nProfiling the network reveals that the largest chunk of the training time is taken up by the dataloader.\nThe training step time remains the same after removing most of the layers.\nTraining runs much faster with constant or random dummy inputs to the model\nIn the examples so far, we have read data from a set of TFRecord files that have our synthetic input data pre-arranged into batches to avoid being limited by data loading (as that would make it difficult to see the speedup from the new changes, which affect operations within the network itself). In TFRecord files, normally each set of inputs is stored as a separate entry and batches are constructed after loading and shuffling data. For datasets with many small features, this can consume significant disk space because each entry is stored and labeled separately. For example, our test model has a binary label, 10 numerical features, and 40 categorical features (three 10-hot and the rest 1-hot). Each entry in a TFRecord of this model\u2019s data contains a single floating-point value for each numerical feature and the appropriate number of integer values for each categorical feature. A dataset of about 4 million inputs takes up 4.1GB on disk in this basic format.\nNow consider a record file where each entry contains an entire batch of 131,072 inputs for this model (so for each numerical feature, the entry will contain 131,072 serialized floating point values). The same dataset of 4 million inputs requires only 803MB on disk in this format, and training is more than 7x faster.\nFigure 5: The training step is over 7x faster after prebatching the input TFRecord dataset. While more thorough shuffling is possible with non-prebatched inputs, overhead is significant compared to negligible overhead from shuffling the order of prebatched input batches.\nDepending on how your data engineering pipeline is set up, you may have to add a component which creates the prebatched data. A side effect of prebatching data is that the batch size and contents are largely predefined at the time of writing the TFRecord. It is possible to work around these limitations (for example, by concatenating multiple batches from the file to increase the batch size at training time) but some flexibility might be lost.\nTensorFlow custom embedding plugins\nThe size and scale of recommenders grow rapidly, and it\u2019s not uncommon to see recommender models in TBs (e.g. Google\u2019s 1.2-TB model). Another great option to accelerate recommender training on NVIDIA GPUs, especially at multi-GPU and multi-node scale, is a TF custom embedding plugin. This CUDA-based plugin distributes large embedding tables across multiple GPUs and nodes for model-parallel multi-GPU training out-of-the-box. It works as a GPU plug-in enhancement for TF native embedding layers such as tf.nn.embedding_lookup and tf.nn.embedding_lookup_sparse. With TensorFlow version 2.5 and above, a single NVIDIA A100 GPU benchmark using a model with 100 ten-hot categorical features shows 7.9x speedup in average training iteration time with the TF custom embedding plugin, and the speedup increases to 23.6x on four NVIDIA A100 GPUs. Check out this article for an overview of this plugin and more information.\nConclusion\nRecommenders present a challenging workload to accelerate. Advancements in NVIDIA GPU technology with increasingly large memories, memory bandwidths, and ever powerful parallel compute greatly benefit modern recommendation systems at scale.\nWe have added GPU implementations of several ops in TensorFlow that did not have one previously, massively improving training times, thus reducing the time a data scientist might spend experimenting and creating recommender models. Moreover, there is another option available to accelerate embedding layers on NVIDIA GPUs through the TF custom embedding plugin.",
    "link": "https://blog.tensorflow.org/2022/01/improved-tensorflow-27-operations-for.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/a/AVvXsEgnyW2sF_lrW1DDnvbaLamNFNVGUtkA0wRnJgMZWUWWKNaZGUD1xxOx4nQwQbsyArvMPE09cKoUQ2L5wsZWQ0iLCa3VgAkaGiSFeDEmpGGJHn2DOKESu4LGpLHH9jySw-cN6DDVdPyve16oPGsJHLm_ba3NOqbk489YVATNwIk3JQ2EYdJ-VwBpSU5p",
      "https://blogger.googleusercontent.com/img/a/AVvXsEgnyW2sF_lrW1DDnvbaLamNFNVGUtkA0wRnJgMZWUWWKNaZGUD1xxOx4nQwQbsyArvMPE09cKoUQ2L5wsZWQ0iLCa3VgAkaGiSFeDEmpGGJHn2DOKESu4LGpLHH9jySw-cN6DDVdPyve16oPGsJHLm_ba3NOqbk489YVATNwIk3JQ2EYdJ-VwBpSU5p",
      "https://blogger.googleusercontent.com/img/a/AVvXsEhDc0oAzY6stON2Hv2DsgFZTxRnuUHkTYC4QSbt8UNH_3qAA9yjyKDVcDbSZpQr4yAoLQSN8d3TlCBpNlgGm5ZYfF6K2tEMHca5yeAcszDfAgoMT5-aUjAgpkoOFn1Iyg2hDRqrLFp3V3-MHG-QP1hxfKLyQX950mSJIK5Gi3l2YR2LHKCjI2KGSOgB",
      "https://blogger.googleusercontent.com/img/a/AVvXsEjZkHHWuWv1HkidysgKaVhZY-S1XNwk_1ejlDzU027cwPgD7jrvp_GeV1Vabxji9Qw75h_LMge8VXRX2TkDFiyvzXkiillpb-5cNm7mEVV92T4g95GsVCQ9C15qxjmeHcBydAUtK9vhuaukbbStarPxQ6kszVMEoaXIJNPvDXwPs8cX79DfBGT0eGuY",
      "https://blogger.googleusercontent.com/img/a/AVvXsEgNW6LBCMQiFLV18IDzXne4WCCvnAzNiNFOtMZvp5SaQ3z8z2B5xrLKAyJZ5L-ue4Uk4g0Yz1w_ZP1ndSJUGPlvF0eD6qeUcbpgRCEchbNpLXqKmnK6egPk4LT2Uf5WAhlOtsZhoC7QSlVW14wQmZVh86aELCelkvld7XU6TQcvqYWAjmS-982fuFVE",
      "https://blogger.googleusercontent.com/img/a/AVvXsEjJaWFOF42k8egFKNd6tLmno10Y6ymXgVw_ny6J5I8JGSyWfXEinVXqu9BduYM09gYg9keBMhhhKzAWxuT871eR-Qbx0VMsrUYqR6_-KinjZ4yxK3vYxpajPUiSSuzfvaw4MZFEQySBG5gvK9FWMghUO0TEAjG_yEasD0A4EZ4WOJg2m_RPIwKbebNU",
      "https://blogger.googleusercontent.com/img/a/AVvXsEhjFPVYh7xAVhbn9MDggO-ZwNtVbLxRoTM_p-oLJu-NQuxE07pzipjaFSgTtquA_tSFYkNAb3E1xSEfaxAtMZRx70RMy6WMYzNesmde1aC7t-h1eLtWJjCsZctzY_xxxivMGeOVSJGI-70FrbogG9jdWlHiD_l45Vkvw5SGJaEt0IGhOQV115VeYm2U"
    ],
    "time": "2023/11/29 00:58:43"
  },
  {
    "title": "Our Summer of Code Project on TF-GAN",
    "content": "Posted by Nived P A, Margaret Maynard-Reid, Joel Shor\nGoogle Summer of Code is a program that brings student developers into open-source projects each summer. This article describes enhancements made to the TensorFlow GAN library (TF-GAN) last summer that were proposed by Nived PA, an undergraduate student of Amrita School of Engineering. The goal of Nived\u2019s project was to improve the TF-GAN library by adding new tutorials, and adding new functionality to the library itself.\nThis article provides an overview of TF-GAN and our accomplishments from last summer. We will share our experience from the perspective of both the student and the mentors, and walk through one of the new tutorials Nived created, an ESRGAN TensorFlow implementation, and show you how easy it is to use TF-GAN to help with training and evaluation.\nWhat is TF-GAN?\nTF-GAN provides common building blocks and infrastructure support for training GANs, and offers easy-to-use, standard techniques for evaluating them. Using TF-GAN helps developers and researchers save time with common GAN tools, and avoids common pitfalls in implementations. In addition, TF-GAN offers a collection of famous examples that include GANs from the image and audio space, as well as GPU and TPU support.\nSince its launch in 2017, the team has updated the infrastructure to work with TensorFlow 2.0, released a self-study GAN course viewed by over 150K people in 2020, and an ML Tech talk on GANs. The project itself has been downloaded over millions of times. Papers using TF-GAN have thousands of citations (e.g. 1, 2, 3, 4, 5).\nThe TF-GAN library can be divided into a number of independent parts, namely Core, Features, Losses, Evaluation and Examples. Each of these different parts can be used to simplify the training or evaluation process of GANs.\nProject Scope\nThe Google Summer of Code 2021 project on TF-GAN was aimed at adding more recent GAN models as examples to the library and additionally add more tutorial notebooks that explored various functionalities of TF-GAN while training and evaluating state-of-the-art GAN models such as ESRGAN. Through this project new loss functions were also added to the library that can improve the training process of GANs. Next, we will walk through the ESRGAN code and demonstrate how to use TF-GAN to help with training and evaluation.\nIf you are new to GANs, a good start is to read this Intro to GANs post written by Margaret (who mentored this project), these GANs tutorials on tensorflow.org and the self-study GAN course on Machine Learning Crash Course as mentioned above.\nESRGAN with TF-GAN\nImage super resolution is an important use case of GANs. Super resolution is the process of reconstructing a high resolution (HR) image from a given low resolution (LR) image. Super resolution can be applied to solve real world problems such as photo editing.\nThe SRGAN paper (Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network) introduced the concept of single-image super resolution and used residual blocks and perception loss to achieve that. The ESRGAN (Enhanced Super-Resolution Generative Adversarial Networks) paper enhanced SRGAN by introducing the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic building block, using relativistic loss and improved the perceptual loss.\nNow let\u2019s walk through how to implement ESRGAN with TensorFlow 2 and evaluate its performance with TF-GAN. There are two versions of Colab notebook: one using GPU and the other one using TPU. We will be going over the Colab notebook TPU version.\nPrerequisites\nFirst let\u2019s make sure that we are set up with Colab TPU and Google Cloud Storage bucket.\nColab TPU\nTo enable TPU runtime in Colab, go to Edit \u2192 Notebook Settings or Runtime\u2192 change runtime type, and then select \u201cTPU\u201d from the Hardware Accelerator drop-down menu.\nGoogle Cloud Storage Bucket\nIn order to train with TPU, we need to first set up a Google Cloud Storage bucket to store dataset and model weights during training. Please refer to the Google Cloud documentation on Creating Storage buckets. After you create a storage bucket, let\u2019s authenticate from Colab so that you can grant Google Cloud SDK access to the bucket:\nbucket = 'enter-your-bucket-name-here'\ntpu_address = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n\nfrom google.colab import auth\nauth.authenticate_user()\n\ntf.config.experimental_connect_to_host(tpu_address)\ntensorflow_gcs_config.configure_gcs_from_colab_auth()\nYou will be prompted to follow a link in your browser to authenticate the connection to the bucket. Click on the link will take you to a new browser tab. Follow the instructions there to get the verification code then go back to the Colab notebook to enter the code. Now you should be able to access the bucket for the rest of the notebook.\nTraining parameters\nNow that we have enabled TPU for Colab and set up GCS cloud bucket to store training data and model weights, we first define some parameters that will be used from data loading to model training, such as the batch size, HR image resolution and the scale by which to downscale the image into LR etc.\nParams = {\n   'batch_size' : 32,    # Number of image samples used in each training step         \n   'hr_dimension' : 256,          # Dimension of a High Resolution (HR) Image\n   'scale' : 4, # Factor by which Low Resolution (LR) Images to be downscaled.\n   'data_name': 'div2k/bicubic_x4',       # Dataset name - loaded using tfds.\n   'trunk_size' : 11,            # Number of Residual blocks used in Generator\n   ...\n}\nData\nWe are using the DIV2K dataset: DIVerse 2k resolution high quality images. We will load the data into our cloud bucket with TensorFlow Datasets (tfds) API.\nWe need both high resolution (HR) and low resolution (LR) data for training. So we will download the original images and scale them down to 96x96 for HR and 28x28 for LR.\nNote: the data downloading and rescaling to store in the cloud bucket could take over 30 minutes.\nVisualize the dataset\nLet\u2019s visualize the dataset downloaded and scaled:\nimg_lr, img_hr = next(iter(train_ds))\n\nlr = Image.fromarray(np.array(img_lr)[0].astype(np.uint8))\nlr = lr.resize([256, 256])\ndisplay(lr)\n\nhr = Image.fromarray(np.array(img_hr)[0].astype(np.uint8))\nhr = hr.resize([256, 256])\ndisplay(hr)\nModel architecture\nWe will first define the generator architecture, the discriminator architecture and the loss functions; and then put everything together to form the ESRGAN model.\nGenerator - as with most GAN generators, the ESRGAN generator upsamples the input a few times. What makes it different is the Residual-in-Residual Block (RRDB) without batch normalization.\nIn the generator we define the function for creating the Conv block, Dense block, RRDB block for upsampling. Then we define a function to create the generator network as follows with Keras Functional API:\ndef generator_network(filter=32,\n                     trunk_size=Params['trunk_size'],\n                     out_channels=3):\n lr_input = layers.Input(shape=(None, None, 3))\n  \n x = layers.Conv2D(filter, kernel_size=[3,3], strides=[1,1],\n                   padding='same', use_bias=True)(lr_input)\n x = layers.LeakyReLU(0.2)(x)\n  ref = x\n for i in range(trunk_size):\n     x = rrdb(x)\n\n x = layers.Conv2D(filter, kernel_size=[3,3], strides=[1,1],\n                   padding='same', use_bias = True)(x)\n x = layers.Add()([x, ref])\n\n x = upsample(x, filter)\n x = upsample(x, filter)\n  x = layers.Conv2D(filter, kernel_size=3, strides=1,\n                   padding='same', use_bias=True)(x)\n x = layers.LeakyReLU(0.2)(x)\n hr_output = layers.Conv2D(out_channels, kernel_size=3, strides=1,\n                           padding='same', use_bias=True)(x)\n\n model = tf.keras.models.Model(inputs=lr_input, outputs=hr_output)\n return model\nDiscriminator\nThe discriminator is a fairly straightforward CNN with Conv2D, BatchNormalization, LeakyReLU and Dense layers. Again, with the Keras Functional API.\ndef discriminator_network(filters = 64, training=True):\n img = layers.Input(shape = (Params['hr_dimension'], Params['hr_dimension'], 3))\n  \n x = layers.Conv2D(filters, [3,3], 1, padding='same', use_bias=False)(img)\n x = layers.BatchNormalization()(x)\n x = layers.LeakyReLU(alpha=0.2)(x)\n \n x = layers.Conv2D(filters, [3,3], 2, padding='same', use_bias=False)(x)\n x = layers.BatchNormalization()(x)\n x = layers.LeakyReLU(alpha=0.2)(x)\n \n x = _conv_block_d(x, filters *2)\n x = _conv_block_d(x, filters *4)\n x = _conv_block_d(x, filters *8)\n  x = layers.Flatten()(x)\n x = layers.Dense(100)(x)\n x = layers.LeakyReLU(alpha=0.2)(x)\n x = layers.Dense(1)(x)\n \n model = tf.keras.models.Model(inputs = img, outputs = x)\n return model\nLoss Functions\nThe ESRGAN model makes use of three loss functions to ensure the balance between visual quality and metrics such as Peak Signal-to- Noise Ratio (PSNR) and encourages the generator to produce more realistic images with natural textures:\nPixel loss - the pixel loss between the generated and ground truth.\nAdversarial loss (used RelativisticGAN) - calculated for both G and D.\nPerceptual loss - calculated using the pre-trained VGG-19 network.\nLet\u2019s dive deeper into the adversarial loss here since this is the most complex one and it\u2019s a function added to the TF-GAN library as part of the project.\nIn GANs the discriminator network classifies the input data as real or fake. The generator is trained to generate fake data and fool the discriminator into mistakenly classifying it as real. As the generator increases the probability of fake data being real, the probability of real data being real should also decrease. This was a missing property of standard GANs as pointed out in this paper, and the relativistic discriminator was introduced to overcome this issue. The relativistic average discriminator estimates the probability that the given real data is more realistic than fake data, on average. This improves the quality of generated data and the stability of the model while training. In the TF-GAN library, see relativistic_generator_loss and relativistic_discriminator_loss for the implementation of this loss function.\ndef ragan_generator_loss(d_real, d_fake):\n real_logits = d_real - tf.reduce_mean(d_fake)\n fake_logits = d_fake - tf.reduce_mean(d_real)\n  real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n     labels=tf.zeros_like(real_logits), logits=real_logits)) \n fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n     labels=tf.ones_like(fake_logits), logits=fake_logits))\n\n return real_loss + fake_loss\n\ndef ragan_discriminator_loss(d_real, d_fake):\n def get_logits(x, y):\n   return x - tf.reduce_mean(y)\n  real_logits = get_logits(d_real, d_fake)\n fake_logits = get_logits(d_fake, d_real)\n\n real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n         labels=tf.ones_like(real_logits), logits=real_logits))\n fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n     labels=tf.zeros_like(fake_logits), logits=fake_logits))\n\n return real_loss + fake_loss\nTraining\nThe ESRGAN model is trained in two phases:\nPhase 1: train the generator network individually and is aimed at improving the PSNR values of generated images by reducing the L1 loss.\nPhase 2: continue training of the same generator model along with the discriminator network. In the second phase, the generator reduces the L1 Loss, Relativistic average GAN (RaGAN) loss which indicates how realistic the generated image looks and the improved Perceptual loss proposed in the paper.\nIf starting from scratch, phase-1 training can be completed within an hour on a free colab TPU, whereas phase-2 can take around 2-3 hours to get good results. As a result saving the weights/checkpoints are important steps during training.\nPhase 1 training\nHere are the steps of phase 1 training:\nDefine the generator and its optimizer\nTake LR, HR image pairs from the training dataset\nInput the LR image to the generator network\nCalculate the L1 loss using the generated image and HR image\nCalculate gradient value and apply it to the optimizer\nUpdate the learning rate of optimizer after every decay steps for better performance\nPhase 2 training\nIn this phase of training:\nLoad the generator network trained in phase 1\nDefine checkpoints that can be useful during training\nUse VGG-19 pretrained network for calculating perceptual loss\nThen we define the training step as follows:\nInput the LR image to the generator network\nCalculate L1 loss, perceptual loss and adversarial loss for both the generator and the discriminator.\nUpdate the optimizers for both networks using the obtained gradient values\nUpdate the learning rate of optimizers after every decay steps for better performance\nTF-GAN's image grid function is used to display the generated images in the validation steps\nPlease refer to the Colab notebook for the complete code implementation.\nDuring training we visualize the 3 images: LR image, HR image (generated), HR image (training data), and these metrics: generator loss, discriminator loss and PSNR.\nstep 0\nGenerator Loss = 0.636057436466217\nDisc Loss = 0.0191921629011631\nPSNR : 20.95576286315918\nHere are some more results at the end of the training which look pretty good.\nEvaluation\nNow that training has completed, we will evaluate the ESRGAN model with 3 metrics: Fr\u00e9chet Inception Distance (FID), Inception Scores and Peak signal-to-noise ratio (PSNR).\nFID and Inception Scores are two common metrics used to evaluate the performance of a GAN model. Peak Signal-to- Noise Ratio (PSNR) is used to quantify the similarity between two images and is used for benchmarking super resolution models.\nInstead of writing the code from scratch to calculate each of the metrics, we are using the TF-GAN library to evaluate our GAN implementation with ease for FID and Inception Scores. Then we make use of the `tf.image` module to calculate PSNR values for evaluating the super resolution algorithm.\nWhy do we need the TF-GAN library for evaluation?\nStandard evaluation metrics for GANs such as Inception Scores, Frechet Distance or Kernel Distance are available inside TF-GAN Evaluation. Various implementations of such metrics can be prone to errors and this can result in unreliable evaluation scores. By using TF-GAN, such errors can be avoided and GAN evaluations can be made easy. For evaluating the ESRGAN model we have made use of the Inception Score (tfgan.eval.inception_score) and Frechet Distance Score (tfgan.eval.frechet_inception_distance) from the TF-GAN library.\n\nHere is how we use tf-gan for evaluation in code.\nFirst we need to install the tf-gan library which should have been part of the imports at the beginning of the notebook. Then we import the library.\n!pip install tensorflow-gan\nimport tensorflow_gan as tfgan\nNow we are ready to use the library for the ESRGAN evaluation!\nFr\u00e9chet inception distance (FID)\n@tf.function\ndef get_fid_score(real_image, gen_image):\n size = tfgan.eval.INCEPTION_DEFAULT_IMAGE_SIZE\n\n resized_real_images = tf.image.resize(real_image, [size, size], method=tf.image.ResizeMethod.BILINEAR)\n resized_generated_images = tf.image.resize(gen_image, [size, size], method=tf.image.ResizeMethod.BILINEAR)\n  num_inception_images = 1\n num_batches = Params['batch_size'] // num_inception_images\n  fid = tfgan.eval.frechet_inception_distance(resized_real_images, resized_generated_images, num_batches=num_batches)\n return fid\nInception Scores\n@tf.function\ndef get_inception_score(images, gen, num_inception_images = 8):\n size = tfgan.eval.INCEPTION_DEFAULT_IMAGE_SIZE\n resized_images = tf.image.resize(images, [size, size], method=tf.image.ResizeMethod.BILINEAR)\n\n num_batches = Params['batch_size'] // num_inception_images\n inc_score = tfgan.eval.inception_score(resized_images, num_batches=num_batches)\n\n return inc_score\nPeak Signal-to- Noise Ratio (PSNR)\ndef get_psnr(real, generated):\n  psnr_value = tf.reduce_mean(tf.image.psnr(generated, real, max_val=256.0))\n  return psnr_value\nGSoC experience\nHere is the Google Summer of Code 2021 experience in our own words:\nNived\nAs a student, Google Summer of Code gave me an opportunity to participate in exciting open source projects for TensorFlow and the mentorship that I got during this period was invaluable. I got to learn a lot about implementing various GAN models, writing tutorial notebooks, using Cloud TPUs for training models and using tools such as Google Cloud Platform. I received a lot of support from Margaret and Joel throughout the program which kept the project on track. From the beginning their suggestions helped define the project scope and during the coding period, Margaret and I met on a weekly basis to clear all my doubts and solve various issues that I was facing. Joel also helped in reviewing all the PRs made to the TF-GAN library. GSoC is indeed a great way of getting involved with various interesting TensorFlow libraries and I look forward to continuing making valuable contributions to the community.\nMargaret\nAs the project mentor, I have been involved since the project selection phase. Mentoring Nived and collaborating with Joel on TF-GAN has been a fulfilling experience. Nived has done an excellent job implementing the ESRGAN paper with TensorFlow 2 and TF-GAN. Nived and I spent a lot of time looking at the various text-to-image GANs to choose one that can potentially be implemented during the GSoC timeframe. Aside from writing the ESRGAN tutorial, he made great progress on ControlGAN for text-to-image generation. I hope this project helps others to learn how to use the TF-GAN library and contribute to TF-GAN and other open source TensorFlow projects.\nJoel\nAs an unofficial technical mentor, I was impressed how independently and effectivly Nived worked. I felt more like I was working with a junior colleague than an intern, in that I helped give technical and project pointers, but ultimately Nived made the decisions. I think the impressive results reflect this: Nived owned the project, and I think as a result the example and Colab are more well-written and cohesive than they otherwise might have been. Furthermore, Nived successfully navigated the multi-timezone reality that is working-from-home!\nWhat\u2019s next\nDuring the GSoC coding period the implementation of the ESRGAN model was completed and the Python code and Colab notebooks were merged to the TF-GAN repo. The implementation of the ControlGAN model for text-to-image generation is still in progress. Once the implementation of ControlGAN is completed, we plan to extend it to serve some real-world applications in areas such as art generation or image editing. We are also planning to write tutorials to explore different models that solve the task of text-to-image translation.\nIf you want to contribute to TF-GAN, you can reach out to `tfgan-users@google.com` to propose a project or addition. Unless you've contributed to OSS Google projects before, it's usually a good idea to check with someone before submitting a large pull request. We look forward to seeing your contributions and working with you!\nAcknowledgements\nWe would like to thank the GSoC program committee and their support, in particular Josh Gordon from the TensorFlow team.\nMany thanks to the support of the Machine Learning (ML) Google Developer Expert (GDE) program, Google Cloud Platform and TensorFlow Research Cloud.",
    "link": "https://blog.tensorflow.org/2022/01/summer-of-code.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/a/AVvXsEiJNfKXPSmXSgaAgKmw_sCQByj1pTMbBFN7pgIKfM8qQRb6TjBMhqP0oEVVpi8sRPpIx9rSNAlY5CvC5kvNAS26lsJjT2Oz2bLeo_wyHI2ahWimkAqLNh9JzflJChONlnoldl7pf3RmSiBfwaZhTPT8dVAye1KgiW5jCTyc0-5sE-lsEGtCQ3FPGIvm",
      "https://blogger.googleusercontent.com/img/a/AVvXsEieVx8SM8KaVsRn3V6SlUW5QWxt3APOSTohIh-lWC0WIFcUcY7Kek9ZHT7EB5KSteY-NfzxIoraKPxmWJEezmMac5bz5zMcsAkmlcCXZN8_7b3kVHAWGs1F3aGkeqerfiw9OgyYV1mwU3nX_W9hG2eZx-LX0RUABTYOV_kCs_oQs0VdtPlHl8sgmbcz",
      "https://blogger.googleusercontent.com/img/a/AVvXsEiVVabEWKWqs9-xNlm4Q0L-gVV4gXgFaD80JZcvhzx6TRHcnGN9KHGzZ1R_8ITGyTA6Kw3YPAnvcZ8_kvwCb4avSEHlkFc41fjwQKywbVgkmoIlCNu7oMm5LpKSVTHugicl6Jemilmw0xhEKUUAKR7zy518k-SYzvgvXLtpYfewyXYbjIJ2e_3FkKpQ",
      "https://blogger.googleusercontent.com/img/a/AVvXsEiJNfKXPSmXSgaAgKmw_sCQByj1pTMbBFN7pgIKfM8qQRb6TjBMhqP0oEVVpi8sRPpIx9rSNAlY5CvC5kvNAS26lsJjT2Oz2bLeo_wyHI2ahWimkAqLNh9JzflJChONlnoldl7pf3RmSiBfwaZhTPT8dVAye1KgiW5jCTyc0-5sE-lsEGtCQ3FPGIvm",
      "https://blogger.googleusercontent.com/img/a/AVvXsEiOTJ5aDbBVW8ZlykPDd0BUUOH70Uerbc6PUwOcwTalO31PmWUpVS3xN5T4tUG5zH6h4OKOUD4WyRrX82ua3YdcXvK2cxGJ8K3cXC7EV_YwUzSH8qYcJX4qNKwpo5X9JL6WS0ZlIOIGzedNoNRebSldfb5eLTIzD86Exf7Sc7ns1u6TeU_-otWTT88-",
      "https://blogger.googleusercontent.com/img/a/AVvXsEgjallpUCxFrEQ8mcpmbVlihaaYAa-9XmnW7fWz-ab_n8VGlShHQMBwBwTGnQ4-ORYP-gz32zCFJLylJOHl4uDSJ4u3f0Vd9jGozp9qQZiQKaLerjnDVrCb1L40pRYv7PLV7Q8JLUb9kQu8lSbNMXT6dGJ5o96OxO-fu9SBNrd1InOY94PaiwwWgwyn"
    ],
    "time": "2023/11/29 00:58:43"
  },
  {
    "title": "Recognizing the 2021 TensorFlow Contributor Awardees",
    "content": "Posted by the TensorFlow team\nTensorFlow wouldn't be where it is today without its bustling global community of contributors. There are many ways these developers contribute. They write code to improve TensorFlow, teach classes, answer questions on forums, and organize and host events.\nWe are thankful to every person that's helped the TensorFlow community over the years. And at this year's TensorFlow Contributor Summit, we wanted to show thanks by recognizing individuals who went above and beyond on their TensorFlow contributions in 2021.\nSo without further ado, we are pleased to introduce the TensorFlow Contributor Awardees of 2021!\nSIG Leadership Award\nAwarded to a highly active SIG\nJason Zaman, SIG Build\nActive SIG Award\nAwarded to an impactful Special Interest Group (SIG) leader\nSean Morgan, SIG Add-ons\nTF Forum Award\nAwarded to a helpful TF Forum user with many liked posts and responses\nEkaterina Dranitsyna\nDiversity and Inclusion Award\nAwarded to the person who made a significant effort to bring diversity into the TensorFlow ecosystem\nMerve Noyan\nEducation Outreach Awards\nAwarded to the people who made significant contributions to educational outreach\nGant Laborde\nSandeep Mistry\nCommunity Management Award\nAwarded to highly active community leaders\nTensorFlow User Group Pune (TFUG Pune)\nYogesh Kulkarni, Shashank Sane, and Aditya Kane\nRegional Awards\nAwarded to top contributors by geographic region\nMargaret Maynard-Reid, Americas\nSayak Paul, South Asia / Oceania\nChansung Park, East Asia\nRuqiya Bin Safi, Middle East / Africa\nM. Yusuf Sar\u0131g\u00f6z, Europe\nArt by Margaret Maynard-Reid\nThank you again to all the TensorFlow contributors! We look forward to recognizing even more of you next year.",
    "link": "https://blog.tensorflow.org/2021/11/2021-TF-Contributor-Awardees.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/a/AVvXsEimuA0gHZDL7prElgpvDtX4R6HmuOv_zCXoTC2Vh_8bo3wU0dEBaDRSprlYy6Ho0c9ENU8CMSOHJhDKydWwEJE4EjBHWKrCSMKP5HjlMcNGPCmIYPrZF5Tsf0SJSp4EtJE91nm_JBByAuqr5ffYB4a519vJwjB6XE37Py1OjTOI_jbtn2Cnxhhz4eML",
      "https://blogger.googleusercontent.com/img/a/AVvXsEimuA0gHZDL7prElgpvDtX4R6HmuOv_zCXoTC2Vh_8bo3wU0dEBaDRSprlYy6Ho0c9ENU8CMSOHJhDKydWwEJE4EjBHWKrCSMKP5HjlMcNGPCmIYPrZF5Tsf0SJSp4EtJE91nm_JBByAuqr5ffYB4a519vJwjB6XE37Py1OjTOI_jbtn2Cnxhhz4eML",
      "https://blogger.googleusercontent.com/img/a/AVvXsEivTKfTjRiXrQRbkRXd1Mu07WYtQnY7-RVSY7SymqyLd1HkSyGZc09kO8McdeIeCIqMIXQAh4D0qVwAN0glCUEfzPB3fByRJ0eJ50BiGbDgSvosUuO-PDup91CZvhJG__fQlXtCjXltohOAbPefvHNATL9cTEZ5JIGQJBrjBzbiDCySvLbZzKBTMl5B"
    ],
    "time": "2023/11/29 00:58:43"
  },
  {
    "title": "ML Community Day 2021 Recap",
    "content": "Posted by the TensorFlow Team\nThanks to everyone who joined our inaugural virtual ML Community Day! It was so great to get the community together and hear incredible talks like how JAX and TPUs make AlphaFold possible from the DeepMind team, and how Edge Impulse makes it easy for developers to work with TinyML using TensorFlow.\nWe also celebrated TensorFlow\u2019s 6th birthday! The TensorFlow ecosystem has come a long way in 6 years, and we love seeing what you all achieve with our tools. From using machine learning to help advance access to human rights information, to creating a custom, TensorFlow-powered drumming arm.\nIn this article are a few of the updates and topics we shared during the event. You can watch the keynote below, and you can find recordings of every talk on the TensorFlow YouTube channel.\n\nModel building\nTensorFlow 2.7 is here! This release offers performance and usability improvements, including TFLite use of XNNPack for mobile inference performance boosts, training improvements on GPUs, and a dramatic improvement in debugging efficiency in Keras and TF.\nKeras has been modularized as a separate pip package on top of TensorFlow (installed by default) and now lives in a separate GitHub repository. This will make it much easier for the community to contribute to the development of Keras. We welcome your PRs!\nResponsible AI\nThe Responsible AI team also announced v0.4 of our Language Interpretability Tool (LIT). LIT is an open-source platform for visualization and understanding of NLP models. This new release includes new interpretability techniques like TCAV, Targeted Concept activation Vector. TCAV is an interpretability method for ML models that shows the importance of high level conceptsfor a predicted class.\nMobile\nWe recently launched on-device training in TensorFlow Lite. When deploying TensorFlow Lite machine learning model to a mobile app, you may want to enable the model to be improved or personalized based on input from the device or end user. Using on-device training techniques allows you to update a model without data leaving your users' devices, improving user privacy, and without requiring users to update the device software. It's currently available on Android.\nAnd we continue to work on making performance better on TensorFlow Lite. As mentioned above, XNNPACK, a library for faster floating point ops, is now turned on by default in TensorFlow Lite. This allows your models to run on an average 2.3x faster on the CPU.\nFind all the talks here\nYou can find all of the content in this playlist, and for your convenience here are direct links to each of the sessions also:\nSimplified machine learning with Google On-Device MLManage MLOps and deploy ML to production with the new and improved TFX\nBuilding fair, ethical and responsible AI with the Responsible AI Toolkit\nIntro to JAX: Accelerating Machine Learning Research\nSimplified distributed training with tf.distribute parameter servers\nCloud TPU v4: Fast, flexible, and easy-to-use\nChip Floorplanning with Deep Reinforcement Learning\nHow to get involved in machine learning",
    "link": "https://blog.tensorflow.org/2021/11/ml-community-day-2021-recap.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/a/AVvXsEj8BkkXu1dLUj00d5yDMNsGK_4qGd8cdd1dhmOOnv1EbFed103NYX-lIMum2DB4c14zLinalPJQn05A56UWzFnH8TdA9sFAy0WxjnzURcGI5O6R_IFqfUgaJ_Iumg68840uFpYS3NPygjaQn1_bBRH7V7UptiA2j9rv_WivnlkJxfvj08w5C_QxK90G",
      "https://blogger.googleusercontent.com/img/a/AVvXsEj8BkkXu1dLUj00d5yDMNsGK_4qGd8cdd1dhmOOnv1EbFed103NYX-lIMum2DB4c14zLinalPJQn05A56UWzFnH8TdA9sFAy0WxjnzURcGI5O6R_IFqfUgaJ_Iumg68840uFpYS3NPygjaQn1_bBRH7V7UptiA2j9rv_WivnlkJxfvj08w5C_QxK90G",
      "https://blogger.googleusercontent.com/img/a/AVvXsEiH0xj6sTvSYmLZ6fUAItEJqJXSTTSNnnwg6MrJxpfVzKRBYy1iIRNAVG_ecnHru5ZIGR6p0_Y-JEwMF9Q_5mWKA4c7-ulbSX9GplpOxbX63ddRxZTRYn_0AH6PT6PTnRQ0czjjcXqyzSQC6bAyGUOwdPRBuoiMrDIBg0uHBwtvsahcq-AWZwn0KEMb"
    ],
    "time": "2023/11/29 00:58:43"
  },
  {
    "title": "ML Community Day: Save the date",
    "content": "Posted by the TensorFlow team\nPlease join us for ML Community Day, a virtual developer event on November 9th. You\u2019ll hear from the TensorFlow, JAX, and Deepmind teams and the community, covering new products, updates, and more.\nThis event takes place on TensorFlow\u2019s sixth birthday (time flies!), and celebrates many years of open-source work by the developer community. We\u2019ll have talks on on-device Machine Learning, JAX, Responsible AI, Cloud TPUs, Machine Learning in production, and sessions from the community.\nYou\u2019ll also learn how you can get involved with the machine learning community by connecting with Google Developer Experts, joining Special Interest Groups, TensorFlow User Groups, and more.\nLater in the day we\u2019ll host the TensorFlow Contributor Summit, in which Google Developer Experts, Special Interest Groups, TensorFlow User Group organizers, and other community leaders gather to connect in a small group to discuss topics such as documentation and how to contribute to the TensorFlow ecosystem. During this event, we will also host the first TensorFlow Awards Ceremony to recognize outstanding community contributions.\nYou can find more details at the event website. We'll see you soon!",
    "link": "https://blog.tensorflow.org/2021/10/ML-community-day.html",
    "imgSource": [
      "https://blogger.googleusercontent.com/img/a/AVvXsEji_7MMI2P_BqUZGhX9UxwuJpww-mI72PP1j65H47qxPgdCqYDd-ersk1LI83DG8YSqyaOTdnA-3NxygSwluM8DI85m0DnmpgpXTfFY4wthFYI5nMgM-s8keMeethI7pBEmmAZcLjrkxsEM8QtqyKxo80p80zKiE4H2gYZkO71ruyWgsC4neKvg3SR9",
      "https://blogger.googleusercontent.com/img/a/AVvXsEji_7MMI2P_BqUZGhX9UxwuJpww-mI72PP1j65H47qxPgdCqYDd-ersk1LI83DG8YSqyaOTdnA-3NxygSwluM8DI85m0DnmpgpXTfFY4wthFYI5nMgM-s8keMeethI7pBEmmAZcLjrkxsEM8QtqyKxo80p80zKiE4H2gYZkO71ruyWgsC4neKvg3SR9"
    ],
    "time": "2023/11/29 00:58:43"
  },
  {
    "title": "Join us at the Women in Machine Learning Symposium",
    "content": "Posted by Jeanine Banks, VP of 3P Core Developer Platforms\nAt Google we believe that diversity and inclusion are core to innovation, and we know there\u2019s work to be done in improving representation to achieve equity. That\u2019s why we\u2019re excited to announce a new event: The Women in Machine Learning Symposium.\nJoin us virtually from 9-12 PDT on October 19, 2021, to hear from leaders in the machine learning (ML) industry.\nAll journeys are different and this event aims to empower the next generation of women leaders in ML. By learning from each other\u2019s stories we want to inspire the creation of a community of support, bringing together women and allies in technology.\nWe\u2019ll have two keynotes discussing the importance of diversity in ML communities and Open Source Software. You can also hear first-hand the stories and experiences of women who are breaking down barriers and ask them questions live!\nLastly, I invite you to attend one of the breakout sessions tailored to what stage you\u2019re at in your career. From learning how to get started in ML, how to switch from being a tech developer to becoming an ML developer, to learning tips for taking your career to the C-level, this event has a place for you.\nRSVP today to reserve your spot and head on over to our website to view the live agenda. I hope to see you there!",
    "link": "https://blog.tensorflow.org/2021/10/join-us-women-in-machine-learning-symposium.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-e-43bRpqRtw/YVvPMH9Rv6I/AAAAAAAAEjw/aXVj5wabUtsU5tw3oJ6dy3KherIea7Z6gCLcBGAsYHQ/s0/WIML_BlogPost_16x9.jpg",
      "https://1.bp.blogspot.com/-e-43bRpqRtw/YVvPMH9Rv6I/AAAAAAAAEjw/aXVj5wabUtsU5tw3oJ6dy3KherIea7Z6gCLcBGAsYHQ/s0/WIML_BlogPost_16x9.jpg"
    ],
    "time": "2023/12/13 00:58:43"
  },
  {
    "title": "TensorFlow Hub\u2019s Experience with Google Summer of Code 2021",
    "content": "Posted by Sayak Paul (MLE at Carted, and GDE) and Morgan Roff (Google)\nWe\u2019re happy to share the work completed by Google Summer of Code students working with TensorFlow Hub this year. If you\u2019re a student who is interested in writing open source code, then you\u2019ll likely be interested in Google\u2019s Summer of Code program.\nThrough this program, students propose project ideas to open source organizations, and if selected, receive a stipend to work with them to complete their projects over the summer. Students have the opportunity to learn directly from mentors within their selected organization, and organizations benefit from the students\u2019 contributions. This year, 17 successful students completed their projects with the TensorFlow organization on many projects. In this article, we\u2019ll focus on some of the work completed on TensorFlow Hub.\nWe\u2019re Sayak and Morgan, two mentors for projects on TensorFlow Hub (TF Hub). Here we share what the students learned about building and publishing state-of-the-art models, training them on large-scale benchmark datasets, what we learned as mentors, and how rewarding summer of code was for each of us, and for the community.\nWe had the opportunity to mentor two students - Aditya Kane and Vasudev Gupta. Aditya successfully implemented several variants of RegNets including one based on this paper, and trained them on the ImageNet-1k dataset. Vasudev ported the pre-trained wav2vec2 weights from this paper to TensorFlow, which required him to implement the model architecture from scratch. He then demonstrated fine-tuning these pre-trained checkpoints on the LibriSpeech dataset, making his work more customizable and relevant for the community.\nWith model training happening at such a large scale, it becomes especially important to follow good engineering practices during the implementation. These include code modularization, unit tests, good design patterns, optimizations, and so on. Models were trained on Cloud TPUs to accelerate training time, and as such, substantial effort was put into the data input pipelines to ensure maximum accelerator utilization.\nAll of these factors collectively contributed to the complexity of the projects. Thanks to the Summer of Code program, students have the opportunity to tackle these challenges with the help of experienced mentors. This also enables students to gain insight into their organizations, and interact with people with many skillsets who cooperate to make large projects possible. A big thank you here to our students, who gracefully handled this engineering work and listened to our feedback.\nVasudev and Aditya contributed significant pre-trained models to TensorFlow Hub, along with tutorials (Wav2Vec, RegNetY) on their use, and TensorFlow implementations for folks who want to dig deeper. In their own words:\nThe last 2-3 months were full of lots of learning and coding. GSoC helped me get into the speech domain and motivated me to explore more about the TensorFlow ecosystem. I am thankful to my mentors for their continuous & timely feedback. I am looking forward to contributing more to the TensorFlow community and other awesome open source projects out there. - Vasudev Gupta\nMore about RegNets and Wav2Vec2\nAlmost 6 years after they were first published, ResNets are still widely used as benchmark architectures across image understanding tasks. Many recent self-supervised and semi-supervised learning frameworks still leverage ResNet50 as their backbone architectures. However, ResNets often do not scale well under larger data regimes and suffer from large training and inference time latencies as they grow. In contrast, RegNets were developed specifically to be a scalable architecture framework that maintains low latency while demonstrating high performance on standard image recognition tasks. Aditya\u2019s models are published on TF Hub, with code and tutorials on GitHub.\nSelf-supervised learning is an important area of machine learning research. Many recent success stories have been focused on NLP and Computer Vision, and for Vasudev\u2019s project, we wanted to explore speech. Last year, a group of researchers released the wav2vec2 framework for learning representations from audio in a self-supervised manner, benefiting downstream tasks like speech-to-text.\nUsing wav2vec2, you can now pre-train speech models without labeled data, and fine-tune those models on downstream tasks like speaker recognition. Vasudev\u2019s models are available on TF Hub, along with a new tutorial on fine-tuning, and code on GitHub.\nWrapping up\nWe\u2019d like to say a heartfelt thank you to all the students, mentors, and organizers who made Summer of Code a success despite this year\u2019s many challenges. We encourage you to check out these models and share what you have built with us by tagging #TFHub on your social media posts, or share your work for the community spotlight program. If you have questions or want to learn more about these new models, you can ask them on discuss.tensorflow.org.",
    "link": "https://blog.tensorflow.org/2021/09/tensorflow-hubs-experience-with-gsoc-2021.html",
    "imgSource": [
      "https://1.bp.blogspot.com/-vwb43vec5M8/YUOesQbkf4I/AAAAAAAAEf4/KyKiH2Fc58opRDaydRP5LrXk4rRlyJTKACLcBGAsYHQ/s0/tensorflow-GSoC-social-01.png",
      "https://1.bp.blogspot.com/-vwb43vec5M8/YUOesQbkf4I/AAAAAAAAEf4/KyKiH2Fc58opRDaydRP5LrXk4rRlyJTKACLcBGAsYHQ/s0/tensorflow-GSoC-social-01.png"
    ],
    "time": "2023/12/13 00:58:43"
  }
]
